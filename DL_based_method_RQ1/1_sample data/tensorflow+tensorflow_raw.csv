BR.BR_id,BR.BR_author,BR.BRopenT,BR.BRcloseT,BR.BR_text.BRsummary,BR.BR_text.BRdescription,BR.comments.comments_0.comment_id,BR.comments.comments_0.comment_author,BR.comments.comments_0.commentT,BR.comments.comments_0.comment_text,BR.comments.comments_1.comment_id,BR.comments.comments_1.comment_author,BR.comments.comments_1.commentT,BR.comments.comments_1.comment_text,BR.comments.comments_2.comment_id,BR.comments.comments_2.comment_author,BR.comments.comments_2.commentT,BR.comments.comments_2.comment_text,commit.commit_id,commit.commit_author,commit.commitT,commit.changed_files.file_0.file_change_type,commit.changed_files.file_0.file_Nmethod,commit.changed_files.file_0.file_old_name,commit.changed_files.file_0.file_new_name,commit.changed_files.file_0.hunks.hunk_0.Ismethod,commit.changed_files.file_0.hunks.hunk_0.added_lines,commit.changed_files.file_0.hunks.hunk_0.deleted_lines,commit.changed_files.file_1.file_change_type,commit.changed_files.file_1.file_Nmethod,commit.changed_files.file_1.file_old_name,commit.changed_files.file_1.file_new_name,BR.comments.comments_3.comment_id,BR.comments.comments_3.comment_author,BR.comments.comments_3.commentT,BR.comments.comments_3.comment_text,BR.comments.comments_4.comment_id,BR.comments.comments_4.comment_author,BR.comments.comments_4.commentT,BR.comments.comments_4.comment_text,BR.comments.comments_5.comment_id,BR.comments.comments_5.comment_author,BR.comments.comments_5.commentT,BR.comments.comments_5.comment_text,commit.changed_files.file_1.hunks.hunk_0.Ismethod,commit.changed_files.file_1.hunks.hunk_0.added_lines,commit.changed_files.file_1.hunks.hunk_0.deleted_lines,commit.changed_files.file_1.hunks.hunk_0.method_info.method_name,commit.changed_files.file_1.hunks.hunk_0.method_info.method_params,commit.changed_files.file_1.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_2.file_change_type,commit.changed_files.file_2.file_Nmethod,commit.changed_files.file_2.file_old_name,commit.changed_files.file_2.file_new_name,commit.changed_files.file_2.hunks.hunk_0.Ismethod,commit.changed_files.file_2.hunks.hunk_0.added_lines,commit.changed_files.file_2.hunks.hunk_0.deleted_lines,commit.changed_files.file_3.file_change_type,commit.changed_files.file_3.file_Nmethod,commit.changed_files.file_3.file_old_name,commit.changed_files.file_3.file_new_name,commit.changed_files.file_3.hunks.hunk_0.Ismethod,commit.changed_files.file_3.hunks.hunk_0.added_lines,commit.changed_files.file_3.hunks.hunk_0.deleted_lines,commit.changed_files.file_0.hunks.hunk_0.method_info.method_name,commit.changed_files.file_0.hunks.hunk_0.method_info.method_params,commit.changed_files.file_0.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_1.Ismethod,commit.changed_files.file_0.hunks.hunk_1.added_lines,commit.changed_files.file_0.hunks.hunk_1.deleted_lines,commit.changed_files.file_0.hunks.hunk_1.method_info.method_name,commit.changed_files.file_0.hunks.hunk_1.method_info.method_params,commit.changed_files.file_0.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_2.Ismethod,commit.changed_files.file_0.hunks.hunk_2.added_lines,commit.changed_files.file_0.hunks.hunk_2.deleted_lines,commit.changed_files.file_0.hunks.hunk_2.method_info.method_name,commit.changed_files.file_0.hunks.hunk_2.method_info.method_params,commit.changed_files.file_0.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_3.Ismethod,commit.changed_files.file_0.hunks.hunk_3.added_lines,commit.changed_files.file_0.hunks.hunk_3.deleted_lines,commit.changed_files.file_0.hunks.hunk_3.method_info.method_name,commit.changed_files.file_0.hunks.hunk_3.method_info.method_params,commit.changed_files.file_0.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_1.Ismethod,commit.changed_files.file_1.hunks.hunk_1.added_lines,commit.changed_files.file_1.hunks.hunk_1.deleted_lines,commit.changed_files.file_1.hunks.hunk_1.method_info.method_name,commit.changed_files.file_1.hunks.hunk_1.method_info.method_params,commit.changed_files.file_1.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_2.Ismethod,commit.changed_files.file_1.hunks.hunk_2.added_lines,commit.changed_files.file_1.hunks.hunk_2.deleted_lines,commit.changed_files.file_1.hunks.hunk_2.method_info.method_name,commit.changed_files.file_1.hunks.hunk_2.method_info.method_params,commit.changed_files.file_1.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_3.Ismethod,commit.changed_files.file_1.hunks.hunk_3.added_lines,commit.changed_files.file_1.hunks.hunk_3.deleted_lines,commit.changed_files.file_1.hunks.hunk_3.method_info.method_name,commit.changed_files.file_1.hunks.hunk_3.method_info.method_params,commit.changed_files.file_1.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_4.Ismethod,commit.changed_files.file_1.hunks.hunk_4.added_lines,commit.changed_files.file_1.hunks.hunk_4.deleted_lines,commit.changed_files.file_1.hunks.hunk_4.method_info.method_name,commit.changed_files.file_1.hunks.hunk_4.method_info.method_params,commit.changed_files.file_1.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_5.Ismethod,commit.changed_files.file_1.hunks.hunk_5.added_lines,commit.changed_files.file_1.hunks.hunk_5.deleted_lines,commit.changed_files.file_1.hunks.hunk_5.method_info.method_name,commit.changed_files.file_1.hunks.hunk_5.method_info.method_params,commit.changed_files.file_1.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_5.method_info.method_endline,BR.comments.comments_6.comment_id,BR.comments.comments_6.comment_author,BR.comments.comments_6.commentT,BR.comments.comments_6.comment_text,commit.changed_files.file_2.hunks.hunk_0.method_info.method_name,commit.changed_files.file_2.hunks.hunk_0.method_info.method_params,commit.changed_files.file_2.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_0.method_info.method_name,commit.changed_files.file_3.hunks.hunk_0.method_info.method_params,commit.changed_files.file_3.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_4.file_change_type,commit.changed_files.file_4.file_Nmethod,commit.changed_files.file_4.file_old_name,commit.changed_files.file_4.file_new_name,commit.changed_files.file_4.hunks.hunk_0.Ismethod,commit.changed_files.file_4.hunks.hunk_0.added_lines,commit.changed_files.file_4.hunks.hunk_0.deleted_lines,commit.changed_files.file_4.hunks.hunk_0.method_info.method_name,commit.changed_files.file_4.hunks.hunk_0.method_info.method_params,commit.changed_files.file_4.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_1.Ismethod,commit.changed_files.file_4.hunks.hunk_1.added_lines,commit.changed_files.file_4.hunks.hunk_1.deleted_lines,commit.changed_files.file_4.hunks.hunk_1.method_info.method_name,commit.changed_files.file_4.hunks.hunk_1.method_info.method_params,commit.changed_files.file_4.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_2.Ismethod,commit.changed_files.file_4.hunks.hunk_2.added_lines,commit.changed_files.file_4.hunks.hunk_2.deleted_lines,commit.changed_files.file_4.hunks.hunk_2.method_info.method_name,commit.changed_files.file_4.hunks.hunk_2.method_info.method_params,commit.changed_files.file_4.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_3.Ismethod,commit.changed_files.file_4.hunks.hunk_3.added_lines,commit.changed_files.file_4.hunks.hunk_3.deleted_lines,commit.changed_files.file_4.hunks.hunk_3.method_info.method_name,commit.changed_files.file_4.hunks.hunk_3.method_info.method_params,commit.changed_files.file_4.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_4.Ismethod,commit.changed_files.file_4.hunks.hunk_4.added_lines,commit.changed_files.file_4.hunks.hunk_4.deleted_lines,commit.changed_files.file_4.hunks.hunk_4.method_info.method_name,commit.changed_files.file_4.hunks.hunk_4.method_info.method_params,commit.changed_files.file_4.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_5.Ismethod,commit.changed_files.file_4.hunks.hunk_5.added_lines,commit.changed_files.file_4.hunks.hunk_5.deleted_lines,commit.changed_files.file_4.hunks.hunk_5.method_info.method_name,commit.changed_files.file_4.hunks.hunk_5.method_info.method_params,commit.changed_files.file_4.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_5.file_change_type,commit.changed_files.file_5.file_Nmethod,commit.changed_files.file_5.file_old_name,commit.changed_files.file_5.file_new_name,commit.changed_files.file_5.hunks.hunk_0.Ismethod,commit.changed_files.file_5.hunks.hunk_0.added_lines,commit.changed_files.file_5.hunks.hunk_0.deleted_lines,commit.changed_files.file_5.hunks.hunk_0.method_info.method_name,commit.changed_files.file_5.hunks.hunk_0.method_info.method_params,commit.changed_files.file_5.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_1.Ismethod,commit.changed_files.file_5.hunks.hunk_1.added_lines,commit.changed_files.file_5.hunks.hunk_1.deleted_lines,commit.changed_files.file_5.hunks.hunk_1.method_info.method_name,commit.changed_files.file_5.hunks.hunk_1.method_info.method_params,commit.changed_files.file_5.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_6.file_change_type,commit.changed_files.file_6.file_Nmethod,commit.changed_files.file_6.file_old_name,commit.changed_files.file_6.file_new_name,commit.changed_files.file_6.hunks.hunk_0.Ismethod,commit.changed_files.file_6.hunks.hunk_0.added_lines,commit.changed_files.file_6.hunks.hunk_0.deleted_lines,commit.changed_files.file_6.hunks.hunk_0.method_info.method_name,commit.changed_files.file_6.hunks.hunk_0.method_info.method_params,commit.changed_files.file_6.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_0.method_info.method_endline,BR.comments.comments_7.comment_id,BR.comments.comments_7.comment_author,BR.comments.comments_7.commentT,BR.comments.comments_7.comment_text,commit.changed_files.file_0.hunks.hunk_4.Ismethod,commit.changed_files.file_0.hunks.hunk_4.added_lines,commit.changed_files.file_0.hunks.hunk_4.deleted_lines,commit.changed_files.file_0.hunks.hunk_4.method_info.method_name,commit.changed_files.file_0.hunks.hunk_4.method_info.method_params,commit.changed_files.file_0.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_5.Ismethod,commit.changed_files.file_0.hunks.hunk_5.added_lines,commit.changed_files.file_0.hunks.hunk_5.deleted_lines,commit.changed_files.file_0.hunks.hunk_5.method_info.method_name,commit.changed_files.file_0.hunks.hunk_5.method_info.method_params,commit.changed_files.file_0.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_1.Ismethod,commit.changed_files.file_2.hunks.hunk_1.added_lines,commit.changed_files.file_2.hunks.hunk_1.deleted_lines,commit.changed_files.file_2.hunks.hunk_1.method_info.method_name,commit.changed_files.file_2.hunks.hunk_1.method_info.method_params,commit.changed_files.file_2.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_2.Ismethod,commit.changed_files.file_2.hunks.hunk_2.added_lines,commit.changed_files.file_2.hunks.hunk_2.deleted_lines,commit.changed_files.file_2.hunks.hunk_2.method_info.method_name,commit.changed_files.file_2.hunks.hunk_2.method_info.method_params,commit.changed_files.file_2.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_3.Ismethod,commit.changed_files.file_2.hunks.hunk_3.added_lines,commit.changed_files.file_2.hunks.hunk_3.deleted_lines,commit.changed_files.file_2.hunks.hunk_3.method_info.method_name,commit.changed_files.file_2.hunks.hunk_3.method_info.method_params,commit.changed_files.file_2.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_1.Ismethod,commit.changed_files.file_3.hunks.hunk_1.added_lines,commit.changed_files.file_3.hunks.hunk_1.deleted_lines,commit.changed_files.file_3.hunks.hunk_1.method_info.method_name,commit.changed_files.file_3.hunks.hunk_1.method_info.method_params,commit.changed_files.file_3.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_2.Ismethod,commit.changed_files.file_3.hunks.hunk_2.added_lines,commit.changed_files.file_3.hunks.hunk_2.deleted_lines,commit.changed_files.file_3.hunks.hunk_2.method_info.method_name,commit.changed_files.file_3.hunks.hunk_2.method_info.method_params,commit.changed_files.file_3.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_3.Ismethod,commit.changed_files.file_3.hunks.hunk_3.added_lines,commit.changed_files.file_3.hunks.hunk_3.deleted_lines,commit.changed_files.file_3.hunks.hunk_3.method_info.method_name,commit.changed_files.file_3.hunks.hunk_3.method_info.method_params,commit.changed_files.file_3.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_4.Ismethod,commit.changed_files.file_3.hunks.hunk_4.added_lines,commit.changed_files.file_3.hunks.hunk_4.deleted_lines,commit.changed_files.file_3.hunks.hunk_4.method_info.method_name,commit.changed_files.file_3.hunks.hunk_4.method_info.method_params,commit.changed_files.file_3.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_5.Ismethod,commit.changed_files.file_3.hunks.hunk_5.added_lines,commit.changed_files.file_3.hunks.hunk_5.deleted_lines,commit.changed_files.file_3.hunks.hunk_5.method_info.method_name,commit.changed_files.file_3.hunks.hunk_5.method_info.method_params,commit.changed_files.file_3.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_6.Ismethod,commit.changed_files.file_3.hunks.hunk_6.added_lines,commit.changed_files.file_3.hunks.hunk_6.deleted_lines,commit.changed_files.file_3.hunks.hunk_6.method_info.method_name,commit.changed_files.file_3.hunks.hunk_6.method_info.method_params,commit.changed_files.file_3.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_7.Ismethod,commit.changed_files.file_3.hunks.hunk_7.added_lines,commit.changed_files.file_3.hunks.hunk_7.deleted_lines,commit.changed_files.file_3.hunks.hunk_7.method_info.method_name,commit.changed_files.file_3.hunks.hunk_7.method_info.method_params,commit.changed_files.file_3.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_8.Ismethod,commit.changed_files.file_3.hunks.hunk_8.added_lines,commit.changed_files.file_3.hunks.hunk_8.deleted_lines,commit.changed_files.file_3.hunks.hunk_8.method_info.method_name,commit.changed_files.file_3.hunks.hunk_8.method_info.method_params,commit.changed_files.file_3.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_2.Ismethod,commit.changed_files.file_5.hunks.hunk_2.added_lines,commit.changed_files.file_5.hunks.hunk_2.deleted_lines,commit.changed_files.file_5.hunks.hunk_2.method_info.method_name,commit.changed_files.file_5.hunks.hunk_2.method_info.method_params,commit.changed_files.file_5.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_1.Ismethod,commit.changed_files.file_6.hunks.hunk_1.added_lines,commit.changed_files.file_6.hunks.hunk_1.deleted_lines,commit.changed_files.file_6.hunks.hunk_1.method_info.method_name,commit.changed_files.file_6.hunks.hunk_1.method_info.method_params,commit.changed_files.file_6.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_2.Ismethod,commit.changed_files.file_6.hunks.hunk_2.added_lines,commit.changed_files.file_6.hunks.hunk_2.deleted_lines,commit.changed_files.file_6.hunks.hunk_2.method_info.method_name,commit.changed_files.file_6.hunks.hunk_2.method_info.method_params,commit.changed_files.file_6.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_3.Ismethod,commit.changed_files.file_6.hunks.hunk_3.added_lines,commit.changed_files.file_6.hunks.hunk_3.deleted_lines,commit.changed_files.file_6.hunks.hunk_3.method_info.method_name,commit.changed_files.file_6.hunks.hunk_3.method_info.method_params,commit.changed_files.file_6.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_7.file_change_type,commit.changed_files.file_7.file_Nmethod,commit.changed_files.file_7.file_old_name,commit.changed_files.file_7.file_new_name,commit.changed_files.file_7.hunks.hunk_0.Ismethod,commit.changed_files.file_7.hunks.hunk_0.added_lines,commit.changed_files.file_7.hunks.hunk_0.deleted_lines,commit.changed_files.file_8.file_change_type,commit.changed_files.file_8.file_Nmethod,commit.changed_files.file_8.file_old_name,commit.changed_files.file_8.file_new_name,commit.changed_files.file_8.hunks.hunk_0.Ismethod,commit.changed_files.file_8.hunks.hunk_0.added_lines,commit.changed_files.file_8.hunks.hunk_0.deleted_lines,BR.comments.comments_8.comment_id,BR.comments.comments_8.comment_author,BR.comments.comments_8.commentT,BR.comments.comments_8.comment_text,BR.comments.comments_9.comment_id,BR.comments.comments_9.comment_author,BR.comments.comments_9.commentT,BR.comments.comments_9.comment_text,BR.comments.comments_10.comment_id,BR.comments.comments_10.comment_author,BR.comments.comments_10.commentT,BR.comments.comments_10.comment_text,BR.comments.comments_11.comment_id,BR.comments.comments_11.comment_author,BR.comments.comments_11.commentT,BR.comments.comments_11.comment_text,BR.comments.comments_12.comment_id,BR.comments.comments_12.comment_author,BR.comments.comments_12.commentT,BR.comments.comments_12.comment_text,BR.comments.comments_13.comment_id,BR.comments.comments_13.comment_author,BR.comments.comments_13.commentT,BR.comments.comments_13.comment_text,BR.comments.comments_14.comment_id,BR.comments.comments_14.comment_author,BR.comments.comments_14.commentT,BR.comments.comments_14.comment_text,BR.comments.comments_15.comment_id,BR.comments.comments_15.comment_author,BR.comments.comments_15.commentT,BR.comments.comments_15.comment_text,BR.comments.comments_16.comment_id,BR.comments.comments_16.comment_author,BR.comments.comments_16.commentT,BR.comments.comments_16.comment_text,BR.comments.comments_17.comment_id,BR.comments.comments_17.comment_author,BR.comments.comments_17.commentT,BR.comments.comments_17.comment_text,BR.comments.comments_18.comment_id,BR.comments.comments_18.comment_author,BR.comments.comments_18.commentT,BR.comments.comments_18.comment_text,BR.comments.comments_19.comment_id,BR.comments.comments_19.comment_author,BR.comments.comments_19.commentT,BR.comments.comments_19.comment_text,BR.comments.comments_20.comment_id,BR.comments.comments_20.comment_author,BR.comments.comments_20.commentT,BR.comments.comments_20.comment_text,BR.comments.comments_21.comment_id,BR.comments.comments_21.comment_author,BR.comments.comments_21.commentT,BR.comments.comments_21.comment_text,BR.comments.comments_22.comment_id,BR.comments.comments_22.comment_author,BR.comments.comments_22.commentT,BR.comments.comments_22.comment_text,BR.comments.comments_23.comment_id,BR.comments.comments_23.comment_author,BR.comments.comments_23.commentT,BR.comments.comments_23.comment_text,BR.comments.comments_24.comment_id,BR.comments.comments_24.comment_author,BR.comments.comments_24.commentT,BR.comments.comments_24.comment_text,BR.comments.comments_25.comment_id,BR.comments.comments_25.comment_author,BR.comments.comments_25.commentT,BR.comments.comments_25.comment_text,BR.comments.comments_26.comment_id,BR.comments.comments_26.comment_author,BR.comments.comments_26.commentT,BR.comments.comments_26.comment_text,BR.comments.comments_27.comment_id,BR.comments.comments_27.comment_author,BR.comments.comments_27.commentT,BR.comments.comments_27.comment_text,BR.comments.comments_28.comment_id,BR.comments.comments_28.comment_author,BR.comments.comments_28.commentT,BR.comments.comments_28.comment_text,BR.comments.comments_29.comment_id,BR.comments.comments_29.comment_author,BR.comments.comments_29.commentT,BR.comments.comments_29.comment_text,BR.comments.comments_30.comment_id,BR.comments.comments_30.comment_author,BR.comments.comments_30.commentT,BR.comments.comments_30.comment_text,BR.comments.comments_31.comment_id,BR.comments.comments_31.comment_author,BR.comments.comments_31.commentT,BR.comments.comments_31.comment_text,BR.comments.comments_32.comment_id,BR.comments.comments_32.comment_author,BR.comments.comments_32.commentT,BR.comments.comments_32.comment_text,commit.changed_files.file_9.file_change_type,commit.changed_files.file_9.file_Nmethod,commit.changed_files.file_9.file_old_name,commit.changed_files.file_9.file_new_name,commit.changed_files.file_10.file_change_type,commit.changed_files.file_10.file_Nmethod,commit.changed_files.file_10.file_old_name,commit.changed_files.file_10.file_new_name,commit.changed_files.file_11.file_change_type,commit.changed_files.file_11.file_Nmethod,commit.changed_files.file_11.file_old_name,commit.changed_files.file_11.file_new_name,commit.changed_files.file_12.file_change_type,commit.changed_files.file_12.file_Nmethod,commit.changed_files.file_12.file_old_name,commit.changed_files.file_12.file_new_name,commit.changed_files.file_13.file_change_type,commit.changed_files.file_13.file_Nmethod,commit.changed_files.file_13.file_old_name,commit.changed_files.file_13.file_new_name,commit.changed_files.file_14.file_change_type,commit.changed_files.file_14.file_Nmethod,commit.changed_files.file_14.file_old_name,commit.changed_files.file_14.file_new_name,commit.changed_files.file_14.hunks.hunk_0.Ismethod,commit.changed_files.file_14.hunks.hunk_0.added_lines,commit.changed_files.file_14.hunks.hunk_0.deleted_lines,commit.changed_files.file_15.file_change_type,commit.changed_files.file_15.file_Nmethod,commit.changed_files.file_15.file_old_name,commit.changed_files.file_15.file_new_name,commit.changed_files.file_15.hunks.hunk_0.Ismethod,commit.changed_files.file_15.hunks.hunk_0.added_lines,commit.changed_files.file_15.hunks.hunk_0.deleted_lines,commit.changed_files.file_15.hunks.hunk_0.method_info.method_name,commit.changed_files.file_15.hunks.hunk_0.method_info.method_params,commit.changed_files.file_15.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_15.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_15.hunks.hunk_1.Ismethod,commit.changed_files.file_15.hunks.hunk_1.added_lines,commit.changed_files.file_15.hunks.hunk_1.deleted_lines,commit.changed_files.file_15.hunks.hunk_1.method_info.method_name,commit.changed_files.file_15.hunks.hunk_1.method_info.method_params,commit.changed_files.file_15.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_15.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_16.file_change_type,commit.changed_files.file_16.file_Nmethod,commit.changed_files.file_16.file_old_name,commit.changed_files.file_16.file_new_name,commit.changed_files.file_16.hunks.hunk_0.Ismethod,commit.changed_files.file_16.hunks.hunk_0.added_lines,commit.changed_files.file_16.hunks.hunk_0.deleted_lines,commit.changed_files.file_16.hunks.hunk_0.method_info.method_name,commit.changed_files.file_16.hunks.hunk_0.method_info.method_params,commit.changed_files.file_16.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_1.Ismethod,commit.changed_files.file_16.hunks.hunk_1.added_lines,commit.changed_files.file_16.hunks.hunk_1.deleted_lines,commit.changed_files.file_16.hunks.hunk_1.method_info.method_name,commit.changed_files.file_16.hunks.hunk_1.method_info.method_params,commit.changed_files.file_16.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_17.file_change_type,commit.changed_files.file_17.file_Nmethod,commit.changed_files.file_17.file_old_name,commit.changed_files.file_17.file_new_name,commit.changed_files.file_17.hunks.hunk_0.Ismethod,commit.changed_files.file_17.hunks.hunk_0.added_lines,commit.changed_files.file_17.hunks.hunk_0.deleted_lines,commit.changed_files.file_17.hunks.hunk_0.method_info.method_name,commit.changed_files.file_17.hunks.hunk_0.method_info.method_params,commit.changed_files.file_17.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_18.file_change_type,commit.changed_files.file_18.file_Nmethod,commit.changed_files.file_18.file_old_name,commit.changed_files.file_18.file_new_name,commit.changed_files.file_18.hunks.hunk_0.Ismethod,commit.changed_files.file_18.hunks.hunk_0.added_lines,commit.changed_files.file_18.hunks.hunk_0.deleted_lines,commit.changed_files.file_18.hunks.hunk_0.method_info.method_name,commit.changed_files.file_18.hunks.hunk_0.method_info.method_params,commit.changed_files.file_18.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_18.hunks.hunk_1.Ismethod,commit.changed_files.file_18.hunks.hunk_1.added_lines,commit.changed_files.file_18.hunks.hunk_1.deleted_lines,commit.changed_files.file_18.hunks.hunk_1.method_info.method_name,commit.changed_files.file_18.hunks.hunk_1.method_info.method_params,commit.changed_files.file_18.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_18.hunks.hunk_2.Ismethod,commit.changed_files.file_18.hunks.hunk_2.added_lines,commit.changed_files.file_18.hunks.hunk_2.deleted_lines,commit.changed_files.file_18.hunks.hunk_2.method_info.method_name,commit.changed_files.file_18.hunks.hunk_2.method_info.method_params,commit.changed_files.file_18.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_19.file_change_type,commit.changed_files.file_19.file_Nmethod,commit.changed_files.file_19.file_old_name,commit.changed_files.file_19.file_new_name,commit.changed_files.file_19.hunks.hunk_0.Ismethod,commit.changed_files.file_19.hunks.hunk_0.added_lines,commit.changed_files.file_19.hunks.hunk_0.deleted_lines,commit.changed_files.file_20.file_change_type,commit.changed_files.file_20.file_Nmethod,commit.changed_files.file_20.file_old_name,commit.changed_files.file_20.file_new_name,commit.changed_files.file_20.hunks.hunk_0.Ismethod,commit.changed_files.file_20.hunks.hunk_0.added_lines,commit.changed_files.file_20.hunks.hunk_0.deleted_lines,commit.changed_files.file_21.file_change_type,commit.changed_files.file_21.file_Nmethod,commit.changed_files.file_21.file_old_name,commit.changed_files.file_21.file_new_name,commit.changed_files.file_21.hunks.hunk_0.Ismethod,commit.changed_files.file_21.hunks.hunk_0.added_lines,commit.changed_files.file_21.hunks.hunk_0.deleted_lines,commit.changed_files.file_22.file_change_type,commit.changed_files.file_22.file_Nmethod,commit.changed_files.file_22.file_old_name,commit.changed_files.file_22.file_new_name,commit.changed_files.file_22.hunks.hunk_0.Ismethod,commit.changed_files.file_22.hunks.hunk_0.added_lines,commit.changed_files.file_22.hunks.hunk_0.deleted_lines,commit.changed_files.file_23.file_change_type,commit.changed_files.file_23.file_Nmethod,commit.changed_files.file_23.file_old_name,commit.changed_files.file_23.file_new_name,commit.changed_files.file_23.hunks.hunk_0.Ismethod,commit.changed_files.file_23.hunks.hunk_0.added_lines,commit.changed_files.file_23.hunks.hunk_0.deleted_lines,commit.changed_files.file_24.file_change_type,commit.changed_files.file_24.file_Nmethod,commit.changed_files.file_24.file_old_name,commit.changed_files.file_24.file_new_name,commit.changed_files.file_24.hunks.hunk_0.Ismethod,commit.changed_files.file_24.hunks.hunk_0.added_lines,commit.changed_files.file_24.hunks.hunk_0.deleted_lines,BR.comments.comments_33.comment_id,BR.comments.comments_33.comment_author,BR.comments.comments_33.commentT,BR.comments.comments_33.comment_text,BR.comments.comments_34.comment_id,BR.comments.comments_34.comment_author,BR.comments.comments_34.commentT,BR.comments.comments_34.comment_text,BR.comments.comments_35.comment_id,BR.comments.comments_35.comment_author,BR.comments.comments_35.commentT,BR.comments.comments_35.comment_text,BR.comments.comments_36.comment_id,BR.comments.comments_36.comment_author,BR.comments.comments_36.commentT,BR.comments.comments_36.comment_text,BR.comments.comments_37.comment_id,BR.comments.comments_37.comment_author,BR.comments.comments_37.commentT,BR.comments.comments_37.comment_text,BR.comments.comments_38.comment_id,BR.comments.comments_38.comment_author,BR.comments.comments_38.commentT,BR.comments.comments_38.comment_text,BR.comments.comments_39.comment_id,BR.comments.comments_39.comment_author,BR.comments.comments_39.commentT,BR.comments.comments_39.comment_text,BR.comments.comments_40.comment_id,BR.comments.comments_40.comment_author,BR.comments.comments_40.commentT,BR.comments.comments_40.comment_text,BR.comments.comments_41.comment_id,BR.comments.comments_41.comment_author,BR.comments.comments_41.commentT,BR.comments.comments_41.comment_text,BR.comments.comments_42.comment_id,BR.comments.comments_42.comment_author,BR.comments.comments_42.commentT,BR.comments.comments_42.comment_text,BR.comments.comments_43.comment_id,BR.comments.comments_43.comment_author,BR.comments.comments_43.commentT,BR.comments.comments_43.comment_text,BR.comments.comments_44.comment_id,BR.comments.comments_44.comment_author,BR.comments.comments_44.commentT,BR.comments.comments_44.comment_text,BR.comments.comments_45.comment_id,BR.comments.comments_45.comment_author,BR.comments.comments_45.commentT,BR.comments.comments_45.comment_text,BR.comments.comments_46.comment_id,BR.comments.comments_46.comment_author,BR.comments.comments_46.commentT,BR.comments.comments_46.comment_text,BR.comments.comments_47.comment_id,BR.comments.comments_47.comment_author,BR.comments.comments_47.commentT,BR.comments.comments_47.comment_text,commit.changed_files.file_5.hunks.hunk_3.Ismethod,commit.changed_files.file_5.hunks.hunk_3.added_lines,commit.changed_files.file_5.hunks.hunk_3.deleted_lines,commit.changed_files.file_5.hunks.hunk_3.method_info.method_name,commit.changed_files.file_5.hunks.hunk_3.method_info.method_params,commit.changed_files.file_5.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_0.method_info.method_name,commit.changed_files.file_7.hunks.hunk_0.method_info.method_params,commit.changed_files.file_7.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_1.Ismethod,commit.changed_files.file_7.hunks.hunk_1.added_lines,commit.changed_files.file_7.hunks.hunk_1.deleted_lines,commit.changed_files.file_7.hunks.hunk_1.method_info.method_name,commit.changed_files.file_7.hunks.hunk_1.method_info.method_params,commit.changed_files.file_7.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_2.Ismethod,commit.changed_files.file_7.hunks.hunk_2.added_lines,commit.changed_files.file_7.hunks.hunk_2.deleted_lines,commit.changed_files.file_7.hunks.hunk_2.method_info.method_name,commit.changed_files.file_7.hunks.hunk_2.method_info.method_params,commit.changed_files.file_7.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_4.Ismethod,commit.changed_files.file_6.hunks.hunk_4.added_lines,commit.changed_files.file_6.hunks.hunk_4.deleted_lines,commit.changed_files.file_6.hunks.hunk_4.method_info.method_name,commit.changed_files.file_6.hunks.hunk_4.method_info.method_params,commit.changed_files.file_6.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_5.Ismethod,commit.changed_files.file_6.hunks.hunk_5.added_lines,commit.changed_files.file_6.hunks.hunk_5.deleted_lines,commit.changed_files.file_6.hunks.hunk_5.method_info.method_name,commit.changed_files.file_6.hunks.hunk_5.method_info.method_params,commit.changed_files.file_6.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_6.Ismethod,commit.changed_files.file_6.hunks.hunk_6.added_lines,commit.changed_files.file_6.hunks.hunk_6.deleted_lines,commit.changed_files.file_6.hunks.hunk_6.method_info.method_name,commit.changed_files.file_6.hunks.hunk_6.method_info.method_params,commit.changed_files.file_6.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_7.Ismethod,commit.changed_files.file_6.hunks.hunk_7.added_lines,commit.changed_files.file_6.hunks.hunk_7.deleted_lines,commit.changed_files.file_6.hunks.hunk_7.method_info.method_name,commit.changed_files.file_6.hunks.hunk_7.method_info.method_params,commit.changed_files.file_6.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_8.Ismethod,commit.changed_files.file_6.hunks.hunk_8.added_lines,commit.changed_files.file_6.hunks.hunk_8.deleted_lines,commit.changed_files.file_6.hunks.hunk_8.method_info.method_name,commit.changed_files.file_6.hunks.hunk_8.method_info.method_params,commit.changed_files.file_6.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_9.Ismethod,commit.changed_files.file_6.hunks.hunk_9.added_lines,commit.changed_files.file_6.hunks.hunk_9.deleted_lines,commit.changed_files.file_6.hunks.hunk_9.method_info.method_name,commit.changed_files.file_6.hunks.hunk_9.method_info.method_params,commit.changed_files.file_6.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_3.Ismethod,commit.changed_files.file_7.hunks.hunk_3.added_lines,commit.changed_files.file_7.hunks.hunk_3.deleted_lines,commit.changed_files.file_7.hunks.hunk_3.method_info.method_name,commit.changed_files.file_7.hunks.hunk_3.method_info.method_params,commit.changed_files.file_7.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_4.Ismethod,commit.changed_files.file_7.hunks.hunk_4.added_lines,commit.changed_files.file_7.hunks.hunk_4.deleted_lines,commit.changed_files.file_7.hunks.hunk_4.method_info.method_name,commit.changed_files.file_7.hunks.hunk_4.method_info.method_params,commit.changed_files.file_7.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_5.Ismethod,commit.changed_files.file_7.hunks.hunk_5.added_lines,commit.changed_files.file_7.hunks.hunk_5.deleted_lines,commit.changed_files.file_7.hunks.hunk_5.method_info.method_name,commit.changed_files.file_7.hunks.hunk_5.method_info.method_params,commit.changed_files.file_7.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_6.Ismethod,commit.changed_files.file_7.hunks.hunk_6.added_lines,commit.changed_files.file_7.hunks.hunk_6.deleted_lines,commit.changed_files.file_7.hunks.hunk_6.method_info.method_name,commit.changed_files.file_7.hunks.hunk_6.method_info.method_params,commit.changed_files.file_7.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_7.Ismethod,commit.changed_files.file_7.hunks.hunk_7.added_lines,commit.changed_files.file_7.hunks.hunk_7.deleted_lines,commit.changed_files.file_7.hunks.hunk_7.method_info.method_name,commit.changed_files.file_7.hunks.hunk_7.method_info.method_params,commit.changed_files.file_7.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_8.Ismethod,commit.changed_files.file_7.hunks.hunk_8.added_lines,commit.changed_files.file_7.hunks.hunk_8.deleted_lines,commit.changed_files.file_7.hunks.hunk_8.method_info.method_name,commit.changed_files.file_7.hunks.hunk_8.method_info.method_params,commit.changed_files.file_7.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_9.Ismethod,commit.changed_files.file_7.hunks.hunk_9.added_lines,commit.changed_files.file_7.hunks.hunk_9.deleted_lines,commit.changed_files.file_7.hunks.hunk_9.method_info.method_name,commit.changed_files.file_7.hunks.hunk_9.method_info.method_params,commit.changed_files.file_7.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_10.Ismethod,commit.changed_files.file_7.hunks.hunk_10.added_lines,commit.changed_files.file_7.hunks.hunk_10.deleted_lines,commit.changed_files.file_7.hunks.hunk_10.method_info.method_name,commit.changed_files.file_7.hunks.hunk_10.method_info.method_params,commit.changed_files.file_7.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_11.Ismethod,commit.changed_files.file_7.hunks.hunk_11.added_lines,commit.changed_files.file_7.hunks.hunk_11.deleted_lines,commit.changed_files.file_7.hunks.hunk_11.method_info.method_name,commit.changed_files.file_7.hunks.hunk_11.method_info.method_params,commit.changed_files.file_7.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_12.Ismethod,commit.changed_files.file_7.hunks.hunk_12.added_lines,commit.changed_files.file_7.hunks.hunk_12.deleted_lines,commit.changed_files.file_7.hunks.hunk_12.method_info.method_name,commit.changed_files.file_7.hunks.hunk_12.method_info.method_params,commit.changed_files.file_7.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_13.Ismethod,commit.changed_files.file_7.hunks.hunk_13.added_lines,commit.changed_files.file_7.hunks.hunk_13.deleted_lines,commit.changed_files.file_7.hunks.hunk_13.method_info.method_name,commit.changed_files.file_7.hunks.hunk_13.method_info.method_params,commit.changed_files.file_7.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_14.Ismethod,commit.changed_files.file_7.hunks.hunk_14.added_lines,commit.changed_files.file_7.hunks.hunk_14.deleted_lines,commit.changed_files.file_7.hunks.hunk_14.method_info.method_name,commit.changed_files.file_7.hunks.hunk_14.method_info.method_params,commit.changed_files.file_7.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_15.Ismethod,commit.changed_files.file_7.hunks.hunk_15.added_lines,commit.changed_files.file_7.hunks.hunk_15.deleted_lines,commit.changed_files.file_7.hunks.hunk_15.method_info.method_name,commit.changed_files.file_7.hunks.hunk_15.method_info.method_params,commit.changed_files.file_7.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_16.Ismethod,commit.changed_files.file_7.hunks.hunk_16.added_lines,commit.changed_files.file_7.hunks.hunk_16.deleted_lines,commit.changed_files.file_7.hunks.hunk_16.method_info.method_name,commit.changed_files.file_7.hunks.hunk_16.method_info.method_params,commit.changed_files.file_7.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_17.Ismethod,commit.changed_files.file_7.hunks.hunk_17.added_lines,commit.changed_files.file_7.hunks.hunk_17.deleted_lines,commit.changed_files.file_7.hunks.hunk_17.method_info.method_name,commit.changed_files.file_7.hunks.hunk_17.method_info.method_params,commit.changed_files.file_7.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_18.Ismethod,commit.changed_files.file_7.hunks.hunk_18.added_lines,commit.changed_files.file_7.hunks.hunk_18.deleted_lines,commit.changed_files.file_7.hunks.hunk_18.method_info.method_name,commit.changed_files.file_7.hunks.hunk_18.method_info.method_params,commit.changed_files.file_7.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_6.Ismethod,commit.changed_files.file_1.hunks.hunk_6.added_lines,commit.changed_files.file_1.hunks.hunk_6.deleted_lines,commit.changed_files.file_1.hunks.hunk_6.method_info.method_name,commit.changed_files.file_1.hunks.hunk_6.method_info.method_params,commit.changed_files.file_1.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_7.Ismethod,commit.changed_files.file_1.hunks.hunk_7.added_lines,commit.changed_files.file_1.hunks.hunk_7.deleted_lines,commit.changed_files.file_1.hunks.hunk_7.method_info.method_name,commit.changed_files.file_1.hunks.hunk_7.method_info.method_params,commit.changed_files.file_1.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_0.method_info.method_name,commit.changed_files.file_8.hunks.hunk_0.method_info.method_params,commit.changed_files.file_8.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_0.Ismethod,commit.changed_files.file_9.hunks.hunk_0.added_lines,commit.changed_files.file_9.hunks.hunk_0.deleted_lines,commit.changed_files.file_9.hunks.hunk_0.method_info.method_name,commit.changed_files.file_9.hunks.hunk_0.method_info.method_params,commit.changed_files.file_9.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_1.Ismethod,commit.changed_files.file_9.hunks.hunk_1.added_lines,commit.changed_files.file_9.hunks.hunk_1.deleted_lines,commit.changed_files.file_9.hunks.hunk_1.method_info.method_name,commit.changed_files.file_9.hunks.hunk_1.method_info.method_params,commit.changed_files.file_9.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_25.file_change_type,commit.changed_files.file_25.file_Nmethod,commit.changed_files.file_25.file_old_name,commit.changed_files.file_25.file_new_name,commit.changed_files.file_26.file_change_type,commit.changed_files.file_26.file_Nmethod,commit.changed_files.file_26.file_old_name,commit.changed_files.file_26.file_new_name,commit.changed_files.file_27.file_change_type,commit.changed_files.file_27.file_Nmethod,commit.changed_files.file_27.file_old_name,commit.changed_files.file_27.file_new_name,commit.changed_files.file_28.file_change_type,commit.changed_files.file_28.file_Nmethod,commit.changed_files.file_28.file_old_name,commit.changed_files.file_28.file_new_name,commit.changed_files.file_28.hunks.hunk_0.Ismethod,commit.changed_files.file_28.hunks.hunk_0.added_lines,commit.changed_files.file_28.hunks.hunk_0.deleted_lines,commit.changed_files.file_29.file_change_type,commit.changed_files.file_29.file_Nmethod,commit.changed_files.file_29.file_old_name,commit.changed_files.file_29.file_new_name,commit.changed_files.file_29.hunks.hunk_0.Ismethod,commit.changed_files.file_29.hunks.hunk_0.added_lines,commit.changed_files.file_29.hunks.hunk_0.deleted_lines,commit.changed_files.file_30.file_change_type,commit.changed_files.file_30.file_Nmethod,commit.changed_files.file_30.file_old_name,commit.changed_files.file_30.file_new_name,commit.changed_files.file_30.hunks.hunk_0.Ismethod,commit.changed_files.file_30.hunks.hunk_0.added_lines,commit.changed_files.file_30.hunks.hunk_0.deleted_lines,commit.changed_files.file_1.hunks.hunk_8.Ismethod,commit.changed_files.file_1.hunks.hunk_8.added_lines,commit.changed_files.file_1.hunks.hunk_8.deleted_lines,commit.changed_files.file_1.hunks.hunk_8.method_info.method_name,commit.changed_files.file_1.hunks.hunk_8.method_info.method_params,commit.changed_files.file_1.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_9.Ismethod,commit.changed_files.file_1.hunks.hunk_9.added_lines,commit.changed_files.file_1.hunks.hunk_9.deleted_lines,commit.changed_files.file_1.hunks.hunk_9.method_info.method_name,commit.changed_files.file_1.hunks.hunk_9.method_info.method_params,commit.changed_files.file_1.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_10.Ismethod,commit.changed_files.file_1.hunks.hunk_10.added_lines,commit.changed_files.file_1.hunks.hunk_10.deleted_lines,commit.changed_files.file_1.hunks.hunk_10.method_info.method_name,commit.changed_files.file_1.hunks.hunk_10.method_info.method_params,commit.changed_files.file_1.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_11.Ismethod,commit.changed_files.file_1.hunks.hunk_11.added_lines,commit.changed_files.file_1.hunks.hunk_11.deleted_lines,commit.changed_files.file_1.hunks.hunk_11.method_info.method_name,commit.changed_files.file_1.hunks.hunk_11.method_info.method_params,commit.changed_files.file_1.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_12.Ismethod,commit.changed_files.file_1.hunks.hunk_12.added_lines,commit.changed_files.file_1.hunks.hunk_12.deleted_lines,commit.changed_files.file_1.hunks.hunk_12.method_info.method_name,commit.changed_files.file_1.hunks.hunk_12.method_info.method_params,commit.changed_files.file_1.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_13.Ismethod,commit.changed_files.file_1.hunks.hunk_13.added_lines,commit.changed_files.file_1.hunks.hunk_13.deleted_lines,commit.changed_files.file_1.hunks.hunk_13.method_info.method_name,commit.changed_files.file_1.hunks.hunk_13.method_info.method_params,commit.changed_files.file_1.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_14.Ismethod,commit.changed_files.file_1.hunks.hunk_14.added_lines,commit.changed_files.file_1.hunks.hunk_14.deleted_lines,commit.changed_files.file_1.hunks.hunk_14.method_info.method_name,commit.changed_files.file_1.hunks.hunk_14.method_info.method_params,commit.changed_files.file_1.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_15.Ismethod,commit.changed_files.file_1.hunks.hunk_15.added_lines,commit.changed_files.file_1.hunks.hunk_15.deleted_lines,commit.changed_files.file_1.hunks.hunk_15.method_info.method_name,commit.changed_files.file_1.hunks.hunk_15.method_info.method_params,commit.changed_files.file_1.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_16.Ismethod,commit.changed_files.file_1.hunks.hunk_16.added_lines,commit.changed_files.file_1.hunks.hunk_16.deleted_lines,commit.changed_files.file_1.hunks.hunk_16.method_info.method_name,commit.changed_files.file_1.hunks.hunk_16.method_info.method_params,commit.changed_files.file_1.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_17.Ismethod,commit.changed_files.file_1.hunks.hunk_17.added_lines,commit.changed_files.file_1.hunks.hunk_17.deleted_lines,commit.changed_files.file_1.hunks.hunk_17.method_info.method_name,commit.changed_files.file_1.hunks.hunk_17.method_info.method_params,commit.changed_files.file_1.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_18.Ismethod,commit.changed_files.file_1.hunks.hunk_18.added_lines,commit.changed_files.file_1.hunks.hunk_18.deleted_lines,commit.changed_files.file_1.hunks.hunk_18.method_info.method_name,commit.changed_files.file_1.hunks.hunk_18.method_info.method_params,commit.changed_files.file_1.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_19.Ismethod,commit.changed_files.file_1.hunks.hunk_19.added_lines,commit.changed_files.file_1.hunks.hunk_19.deleted_lines,commit.changed_files.file_1.hunks.hunk_19.method_info.method_name,commit.changed_files.file_1.hunks.hunk_19.method_info.method_params,commit.changed_files.file_1.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_20.Ismethod,commit.changed_files.file_1.hunks.hunk_20.added_lines,commit.changed_files.file_1.hunks.hunk_20.deleted_lines,commit.changed_files.file_1.hunks.hunk_20.method_info.method_name,commit.changed_files.file_1.hunks.hunk_20.method_info.method_params,commit.changed_files.file_1.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_21.Ismethod,commit.changed_files.file_1.hunks.hunk_21.added_lines,commit.changed_files.file_1.hunks.hunk_21.deleted_lines,commit.changed_files.file_1.hunks.hunk_21.method_info.method_name,commit.changed_files.file_1.hunks.hunk_21.method_info.method_params,commit.changed_files.file_1.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_22.Ismethod,commit.changed_files.file_1.hunks.hunk_22.added_lines,commit.changed_files.file_1.hunks.hunk_22.deleted_lines,commit.changed_files.file_1.hunks.hunk_22.method_info.method_name,commit.changed_files.file_1.hunks.hunk_22.method_info.method_params,commit.changed_files.file_1.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_23.Ismethod,commit.changed_files.file_1.hunks.hunk_23.added_lines,commit.changed_files.file_1.hunks.hunk_23.deleted_lines,commit.changed_files.file_1.hunks.hunk_23.method_info.method_name,commit.changed_files.file_1.hunks.hunk_23.method_info.method_params,commit.changed_files.file_1.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_24.Ismethod,commit.changed_files.file_1.hunks.hunk_24.added_lines,commit.changed_files.file_1.hunks.hunk_24.deleted_lines,commit.changed_files.file_1.hunks.hunk_24.method_info.method_name,commit.changed_files.file_1.hunks.hunk_24.method_info.method_params,commit.changed_files.file_1.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_0.Ismethod,commit.changed_files.file_10.hunks.hunk_0.added_lines,commit.changed_files.file_10.hunks.hunk_0.deleted_lines,commit.changed_files.file_11.hunks.hunk_0.Ismethod,commit.changed_files.file_11.hunks.hunk_0.added_lines,commit.changed_files.file_11.hunks.hunk_0.deleted_lines,commit.changed_files.file_12.hunks.hunk_0.Ismethod,commit.changed_files.file_12.hunks.hunk_0.added_lines,commit.changed_files.file_12.hunks.hunk_0.deleted_lines,commit.changed_files.file_13.hunks.hunk_0.Ismethod,commit.changed_files.file_13.hunks.hunk_0.added_lines,commit.changed_files.file_13.hunks.hunk_0.deleted_lines,commit.changed_files.file_2.hunks.hunk_4.Ismethod,commit.changed_files.file_2.hunks.hunk_4.added_lines,commit.changed_files.file_2.hunks.hunk_4.deleted_lines,commit.changed_files.file_2.hunks.hunk_4.method_info.method_name,commit.changed_files.file_2.hunks.hunk_4.method_info.method_params,commit.changed_files.file_2.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_5.Ismethod,commit.changed_files.file_2.hunks.hunk_5.added_lines,commit.changed_files.file_2.hunks.hunk_5.deleted_lines,commit.changed_files.file_2.hunks.hunk_5.method_info.method_name,commit.changed_files.file_2.hunks.hunk_5.method_info.method_params,commit.changed_files.file_2.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_6.Ismethod,commit.changed_files.file_2.hunks.hunk_6.added_lines,commit.changed_files.file_2.hunks.hunk_6.deleted_lines,commit.changed_files.file_2.hunks.hunk_6.method_info.method_name,commit.changed_files.file_2.hunks.hunk_6.method_info.method_params,commit.changed_files.file_2.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_7.Ismethod,commit.changed_files.file_2.hunks.hunk_7.added_lines,commit.changed_files.file_2.hunks.hunk_7.deleted_lines,commit.changed_files.file_2.hunks.hunk_7.method_info.method_name,commit.changed_files.file_2.hunks.hunk_7.method_info.method_params,commit.changed_files.file_2.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_8.Ismethod,commit.changed_files.file_2.hunks.hunk_8.added_lines,commit.changed_files.file_2.hunks.hunk_8.deleted_lines,commit.changed_files.file_2.hunks.hunk_8.method_info.method_name,commit.changed_files.file_2.hunks.hunk_8.method_info.method_params,commit.changed_files.file_2.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_9.Ismethod,commit.changed_files.file_2.hunks.hunk_9.added_lines,commit.changed_files.file_2.hunks.hunk_9.deleted_lines,commit.changed_files.file_2.hunks.hunk_9.method_info.method_name,commit.changed_files.file_2.hunks.hunk_9.method_info.method_params,commit.changed_files.file_2.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_10.Ismethod,commit.changed_files.file_2.hunks.hunk_10.added_lines,commit.changed_files.file_2.hunks.hunk_10.deleted_lines,commit.changed_files.file_2.hunks.hunk_10.method_info.method_name,commit.changed_files.file_2.hunks.hunk_10.method_info.method_params,commit.changed_files.file_2.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_11.Ismethod,commit.changed_files.file_2.hunks.hunk_11.added_lines,commit.changed_files.file_2.hunks.hunk_11.deleted_lines,commit.changed_files.file_2.hunks.hunk_11.method_info.method_name,commit.changed_files.file_2.hunks.hunk_11.method_info.method_params,commit.changed_files.file_2.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_12.Ismethod,commit.changed_files.file_2.hunks.hunk_12.added_lines,commit.changed_files.file_2.hunks.hunk_12.deleted_lines,commit.changed_files.file_2.hunks.hunk_12.method_info.method_name,commit.changed_files.file_2.hunks.hunk_12.method_info.method_params,commit.changed_files.file_2.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_13.Ismethod,commit.changed_files.file_2.hunks.hunk_13.added_lines,commit.changed_files.file_2.hunks.hunk_13.deleted_lines,commit.changed_files.file_2.hunks.hunk_13.method_info.method_name,commit.changed_files.file_2.hunks.hunk_13.method_info.method_params,commit.changed_files.file_2.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_14.Ismethod,commit.changed_files.file_2.hunks.hunk_14.added_lines,commit.changed_files.file_2.hunks.hunk_14.deleted_lines,commit.changed_files.file_2.hunks.hunk_14.method_info.method_name,commit.changed_files.file_2.hunks.hunk_14.method_info.method_params,commit.changed_files.file_2.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_15.Ismethod,commit.changed_files.file_2.hunks.hunk_15.added_lines,commit.changed_files.file_2.hunks.hunk_15.deleted_lines,commit.changed_files.file_2.hunks.hunk_15.method_info.method_name,commit.changed_files.file_2.hunks.hunk_15.method_info.method_params,commit.changed_files.file_2.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_16.Ismethod,commit.changed_files.file_2.hunks.hunk_16.added_lines,commit.changed_files.file_2.hunks.hunk_16.deleted_lines,commit.changed_files.file_2.hunks.hunk_16.method_info.method_name,commit.changed_files.file_2.hunks.hunk_16.method_info.method_params,commit.changed_files.file_2.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_17.Ismethod,commit.changed_files.file_2.hunks.hunk_17.added_lines,commit.changed_files.file_2.hunks.hunk_17.deleted_lines,commit.changed_files.file_2.hunks.hunk_17.method_info.method_name,commit.changed_files.file_2.hunks.hunk_17.method_info.method_params,commit.changed_files.file_2.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_18.Ismethod,commit.changed_files.file_2.hunks.hunk_18.added_lines,commit.changed_files.file_2.hunks.hunk_18.deleted_lines,commit.changed_files.file_2.hunks.hunk_18.method_info.method_name,commit.changed_files.file_2.hunks.hunk_18.method_info.method_params,commit.changed_files.file_2.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_19.Ismethod,commit.changed_files.file_2.hunks.hunk_19.added_lines,commit.changed_files.file_2.hunks.hunk_19.deleted_lines,commit.changed_files.file_2.hunks.hunk_19.method_info.method_name,commit.changed_files.file_2.hunks.hunk_19.method_info.method_params,commit.changed_files.file_2.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_20.Ismethod,commit.changed_files.file_2.hunks.hunk_20.added_lines,commit.changed_files.file_2.hunks.hunk_20.deleted_lines,commit.changed_files.file_2.hunks.hunk_20.method_info.method_name,commit.changed_files.file_2.hunks.hunk_20.method_info.method_params,commit.changed_files.file_2.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_21.Ismethod,commit.changed_files.file_2.hunks.hunk_21.added_lines,commit.changed_files.file_2.hunks.hunk_21.deleted_lines,commit.changed_files.file_2.hunks.hunk_21.method_info.method_name,commit.changed_files.file_2.hunks.hunk_21.method_info.method_params,commit.changed_files.file_2.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_22.Ismethod,commit.changed_files.file_2.hunks.hunk_22.added_lines,commit.changed_files.file_2.hunks.hunk_22.deleted_lines,commit.changed_files.file_2.hunks.hunk_22.method_info.method_name,commit.changed_files.file_2.hunks.hunk_22.method_info.method_params,commit.changed_files.file_2.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_23.Ismethod,commit.changed_files.file_2.hunks.hunk_23.added_lines,commit.changed_files.file_2.hunks.hunk_23.deleted_lines,commit.changed_files.file_2.hunks.hunk_23.method_info.method_name,commit.changed_files.file_2.hunks.hunk_23.method_info.method_params,commit.changed_files.file_2.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_24.Ismethod,commit.changed_files.file_2.hunks.hunk_24.added_lines,commit.changed_files.file_2.hunks.hunk_24.deleted_lines,commit.changed_files.file_2.hunks.hunk_24.method_info.method_name,commit.changed_files.file_2.hunks.hunk_24.method_info.method_params,commit.changed_files.file_2.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_25.Ismethod,commit.changed_files.file_2.hunks.hunk_25.added_lines,commit.changed_files.file_2.hunks.hunk_25.deleted_lines,commit.changed_files.file_2.hunks.hunk_25.method_info.method_name,commit.changed_files.file_2.hunks.hunk_25.method_info.method_params,commit.changed_files.file_2.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_26.Ismethod,commit.changed_files.file_2.hunks.hunk_26.added_lines,commit.changed_files.file_2.hunks.hunk_26.deleted_lines,commit.changed_files.file_2.hunks.hunk_26.method_info.method_name,commit.changed_files.file_2.hunks.hunk_26.method_info.method_params,commit.changed_files.file_2.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_27.Ismethod,commit.changed_files.file_2.hunks.hunk_27.added_lines,commit.changed_files.file_2.hunks.hunk_27.deleted_lines,commit.changed_files.file_2.hunks.hunk_27.method_info.method_name,commit.changed_files.file_2.hunks.hunk_27.method_info.method_params,commit.changed_files.file_2.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_28.Ismethod,commit.changed_files.file_2.hunks.hunk_28.added_lines,commit.changed_files.file_2.hunks.hunk_28.deleted_lines,commit.changed_files.file_2.hunks.hunk_28.method_info.method_name,commit.changed_files.file_2.hunks.hunk_28.method_info.method_params,commit.changed_files.file_2.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_29.Ismethod,commit.changed_files.file_2.hunks.hunk_29.added_lines,commit.changed_files.file_2.hunks.hunk_29.deleted_lines,commit.changed_files.file_2.hunks.hunk_29.method_info.method_name,commit.changed_files.file_2.hunks.hunk_29.method_info.method_params,commit.changed_files.file_2.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_30.Ismethod,commit.changed_files.file_2.hunks.hunk_30.added_lines,commit.changed_files.file_2.hunks.hunk_30.deleted_lines,commit.changed_files.file_2.hunks.hunk_30.method_info.method_name,commit.changed_files.file_2.hunks.hunk_30.method_info.method_params,commit.changed_files.file_2.hunks.hunk_30.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_30.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_31.Ismethod,commit.changed_files.file_2.hunks.hunk_31.added_lines,commit.changed_files.file_2.hunks.hunk_31.deleted_lines,commit.changed_files.file_2.hunks.hunk_31.method_info.method_name,commit.changed_files.file_2.hunks.hunk_31.method_info.method_params,commit.changed_files.file_2.hunks.hunk_31.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_31.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_32.Ismethod,commit.changed_files.file_2.hunks.hunk_32.added_lines,commit.changed_files.file_2.hunks.hunk_32.deleted_lines,commit.changed_files.file_2.hunks.hunk_32.method_info.method_name,commit.changed_files.file_2.hunks.hunk_32.method_info.method_params,commit.changed_files.file_2.hunks.hunk_32.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_32.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_33.Ismethod,commit.changed_files.file_2.hunks.hunk_33.added_lines,commit.changed_files.file_2.hunks.hunk_33.deleted_lines,commit.changed_files.file_2.hunks.hunk_33.method_info.method_name,commit.changed_files.file_2.hunks.hunk_33.method_info.method_params,commit.changed_files.file_2.hunks.hunk_33.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_33.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_34.Ismethod,commit.changed_files.file_2.hunks.hunk_34.added_lines,commit.changed_files.file_2.hunks.hunk_34.deleted_lines,commit.changed_files.file_2.hunks.hunk_34.method_info.method_name,commit.changed_files.file_2.hunks.hunk_34.method_info.method_params,commit.changed_files.file_2.hunks.hunk_34.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_34.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_35.Ismethod,commit.changed_files.file_2.hunks.hunk_35.added_lines,commit.changed_files.file_2.hunks.hunk_35.deleted_lines,commit.changed_files.file_2.hunks.hunk_35.method_info.method_name,commit.changed_files.file_2.hunks.hunk_35.method_info.method_params,commit.changed_files.file_2.hunks.hunk_35.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_35.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_36.Ismethod,commit.changed_files.file_2.hunks.hunk_36.added_lines,commit.changed_files.file_2.hunks.hunk_36.deleted_lines,commit.changed_files.file_2.hunks.hunk_36.method_info.method_name,commit.changed_files.file_2.hunks.hunk_36.method_info.method_params,commit.changed_files.file_2.hunks.hunk_36.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_36.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_37.Ismethod,commit.changed_files.file_2.hunks.hunk_37.added_lines,commit.changed_files.file_2.hunks.hunk_37.deleted_lines,commit.changed_files.file_2.hunks.hunk_37.method_info.method_name,commit.changed_files.file_2.hunks.hunk_37.method_info.method_params,commit.changed_files.file_2.hunks.hunk_37.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_37.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_9.Ismethod,commit.changed_files.file_3.hunks.hunk_9.added_lines,commit.changed_files.file_3.hunks.hunk_9.deleted_lines,commit.changed_files.file_3.hunks.hunk_9.method_info.method_name,commit.changed_files.file_3.hunks.hunk_9.method_info.method_params,commit.changed_files.file_3.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_10.Ismethod,commit.changed_files.file_3.hunks.hunk_10.added_lines,commit.changed_files.file_3.hunks.hunk_10.deleted_lines,commit.changed_files.file_3.hunks.hunk_10.method_info.method_name,commit.changed_files.file_3.hunks.hunk_10.method_info.method_params,commit.changed_files.file_3.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_11.Ismethod,commit.changed_files.file_3.hunks.hunk_11.added_lines,commit.changed_files.file_3.hunks.hunk_11.deleted_lines,commit.changed_files.file_3.hunks.hunk_11.method_info.method_name,commit.changed_files.file_3.hunks.hunk_11.method_info.method_params,commit.changed_files.file_3.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_12.Ismethod,commit.changed_files.file_3.hunks.hunk_12.added_lines,commit.changed_files.file_3.hunks.hunk_12.deleted_lines,commit.changed_files.file_3.hunks.hunk_12.method_info.method_name,commit.changed_files.file_3.hunks.hunk_12.method_info.method_params,commit.changed_files.file_3.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_13.Ismethod,commit.changed_files.file_3.hunks.hunk_13.added_lines,commit.changed_files.file_3.hunks.hunk_13.deleted_lines,commit.changed_files.file_3.hunks.hunk_13.method_info.method_name,commit.changed_files.file_3.hunks.hunk_13.method_info.method_params,commit.changed_files.file_3.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_2.Ismethod,commit.changed_files.file_9.hunks.hunk_2.added_lines,commit.changed_files.file_9.hunks.hunk_2.deleted_lines,commit.changed_files.file_9.hunks.hunk_2.method_info.method_name,commit.changed_files.file_9.hunks.hunk_2.method_info.method_params,commit.changed_files.file_9.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_3.Ismethod,commit.changed_files.file_9.hunks.hunk_3.added_lines,commit.changed_files.file_9.hunks.hunk_3.deleted_lines,commit.changed_files.file_9.hunks.hunk_3.method_info.method_name,commit.changed_files.file_9.hunks.hunk_3.method_info.method_params,commit.changed_files.file_9.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_4.Ismethod,commit.changed_files.file_9.hunks.hunk_4.added_lines,commit.changed_files.file_9.hunks.hunk_4.deleted_lines,commit.changed_files.file_9.hunks.hunk_4.method_info.method_name,commit.changed_files.file_9.hunks.hunk_4.method_info.method_params,commit.changed_files.file_9.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_0.method_info.method_name,commit.changed_files.file_10.hunks.hunk_0.method_info.method_params,commit.changed_files.file_10.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_1.Ismethod,commit.changed_files.file_10.hunks.hunk_1.added_lines,commit.changed_files.file_10.hunks.hunk_1.deleted_lines,commit.changed_files.file_10.hunks.hunk_1.method_info.method_name,commit.changed_files.file_10.hunks.hunk_1.method_info.method_params,commit.changed_files.file_10.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_2.Ismethod,commit.changed_files.file_10.hunks.hunk_2.added_lines,commit.changed_files.file_10.hunks.hunk_2.deleted_lines,commit.changed_files.file_10.hunks.hunk_2.method_info.method_name,commit.changed_files.file_10.hunks.hunk_2.method_info.method_params,commit.changed_files.file_10.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_3.Ismethod,commit.changed_files.file_10.hunks.hunk_3.added_lines,commit.changed_files.file_10.hunks.hunk_3.deleted_lines,commit.changed_files.file_10.hunks.hunk_3.method_info.method_name,commit.changed_files.file_10.hunks.hunk_3.method_info.method_params,commit.changed_files.file_10.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_4.Ismethod,commit.changed_files.file_10.hunks.hunk_4.added_lines,commit.changed_files.file_10.hunks.hunk_4.deleted_lines,commit.changed_files.file_10.hunks.hunk_4.method_info.method_name,commit.changed_files.file_10.hunks.hunk_4.method_info.method_params,commit.changed_files.file_10.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_5.Ismethod,commit.changed_files.file_10.hunks.hunk_5.added_lines,commit.changed_files.file_10.hunks.hunk_5.deleted_lines,commit.changed_files.file_10.hunks.hunk_5.method_info.method_name,commit.changed_files.file_10.hunks.hunk_5.method_info.method_params,commit.changed_files.file_10.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_6.Ismethod,commit.changed_files.file_10.hunks.hunk_6.added_lines,commit.changed_files.file_10.hunks.hunk_6.deleted_lines,commit.changed_files.file_10.hunks.hunk_6.method_info.method_name,commit.changed_files.file_10.hunks.hunk_6.method_info.method_params,commit.changed_files.file_10.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_7.Ismethod,commit.changed_files.file_10.hunks.hunk_7.added_lines,commit.changed_files.file_10.hunks.hunk_7.deleted_lines,commit.changed_files.file_10.hunks.hunk_7.method_info.method_name,commit.changed_files.file_10.hunks.hunk_7.method_info.method_params,commit.changed_files.file_10.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_8.Ismethod,commit.changed_files.file_10.hunks.hunk_8.added_lines,commit.changed_files.file_10.hunks.hunk_8.deleted_lines,commit.changed_files.file_10.hunks.hunk_8.method_info.method_name,commit.changed_files.file_10.hunks.hunk_8.method_info.method_params,commit.changed_files.file_10.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_9.Ismethod,commit.changed_files.file_10.hunks.hunk_9.added_lines,commit.changed_files.file_10.hunks.hunk_9.deleted_lines,commit.changed_files.file_10.hunks.hunk_9.method_info.method_name,commit.changed_files.file_10.hunks.hunk_9.method_info.method_params,commit.changed_files.file_10.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_10.Ismethod,commit.changed_files.file_10.hunks.hunk_10.added_lines,commit.changed_files.file_10.hunks.hunk_10.deleted_lines,commit.changed_files.file_10.hunks.hunk_10.method_info.method_name,commit.changed_files.file_10.hunks.hunk_10.method_info.method_params,commit.changed_files.file_10.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_11.Ismethod,commit.changed_files.file_10.hunks.hunk_11.added_lines,commit.changed_files.file_10.hunks.hunk_11.deleted_lines,commit.changed_files.file_10.hunks.hunk_11.method_info.method_name,commit.changed_files.file_10.hunks.hunk_11.method_info.method_params,commit.changed_files.file_10.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_12.Ismethod,commit.changed_files.file_10.hunks.hunk_12.added_lines,commit.changed_files.file_10.hunks.hunk_12.deleted_lines,commit.changed_files.file_10.hunks.hunk_12.method_info.method_name,commit.changed_files.file_10.hunks.hunk_12.method_info.method_params,commit.changed_files.file_10.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_13.Ismethod,commit.changed_files.file_10.hunks.hunk_13.added_lines,commit.changed_files.file_10.hunks.hunk_13.deleted_lines,commit.changed_files.file_10.hunks.hunk_13.method_info.method_name,commit.changed_files.file_10.hunks.hunk_13.method_info.method_params,commit.changed_files.file_10.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_14.Ismethod,commit.changed_files.file_10.hunks.hunk_14.added_lines,commit.changed_files.file_10.hunks.hunk_14.deleted_lines,commit.changed_files.file_10.hunks.hunk_14.method_info.method_name,commit.changed_files.file_10.hunks.hunk_14.method_info.method_params,commit.changed_files.file_10.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_15.Ismethod,commit.changed_files.file_10.hunks.hunk_15.added_lines,commit.changed_files.file_10.hunks.hunk_15.deleted_lines,commit.changed_files.file_10.hunks.hunk_15.method_info.method_name,commit.changed_files.file_10.hunks.hunk_15.method_info.method_params,commit.changed_files.file_10.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_16.Ismethod,commit.changed_files.file_10.hunks.hunk_16.added_lines,commit.changed_files.file_10.hunks.hunk_16.deleted_lines,commit.changed_files.file_10.hunks.hunk_16.method_info.method_name,commit.changed_files.file_10.hunks.hunk_16.method_info.method_params,commit.changed_files.file_10.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_17.Ismethod,commit.changed_files.file_10.hunks.hunk_17.added_lines,commit.changed_files.file_10.hunks.hunk_17.deleted_lines,commit.changed_files.file_10.hunks.hunk_17.method_info.method_name,commit.changed_files.file_10.hunks.hunk_17.method_info.method_params,commit.changed_files.file_10.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_18.Ismethod,commit.changed_files.file_10.hunks.hunk_18.added_lines,commit.changed_files.file_10.hunks.hunk_18.deleted_lines,commit.changed_files.file_10.hunks.hunk_18.method_info.method_name,commit.changed_files.file_10.hunks.hunk_18.method_info.method_params,commit.changed_files.file_10.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_19.Ismethod,commit.changed_files.file_10.hunks.hunk_19.added_lines,commit.changed_files.file_10.hunks.hunk_19.deleted_lines,commit.changed_files.file_10.hunks.hunk_19.method_info.method_name,commit.changed_files.file_10.hunks.hunk_19.method_info.method_params,commit.changed_files.file_10.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_20.Ismethod,commit.changed_files.file_10.hunks.hunk_20.added_lines,commit.changed_files.file_10.hunks.hunk_20.deleted_lines,commit.changed_files.file_10.hunks.hunk_20.method_info.method_name,commit.changed_files.file_10.hunks.hunk_20.method_info.method_params,commit.changed_files.file_10.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_21.Ismethod,commit.changed_files.file_10.hunks.hunk_21.added_lines,commit.changed_files.file_10.hunks.hunk_21.deleted_lines,commit.changed_files.file_10.hunks.hunk_21.method_info.method_name,commit.changed_files.file_10.hunks.hunk_21.method_info.method_params,commit.changed_files.file_10.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_22.Ismethod,commit.changed_files.file_10.hunks.hunk_22.added_lines,commit.changed_files.file_10.hunks.hunk_22.deleted_lines,commit.changed_files.file_10.hunks.hunk_22.method_info.method_name,commit.changed_files.file_10.hunks.hunk_22.method_info.method_params,commit.changed_files.file_10.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_23.Ismethod,commit.changed_files.file_10.hunks.hunk_23.added_lines,commit.changed_files.file_10.hunks.hunk_23.deleted_lines,commit.changed_files.file_10.hunks.hunk_23.method_info.method_name,commit.changed_files.file_10.hunks.hunk_23.method_info.method_params,commit.changed_files.file_10.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_24.Ismethod,commit.changed_files.file_10.hunks.hunk_24.added_lines,commit.changed_files.file_10.hunks.hunk_24.deleted_lines,commit.changed_files.file_10.hunks.hunk_24.method_info.method_name,commit.changed_files.file_10.hunks.hunk_24.method_info.method_params,commit.changed_files.file_10.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_25.Ismethod,commit.changed_files.file_10.hunks.hunk_25.added_lines,commit.changed_files.file_10.hunks.hunk_25.deleted_lines,commit.changed_files.file_10.hunks.hunk_25.method_info.method_name,commit.changed_files.file_10.hunks.hunk_25.method_info.method_params,commit.changed_files.file_10.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_26.Ismethod,commit.changed_files.file_10.hunks.hunk_26.added_lines,commit.changed_files.file_10.hunks.hunk_26.deleted_lines,commit.changed_files.file_10.hunks.hunk_26.method_info.method_name,commit.changed_files.file_10.hunks.hunk_26.method_info.method_params,commit.changed_files.file_10.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_27.Ismethod,commit.changed_files.file_10.hunks.hunk_27.added_lines,commit.changed_files.file_10.hunks.hunk_27.deleted_lines,commit.changed_files.file_10.hunks.hunk_27.method_info.method_name,commit.changed_files.file_10.hunks.hunk_27.method_info.method_params,commit.changed_files.file_10.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_28.Ismethod,commit.changed_files.file_10.hunks.hunk_28.added_lines,commit.changed_files.file_10.hunks.hunk_28.deleted_lines,commit.changed_files.file_10.hunks.hunk_28.method_info.method_name,commit.changed_files.file_10.hunks.hunk_28.method_info.method_params,commit.changed_files.file_10.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_29.Ismethod,commit.changed_files.file_10.hunks.hunk_29.added_lines,commit.changed_files.file_10.hunks.hunk_29.deleted_lines,commit.changed_files.file_10.hunks.hunk_29.method_info.method_name,commit.changed_files.file_10.hunks.hunk_29.method_info.method_params,commit.changed_files.file_10.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_0.method_info.method_name,commit.changed_files.file_11.hunks.hunk_0.method_info.method_params,commit.changed_files.file_11.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_1.Ismethod,commit.changed_files.file_11.hunks.hunk_1.added_lines,commit.changed_files.file_11.hunks.hunk_1.deleted_lines,commit.changed_files.file_11.hunks.hunk_1.method_info.method_name,commit.changed_files.file_11.hunks.hunk_1.method_info.method_params,commit.changed_files.file_11.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_2.Ismethod,commit.changed_files.file_11.hunks.hunk_2.added_lines,commit.changed_files.file_11.hunks.hunk_2.deleted_lines,commit.changed_files.file_11.hunks.hunk_2.method_info.method_name,commit.changed_files.file_11.hunks.hunk_2.method_info.method_params,commit.changed_files.file_11.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_3.Ismethod,commit.changed_files.file_11.hunks.hunk_3.added_lines,commit.changed_files.file_11.hunks.hunk_3.deleted_lines,commit.changed_files.file_11.hunks.hunk_3.method_info.method_name,commit.changed_files.file_11.hunks.hunk_3.method_info.method_params,commit.changed_files.file_11.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_4.Ismethod,commit.changed_files.file_11.hunks.hunk_4.added_lines,commit.changed_files.file_11.hunks.hunk_4.deleted_lines,commit.changed_files.file_11.hunks.hunk_4.method_info.method_name,commit.changed_files.file_11.hunks.hunk_4.method_info.method_params,commit.changed_files.file_11.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_5.Ismethod,commit.changed_files.file_11.hunks.hunk_5.added_lines,commit.changed_files.file_11.hunks.hunk_5.deleted_lines,commit.changed_files.file_11.hunks.hunk_5.method_info.method_name,commit.changed_files.file_11.hunks.hunk_5.method_info.method_params,commit.changed_files.file_11.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_6.Ismethod,commit.changed_files.file_11.hunks.hunk_6.added_lines,commit.changed_files.file_11.hunks.hunk_6.deleted_lines,commit.changed_files.file_11.hunks.hunk_6.method_info.method_name,commit.changed_files.file_11.hunks.hunk_6.method_info.method_params,commit.changed_files.file_11.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_7.Ismethod,commit.changed_files.file_11.hunks.hunk_7.added_lines,commit.changed_files.file_11.hunks.hunk_7.deleted_lines,commit.changed_files.file_11.hunks.hunk_7.method_info.method_name,commit.changed_files.file_11.hunks.hunk_7.method_info.method_params,commit.changed_files.file_11.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_8.Ismethod,commit.changed_files.file_11.hunks.hunk_8.added_lines,commit.changed_files.file_11.hunks.hunk_8.deleted_lines,commit.changed_files.file_11.hunks.hunk_8.method_info.method_name,commit.changed_files.file_11.hunks.hunk_8.method_info.method_params,commit.changed_files.file_11.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_9.Ismethod,commit.changed_files.file_11.hunks.hunk_9.added_lines,commit.changed_files.file_11.hunks.hunk_9.deleted_lines,commit.changed_files.file_11.hunks.hunk_9.method_info.method_name,commit.changed_files.file_11.hunks.hunk_9.method_info.method_params,commit.changed_files.file_11.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_10.Ismethod,commit.changed_files.file_11.hunks.hunk_10.added_lines,commit.changed_files.file_11.hunks.hunk_10.deleted_lines,commit.changed_files.file_11.hunks.hunk_10.method_info.method_name,commit.changed_files.file_11.hunks.hunk_10.method_info.method_params,commit.changed_files.file_11.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_11.Ismethod,commit.changed_files.file_11.hunks.hunk_11.added_lines,commit.changed_files.file_11.hunks.hunk_11.deleted_lines,commit.changed_files.file_11.hunks.hunk_11.method_info.method_name,commit.changed_files.file_11.hunks.hunk_11.method_info.method_params,commit.changed_files.file_11.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_12.Ismethod,commit.changed_files.file_11.hunks.hunk_12.added_lines,commit.changed_files.file_11.hunks.hunk_12.deleted_lines,commit.changed_files.file_11.hunks.hunk_12.method_info.method_name,commit.changed_files.file_11.hunks.hunk_12.method_info.method_params,commit.changed_files.file_11.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_13.Ismethod,commit.changed_files.file_11.hunks.hunk_13.added_lines,commit.changed_files.file_11.hunks.hunk_13.deleted_lines,commit.changed_files.file_11.hunks.hunk_13.method_info.method_name,commit.changed_files.file_11.hunks.hunk_13.method_info.method_params,commit.changed_files.file_11.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_14.Ismethod,commit.changed_files.file_11.hunks.hunk_14.added_lines,commit.changed_files.file_11.hunks.hunk_14.deleted_lines,commit.changed_files.file_11.hunks.hunk_14.method_info.method_name,commit.changed_files.file_11.hunks.hunk_14.method_info.method_params,commit.changed_files.file_11.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_15.Ismethod,commit.changed_files.file_11.hunks.hunk_15.added_lines,commit.changed_files.file_11.hunks.hunk_15.deleted_lines,commit.changed_files.file_11.hunks.hunk_15.method_info.method_name,commit.changed_files.file_11.hunks.hunk_15.method_info.method_params,commit.changed_files.file_11.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_16.Ismethod,commit.changed_files.file_11.hunks.hunk_16.added_lines,commit.changed_files.file_11.hunks.hunk_16.deleted_lines,commit.changed_files.file_11.hunks.hunk_16.method_info.method_name,commit.changed_files.file_11.hunks.hunk_16.method_info.method_params,commit.changed_files.file_11.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_17.Ismethod,commit.changed_files.file_11.hunks.hunk_17.added_lines,commit.changed_files.file_11.hunks.hunk_17.deleted_lines,commit.changed_files.file_11.hunks.hunk_17.method_info.method_name,commit.changed_files.file_11.hunks.hunk_17.method_info.method_params,commit.changed_files.file_11.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_18.Ismethod,commit.changed_files.file_11.hunks.hunk_18.added_lines,commit.changed_files.file_11.hunks.hunk_18.deleted_lines,commit.changed_files.file_11.hunks.hunk_18.method_info.method_name,commit.changed_files.file_11.hunks.hunk_18.method_info.method_params,commit.changed_files.file_11.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_19.Ismethod,commit.changed_files.file_11.hunks.hunk_19.added_lines,commit.changed_files.file_11.hunks.hunk_19.deleted_lines,commit.changed_files.file_11.hunks.hunk_19.method_info.method_name,commit.changed_files.file_11.hunks.hunk_19.method_info.method_params,commit.changed_files.file_11.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_1.Ismethod,commit.changed_files.file_8.hunks.hunk_1.added_lines,commit.changed_files.file_8.hunks.hunk_1.deleted_lines,commit.changed_files.file_8.hunks.hunk_1.method_info.method_name,commit.changed_files.file_8.hunks.hunk_1.method_info.method_params,commit.changed_files.file_8.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_2.Ismethod,commit.changed_files.file_8.hunks.hunk_2.added_lines,commit.changed_files.file_8.hunks.hunk_2.deleted_lines,commit.changed_files.file_8.hunks.hunk_2.method_info.method_name,commit.changed_files.file_8.hunks.hunk_2.method_info.method_params,commit.changed_files.file_8.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_12.hunks.hunk_0.method_info.method_name,commit.changed_files.file_12.hunks.hunk_0.method_info.method_params,commit.changed_files.file_12.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_13.hunks.hunk_0.method_info.method_name,commit.changed_files.file_13.hunks.hunk_0.method_info.method_params,commit.changed_files.file_13.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_13.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_6.Ismethod,commit.changed_files.file_0.hunks.hunk_6.added_lines,commit.changed_files.file_0.hunks.hunk_6.deleted_lines,commit.changed_files.file_0.hunks.hunk_6.method_info.method_name,commit.changed_files.file_0.hunks.hunk_6.method_info.method_params,commit.changed_files.file_0.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_6.Ismethod,commit.changed_files.file_4.hunks.hunk_6.added_lines,commit.changed_files.file_4.hunks.hunk_6.deleted_lines,commit.changed_files.file_4.hunks.hunk_6.method_info.method_name,commit.changed_files.file_4.hunks.hunk_6.method_info.method_params,commit.changed_files.file_4.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_7.Ismethod,commit.changed_files.file_0.hunks.hunk_7.added_lines,commit.changed_files.file_0.hunks.hunk_7.deleted_lines,commit.changed_files.file_0.hunks.hunk_7.method_info.method_name,commit.changed_files.file_0.hunks.hunk_7.method_info.method_params,commit.changed_files.file_0.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_8.Ismethod,commit.changed_files.file_0.hunks.hunk_8.added_lines,commit.changed_files.file_0.hunks.hunk_8.deleted_lines,commit.changed_files.file_0.hunks.hunk_8.method_info.method_name,commit.changed_files.file_0.hunks.hunk_8.method_info.method_params,commit.changed_files.file_0.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_9.Ismethod,commit.changed_files.file_0.hunks.hunk_9.added_lines,commit.changed_files.file_0.hunks.hunk_9.deleted_lines,commit.changed_files.file_0.hunks.hunk_9.method_info.method_name,commit.changed_files.file_0.hunks.hunk_9.method_info.method_params,commit.changed_files.file_0.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_25.hunks.hunk_0.Ismethod,commit.changed_files.file_25.hunks.hunk_0.added_lines,commit.changed_files.file_25.hunks.hunk_0.deleted_lines,commit.changed_files.file_25.hunks.hunk_0.method_info.method_name,commit.changed_files.file_25.hunks.hunk_0.method_info.method_params,commit.changed_files.file_25.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_25.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_25.hunks.hunk_1.Ismethod,commit.changed_files.file_25.hunks.hunk_1.added_lines,commit.changed_files.file_25.hunks.hunk_1.deleted_lines,commit.changed_files.file_25.hunks.hunk_1.method_info.method_name,commit.changed_files.file_25.hunks.hunk_1.method_info.method_params,commit.changed_files.file_25.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_25.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_25.hunks.hunk_2.Ismethod,commit.changed_files.file_25.hunks.hunk_2.added_lines,commit.changed_files.file_25.hunks.hunk_2.deleted_lines,commit.changed_files.file_25.hunks.hunk_2.method_info.method_name,commit.changed_files.file_25.hunks.hunk_2.method_info.method_params,commit.changed_files.file_25.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_25.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_25.hunks.hunk_3.Ismethod,commit.changed_files.file_25.hunks.hunk_3.added_lines,commit.changed_files.file_25.hunks.hunk_3.deleted_lines,commit.changed_files.file_25.hunks.hunk_3.method_info.method_name,commit.changed_files.file_25.hunks.hunk_3.method_info.method_params,commit.changed_files.file_25.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_25.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_26.hunks.hunk_0.Ismethod,commit.changed_files.file_26.hunks.hunk_0.added_lines,commit.changed_files.file_26.hunks.hunk_0.deleted_lines,commit.changed_files.file_0.hunks.hunk_10.Ismethod,commit.changed_files.file_0.hunks.hunk_10.added_lines,commit.changed_files.file_0.hunks.hunk_10.deleted_lines,commit.changed_files.file_0.hunks.hunk_10.method_info.method_name,commit.changed_files.file_0.hunks.hunk_10.method_info.method_params,commit.changed_files.file_0.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_11.Ismethod,commit.changed_files.file_0.hunks.hunk_11.added_lines,commit.changed_files.file_0.hunks.hunk_11.deleted_lines,commit.changed_files.file_0.hunks.hunk_11.method_info.method_name,commit.changed_files.file_0.hunks.hunk_11.method_info.method_params,commit.changed_files.file_0.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_12.Ismethod,commit.changed_files.file_0.hunks.hunk_12.added_lines,commit.changed_files.file_0.hunks.hunk_12.deleted_lines,commit.changed_files.file_0.hunks.hunk_12.method_info.method_name,commit.changed_files.file_0.hunks.hunk_12.method_info.method_params,commit.changed_files.file_0.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_13.Ismethod,commit.changed_files.file_0.hunks.hunk_13.added_lines,commit.changed_files.file_0.hunks.hunk_13.deleted_lines,commit.changed_files.file_0.hunks.hunk_13.method_info.method_name,commit.changed_files.file_0.hunks.hunk_13.method_info.method_params,commit.changed_files.file_0.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_14.Ismethod,commit.changed_files.file_0.hunks.hunk_14.added_lines,commit.changed_files.file_0.hunks.hunk_14.deleted_lines,commit.changed_files.file_0.hunks.hunk_14.method_info.method_name,commit.changed_files.file_0.hunks.hunk_14.method_info.method_params,commit.changed_files.file_0.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_15.Ismethod,commit.changed_files.file_0.hunks.hunk_15.added_lines,commit.changed_files.file_0.hunks.hunk_15.deleted_lines,commit.changed_files.file_0.hunks.hunk_15.method_info.method_name,commit.changed_files.file_0.hunks.hunk_15.method_info.method_params,commit.changed_files.file_0.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_15.method_info.method_endline
10303,blake-varden,2017-05-30T21:37:53Z,2017-06-14T03:56:30Z,TensorflowDebugger does not dump Stack/Pack/Concat nodes,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Ubuntu 14.04
 TensorFlow installed from (source or binary):
 Binary
 TensorFlow version (use command below):
 1.1
 Bazel version (if compiling from source):
 CUDA/cuDNN version:
 8.0/5.1.5
 GPU model and memory:
 Titan X Pascal
 Exact command to reproduce:
 
 <denchmark-code>import sys
 import tensorflow as tf
 from tensorflow.python import debug as tf_debug
 
 base = tf.ones([10], dtype=tf.float32, name='base')
 stacked = tf.stack([base, base], name='stacked')
 concat = tf.concat([[base], [base]], axis=0, name='concat')
 
 session = tf.Session()
 session = tf_debug.LocalCLIDebugWrapperSession(session)
 
 with session.as_default():
     res = session.run([ stacked, concat])
 print res
 </denchmark-code>
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 When using the TensorflowDebugger with stacked/concated, the stacked/concated nodes do not appear in the set of dumped nodes once a run has completed.  In addition nodes that fed into these nodes are not dumped.
 	",1.0,blake-varden,2017-05-31T00:00:38Z,"
 		<denchmark-link:https://github.com/caisq>@caisq</denchmark-link>
  : Mind taking a look?
 		",2.0,blake-varden,2017-05-31T00:36:46Z,"
 		<denchmark-link:https://github.com/blake-varden>@blake-varden</denchmark-link>
  This is not a bug. The reason why you see no data dumped is because every node is constant folded in the graph set up by your code.  defines a TF constant. So all the downstream tensors like  and  are effectively constant. TF's graph optimization knows that and folds all nodes into one for each fetched tensor.
 If you replace tf.ones with a tf.Variable, you'll see the data dumped:
 import numpy as np
 import tensorflow as tf
 from tensorflow.python import debug as tf_debug
 
 base = tf.Variable(np.ones([10]), dtype=tf.float32, name=""base"")
 stacked = tf.stack([base, base], name='stacked')
 concat = tf.concat([[base], [base]], axis=0, name='concat')
 
 session = tf.Session()
 session.run(tf.global_variables_initializer())
 
 session = tf_debug.LocalCLIDebugWrapperSession(session)
 res = session.run([ stacked, concat])
 This behavior is documented in a relatively obscure place:
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tfdbg/watch_graph>https://www.tensorflow.org/api_docs/python/tfdbg/watch_graph</denchmark-link>
 
 
 N.B.: 1. Under certain circumstances, the Tensor may not get actually watched (e.g., if the node of the Tensor is constant-folded during runtime).
 
 For more on constant folding in TF, see:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L83>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/config.proto#L83</denchmark-link>
 
 and
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/OptimizerOptions>https://www.tensorflow.org/api_docs/python/tf/OptimizerOptions</denchmark-link>
 
 TFDBG is working as intended as I just checked in the tensorflow/tensorflow:nightly docker image.
 		",3.0,blake-varden,2017-05-31T00:42:32Z,"
 		Re-opening the issue as a doc bug. I plan to add a Q&A item for this behavior.
 		",6f36e6b27106fb4de065db18b9333a3c6c2fbb89,Shanqing Cai,2017-06-13 09:11:50-07:00,MODIFY,0,tensorflow\docs_src\programmers_guide\debugger.md,tensorflow\docs_src\programmers_guide\debugger.md,0.0,"1,3,5,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,34,35,36,38,39,40,41,42,44,45,46,47,51,52,53,54,55,68,71,72,73,76,79,80,81,84,92,93,105,110,111,112,113,114,116,117,121,122,123,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,213,214,224,225,226,227,228,287,302,303,305,306,318,319,323,324,325,326,327,328,329,330,331,337,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,403,404,405,406,407,408,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,448,449,450,451,452,453,454,471,476,477,478,479,487,488,489,502,507,508,515,517,518,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,572,597,598,611,612,613,614,615,619,620,621,622,632,633,634,651,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680","1,3,5,8,9,10,11,12,13,14,15,16,17,23,24,25,27,29,30,31,32,33,34,38,39,40,53,56,57,58,59,62,65,68,69,70,71,72,73,81,82,94,99,100,101,102,103,105,106,107,108,109,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,155,165,166,167,168,169,179,180,181,182,183,233,248,249,251,252,264,268,269,275,281,282,283,285,287,288,289,290,296,297,314,319,320,321,329,330,343,354,356,357,358,359,360,361,362,364,365,366,396,421,422,435,436,437,441,442,443,453,454,455,472,474,475,476,477",DELETE,0.0,tensorflow\docs_src\programmers_guide\tfdbg-tflearn.md,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10428,orome,2017-06-04T12:59:01Z,2017-06-16T19:11:21Z,TensorBoard graph key does not match documentation,"
 The key in the TensorBoard UI indicates a ""Reference edge"" as a single-headed arrow:
 <denchmark-link:https://i.stack.imgur.com/5MhdF.png></denchmark-link>
 
 while the documentation shows these as double-headed arrows:
 <denchmark-link:https://i.stack.imgur.com/fWRZL.png></denchmark-link>
 
 Moreover, it appears that the edges indicated as references edges in the UI (according to the key there) are not in fact such edges. For example neither
 <denchmark-code>cs = tf.constant([1,2,3], name='const_share')
 vs = tf.Variable([1,2,3], name='var_share')
 tf.add(cs, vs, name='opVS1')
 tf.add(vs, cs, name='opVS2')
 </denchmark-code>
 
 <denchmark-link:https://camo.githubusercontent.com/4dd443e86357435723699846a7e12c6bf583d70bebfdc6cdd46fac63adc58654/68747470733a2f2f692e737461636b2e696d6775722e636f6d2f387a31426e2e706e67></denchmark-link>
 
 note
 <denchmark-code>tf.add([4],[3], name='opA')
 </denchmark-code>
 
 <denchmark-link:https://i.stack.imgur.com/eZHmm.png></denchmark-link>
 
 should include reference edges (should they?). But in both cases the key in the UI says that they do.
 	",1.0,orome,2017-06-04T13:04:33Z,"
 		<denchmark-link:https://stackoverflow.com/q/44345863/656912>Related SO question</denchmark-link>
 .
 		",2.0,orome,2017-06-04T18:10:48Z,"
 		By convention, dataflow edges are directed upwards, so that is why the ""dataflow edges"" lack arrowheads - we assume the reader understands the convention.
 If we must make a dataflow edge that points downwards (or close to downwards like in your second picture) - and sometimes that is necessary, we add an arrowhead to make that clear.
 Several people (including some folks on deep mind) have told me that this is unclear. For starters, perhaps we can update the documentation to clarify the convention as well as why only some dataflow edges have arrowheads (They just flow downwards).
 		",3.0,orome,2017-06-04T21:50:47Z,"
 		<denchmark-link:https://github.com/chihuahua>@chihuahua</denchmark-link>
  Indeed, that wasn't at all clear (I'm glad to have company at DeepMind).
 IMV it would be a lot clearer if direction of flow was always indicated, and there was something distinct for ""reference edges"" (whatever those are).
 And of course: the documentation should agree with the docs!
 		",65ce8c723da2da639af0f1dd237d50d2680a4cd9,A. Unique TensorFlower,2017-06-06 15:03:28-07:00,MODIFY,0,tensorflow\tensorboard\components\tf_graph\tf-graph-scene.html,tensorflow\tensorboard\components\tf_graph\tf-graph-scene.html,0.0,"279,280,281,282,283,284,445,449,450,451,452,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,554,555,556,558,559,560,562,563,564","439,519,520,521,522,523,525,526,527,529,530,531,533,534,535",MODIFY,1.0,tensorflow\tensorboard\components\tf_graph_common\edge.ts,tensorflow\tensorboard\components\tf_graph_common\edge.ts,4.0,orome,2017-06-05T04:28:34Z,"
 		Assigning to <denchmark-link:https://github.com/chihuahua>@chihuahua</denchmark-link>
  as a documentation issue.
 		",5.0,orome,2017-06-06T06:19:35Z,"
 		FYI, I am moving forward with an internal code change that
 
 Makes reference edges orange (and dataflow edges the same grey as before). The final colors might differ based on input from a designer, but the bottom line is that dataflow and reference edges will contrast in color.
 Adds arrows to all dataflow edges no matter where they point.
 
 I will subsequently update tensorflow.org docs to match.
 Here is a preview. The edges from the ""save"" node are reference edges because ""assign ops"" can modify other tensor values.
 <denchmark-link:https://cloud.githubusercontent.com/assets/4221553/26816188/4324c082-4a45-11e7-9e6e-2b21b61bd5ca.png></denchmark-link>
 
 		",6.0,orome,2017-06-16T19:11:21Z,"
 		I have migrated this issue to <denchmark-link:https://github.com/tensorflow/tensorboard/issues/48>tensorflow/tensorboard#48</denchmark-link>
 .
 The issue is almost fixed (It is already in the code.). The documentation code just has to be updated, so <denchmark-link:https://www.tensorflow.org/get_started/graph_viz>this tensorflow.org page</denchmark-link>
  changes.
 		",1.0,"251,252,253,254",,arrowheadMap,strokeWidth,251,359,MODIFY,0.0,tensorflow\tensorboard\components\tf_graph_common\scene.ts,tensorflow\tensorboard\components\tf_graph_common\scene.ts,0.0,45,,MODIFY,0.0,tensorflow\tensorboard\components\tf_graph_controls\tf-graph-controls.html,tensorflow\tensorboard\components\tf_graph_controls\tf-graph-controls.html,0.0,"348,357,367,515,533,551,569,585,601,612,613,614,617,618,619,626,637,644,654,655,656,657,658,659,660,661,662,669","348,357,367,515,533,551,569,585,601,612,613,614,617,618,625,636,643,653,654,655,656,657,664",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10519,ddtm,2017-06-08T03:26:09Z,2017-06-22T00:34:51Z,tf.contrib.data: tf-slim training pipeline gets stuck,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux leto28 3.16.0-4-amd64 1 SMP Debian 3.16.39-1+deb8u2 (2017-03-07) x86_64 GNU/Linux
 VERSION_ID=""8""
 VERSION=""8 (jessie)""
 
 
 TensorFlow installed from (source or binary):
 Binary
 
 
 TensorFlow version (use command below):
 tf.VERSION = 1.2.0-rc2
 tf.GIT_VERSION = v1.2.0-rc1-24-gce1d6ec
 tf.COMPILER_VERSION = v1.2.0-rc1-24-gce1d6ec
 
 
 Bazel version (if compiling from source):
 None
 
 
 CUDA/cuDNN version:
 8.0/5.1
 
 
 GPU model and memory:
 TITAN X (Pascal), 12189MiB
 
 
 Exact command to reproduce:
 python ./mwe.py
 
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 I recently ported my dataset handling to the new dataset API from . Now it seems that the  training pipeline stalls if I request just 1 or 2 CPUs for my job (it used to work just fine with the dataset API provided by ). I does work if I grab 4 CPUs. I tried to come up with a MWE (see below). The interesting thing is that it is not getting stuck if I remove one of the s or  at line 39. I suspect this issue is related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/10369>#10369</denchmark-link>
 .
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 import os
 import tensorflow as tf
 import tensorflow.contrib.data as tcd
 import tensorflow.contrib.slim as slim
 
 from tensorflow.contrib.data.python.ops.dataset_ops import _get_file_names
 
 DATASET_DIR = '/path_to_the_dataset'
 FILE_PATTERN = 'shapes_{}_*.tfrecord'
 IMAGE_SHAPE = [48, 48, 3]
 
 
 def _parse_function(example_proto):
     features = {
         ""image/encoded"": tf.FixedLenFeature(
             (), tf.string, default_value=""""),
         'image/annotation/color': tf.FixedLenFeature(
             (), tf.int64, default_value=0),
         'image/annotation/shape': tf.FixedLenFeature(
             (), tf.int64, default_value=0),
     }
     parsed_features = tf.parse_single_example(example_proto, features)
     image_decoded = tf.image.decode_image(parsed_features[""image/encoded""])
     color = parsed_features['image/annotation/color']
     shape = parsed_features['image/annotation/shape']
 
     return image_decoded, color, shape
 
 
 def get_batch(batch_size=32, group_size=3, split_name='train'):
     file_pattern = os.path.join(
         DATASET_DIR, FILE_PATTERN.format(split_name))
 
     file_names = _get_file_names(file_pattern, randomize_input=True)
 
     dataset = tcd.TFRecordDataset(file_names)
     dataset = dataset.map(_parse_function)
 
     dataset = dataset.map(lambda image, color, shape: image)
     dataset = dataset.shuffle(buffer_size=10000)
     dataset = dataset.repeat().batch(group_size * batch_size)
 
     iterator = dataset.make_one_shot_iterator()
     images = iterator.get_next()
 
     images = tf.split(images, group_size, axis=0)
     images = [tf.reshape(x, [batch_size] + IMAGE_SHAPE) for x in images]
 
     return images
 
 
 if __name__ == ""__main__"":
     with tf.Graph().as_default():
         x_1, x_2, x_3 = get_batch(batch_size=32,
                                   group_size=3)
 
         val = tf.reduce_sum(tf.add_n([x_1, x_2, x_3]))
         val = tf.Print(val, [tf.constant(0)], ""I'm alive! "")
 
         global_step = slim.get_or_create_global_step()
         with tf.control_dependencies([val]):
             update_global_step_op = tf.assign_add(global_step, 1)
 
         train_op = update_global_step_op
 
         tf.summary.scalar('Summary 1', val)
         tf.summary.scalar('Summary 2', train_op)
 
         logdir = 'mwe_logdir'
         slim.learning.train(
             train_op=train_op,
             logdir=logdir,
             number_of_steps=1000000)
 	",1.0,ddtm,2017-06-08T05:00:43Z,"
 		Thanks for reporting this... it definitely looks like a bug. I think I've tracked it down to the ""OneShotIterator"" op, which internally blocks on this line while a function executes to build the dataset:
 
 
 
 tensorflow/tensorflow/core/kernels/iterator_ops.cc
 
 
          Line 248
       in
       91cb809
 
 
 
 
 
 
  n.WaitForNotification(); 
 
 
 
 
 
 That will block one of the inter-op thread pool threads for the (typically short) execution time of the dataset construction function. The number of CPUs determines the default number of threads in that thread pool: when you have only 1 CPU, the system will deadlock as soon as you hit that line (because no more thread pool threads are available to run the function that will unblock it). When you have 2 CPUs, it can work, but slim.learning.train() uses tf.train.Supervisor, which asynchronously runs a background thread... that runs the same ""OneShotIterator"" op. To ensure that the op only initializes once, the initialization runs under a lock, acquired here:
 
 
 
 tensorflow/tensorflow/core/kernels/iterator_ops.cc
 
 
          Line 194
       in
       91cb809
 
 
 
 
 
 
  mutex_lock l(mu_); 
 
 
 
 
 
 The problem is probably starting to become clear: two concurrent executions of the same ""OneShotIterator"" kernel will potentially block two inter-op thread pool threads, leading to deadlock in a 2-CPU system, because there are no more threads available to run the function that will unblock them.
 Anyway, mea culpa, and thanks again for finding the bug. I'll be working on a fix, although it might not make it into the final 1.2 release. In the mean time, there are a couple of workarounds:
 
 
 Increase the number of threads to more than 2 in the inter-op thread pool. You can do this by passing session_config=tf.ConfigProto(inter_op_parallelism_threads=3) to slim.learning.train().
 
 
 Use dataset.make_initializable_iterator() instead of dataset.make_one_shot_iterator(). This comes with the additional requirement that you have to run iterator.initializer, which is not completely trivial with slim.learning.train() because you don't have access to the tf.Session. One possibility is to pass local_init_op=tf.group(tf.local_variables_initializer(), tf.tables_initializer(), iterator.initializer) to slim.learning.train().
 
 
 		",2.0,ddtm,2017-06-08T05:31:29Z,"
 		Wow, thanks for the swift response! The first workaround seems to be working perfectly.
 On a side note, I'm quite happy with the tf.contrib.data. My code has become way cleaner.
 		",,,,,f5fcd1fdcf896f46aed03c7e61525b48b75d1acc,Derek Murray,2017-06-14 14:09:31-07:00,MODIFY,4,tensorflow\contrib\data\python\kernel_tests\iterator_ops_test.py,tensorflow\contrib\data\python\kernel_tests\iterator_ops_test.py,1.0,"150,151,152,153,154",,MODIFY,6.0,tensorflow\core\kernels\iterator_ops.cc,tensorflow\core\kernels\iterator_ops.cc,,,,,,,,,,,,,1.0,"200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,222,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276","193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,272,273,274,275",tensorflow::OneShotIteratorOp::Compute,ctx,193,276,,,,,,,,,,,,,,,testOneShotIteratorNonBlocking.consumer_thread,,150,154,1.0,"133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167",,testOneShotIteratorNonBlocking,self,133,167,1.0,"169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196",,testOneShotIteratorInitializerFails,self,169,196,1.0,"186,187,188",,testOneShotIteratorInitializerFails.consumer_thread,,186,188,1.0,"226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246","226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246",tensorflow::OneShotIteratorOp::Init,"ctx,done",226,246,1.0,"166,167,168,169,170,171,172,173,174,180,181,182","171,172",tensorflow::OneShotIteratorOp::OneShotIteratorOp,ctx,166,185,1.0,"206,207,208,209,210,211,212,213,214,215,216,217,218,222","206,207,208,209,210,211,212,213,214,217,218,219,220,221,222,223",tensorflow::OneShotIteratorOp::ComputeAsync,"ctx,done",206,223,1.0,"321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337",,tensorflow::OneShotIteratorOp::ProduceOutput,"ctx,done",321,337,1.0,"248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319","248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,272,273,274,275,279,285",tensorflow::OneShotIteratorOp::TryInit,"ctx,iterator,cinfo",248,319,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10641,JerrikEph,2017-06-12T02:16:00Z,2018-02-08T17:44:56Z,bug: BeamSearchDecoder should not assume that  when time &gt; 0 beam will be full,"
 <denchmark-code>  scores_flat = control_flow_ops.cond(
       time > 0,
       lambda: array_ops.reshape(scores, [batch_size, -1]),
       lambda: scores[:, 0])
   num_available_beam = control_flow_ops.cond(
       time > 0,
       lambda: math_ops.reduce_prod(scores_shape[1:]),
       lambda: math_ops.reduce_prod(scores_shape[2:]))
 
   # Pick the next beams according to the specified successors function
   next_beam_size = math_ops.minimum(
       ops.convert_to_tensor(
           beam_width, dtype=dtypes.int32, name=""beam_width""),
       num_available_beam)
   next_beam_scores, word_indices = nn_ops.top_k(scores_flat, k=next_beam_size)
   next_beam_scores.set_shape([static_batch_size, beam_width])
   word_indices.set_shape([static_batch_size, beam_width])
 </denchmark-code>
 
 code start from
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L510>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py#L510</denchmark-link>
 
 Correct me if I am wrong, but I think this code is assuming that, when time > 0 the beam will be full. It is true when the vocabulary is big such as is the case in machine translation. but if the vocabulary is small, the beam might won't be full when time > 0 and might pose a problem.  the value of next_beam_size  in the code seems must be beam_width or it will raise an error since next_beam_scores.set_shape([static_batch_size, beam_width]), which make  next_beam_size = math_ops.minimum useless.
 I am trying to write a Pointer Network BeamSearch Decoder by modifying this source file. And the vocabulary is usually small, so there is a possibility that when time == 1 the beam won't be fully filled.
 I appreciate finally some one wrote a general BeamSeach decoder, that will make my life easier.
 	",1.0,JerrikEph,2017-06-13T17:27:39Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 , could you please take a look, it's beyond my expertise.
 		",2.0,JerrikEph,2017-06-13T18:07:49Z,"
 		This is indeed the assumption.  Unfortunately it's unlikely to be fixed
 until after the 1.2 release and I'm away this week.  If you would like to
 send a PR to fix this by using the tf.shape() on the input Tensor instead
 of assuming beam_width, and adding a unit test, I can review when I return.
 
 On Jun 13, 2017 10:28 AM, ""Andrew Selle"" <notifications@github.com> wrote:
 
 <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  <<denchmark-link:https://github.com/ebrevdo>https://github.com/ebrevdo</denchmark-link>
 >, could you please take a look, it's
 beyond my expertise.
 
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 <<denchmark-link:https://github.com/tensorflow/tensorflow/issues/10641#issuecomment-308189704>#10641 (comment)</denchmark-link>
 >,
 or mute the thread
 <<denchmark-link:https://github.com/notifications/unsubscribe-auth/ABtim6kdt62nhk_I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D>https://github.com/notifications/unsubscribe-auth/ABtim6kdt62nhk_I7Ie5cDr45ccAy5MRks5sDsbEgaJpZM4N2l0D</denchmark-link>
 >
 .
 		",3.0,JerrikEph,2017-06-14T15:01:17Z,"
 		<denchmark-code>def initialize(self, name=None):
     """"""Initialize the decoder.
 
     Args:
       name: Name scope for any created operations.
 
     Returns:
       `(finished, start_inputs, initial_state)`.
     """"""
     finished, start_inputs = self._finished, self._start_inputs
 
     log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)
         array_ops.ones([self._batch_size], dtype=dtypes.int32),
         depth=self._beam_width, dtype=dtypes.bool)
 
     log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)
                     dtype=nest.flatten(self._initial_cell_state)[0].dtype)
     log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)
                     dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')
 
     log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)
 
     initial_state = BeamSearchDecoderState(
         cell_state=self._initial_cell_state,
         log_probs=log_probs,
         finished=finished,
         lengths=array_ops.zeros(
             [self._batch_size, self._beam_width], dtype=dtypes.int32))
 
     return (finished, start_inputs, initial_state)
 </denchmark-code>
 
 <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 
 It probably is not  a good idea to push tensors with variant shape to TensorArray.
 Actually I think it's a good idea to just set  to negative infinity in initialize function.
 and of course set
 <denchmark-code>  scores_flat = control_flow_ops.cond(
       time > 0,
       lambda: array_ops.reshape(scores, [batch_size, -1]),
       lambda: scores[:, 0])
   num_available_beam = control_flow_ops.cond(
       time > 0,
       lambda: math_ops.reduce_prod(scores_shape[1:]),
       lambda: math_ops.reduce_prod(scores_shape[2:]))
 
   # Pick the next beams according to the specified successors function
   next_beam_size = math_ops.minimum(
       ops.convert_to_tensor(
           beam_width, dtype=dtypes.int32, name=""beam_width""),
       num_available_beam)
 </denchmark-code>
 
 to a  simple reshape
 <denchmark-code>  scores_flat = array_ops.reshape(scores, [batch_size, -1])
 </denchmark-code>
 
 I will add a test unit and test it if you think it's ok.
 		",be1b702ff357e851eb4a7237728d80fe08220816,JerrikEph,2018-01-20 14:06:16-08:00,MODIFY,3,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_decoder_test.py,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_decoder_test.py,1.0,"243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314",,MODIFY,1.0,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,4.0,JerrikEph,2017-12-20T19:15:33Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",5.0,JerrikEph,2018-01-04T19:21:30Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",6.0,JerrikEph,2018-01-23T23:07:33Z,"
 		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
 		",1.0,"303,304,305,306,307,308,311","303,304,305",initialize,"self,name",292,316,,,,,,,,,,,,,,,test_step,self,243,314,1.0,"234,235,236,237,238,239,240",,setUp,self,234,240,1.0,"244,245,246,247,248,249,250,251,252,253,254,255,256,257,258",,test_step.get_probs,,244,258,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,JerrikEph,2018-02-07T13:47:33Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10729,jkschin,2017-06-15T10:33:01Z,2017-08-03T21:05:01Z,tf.nn.max_pool wrong docs?,"
 <denchmark-h:h3>System information</denchmark-h>
 
 Not Applicable
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/max_pool>API</denchmark-link>
  states that ksize has length >= 4, the size of window for each dimension of the input tensor. However, value is a 4-D Tensor so doesn't this mean that ksize should be length == 4? Same for strides.
 Digging into maxpooling_op.cc shows that there's some check that does ==. Line 212:
 <denchmark-code>    OP_REQUIRES(context, ksize_.size() == 4,
                 errors::InvalidArgument(""Sliding window ksize field must ""
                                         ""specify 4 dimensions""));
 </denchmark-code>
 
 	",1.0,jkschin,2017-06-15T23:48:21Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
 , <denchmark-link:https://github.com/josh11b>@josh11b</denchmark-link>
  , this seems to come from the attribute specifier only allowing a >= optionally and not ==.
 i.e.
 <denchmark-code>REGISTER_OP(""AvgPool"")
     .Input(""value: T"")
     .Output(""output: T"")
     .Attr(""ksize: list(int) >= 4"")
     .Attr(""strides: list(int) >= 4"")
     .Attr(GetPaddingAttrString())
     .Attr(GetConvnetDataFormatAttrString())
     .Attr(""T: {half, float, double}"")
     .SetShapeFn(shape_inference::AvgPoolShape)
     .Doc(R""doc(
 </denchmark-code>
 
 We can't make it list(int) == 4, because the op registration class doesn't support that for Attr. However, since the attribute constraint gets auto placed in the docs i.e. here:
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/max_pool>https://www.tensorflow.org/api_docs/python/tf/nn/max_pool</denchmark-link>
 
 It is confusing, maybe we should
 
 not  include the list constraints in auto-generated documentation.
 augment list constraints to allow == as well as >=.
 
 		",2.0,jkschin,2017-06-16T00:19:45Z,"
 		Two things we can do: support more constraint types (good, but work), and add support for = at the beginning of the doc string for attrs suppressing the generated prefix (which currently works for tensor inputs).
 		",3.0,jkschin,2017-06-16T00:29:32Z,"
 		If you are accepting PRs for this and could point me to some resources to study how this works in detail (like the op registration class for one), I'd be happy to do one.
 		",d4a21196ac06e49f2581e27af62efc4efd5387c4,Yong Tang,2017-08-03 14:05:00-07:00,MODIFY,2,tensorflow\python\ops\nn_ops.py,tensorflow\python\ops\nn_ops.py,1.0,"1753,1755","1753,1755",,,,,4.0,jkschin,2017-06-16T19:18:48Z,"
 		If you want to fix the >= / = / == parsing issue, the code that processes that lives here in tensorflow/core/framework/op_def_builder.cc line 221ff. You could allow == as a token, and then we could use that in the Op registrations. There's no documentation for this, just the code.
 We would love a PR for that.
 		",5.0,jkschin,2017-06-16T21:02:22Z,"
 		Josh also suggested there is a := syntax that lets you remove the constraint part in the docs and provide your own text completely.
 		",6.0,jkschin,2017-08-01T02:04:00Z,"
 		Added PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11925>#11925</denchmark-link>
  for the fix. As both  and  are wrapped in the python code, the fix only changes the docstring in tensorflow/python/ops/nn_ops.py.
 		",,,,,,,,,,,,,,,,,,,,,,max_pool,"value,ksize,strides,padding,data_format,name",1747,1772,1.0,"1724,1726","1724,1726",avg_pool,"value,ksize,strides,padding,data_format,name",1715,1744,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10741,daviddengcn,2017-06-15T20:21:34Z,2017-06-16T18:21:30Z,[go] bug in Shape.size for dim == NumDimensions,"
 <denchmark-h:h3>System information</denchmark-h>
 
 This does not matter.
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 (go) when dim equals s.NumDimensions(), the function should return -1, instead it panics.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 In <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/shape.go#L62>shape.go</denchmark-link>
 ,  method
 <denchmark-code>    func (s Shape) Size(dim int) int64 {
 ---   if dim < 0 || dim > s.NumDimensions() {
         return -1
 </denchmark-code>
 
 should be:
 <denchmark-code>    func (s Shape) Size(dim int) int64 {
 +++   if dim < 0 || dim >= s.NumDimensions() {
         return -1
 </denchmark-code>
 
 	",1.0,daviddengcn,2017-06-15T21:57:15Z,"
 		Thanks for the report, please specify the filename (link is even better) in the future to make it clear what you are talking about.  Thanks!
 		",,,,,,,,,76a0a15cb90c370e41766d124a7a11b28c18089a,Andrew Selle,2017-06-16 11:21:29-07:00,MODIFY,1,tensorflow\go\shape.go,tensorflow\go\shape.go,1.0,62,62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Size,int,61,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11016,c810armyHuan,2017-06-23T17:16:38Z,2017-06-30T19:58:01Z,map_func of tf.contrib.data.Dataset.map gets dict keys instead of values when the nested structure of Dataset is dict,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): b'0.5.0-12520-g1111e06d9' 1.2.0-rc2
 Bazel version (if compiling from source): 0.4.5
 CUDA/cuDNN version: 8.0/6
 GPU model and memory:
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 If the nested structure of  is ,  will call <denchmark-link:https://github.com/tensorflow/tensorflow/blob/1111e06d9cd691cbdfcb67cf9f234a504f4e0f6d/tensorflow/contrib/data/python/ops/dataset_ops.py#L1463>map_func(*nested_args)</denchmark-link>
  and pass the keys of  instead of components in the dataset to . It seems that  or  need to be passed to , so that  could transform the elements in the dataset.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 import tensorflow as tf
 
 def foo(*args, **kwargs):
     print(args, kwargs) # ('a', 'b') {}
     return 1 * 100, 2 * 200
 
 tf.contrib.data.Dataset.from_tensors({'a': [1], 'b': [2]}).map(foo)
 	",1.0,c810armyHuan,2017-06-23T22:46:15Z,"
 		I tried reproducing and I think that Dicts are not supported as inputs in general
 
 
 
 a = tf.contrib.data.Dataset.from_tensors({'a': [1], 'b': [2]})
 Traceback (most recent call last):
 File """", line 1, in 
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 460, in from_tensors
 return TensorDataset(tensors)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/data/python/ops/dataset_ops.py"", line 864, in init
 for i, t in enumerate(nest.flatten(tensors))
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 676, in convert_to_tensor
 as_ref=False)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 741, in internal_convert_to_tensor
 ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 113, in _constant_tensor_conversion_function
 return constant(v, dtype=dtype, name=name)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/constant_op.py"", line 102, in constant
 tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape, verify_shape=verify_shape))
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.py"", line 462, in make_tensor_proto
 ""supported type."" % (type(values), values))
 TypeError: Failed to convert object of type <type 'dict'> to Tensor. Contents: {'a': [1], 'b': [2]}. Consider casting elements to a supported type.
 
 
 
 My tensorflow version was 1.2.0 though.
 		",2.0,c810armyHuan,2017-06-24T01:35:18Z,"
 		It seems that <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f3f53e8b394bdcaddc707f7bde8dcc98a73531e7>f3f53e8</denchmark-link>
  adds support for  as nested structures of . I built master branch from source.
 		",3.0,c810armyHuan,2017-06-28T23:37:49Z,"
 		This is definitely a bug. Thanks for catching it! I have a fix in the works.
 		",9b11f458196f6f0528c9974238497a6c8b6da547,Derek Murray,2017-06-29 11:15:12-07:00,MODIFY,3,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py,1.0,"260,261,262,263,264,265,266,267,268,269,273,276,277,294,296","260,261,265,268,269,286,288",MODIFY,1.0,tensorflow\contrib\data\python\kernel_tests\filter_dataset_op_test.py,tensorflow\contrib\data\python\kernel_tests\filter_dataset_op_test.py,4.0,c810armyHuan,2017-06-29T01:36:30Z,"
 		It seemed that the parameter padded_shapes for tf.contrib.data.Dataset.padded_batch can't be dict too.
 		",5.0,c810armyHuan,2017-06-29T15:05:50Z,"
 		Thanks! That turned up when I was testing the fix.
 		",,,,,1.0,"86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101",,testFilterDict,self,86,101,MODIFY,1.0,tensorflow\contrib\data\python\kernel_tests\flat_map_dataset_op_test.py,tensorflow\contrib\data\python\kernel_tests\flat_map_dataset_op_test.py,1.0,"105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120",,MODIFY,1.0,tensorflow\contrib\data\python\kernel_tests\map_dataset_op_test.py,tensorflow\contrib\data\python\kernel_tests\map_dataset_op_test.py,1.0,"327,328,329,330,331,332,333,334,335,336,337,338,339,340",,testEvenOddBucketsFilterOutAllOdd,self,258,296,1.0,"264,265,266,267,268,269","265,268,269",testEvenOddBucketsFilterOutAllOdd._dynamic_pad_fn,"bucket,window,_",264,269,1.0,"260,261,262","260,261",testEvenOddBucketsFilterOutAllOdd._map_fn,v,259,262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testMapDict,self,105,120,testMapDict,self,327,340,MODIFY,6.0,tensorflow\contrib\data\python\ops\dataset_ops.py,tensorflow\contrib\data\python\ops\dataset_ops.py,1.0,1491,1486,tf_map_func,args,1483,1503,1.0,1402,1397,__init__,"self,input_dataset,key_func,reduce_func,window_size",1389,1429,1.0,1617,1612,__init__,"self,input_dataset,predicate",1603,1630,1.0,"1360,1361,1362",,_should_unpack_args,args,1360,1362,1.0,1402,1397,__init__.tf_key_func,args,1396,1409,1.0,1617,1612,__init__.tf_predicate,args,1609,1627,MODIFY,2.0,tensorflow\contrib\data\python\util\nest.py,tensorflow\contrib\data\python\util\nest.py,1.0,"289,290",289,_yield_flat_up_to,"shallow_tree,input_tree",286,294,1.0,499,,map_structure_up_to,"shallow_tree,func,inputs",429,501,MODIFY,1.0,tensorflow\contrib\data\python\util\nest_test.py,tensorflow\contrib\data\python\util\nest_test.py,1.0,"290,291,292,293,294,295,296",,testFlattenUpTo,self,190,296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11017,rubenvereecken,2017-06-23T17:18:55Z,2017-07-07T01:10:24Z,Tfdbg does not work with Coordinator/QueueRunners,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 18
 TensorFlow installed from (source or binary): Binary (pip)
 TensorFlow version (use command below): v1.2.0-rc2-21-g12f033d 1.2.0
 Bazel version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce: N/A
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 The Tensorflow debugger does not seem to be working with Queues; data never seems to be fetched by the QueueRunner threads, be it from a file (using tf.TFRecordReader and tf.parse_single_example) or preloaded (using tf.train.slice_input_producer). Instead, the coordinator.should_stop() is True right away. This is only the case after wrapping the session in a tf.python.debug.LocalCLIDebugWrapperSession. The example should make things clearer.
 Moreover, another error occurs at coordinator.join(threads).
 I am aware of the <denchmark-link:https://www.tensorflow.org/programmers_guide/debugger>FAQ entry on Threads</denchmark-link>
 , but that does not explain why the data fetching threads would not be working.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 To make it easiest to replicate, I simply took the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_preloaded.py>example on working with preloaded data</denchmark-link>
 , and wrapped the session in there with the debugger. I uploaded the gist with two lines added to <denchmark-link:https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92>https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92</denchmark-link>
 .
 To reproduce, run the file. Once you drop in the debugger, run once. It then exits. The full output is below:
 <denchmark-code>Extracting /tmp/data/train-images-idx3-ubyte.gz
 Extracting /tmp/data/train-labels-idx1-ubyte.gz
 Extracting /tmp/data/t10k-images-idx3-ubyte.gz
 Extracting /tmp/data/t10k-labels-idx1-ubyte.gz
 Traceback (most recent call last):
   File ""ex.py"", line 191, in <module>
     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
   File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
     _sys.exit(main(_sys.argv[:1] + flags_passthrough))
   File ""ex.py"", line 143, in main
     run_training()
   File ""ex.py"", line 138, in run_training
     coord.join(threads)
   File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
     six.reraise(*self._exc_info_to_raise)
   File ""/home/ruben/anaconda3/lib/python3.6/site-packages/six.py"", line 686, in reraise
     raise value
   File ""/home/ruben/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/queue_runner_impl.py"", line 233, in _run
     enqueue_callable = sess.make_callable(enqueue_op)
 AttributeError: 'LocalCLIDebugWrapperSession' object has no attribute 'make_callable'
 </denchmark-code>
 
 The stacktrace is about coord.join(threads), but this is only possible because coord.should_stop() never seems to be False, which would indicate there is data to load. Without the added debugger lines, the example simply works.
 	",1.0,rubenvereecken,2017-06-23T18:36:48Z,"
 		cc <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 
 <denchmark-link:https://github.com/rubenvereecken>@rubenvereecken</denchmark-link>
  Thanks for reporting this issue. We are aware of it and will push a fix to it soon.
 		",2.0,rubenvereecken,2017-06-23T19:21:33Z,"
 		<denchmark-link:https://github.com/caisq>@caisq</denchmark-link>
  thank you so much, I look forward to it.
 		",3.0,rubenvereecken,2017-06-23T19:25:28Z,"
 		<denchmark-link:https://github.com/rubenvereecken>@rubenvereecken</denchmark-link>
  While you wait for the fix, I want to ask you whether you are trying to debug the data input queues, or the training operation on the main thread. If the latter, there is a workaround for that.
 		",41bc76d28b8b301c546cc5624abd37fd8b97b64c,Shanqing Cai,2017-07-05 18:22:03-07:00,MODIFY,6,tensorflow\python\client\session.py,tensorflow\python\client\session.py,1.0,"889,890,891,900,901","887,888,889,890,891,892,901",MODIFY,3.0,tensorflow\python\client\session_test.py,tensorflow\python\client\session_test.py,4.0,rubenvereecken,2017-06-23T23:02:28Z,"
 		<denchmark-link:https://github.com/caisq>@caisq</denchmark-link>
  Ah actually the former, but I'd be working my way towards the latter. Is there a way to debug these training ops while still using data fed from queues? Either way, could you point me at the workaround? Much appreciated!
 		",5.0,rubenvereecken,2017-06-23T23:07:52Z,"
 		<denchmark-link:https://github.com/rubenvereecken>@rubenvereecken</denchmark-link>
  The workaround is based on the assumption that the train op runs on the Python main thread, while the data queues run on the child threads, which should usually be the case.
 You can just use the thread_name_filter kwarg of the wrapper's constructor to limit the debugging to the train op.
 sess = tf_debug.LocalCLIDebugWrapperSession(sess, thread_name_filter=""MainThread$"")
 This is talked about in the FAQ. Doing this doesn't change the source of the input data. They still come from the queues; they just don't break into the TFDBG UI when they run.
 		",6.0,rubenvereecken,2017-06-23T23:55:17Z,"
 		Oops. I may have given incomplete suggestion. In your code at <denchmark-link:https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92>https://gist.github.com/rubenvereecken/079cdf1abc76866714ff6f752167481d#file-fully_connected_preloaded_debug-py-L92</denchmark-link>
 
 make sure that your wrapped Session object is used only to run the train_op. You can do something like this:
 Move the line sess = tf_debug.LocalCLIDebugWrapperSession(sess) after line 100. That makes sure that when the data input queue ops are created, the make_callable() method of the original session, not that of the wrapped session is called. The wrapped session doesn't have a make_callable() method, which was recently added. This was the root cause of the issue you are seeing.
 		",1.0,"1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259",,testMakeCallableOnOperationWithRunOptions,self,1247,1259,MODIFY,4.0,tensorflow\python\debug\cli\cli_shared.py,tensorflow\python\debug\cli\cli_shared.py,1.0,"404,405,406,407",,MODIFY,9.0,tensorflow\python\debug\wrappers\framework.py,tensorflow\python\debug\wrappers\framework.py,1.0,"564,565,566,567,568,569,570",,run,"self,fetches,feed_dict,options,run_metadata",787,904,1.0,"1194,1195,1196,1197,1198,1199",,_callable_template_with_options_and_metadata,"fetch_list_as_strings,target_list_as_strings,fetch_handler,options,run_metadata",1194,1199,1.0,"1177,1180",,_generic_run,"feed_args,kwargs",1177,1180,1.0,"1129,1130,1131,1132",1129,make_callable,"self,fetches,feed_list,accept_options",1129,1132,1.0,"1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275",,testMakeCallableWithFeedListAndRunOptions,self,1262,1275,1.0,"1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244",,testMakeCallableOnTensorWithRunOptions,self,1234,1244,,,,,,,,,,,,,,,,,,,,,,7.0,rubenvereecken,2017-07-10T13:14:50Z,"
 		🎉
 		",get_run_short_description,"run_call_count,fetches,feed_dict,is_callable_runner",404,407,wrapped_runner,"runner_args,kwargs",564,570,MODIFY,2.0,tensorflow\python\debug\wrappers\framework_test.py,tensorflow\python\debug\wrappers\framework_test.py,1.0,"406,407,408,409,410,411,412,413,414,415,416,417,418,419",,testWrapperHasAllPublicMethodsOfSession,self,406,419,1.0,"399,400,401",,_is_public_method_name,method_name,399,401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,tensorflow\python\debug\wrappers\local_cli_wrapper.py,tensorflow\python\debug\wrappers\local_cli_wrapper.py,1.0,"536,537,538,539,540,550,551,555,556","535,548,549,550,553,554,555,556",_update_run_calls_state,"self,run_call_count,fetches,feed_dict",535,556,1.0,"536,537,538,539,540",,_update_run_calls_state,"self,run_call_count,fetches,feed_dict,is_callable_runner",536,540,MODIFY,4.0,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,1.0,"320,321,322,323,324,325,326,327,328",,testDebuggingMakeCallableTensorRunnerWorks,self,320,328,8.0,rubenvereecken,2018-01-26T01:23:35Z,"
 		For the user of TF-Slim,  The usage of thread_name_filter
 <denchmark-code>session_wrapper_main_thread =  functools.partial(
   tf_debug.LocalCLIDebugWrapperSession,
   thread_name_filter=""MainThread$"")
 
 slim.learning.train(
   ... 
   session_wrapper=session_wrapper_main_thread,
   ... ) 
 
 </denchmark-code>
 
 		",1.0,,"1168,1171",make_callable._generic_run,feed_args,1168,1171,1.0,"1129,1130,1131,1132,1152,1153,1154,1155,1156,1157,1177,1180,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215","1129,1168,1171,1185",make_callable,"self,fetches,feed_list",1129,1215,1.0,"404,405,406,407,416,417,423,424",395,get_run_short_description,"run_call_count,fetches,feed_dict",395,432,1.0,"289,290,291",292,get_run_start_intro,"run_call_count,fetches,feed_dict,tensor_filters",289,292,1.0,"295,296",292,get_run_start_intro,"run_call_count,fetches,feed_dict,tensor_filters,is_callable_runner",292,296,1.0,"555,556",,reset,"self,args,kwargs",555,556,1.0,196,196,__init__,"self,fetches,feed_dict,run_options,run_metadata,run_call_count",195,196,1.0,"400,401,402,403,404,405,406,414,415,416,422,423,425,426,427,428,429,430,431,434,435,436,437,438,439,440,445,446,469,470,471,472,473,474,475,476,477,492,493,494,495,496,497,498,499","397,410,413,415,416,417,418,423,446,447,448,449,464,465,466,467,472",run,"self,fetches,feed_dict,options,run_metadata",397,499,1.0,196,196,__init__,"self,fetches,feed_dict,run_options,run_metadata,run_call_count,is_callable_runner",195,196,1.0,"558,559,560,561",,make_callable,"self,fetches,feed_list,accept_options",558,561,1.0,"552,553",,list_devices,"self,args,kwargs",552,553,1.0,"680,681",,__del__,self,680,681,1.0,"400,401,402,403,404,405,406",,run,"self,fetches,feed_dict,options,run_metadata,callable_runner,callable_runner_args",400,406,1.0,"221,222,223","221,222",on_run_start,"self,request",208,254,1.0,"356,357,358,359,360,361,362,363,364,365",,testDebuggingMakeCallableRunnerWithFeedListWorks,self,356,365,1.0,"330,331,332,333,334,335,336,337,338,339,340,341,342",,testDebuggingMakeCallableTensorRunnerWithCustomRunOptionsWorks,self,330,342,1.0,"344,345,346,347,348,349,350,351,352,353,354",,testDebuggingMakeCallableOperationRunnerWorks,self,344,354,MODIFY,0.0,tensorflow\tools\api\golden\tensorflow.-interactive-session.pbtxt,tensorflow\tools\api\golden\tensorflow.-interactive-session.pbtxt,0.0,37,37,MODIFY,0.0,tensorflow\tools\api\golden\tensorflow.-session.pbtxt,tensorflow\tools\api\golden\tensorflow.-session.pbtxt,0.0,37,37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11091,anishathalye,2017-06-27T20:57:44Z,2017-07-17T16:04:54Z,tf.nn.elu: incorrect second derivative,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.2.0
 Bazel version (if compiling from source): N/A
 CUDA/cuDNN version: CUDA 8.0 / cuDNN 6.0
 GPU model and memory: GTX 1080 Ti 11GB
 Exact command to reproduce: see below
 
 tf.nn.elu gives incorrect second derivatives:
 Consider the graph y = 2 * elu(-x).
 x = tf.placeholder(tf.float32, ())
 y = 2 * tf.nn.elu(-x)
 We'll be evaluating at x=1:
 x_ = 1
 We can evaluate first derivatives with automatic differentiation:
 dy, = tf.gradients(y, x)
 dy.eval({x: x_})
 => -0.7357589
 This lines up with the analytic answer: y' = -2e^(-x)
 However, for the second derivative:
 ddy, = tf.gradients(dy, x)
 ddy.eval({x: x_})
 => 0.36787945
 Whoops, this doesn't look right! Analytically, the derivative is y'' = 2e^(-x). Evaluated at x=1, this is 0.7357588!
 <denchmark-h:h3>Workaround</denchmark-h>
 
 Just in case anyone else needs to work around this until it's fixed:
 def elu(x):
     return tf.where(x >= 0.0, x, tf.exp(x) - 1)
 Looks like second derivatives work with that.
 	",1.0,anishathalye,2017-06-27T20:59:19Z,"
 		Possibly related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/7403>#7403</denchmark-link>
 
 		",2.0,anishathalye,2017-06-29T18:31:13Z,"
 		<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  do you know why this might be?
 <denchmark-link:https://github.com/anishathalye>@anishathalye</denchmark-link>
  is it possible to make a simpler repro script that you can share?
 		",3.0,anishathalye,2017-06-29T18:37:31Z,"
 		Can you use the tensorflow gradient checker to test the gradient of your particular graph? See <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradient_checker.py</denchmark-link>
  and usages of it in the tensorflow tests.
 		",e121535a7d04cfc7c7dbb09d8694c01eb29da26f,Alexandre Passos,2017-07-10 12:52:34-07:00,MODIFY,2,tensorflow\python\kernel_tests\relu_op_test.py,tensorflow\python\kernel_tests\relu_op_test.py,1.0,"276,277,278,279,280,281,282,283,284,285",,MODIFY,1.0,tensorflow\python\ops\nn_grad.py,tensorflow\python\ops\nn_grad.py,4.0,anishathalye,2017-07-05T22:15:30Z,"
 		Okay, so I tried the following:
 I have a network y(x), where y contains no tf.gradient ops, and I checked:
 
 
 tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok
 
 
 tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is that okay? in any case, for training, first derivatives seem to work)
 
 
 tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result is ok
 
 
 tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) - result is ~ 1522423936
 
 
 Finite differences probably isn't producing great results because y is a fairly big graph. But still, having a maximum error of 1e9 seems kind of large.
 What do you suggest looking into next?
 		",5.0,anishathalye,2017-07-05T22:30:59Z,"
 		This is really hard to debug without having access to the full graph.
 
 What I'd do if I did have access  to the full graph would be bisect it;
 removing chunks of graph at a time until the error in second derivative
 goes down to see if there's some part of the code which is less numerically
 stable than it should be.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Wed, Jul 5, 2017 at 3:17 PM, Anish Athalye ***@***.***> wrote:
  Okay, so I tried the following:
 
  I have a network y(x), where y contains no tf.gradient ops, and I checked:
 
     1.
 
     tf.check_numerics(tf.gradients(y, x)[0]) -- result is ok
     2.
 
     tf.test.compute_gradient_error(x, ..., y, ...) -- result is ~ 983 (is
     that okay? in any case, for training, first derivatives seem to work)
     3.
 
     tf.check_numerics(tf.gradients(tf.gradients(y, x)[0], x)[0]) -- result
     is ok
     4.
 
     tf.test.compute_gradient_error(x, ..., tf.gradients(y, x)[0], ...) -
     result is ~ 1522423936
 
  Finite differences probably isn't producing great results because y is a
  fairly big graph. But still, having a maximum error of 1e9 seems kind of
  large.
 
  What do you suggest looking into next?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#11091 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AAATxdLiF38xxI13fIJsHsvl7rooT44Cks5sLAuTgaJpZM4OHLlJ>
  .
 
 
 -- 
  - Alex
 
 		",6.0,anishathalye,2017-07-06T23:42:52Z,"
 		Ok, I have some more evidence indicating that there is in fact a bug. Also, I can share the full graph with you.
 I don't want to post it publicly, so I've sent you an email with this additional information.
 		",1.0,"330,332,333,334,335","330,332,333,334",_EluGradGrad,"op,grad",329,335,,,,,,,,,,,,,,,testGradGrad,self,276,285,1.0,"36,37,38,39",,_elu_grad_grad,activation,36,39,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,anishathalye,2017-07-07T21:57:16Z,"
 		I found the bug: I updated the original post.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,anishathalye,2017-07-17T16:05:06Z,"
 		The commit from last week should have fixed this.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11132,galeone,2017-06-29T08:49:31Z,2017-07-07T01:10:25Z,Go: SIGABRT when executing the same node more than once,"
 <denchmark-h:h2>Problem</denchmark-h>
 
 In Go, when we pass the same node to the fetches list more then once SIGABRT is raised.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 package poc_test
 
 import (
         ""fmt""
         tf ""github.com/tensorflow/tensorflow/tensorflow/go""
         ""github.com/tensorflow/tensorflow/tensorflow/go/op""
         ""testing""
 )
 
 func TestFunc(t *testing.T) {
         // Create root scope
         root := op.NewScope()
 
         // Define graph
 
         // Create a constant matrix
         A := op.Const(root.SubScope(""A""), [2][2]int32{{1, 2}, {-1, -2}})
         // Create a constant column vector
         b := op.Const(root.SubScope(""b""), [2][1]int32{{10}, {100}})
         // Create a matmul operation
         mul := op.MatMul(root.SubScope(""MatMul""), A, b)
 
         // Finalize the graph
         graph, _ := root.Finalize()
 
         // Create the session
         var sess *tf.Session
         sess, _ = tf.NewSession(graph, &tf.SessionOptions{})
         // Run
         var results []*tf.Tensor
         var err error
         if results, err = sess.Run(nil, []tf.Output{mul, mul}, nil); err != nil {
                 t.Errorf(err.Error())
         }
         fmt.Println(results[0].Value())
 }
 Here's the output:
 <denchmark-code>go test poc_test.go 
 2017-06-29 10:46:09.154744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2017-06-29 10:46:09.155330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] Found device 0 with properties: 
 name: GeForce GTX 1080 Ti
 major: 6 minor: 1 memoryClockRate (GHz) 1.582
 pciBusID 0000:03:00.0
 Total memory: 10.91GiB
 Free memory: 249.38MiB
 2017-06-29 10:46:09.267778: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x1f22910 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
 2017-06-29 10:46:09.268001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2017-06-29 10:46:09.268357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:938] Found device 1 with properties: 
 name: GeForce GTX 1060 6GB
 major: 6 minor: 1 memoryClockRate (GHz) 1.7845
 pciBusID 0000:01:00.0
 Total memory: 5.93GiB
 Free memory: 5.34GiB
 2017-06-29 10:46:09.268390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:830] Peer access not supported between device ordinals 0 and 1
 2017-06-29 10:46:09.268399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:830] Peer access not supported between device ordinals 1 and 0
 2017-06-29 10:46:09.268409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:959] DMA: 0 1 
 2017-06-29 10:46:09.268415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:969] 0:   Y N 
 2017-06-29 10:46:09.268421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:969] 1:   N Y 
 2017-06-29 10:46:09.268433: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1028] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0)
 2017-06-29 10:46:09.268440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1028] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0)
 2017-06-29 10:46:09.290295: F tensorflow/c/c_api.cc:488] Check failed: nelems == 0 (2 vs. 0)
 SIGABRT: abort
 PC=0x7fa8684dc670 m=0 sigcode=18446744073709551610
 signal arrived during cgo execution
 
 goroutine 5 [syscall, locked to thread]:
 runtime.cgocall(0x50d580, 0xc420043d68, 0x530100)
         /usr/lib/go/src/runtime/cgocall.go:131 +0xe2 fp=0xc420043d20 sp=0xc420043ce0
 github.com/tensorflow/tensorflow/tensorflow/go._Cfunc_TF_SessionRun(0x2beb8a0, 0x0, 0x0, 0x0, 0x0, 0xc42000ce40, 0xc420011040, 0xc400000002, 0x0, 0x0, ...)
         github.com/tensorflow/tensorflow/tensorflow/go/_obj/_cgo_gotypes.go:703 +0x45 fp=0xc420043d68 sp=0xc420043d20
 github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run.func1(0x2beb8a0, 0x0, 0x0, 0x0, 0x0, 0xc42000ce40, 0xc420011040, 0xc400000002, 0x0, 0x0, ...)
         /home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:87 +0x23a fp=0xc420043dd8 sp=0xc420043d68
 github.com/tensorflow/tensorflow/tensorflow/go.(*Session).Run(0xc42000ce20, 0x0, 0xc420043f50, 0x2, 0x2, 0x0, 0x0, 0x0, 0x0, 0x0, ...)
         /home/pgaleone/projects/go/src/github.com/tensorflow/tensorflow/tensorflow/go/session.go:91 +0x243 fp=0xc420043e70 sp=0xc420043dd8
 command-line-arguments_test.TestFunc(0xc4200665b0)
         /home/pgaleone/projects/go/src/github.com/galeone/asd/poc_test.go:32 +0x35d fp=0xc420043fa8 sp=0xc420043e70
 testing.tRunner(0xc4200665b0, 0x562278)
         /usr/lib/go/src/testing/testing.go:657 +0x96 fp=0xc420043fd0 sp=0xc420043fa8
 runtime.goexit()
         /usr/lib/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc420043fd8 sp=0xc420043fd0
 created by testing.(*T).Run
         /usr/lib/go/src/testing/testing.go:697 +0x2ca
 
 goroutine 1 [chan receive]:
 testing.(*T).Run(0xc4200664e0, 0x559659, 0x8, 0x562278, 0xc420053d20)
         /usr/lib/go/src/testing/testing.go:698 +0x2f4
 testing.runTests.func1(0xc4200664e0)
         /usr/lib/go/src/testing/testing.go:882 +0x67
 testing.tRunner(0xc4200664e0, 0xc420053de0)
         /usr/lib/go/src/testing/testing.go:657 +0x96
 testing.runTests(0xc42000cd80, 0x7ecf80, 0x1, 0x1, 0x4131ac)
         /usr/lib/go/src/testing/testing.go:888 +0x2c1
 testing.(*M).Run(0xc420053f20, 0xc420053f20)
         /usr/lib/go/src/testing/testing.go:822 +0xfc
 main.main()
         command-line-arguments/_test/_testmain.go:42 +0xf7
 
 goroutine 17 [syscall, locked to thread]:
 runtime.goexit()
         /usr/lib/go/src/runtime/asm_amd64.s:2197 +0x1
 
 rax    0x0
 rbx    0x6
 rcx    0x7fa8684dc670
 rdx    0x0
 rdi    0x2
 rsi    0x7ffe4515bf50
 rbp    0x7ffe4515c1a0
 rsp    0x7ffe4515bf50
 r8     0x0
 r9     0x7ffe4515bf50
 r10    0x8
 r11    0x246
 r12    0x2
 r13    0x2
 r14    0x2bf8160
 r15    0x20
 rip    0x7fa8684dc670
 rflags 0x246
 cs     0x33
 fs     0x0
 gs     0x0
 FAIL    command-line-arguments  0.544s
 </denchmark-code>
 
 The same logic,  in python, works without any issue:
 import tensorflow as tf
 
 A = tf.constant([[1,2], [-1, -2]])
 b = tf.constant([[10], [100]])
 
 mul = tf.matmul(A, b)
 
 with tf.Session() as sess:
     print(sess.run([mul, mul]))
 outputs
 <denchmark-code>[array([[ 210],
        [-210]], dtype=int32), array([[ 210],
        [-210]], dtype=int32)]
 </denchmark-code>
 
 as expected.
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): 1.2.0
 Bazel version (if compiling from source): 0.5.1
 CUDA/cuDNN version: cuda 8, cudnn 5.1
 GPU model and memory:  GeForce GTX 1080
 Exact command to reproduce: go test
 
 	",1.0,galeone,2017-06-29T14:48:06Z,"
 		Thanks for the report, it certainly shouldn't fail with a SIGABRT.
 (That said though, am curious about the use case for fetching the same value multiple times).
 		",2.0,galeone,2017-06-29T15:11:23Z,"
 		I was showing that in <denchmark-link:https://github.com/galeone/tfgo>tfgo</denchmark-link>
  when one assigns a go variable to another go variable it needs to ""clone"" it before the assignment in order to create a different and new node in the graph.
 Otherwise, the assignment only exists in Go but the underlying reference points to the same node in the graph.
 (In short, I was showing how to use tf.assing and not the assignment operator of the lanuage used).
 To empathize this, I'd like to show that those 2 Go variables when evaluated contain the same value.
 But I can't because of that bug, thus I fallback showing it in another way. The first example here: <denchmark-link:https://github.com/galeone/tfgo#getting-started>https://github.com/galeone/tfgo#getting-started</denchmark-link>
 
 		",,,,,66604b0355b32961f9a532792be2e008cc22221f,Asim Shankar,2017-07-05 21:35:40-07:00,MODIFY,1,tensorflow\core\common_runtime\direct_session.cc,tensorflow\core\common_runtime\direct_session.cc,1.0,"622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,640,641,642,643,644,645,646,647,648","624,625,626,627",MODIFY,1.0,tensorflow\core\common_runtime\direct_session_test.cc,tensorflow\core\common_runtime\direct_session_test.cc,,,,,,,,,,,,,1.0,"409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435",,tensorflow::TEST,"DirectSessionTest,FetchMultipleTimes",409,435,MODIFY,1.0,tensorflow\core\distributed_runtime\rpc\grpc_session.cc,tensorflow\core\distributed_runtime\rpc\grpc_session.cc,1.0,"193,194,196,197,198,199,200,226,227,228,229,230,231,232,233,234,235,236","193,195,196,197,198",MODIFY,1.0,tensorflow\core\distributed_runtime\rpc\grpc_session_test.cc,tensorflow\core\distributed_runtime\rpc\grpc_session_test.cc,1.0,"186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211",,tensorflow::DirectSession::Run,"run_options,inputs,output_names,target_nodes,outputs,run_metadata",434,689,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::GrpcSession::RunHelper,"run_options,inputs,output_tensor_names,target_node_names,outputs,run_metadata,prun_handle",166,243,tensorflow::TEST,"GrpcSessionTest,FetchMultipleTimes",186,211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11411,ahaider3,2017-07-10T15:50:23Z,2018-03-26T20:14:33Z,Fetching data in Distributed Tensorflow has too much latency,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 
 <denchmark-link:https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d>https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d</denchmark-link>
 
 The above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring.
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Redhat
 
 
 TensorFlow installed from (source or binary):
 source
 
 
 TensorFlow version (use command below):
 tensorflow 1.2
 
 
 Python version:
 python 2.7.7
 
 
 Exact command to reproduce:
 
 
 python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &
 python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker
 By increasing batch_size, the timing difference between local/remote computation eventually becomes negligible.
 However, for small batch sizes the overhead can become 2x/3x:
 For example, here are two runs for different model/dataset sizes:
 
 128 features, batch size of 32, hidden layer size of 256  returns:
 Local GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798
 
 
 256 features, batch size of 128, hidden layer size of 128  returns:
 Local GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Distributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data.
 This is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus.
 Fetching small variables provides a constant overhead which limits scaling and efficiency .
 This overhead creates two issues in Distributed Tensorflow:
 
 I have to add several workers just to equal the performance of a single process.
 The overhead of fetching model parameters doesn't scale but the amount of computation does
 decrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters.
 
 There have been several issues posted with distributed tensorflow. <denchmark-link:https://github.com/tensorflow/tensorflow/issues/6116>#6116</denchmark-link>
  is an improvement to large tensor transfer while this problem exists for small tensors. <denchmark-link:https://github.com/tensorflow/tensorflow/issues/4498>#4498</denchmark-link>
  might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-link:https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d>https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d</denchmark-link>
 
 	",1.0,ahaider3,2017-07-10T18:31:54Z,"
 		This is a known issue (<denchmark-link:https://github.com/tensorflow/tensorflow/issues/4498>#4498</denchmark-link>
 , <denchmark-link:https://github.com/tensorflow/tensorflow/issues/6116>#6116</denchmark-link>
 , <denchmark-link:https://github.com/tensorflow/tensorflow/issues/11196>#11196</denchmark-link>
 ) with several workarounds:
 
 Wait until #7466 is merged, which gives roughly 2-3x speedup to gRPC large tensor transport.
 Use alternative communication protocol, such as grpc+verbs or grpc+mpi. This requires RDMA capable hardware.
 Refactor your code and try to use intra-process communication.
 
 		",2.0,ahaider3,2017-07-10T22:07:59Z,"
 		I am mostly interested in the performance of small tensors. I have tried grpc+verbs and haven't seen any significant benefits. I will try with MPI.
 		",3.0,ahaider3,2017-07-11T02:03:36Z,"
 		I see both of your model size and batch size is small (128/256?). What's the time spent on computation each round before fetching model parameters from PS? If it only spends tens to hundreds of microseconds, I don't think the current distributed runtime could reach a performance on par to that of a single process, as there's a fixed overhead on setting up interprocess communication.
 		",11e2aef14f7f2d862363c350ca1d67b87ea6a57b,Bairen Yi,2017-08-09 08:58:12-07:00,MODIFY,1,configure.py,configure.py,1.0,"943,944",,MODIFY,0.0,tensorflow\BUILD,tensorflow\BUILD,4.0,ahaider3,2017-07-11T16:03:56Z,"
 		Without fetching model parameters it only takes a few hundred microseconds. Yes, that's the problem I have. My compute time per iteration is low and so going to distributed requires double/triple the time per iteration due to the fixed overhead.
 I didn't see much benefit when I was testing with grpc+verbs, but that was with larger tensors. Is there a difference between grpc+mpi or grpc+verbs when compared to grpc besides the actual communication protocol? Are tensors still serialized in the same manner? The actual communication across the network is not the bottleneck from my testing.
 		",5.0,ahaider3,2017-07-11T16:11:19Z,"
 		If you are in doubt, you could try my own GPU Direct patch <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11392>#11392</denchmark-link>
  which is theoretically of the lowest latency. It does neither memory copy nor serialization. Btw if your computation is not that complicated, you should avoid using GPU; copying data from/to GPU adds nontrivial overhead in your communication pipeline.
 		",6.0,ahaider3,2017-07-11T19:42:04Z,"
 		I'm also interested in this issue, with the application of training small feedforward nets (say 200-100-50-4) as quickly as possible. Note this only has ~100KB of parameters. I'd like to run on an algorithmic minibatch size of 8K, split up among as many workers as possible (so 8 workers means each handles 1K examples per iteration). As I add more workers, the batch size per worker gets smaller, which makes connection overhead more significant (per Amdahl's law).
 I'm working on training these on CPU, storing the variables on a parameter server. I've found the connection overhead destroys parallelism, as this issue points out. The particular problem here is not low throughput for transferring large parameters, it is high latency in moving even very small amounts of data. My guess is this comes from connection setup time, and that steps such as serialization and extra copies do not help.
 I have not tried gprc+verbs or grpc+mpi, and I don't expect them to help significantly as my understanding is these protocols do the same connection establishment and then just move the tensors themselves (tiny amounts of data in my case!) through fast networking. Is this a valid understanding? Is there anywhere (docs or code) that I can learn more about network protocol involved in fetching tensors from parameter server?
 I implemented the same model with data parallelism using MPI (instead of TensorFlow's networking stack) to allreduce the gradients and found hugely better performance (running at 2 nodes results in a ~1.8x speedup instead of a slowdown).
 <denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  Is their any way to avoid tensor serialization on CPU?
 		",0.0,"185,186,187,188,189,190",,,,,,MODIFY,1.0,tensorflow\c\c_api.cc,tensorflow\c\c_api.cc,1.0,158,158,ADD,0.0,None,tensorflow\contrib\gdr\BUILD,,,,main,,913,977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ahaider3,2017-07-11T20:23:42Z,"
 		<denchmark-link:https://github.com/eamartin>@eamartin</denchmark-link>
  In my patch, tensors are transferred in terms of direct memory access (DMA) of the underlying tensor buffer, so there is no serialization nor memory copy. It is titled ""GPU Direct RDMA"", but certainly works for CPU as well.
 		",deallocate_buffer,"data,len,arg",157,165,,,,,ADD,0.0,None,tensorflow\contrib\gdr\README.md,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\contrib\gdr\gdr.proto,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\contrib\gdr\gdr_memory_manager.cc,,,,,,,,8.0,ahaider3,2017-07-12T23:55:54Z,"
 		Thanks for helping our friend <denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\contrib\gdr\gdr_memory_manager.h,,,,ADD,0.0,None,tensorflow\contrib\gdr\gdr_rendezvous_mgr.cc,,,,9.0,ahaider3,2017-07-13T00:03:00Z,"
 		Can we pause on closing this until the mentioned patch is actually merged? We have not yet confirmed that serialization or memcpy's caused the overhead. Other culprits could be multiple network roundtrips causing extra latency (and killing bandwidth in the small tensor limit).
 		",10.0,ahaider3,2017-07-13T00:59:40Z,"
 		Take a look at <denchmark-link:https://github.com/tensorflow/tensorflow/issues/10530>#10530</denchmark-link>
  and you may find something that interests you.
 Let me know if you have further questions; I am more than willing to help you out.
 		",11.0,ahaider3,2017-07-13T07:53:35Z,"
 		I run the script provided by@yaroslavvb in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/4498>#4498</denchmark-link>
 , result for  is
 <denchmark-code>Local rate:       15962.31 MB per second
 Distributed rate: 335.68 MB per second
 </denchmark-code>
 
 and grpc+verbs is
 <denchmark-code>Local rate:       15514.35 MB per second
 Distributed rate: 1306.15 MB per second
 </denchmark-code>
 
 I'm using a 56Gbps ib network.
 		",12.0,ahaider3,2017-07-13T09:58:36Z,"
 		I just ran a <denchmark-link:https://gist.github.com/shamoya/731a81a1fe3d12a2b098f8163eaab7dd/>similar script</denchmark-link>
  in a 40Gbps RoCE setup for my GDR patch, and here's the result:
 <denchmark-code>Adding data in 100 MB chunks
 Local rate: 5243.48 MB per second
 Distributed rate: 2679.18 MB per second
 </denchmark-code>
 
 Numbers for grpc and grpc+verbs are similar to what you got (~300 MB/s and ~1300MB/s).
 To try out the result in your own environment, do change the host to one of the IB interface address you actually use, as my patch will not work for localhost or 127.0.0.1.
 		",13.0,ahaider3,2017-07-13T23:29:12Z,"
 		<denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
  I am not seeing that benefit from  on my ib network. Are your results with an updated patch? I am running with TF 1.2.0.
 Here are tests with different chunk sizes:
 For 100 MB chunks with grpc:
 
 Local rate:       19761.24 MB per second
 Distributed rate: 339.71 MB per second
 
 For 512 KB chunks with grpc:
 
 Local rate:       3538.90 MB per second
 Distributed rate: 616.97 MB per second
 
 So decreasing the chunk size by 200x only increases throughput by 2x. I think the problem of large tensors is important, but small tensor transfer is also slow.
 		",14.0,ahaider3,2017-07-14T00:25:54Z,"
 		<denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  Please be sure to add  to your commit message in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11392>#11392</denchmark-link>
 .
 		",15.0,ahaider3,2017-07-14T08:54:12Z,"
 		<denchmark-link:https://github.com/ahaider3>@ahaider3</denchmark-link>
  , I built from official master branch, I'll try <denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  's branch. I find there is not much difference between benchmark scripts which run distributed tensorflow on same machine or different machines. The script in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/4498>#4498</denchmark-link>
  performs bettern on different machines than on a single machine, weird.
 		",16.0,ahaider3,2017-07-14T09:30:20Z,"
 		I find that when test with small data(10MB for example), grpc+rdma 's performance is very bad, speed decreases from 1300MB/s to less than 1000MB/s(300MB/s or 900MB/s, not stable).
 		",17.0,ahaider3,2017-07-14T09:41:58Z,"
 		<denchmark-link:https://github.com/ahaider3>@ahaider3</denchmark-link>
  <denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
  There is an important patch <denchmark-link:https://github.com/tensorflow/tensorflow/pull/10531>#10531</denchmark-link>
  got merged after the 1.2 release, so it is expected that the current master is faster than 1.2 for  runtime.
 		",18.0,ahaider3,2017-07-14T11:22:39Z,"
 		I find assign is much slow than assign_add, but I think assign should be as fast as assign_add at least.
 		",19.0,ahaider3,2017-07-14T22:05:29Z,"
 		<denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
 
 I've noticed this issue, too. I'm testing my GDR patch using the <denchmark-link:https://gist.github.com/yaroslavvb/e196107b5e0afc834652bd3153030c42>benchmark_grpc_recv.py</denchmark-link>
  script (courtesy <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
 ).
 For assign, 3 measurements in a row:
 <denchmark-code>Local rate:       6944.22 MB/s
 Distributed rate: 2690.64 MB/s
 ---
 Local rate:       5084.81 MB/s
 Distributed rate: 2910.57 MB/s
 ---
 Local rate:       5864.24 MB/s
 Distributed rate: 2588.69 MB/s
 </denchmark-code>
 
 For assign_add:
 <denchmark-code>Local rate:       16558.85 MB/s
 Distributed rate: 3248.83 MB/s
 ---
 Local rate:       9952.02 MB/s
 Distributed rate: 3681.21 MB/s
 ---
 Local rate:       16090.50 MB/s
 Distributed rate: 3418.83 MB/s
 </denchmark-code>
 
 But the variance of these measurements (running on the same machine) seems to be just as large as the throughput gap. So I would rather not take it as a serious issue or a performance bug.
 		",20.0,ahaider3,2017-08-09T18:22:30Z,"
 		<denchmark-link:https://github.com/eamartin>@eamartin</denchmark-link>
  As GDR is available in current master <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11392>#11392</denchmark-link>
 , would you mind to try again? It does no tensor serialisation nor memory copies for tensor data.
 Since all three extension protocol still use grpc heavily for the control plane, <denchmark-link:https://github.com/ahaider3>@ahaider3</denchmark-link>
  you might not see much difference as your computation is way too fast compared to the fixed overhead of setting up each tensor transmission (it is rather a latency issue, not a throughput one). I personally would try to port grpc from HTTP2 to RDMA, but it will be a patch unlikely to be accepted by the grpc project as indicated <denchmark-link:https://github.com/tensorflow/tensorflow/issues/2916#issuecomment-253924638>here</denchmark-link>
 .
 <denchmark-link:https://github.com/jart>@jart</denchmark-link>
  Any comments?
 		",21.0,ahaider3,2017-08-16T09:21:17Z,"
 		<denchmark-link:https://github.com/caffe2/caffe2/tree/master/caffe2/contrib/gloo>https://github.com/caffe2/caffe2/tree/master/caffe2/contrib/gloo</denchmark-link>
 
 <denchmark-link:https://github.com/facebookincubator/gloo#benchmarking>https://github.com/facebookincubator/gloo#benchmarking</denchmark-link>
 
 		",22.0,ahaider3,2017-08-17T22:47:13Z,"
 		I ran the tests again using TF 1.2 with the different extension protocols that I can run on my system. In my case, I can only use grpc+verbs.
 I found minimal to no benefit from using this protocol for my small model. I am measuring the throughput of fetching a parameter and then doing a GEMM.
 
 128 features; 32 batch size; 256 hidden layer size
 grpc:  192.81 MB/sec
 grpc+verbs: 179.18 MB/sec
 
 This was the average of three runs. I would conclude that these protocols are limited by GRPC because of the latency it introduces, as suggested by <denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  .
 There has been interesting work by Uber to make communication all-around more efficient: <denchmark-link:https://github.com/uber/horovod>https://github.com/uber/horovod</denchmark-link>
 
 <denchmark-link:https://github.com/jart>@jart</denchmark-link>
  what do you think about uber's method for distributed training.
 		",23.0,ahaider3,2017-08-17T23:13:44Z,"
 		Horovod uses NCCL 2, which supports InfiniBand but not RoCE. See <denchmark-link:https://github.com/uber/horovod/issues/5>here</denchmark-link>
 .
 		",24.0,ahaider3,2017-11-03T13:30:27Z,"
 		I report the same problem as <denchmark-link:https://github.com/ahaider3>@ahaider3</denchmark-link>
  that small mini-batch on small model is terrible for distributed training using cpu.
 When I train a deep&wide model with about 15 feas, embedding_size=32, I tried the 2/3/4 machine of ps+worker and ends up with the same speed as one machine of all in one process.
 Only after increasing embedding_size to 256 give a 2x speed up with 4 machine.
 		",25.0,ahaider3,2017-12-20T19:10:03Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",26.0,ahaider3,2018-01-04T19:07:54Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",27.0,ahaider3,2018-01-24T13:25:09Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",28.0,ahaider3,2018-02-08T19:33:48Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",29.0,ahaider3,2018-02-23T14:12:27Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",30.0,ahaider3,2018-03-10T13:20:03Z,"
 		Nagging Assignee <denchmark-link:https://github.com/jart>@jart</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",31.0,ahaider3,2018-03-25T12:40:35Z,"
 		Nagging Assignee <denchmark-link:https://github.com/jart>@jart</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",32.0,ahaider3,2018-03-26T08:33:11Z,"
 		I think we should close this issue as it appears to be resolved?
 		",33.0,ahaider3,2018-03-26T20:14:33Z,"
 		Thanks for the tip. Closing now that PR is merged. <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11392>#11392</denchmark-link>
 
 		",ADD,0.0,None,tensorflow\contrib\gdr\gdr_rendezvous_mgr.h,ADD,0.0,None,tensorflow\contrib\gdr\gdr_server_lib.cc,ADD,0.0,None,tensorflow\contrib\gdr\gdr_server_lib.h,ADD,0.0,None,tensorflow\contrib\gdr\gdr_worker.cc,ADD,0.0,None,tensorflow\contrib\gdr\gdr_worker.h,MODIFY,0.0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,"119,1338,1339",1337,MODIFY,2.0,tensorflow\core\common_runtime\gpu\gpu_device.cc,tensorflow\core\common_runtime\gpu\gpu_device.cc,1.0,124,123,tensorflow::EigenCudaStreamDevice::deallocate,buffer,123,132,1.0,"116,117",116,tensorflow::EigenCudaStreamDevice::allocate,num_bytes,104,122,MODIFY,2.0,tensorflow\core\common_runtime\gpu\process_state.cc,tensorflow\core\common_runtime\gpu\process_state.cc,1.0,"240,247","240,247",tensorflow::ProcessState::GetCUDAHostAllocator,numa_node,202,262,1.0,"170,195","170,195",tensorflow::ProcessState::GetCPUAllocator,numa_node,154,200,MODIFY,1.0,tensorflow\core\distributed_runtime\rpc\grpc_remote_worker.cc,tensorflow\core\distributed_runtime\rpc\grpc_remote_worker.cc,1.0,"136,139,183","132,133,134,135,136,137,138,143,145,146,147,148,149,150,152,153",tensorflow::GrpcRemoteWorker::RecvTensorAsync,"call_opts,request,response,done",128,184,MODIFY,3.0,tensorflow\core\distributed_runtime\rpc\grpc_server_lib.cc,tensorflow\core\distributed_runtime\rpc\grpc_server_lib.cc,1.0,250,,tensorflow::GrpcServer::Init,,250,250,1.0,"244,245,246,247,248",,tensorflow::GrpcServer::Init,"service_func,rendezvous_mgr_func",244,248,1.0,"108,109,187,188","108,186,242",tensorflow::GrpcServer::Init,"service_func,rendezvous_mgr_func,worker_func",106,242,MODIFY,0.0,tensorflow\core\distributed_runtime\rpc\grpc_server_lib.h,tensorflow\core\distributed_runtime\rpc\grpc_server_lib.h,0.0,"48,49,50,51,71,72,73,74",,MODIFY,0.0,tensorflow\core\distributed_runtime\rpc\grpc_worker_service.h,tensorflow\core\distributed_runtime\rpc\grpc_worker_service.h,0.0,"37,38,39,40","37,38",MODIFY,0.0,tensorflow\core\platform\default\build_config.bzl,tensorflow\core\platform\default\build_config.bzl,0.0,"296,297,298,299,300,301",,MODIFY,0.0,tensorflow\core\platform\default\build_config_root.bzl,tensorflow\core\platform\default\build_config_root.bzl,0.0,"42,43,44,45,46,47,48,49",,MODIFY,0.0,tensorflow\python\BUILD,tensorflow\python\BUILD,0.0,"33,2863,2864",2862,MODIFY,0.0,tensorflow\tools\pip_package\build_pip_package.sh,tensorflow\tools\pip_package\build_pip_package.sh,0.0,"55,56",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11692,meijun,2017-07-23T15:20:45Z,2017-08-08T20:01:28Z,A bug of tf.reduce_logsumexp with `-inf`,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux CentOS 7
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.2.0
 Python version: 2.7.13
 Bazel version (if compiling from source):
 CUDA/cuDNN version: 8.0/5.1.3
 GPU model and memory: Tesla K40m, 11439MiB
 Exact command to reproduce:  python -c ""import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))""
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 The doc of tf.reduce_logsumexp says it
 
 Computes log(sum(exp(elements across dimensions of a tensor))).
 
 However, it does not when the tensor is -inf.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>python -c ""import tensorflow as tf; print tf.Session().run(tf.reduce_logsumexp(float('-inf')))""
 </denchmark-code>
 
 prints
 <denchmark-code>nan
 </denchmark-code>
 
 <denchmark-h:hr></denchmark-h>
 
 <denchmark-code>python -c ""import tensorflow as tf; print tf.Session().run(tf.log(tf.reduce_sum(tf.exp(float('-inf')))))""
 </denchmark-code>
 
 prints
 <denchmark-code>-inf
 </denchmark-code>
 
 	",1.0,meijun,2017-07-24T07:25:19Z,"
 		The tf.reduce_logsumexp source code currently is:
 def reduce_logsumexp(input_tensor,
                      axis=None,
                      keep_dims=False,
                      name=None,
                      reduction_indices=None):
   """"""TF 1.2.0 source code""""""
   with ops.name_scope(name, ""ReduceLogSumExp"", [input_tensor]) as name:
     my_max = array_ops.stop_gradient(
         reduce_max(
             input_tensor,
             axis=axis,
             reduction_indices=reduction_indices,
             keep_dims=True))
     result = gen_math_ops.log(
         reduce_sum(
             gen_math_ops.exp(input_tensor - my_max),
             axis,
             keep_dims=True,
             reduction_indices=reduction_indices)) + my_max
     if not keep_dims:
       if isinstance(axis, int):
         axis = [axis]
       result = array_ops.squeeze(result, axis)
     return result
 I have written my own tf_reduce_logsumexp to fix the bug:
 def tf_reduce_logsumexp(input_tensor,
                         axis=None,
                         keep_dims=False,
                         name=None,
                         reduction_indices=None):
     """"""Fix tf.reduce_logsumexp""""""
     with tf.name_scope(name, ""tf_ReduceLogSumExp"", [input_tensor]) as name:
         raw_max = tf.reduce_max(
             input_tensor,
             axis=axis,
             reduction_indices=reduction_indices,
             keep_dims=True)
         my_max = tf.stop_gradient(
             tf.where(
                 tf.is_finite(raw_max),
                 raw_max,
                 tf.zeros_like(raw_max)))
         result = tf.log(
             tf.reduce_sum(
                 tf.exp(input_tensor - my_max),
                 axis,
                 keep_dims=True,
                 reduction_indices=reduction_indices)) + my_max
         if not keep_dims:
             if isinstance(axis, int):
                 axis = [axis]
             result = tf.squeeze(result, axis)
         return result
 		",2.0,meijun,2017-07-25T01:15:32Z,"
 		<denchmark-link:https://github.com/fastturtle>@fastturtle</denchmark-link>
 , could you take a look?
 		",3.0,meijun,2017-07-25T16:24:53Z,"
 		This looks like a reasonable fix, would you be willing to submit it as a pull-request?
 		",30c13a450841b213d72dea93d9447a25169be0a7,Jun MEI,2017-08-08 13:01:28-07:00,MODIFY,0,tensorflow\python\ops\math_ops.py,tensorflow\python\ops\math_ops.py,0.0,"1642,1643,1644,1645,1646,1648,1649,1650,1651","1643,1644,1645,1646,1647",MODIFY,1.0,tensorflow\python\ops\math_ops_test.py,tensorflow\python\ops\math_ops_test.py,4.0,meijun,2017-07-25T17:12:20Z,"
 		<denchmark-link:https://github.com/aselle>@aselle</denchmark-link>
  Sure, I will try it.
 		",,,,,,,,,1.0,"137,138,139,140",,testInfinity,self,137,140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11725,shamoya,2017-07-24T20:01:58Z,2017-08-24T05:21:42Z,tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, running tf_cnn_benchmarks.py from benchmarks repo
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.2 LTS
 TensorFlow installed from (source or binary): Unmodified source with RDMA Verbs enabled
 TensorFlow version (use command below): 1.3.0-rc0
 Python version: 2.7.12
 Bazel version (if compiling from source): 0.5.1
 CUDA/cuDNN version: 8.0/6
 GPU model and memory: NVIDIA Tesla P100 PCIe 16GB (8 per node)
 Exact command to reproduce:
 
 PS: CUDA_VISIBLE_DEVICES='' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=ps --task_index=0 --server_protocol grpc+verbs
 Worker0: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=0 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs
 Worker1: CUDA_VISIBLE_DEVICES='0,1,2,3,4,5,6,7' python tf_cnn_benchmarks.py --ps_hosts 12.12.12.43:20000 --worker_hosts 12.12.12.44:20000,12.12.12.41:20000 --batch_size=64 --model=inception3 --variable_update=parameter_server --local_parameter_device=cpu --job_name=worker --task_index=1 --num_gpus=8 --data_dir=/data/imagenet_data/ --train_dir=/data/imagenet_train/ --server_protocol grpc+verbs
 
 RDMA driver version: MLNX_OFED_LINUX-4.1-1.0.2.0
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 When running the above commands (Inception V3 synchronized data parallelism training with 2 workers and 1 external ps), the tf_cnn_benchmarks application hangs forever after some iterations (usually in warm up).
 It happens only when real data is involved (ImageNet), and with >4 GPUs. (More GPUs, less iterations before it hangs). Doesn't happen with grpc protocol, or when running with ""synthetic"" data.
 The master_service in the workers is stuck <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608>here</denchmark-link>
 , which I guess means some operations in the computation have not been completed.
 The RDMA protocol looks valid and clean, all messages corresponds to the protocol (see below logs).
 There some tensors requested by the workers which they don't receive, but they are passed by the RDMA Verbs transport to the BaseRendezvoudMgr with RecvLocalAsync in a valid way, and for some reason the higher level worker service doesn't trigger the Send kernel on those tensors.
 Any help is much appreciated!
 If there are some debug mechanisms I can use to understand which tensors/operations have not been completed it can greatly help. I was mostly debugging this from the RDMA Verbs layer till now, without much success, and I feel I don't have enough information there to understand what's missing.
 Also I feel we don't have enough knowledge on how the step_id acts (diving into this in the code now, but there's some higher level documentation it can greatly help).
 My initial guess was an occurrence of a racy condition when loading the data, since it creates a gap in execution time (worker0 starts the first training step 30-60 seconds after worker1, since it does the preprocessing of the data twice for a reason I couldn't understand yet), but after the first iteration (which usually passes successfully) the time is synchronized between workers.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Those are the logs of the runtime after moving the logging in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc>rdma.cc</denchmark-link>
  to VLOG(0) (also adding Tensor name and step id for all cases, in some cases the step_id doesn't mean anything like BUFFER_REQUEST/RESPONSE for example), and also some VLOG in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc>master_session.cc</denchmark-link>
 
 <denchmark-link:https://gist.github.com/shamoya/15a42f421e088473b8f02bf00c16d0fc>worker0</denchmark-link>
 
 <denchmark-link:https://gist.github.com/shamoya/dd3126c02c73990a6e28b534d9a9ddf6>worker1</denchmark-link>
 
 <denchmark-link:https://gist.github.com/shamoya/0c856365802ae4d42b38baf988149574>ps</denchmark-link>
 
 Unfortunately they are fairly large, but it's better then to cut the log files IMO.
 Example for analysis I did in the verbs layer, comparing the Sent Tensor requests to the actual received tensors writes in both workers:
 worker 0:
 
 /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:0/gpu:0;edge_116943_group_deps_2/NoOp_1;0:0 80661058974090965
 /job:worker/replica:0/task:1/cpu:0;1a50d5c51cd9c5d1;/job:worker/replica:0/task:0/gpu:0;edge_116947_group_deps_3/NoOp_1;0:0 80661058974090965
 /job:worker/replica:0/task:1/gpu:2;7f00fadabfe781f5;/job:worker/replica:0/task:0/gpu:0;edge_111078_group_deps_1/NoOp_2;0:0 80661058974090965
 /job:worker/replica:0/task:1/gpu:4;b07185dd19f62088;/job:worker/replica:0/task:0/gpu:0;edge_111080_group_deps_1/NoOp_4;0:0 80661058974090965
 
 worker 1:
 
 /job:ps/replica:0/task:0/cpu:0;f3c10d28b54074c0;/job:worker/replica:0/task:1/cpu:0;edge_155113_AssignAdd;0:0 80661058974090965
 /job:worker/replica:0/task:0/gpu:0;f3df8abf03739fe8;/job:worker/replica:0/task:1/cpu:0;edge_116948_group_deps_3;0:0 80661058974090965
 
 The tensors requests received well by the other side and passed to RecvLocalAsync, but are not called later.
 Thanks a lot.
 	",1.0,shamoya,2017-07-24T23:11:14Z,"
 		I was able to reproduce the issue. I also tried 'alexnet', it hung as well. I will take a close look in the coming days. Thanks for reporting.
 		",2.0,shamoya,2017-07-25T00:14:15Z,"
 		Thank you for looking into this <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
 !
 		",3.0,shamoya,2017-07-25T11:01:18Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  <denchmark-link:https://github.com/jhseu>@jhseu</denchmark-link>
  <denchmark-link:https://github.com/poxvoculi>@poxvoculi</denchmark-link>
  , just a small question.
 I'm trying to understand why execution hangs in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/master_session.cc#L608>here</denchmark-link>
 .
 What (I think) I need is to get the Executor(s) which doesn't end in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/graph_mgr.cc#L541>here</denchmark-link>
 .
 I tried passing tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE) to sess.run but then I get a huge file I can't really understand.  Is this the right way ?
 		",e650dcfb462a9efc9236e33cae87ac5bbf55d9f7,yanivbl6,2017-08-23 21:57:07-07:00,MODIFY,2,tensorflow\contrib\verbs\rdma.cc,tensorflow\contrib\verbs\rdma.cc,1.0,"774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848","774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820",MODIFY,0.0,tensorflow\contrib\verbs\rdma.h,tensorflow\contrib\verbs\rdma.h,4.0,shamoya,2017-07-25T11:51:33Z,"
 		I have met stuck problem(I think it may happen in warm up if I'm right.) when all workers finished all steps. I think  should be protected by a lock.I raised <denchmark-link:https://github.com/tensorflow/benchmarks/issues/38>an issue</denchmark-link>
 . I'm not sure if tensorflow has some special code for .
 		",5.0,shamoya,2017-07-25T16:41:12Z,"
 		When I'm trying to debug a hung concurrent program the first step is
 usually to try to find a small case that exhibits the problem, i.e. try to
 find the least number of workers and GPUs that still hangs.  It's much
 easier to debug a small case than a large one.  Then, if it's a really
 small case you might be able to set the logging level high and read all the
 log files.  Usually though, I form some hypotheses about where the problem
 might be (e.g. a missing lock, a race that might result in deadlock, logic
 error that always deadlocks) and start putting in LOG(INFO) statements
 around the suspicious points to confirm or refute each hypothesis.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Tue, Jul 25, 2017 at 4:53 AM, Ziming Dong ***@***.***> wrote:
  I think global_step should be protected by a lock.I raised an issue
  <tensorflow/benchmarks#38>. I'm not sure if
  tensorflow has some special code for global_step.
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#11725 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AO818aHj9Ak6IS6emES-wdAhL-n1M4oUks5sRdcmgaJpZM4OhqyQ>
  .
 
 
 
 		",6.0,shamoya,2017-07-25T17:09:14Z,"
 		<denchmark-link:https://github.com/tfboyd>@tfboyd</denchmark-link>
  I'm unassigning myself because I know very little about the RPC layer and know nothing about VERBs. I don't think this is a bug with tf_cnn_benchmarks, because <denchmark-link:https://github.com/tensorflow/tensorflow/issues/11416>#11416</denchmark-link>
  also has the same issue with a different model.
 <denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
  I don't think <denchmark-link:https://github.com/tensorflow/benchmarks/issues/38>tensorflow/benchmarks#38</denchmark-link>
  is the same issue, since this issue only occurs with verbs, and it occurs with another model with verbs. I'll take a look at <denchmark-link:https://github.com/tensorflow/benchmarks/issues/38>tensorflow/benchmarks#38</denchmark-link>
  though.
 		",0.0,"31,229,230,231,232,233,234",,,,,,MODIFY,2.0,tensorflow\contrib\verbs\rdma_rendezvous_mgr.cc,tensorflow\contrib\verbs\rdma_rendezvous_mgr.cc,1.0,"122,123,124,125,126,127,128,129,130,131,132,133,152","116,117,118,119,138,139,140,141,142,143,144,145,146,147,148,149",MODIFY,3.0,tensorflow\contrib\verbs\verbs_util.cc,tensorflow\contrib\verbs\verbs_util.cc,1.0,,"41,42,43,44,45,46,47,48,49,50,51,52,53,54,55",tensorflow::RdmaTensorBuffer::PostCopyOperations,"can_memcpy,buffer_size,tensor_bytes,key,in,step_id,is_dead,key_with_step_id,copy,proto,copy_buf",774,848,1.0,"723,724,725,726,727,728,729,730,731,732,733,734,735,737,738,739,740,741,742,743,744,745,746,747,748,749,753,756,759,761,762,763,764,768","710,724,725,726,727,729,730,731,732,741,742,743,744,745,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772",tensorflow::RdmaTensorBuffer::SendNextItem,,659,772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,shamoya,2017-07-26T13:26:29Z,"
 		Thanks <denchmark-link:https://github.com/poxvoculi>@poxvoculi</denchmark-link>
 
 Sadly the smallest case which repro the issue is 3 nodes (2 workers 1 ps) with InveptionV3 and ImageNet, with the tf_cnn_benchmark.py (haven't checked others).
 I had ~15 hypotheses which turn out to be false.
 Added a lot of prints, but still couldn't understand which tensor is the rebel.
 I feel the trace_log mechanism (tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)) can help me here, but I have no idea how to parse it. Is there a parsing tool or format for it ?
 		",tensorflow::RdmaRemoteRendezvous::RecvFromRemoteAsync,"parsed,recv_args,done",54,164,tensorflow::VerbsUtil::CopyCPUTensorToGPUSync,"cpu_tensor,device_context,gpu_device,gpu_tensor",41,55,MODIFY,0.0,tensorflow\contrib\verbs\verbs_util.h,tensorflow\contrib\verbs\verbs_util.h,0.0,,"31,32,33,34,35,36,37,38,39,40,41,42,43,44",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,shamoya,2017-07-26T14:19:07Z,"
 		after adding  a lock to global step, I didn't meet stuck problem.
 but I got error when running vgg benchmark code, maybe my network environment is a little unstable.
 <denchmark-code>2017-07-26 21:21:26.691844: F tensorflow/contrib/verbs/rdma.cc:683] Check failed: status.ok() RecvLocalAsync was not ok, key/job:worker/replica:0/task:2/gpu:1;b0903effc6e4881e;/job:ps/replica:0/task:0/cpu:0;edge_2493_Mul_1;0:0;138139939197237012 error message: Step 138139939197237012
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,1.0,"166,167,168,169,170,171,172,173,174,175,176,177,178,179,180",,tensorflow::RdmaRemoteRendezvous::RecvPostCopyOps,"key,key_with_step_id,recv_args,done,rm,rc,val,s",166,180,,,,,,,,,,,,,,,1.0,,"24,25,26,27,28,29,30,31,32,33,34,35,36,37,38",tensorflow::VerbsUtil::CopyGPUTensorToCPUSync,"gpu_device,device_context,gpu_tensor,cpu_tensor",24,38,1.0,,"58,59,60,61,62,63,64,65,66,67,68,69,70",tensorflow::VerbsUtil::SetProtoFromGPUSync,"tensor,dev,device_context,proto,is_dead",58,70,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,shamoya,2017-07-26T15:11:05Z,"
 		<denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
  Your error might be caused by another process (PS or worker) in your job. We have met the same problem and it turns out when one process terminated the connection, its peers will print out such odd error log.
 		",10.0,shamoya,2017-07-26T15:13:50Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  For a simpler model, you could try  (the default one) and set your number of batches to a smaller value.
 Sorry that I am busying working on migrating my own patch and have little time to reproduce this particular bug right now.
 		",11.0,shamoya,2017-07-26T15:45:04Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  Can you try what <denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
  and <denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  suggested.
 
 
 try the simple model to see if it hangs --model=trivial.
 
 
 With inception3, add a lock around global_step. something like
 
 
 <denchmark-code>  with self.lock:
         inc_global_step = global_step.assign_add(1)
         fetches.append(inc_global_step)
 </denchmark-code>
 
 some where in the init function, you define the lock:
 self.lock = threading.Lock()
 I was unable to get an infiniband setup yesterday. I will try again today.
 		",12.0,shamoya,2017-07-26T16:48:34Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
 
 The tracing data is available in a pre-parsed (protobuf) format.  See the run_metadata option to Session.run, which returns tracing data in StepStats.
 		",13.0,shamoya,2017-07-26T17:39:51Z,"
 		<denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  <denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
 
 note that  is also a part of computing graph,  op has a  arg.
 		",14.0,shamoya,2017-07-26T21:32:04Z,"
 		<denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  , you're the best ! it's reproduced on trivial case (with ImageNet data).
 Now it will be much easier to debug, going full VLOG now - let's hope I'll have update soon.
 <denchmark-link:https://github.com/suiyuan2009>@suiyuan2009</denchmark-link>
 , it's not related to the global_step.
 I changed to have only the chief increment it and it doesn't have any affect.
 		",15.0,shamoya,2017-07-27T02:43:39Z,"
 		<denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
 , do your GDR patch have met same problem like stuck or failure?
 		",16.0,shamoya,2017-07-27T05:52:22Z,"
 		No I have not met such kind of problems, at least not during our internal testing. But my patch uses vastly different design compared to current verbs implementation so there is little I can tell about this particular issue.
 		",17.0,shamoya,2017-07-27T06:47:04Z,"
 		My observation agrees with <denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  's.
 
 the program got stuck with the trivial model, but it needed 8 GPUs per worker. Even 7 GPUs do not result in hang (after running 1000 iterations).
 the program got stuck either at warm-up or early iterations (< 100).
 locking global_step does not help.
 At the very beginning, some tensors changed size as shown below. This may not be an issue, but it will be good to find out why.
 
 <denchmark-code>tensor and buffer size do not agree! buffer_size = 653 requested tensor size = 593 Tensor<type: int64 shape: [0,1] values: > key = /job:ps/replica:0/task:0/cpu:0;ccc0db7aa974ba53;/job:worker/replica:0/task:0/gpu:0;edge_37_report_uninitialized_variables/boolean_mask/Where;0:0
 
 tensor and buffer size do not agree! buffer_size = 649 requested tensor size = 589 Tensor<type: int64 shape: [0] values: > key = /job:worker/replica:0/task:0/gpu:0;62e354fc58da5afa;/job:ps/replica:0/task:0/cpu:0;edge_39_report_uninitialized_variables/boolean_mask/Squeeze;0:0
 </denchmark-code>
 
 		",18.0,shamoya,2017-07-27T14:49:16Z,"
 		<denchmark-link:https://github.com/katyakats>@katyakats</denchmark-link>
  <denchmark-link:https://github.com/bkovalev>@bkovalev</denchmark-link>
 
 Ok, after reviewing the full logs, this is what we think is the root cause:
 A single GPU (in worker1) doesn't complete loading of the model parameters from the CPU.
 For this GPU we don't see ""Async kernel done"" for the SEND/RECV operation (CPU:0 -> GPU:x locally).
 The reason why it happens only with RDMA (and not with gRPC) is not known yet.
 Thought about possible interrupts issue due to excessive interrupts (8 GPUs + NIC + NVMe drive which holds the ImageNet data, all on the same PCIe bus). however polling mode of Process_CQ (no interrupts from the NIC at all) didn't resolve the issue.
 <denchmark-link:https://gist.github.com/shamoya/824d452be527d95902f20b59f868b391>This</denchmark-link>
  is the problematic GPU relevant logs.
 This is what I get from grepping one of the model parameters tensors  in the problematic GPU log (as above) VS one of the other valid GPUs:
 idos@MTR-IDOS $cat gpu2.log | grep ""affine1/biases""
 2017-07-27 16:04:48.740960: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G109 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:2"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-5325932350616196133, tensor_name=""edge_609_v/affine1/biases/read"", tensor_type=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:2""</denchmark-link>
  is dead: 0
 2017-07-27 16:04:48.748463: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35506 step 286 v/affine1/biases/read_G108 = _Send<denchmark-link:v/affine1/biases/read>T=DT_FLOAT, client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:2"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-5325932350616196133, tensor_name=""edge_609_v/affine1/biases/read"", _device=""/job:worker/replica:0/task:1/cpu:0""</denchmark-link>
  is dead: 0
 idos@MTR-IDOS $cat gpu4.log | grep ""affine1/biases""
 2017-07-27 16:04:48.740332: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 8 step 286 v/affine1/biases/read_G105 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:4"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-5325932350616196133, tensor_name=""edge_605_v/affine1/biases/read"", tensor_type=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:4""</denchmark-link>
  is dead: 0
 2017-07-27 16:04:48.748482: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 35508 step 286 v/affine1/biases/read_G104 = _Send<denchmark-link:v/affine1/biases/read>T=DT_FLOAT, client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:4"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-5325932350616196133, tensor_name=""edge_605_v/affine1/biases/read"", _device=""/job:worker/replica:0/task:1/cpu:0""</denchmark-link>
  is dead: 0
 2017-07-27 16:04:48.764311: I tensorflow/core/common_runtime/executor.cc:1612] 0x7f1fc4e67ab0 Async kernel done: v/affine1/biases/read_G105 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:worker/replica:0/task:1/gpu:4"", send_device=""/job:worker/replica:0/task:1/cpu:0"", send_device_incarnation=-5325932350616196133, tensor_name=""edge_605_v/affine1/biases/read"", tensor_type=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:4""</denchmark-link>
 
 2017-07-27 16:04:48.765221: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 69 step 286 v_4/tower_4/gradients/v_4/tower_4/L2Loss_3_grad/mul = Mul[T=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:4""](v/affine1/biase
 /read_G105, v_4/tower_4/gradients/v_4/tower_4/mul_1_grad/Reshape_1) is dead: 0
 2017-07-27 16:04:48.765199: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 9 step 286 v_4/tower_4/L2Loss_3 = L2Loss<denchmark-link:v/affine1/biases/read_G105>T=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:4""</denchmark-link>
  is dead: 0
 2017-07-27 16:04:48.765338: I tensorflow/core/common_runtime/executor.cc:1558] Process node: 38 step 286 v_4/tower_4/affine1/add = Add[T=DT_FLOAT, _device=""/job:worker/replica:0/task:1/gpu:4""](v_4/tower_4/affine1/MatMul, v/affine1/biase
 /read_G105) is dead: 0
 		",19.0,shamoya,2017-07-27T16:33:53Z,"
 		Just a wild guess, if the callback passed into the local async recv kernel is an erroneous remote send, would it appear to be stuck at the local recv? What's the downstream operator of that local recv?
 		",20.0,shamoya,2017-07-27T20:01:26Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  intriguing discovery. Local tensor transfer is handled by BaseRemoteRendezvous. There is one difference between gRPC and RDMA: tolerate_dup_recv is set to  in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/distributed_runtime/rpc/rpc_rendezvous_mgr.cc#L42>the former</denchmark-link>
  and  in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/contrib/verbs/rdma_rendezvous_mgr.cc#L34>the latter</denchmark-link>
 . The reason: gRPC only receives a tensor once, with verbs we have multiple receive attempts, since the tx/rx buffer may not be ready at early attempts. I ran some experiments by changing that to  in gPRC model, no hang was observed. However <denchmark-link:https://github.com/tensorflow/tensorflow/commit/cbfd50ff0f01e1825922230a8bc6e5766da98dd7#diff-b9ae16e68ba80801fe243bb5e19bac51>this patch</denchmark-link>
  totally broke the verbs code. I had to raise an issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/11825>here</denchmark-link>
 . Something to worry about after this debug.
 A correction to what I said early, I finally saw hang with 7 GPUs per worker, after 2000 steps.
 		",21.0,shamoya,2017-07-27T20:48:27Z,"
 		Thanks <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  for noticing this commit.
 Not sure I understand why you say BaseRemoteRendezvous is used in the case of local tensor transfer between GPU and CPU on the same worker.
 <denchmark-link:https://gist.github.com/shamoya/36beb1d093d4b95a523e27d4deda16ea>This</denchmark-link>
  is a more detailed log of the all the occurences of affine1/biases in the last iteration in worker1. We can see the RdmaRemoteRendezvous has completed successfully from the PS.
 I'm not so sure anymore this issue is even related to the Verbs code.
 		",22.0,shamoya,2017-07-27T21:53:37Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  RdmaRemoteRendezvous is a derived class of BaseRemoteRendezvous. The local send/receive functions are defined in the base class. RdmaRemoteRendezvous only overrides RecvFromRemoteAsync().
 		",23.0,shamoya,2017-08-07T16:53:00Z,"
 		<denchmark-link:https://github.com/katyakats>@katyakats</denchmark-link>
  When you reverted commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/626d8d905aa412aaca02d171e5e0b4a1c407656b>626d8d9</denchmark-link>
  (Improve RDMA rendezvous speed), where did the code hang? SetProtoFromGPUSync?
 		",24.0,shamoya,2017-08-08T07:31:17Z,"
 		<denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  thank you! we will check this direction!
 I did not check what method is stuck when I reverted the commit. Will check this as well
 		",25.0,shamoya,2017-08-09T13:15:21Z,"
 		Hi <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  , following the discussion <denchmark-link:https://github.com/tensorflow/tensorflow/issues/11825>here</denchmark-link>
  and in this bug and after I review the code, I think we have a serious issue with tolerate_dup_recv mechanism (maybe this is why it was removed ? ).
 I think <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L209>this</denchmark-link>
  is wrong in case we don't do a duplicate receive (if we have a buffer already in some step), becasue no one will UnRef it, in case the Recv will not be called.
 Do u think it might be the issue ?
 		",26.0,shamoya,2017-08-09T13:34:36Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  UnRef is in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L264>Recv</denchmark-link>
 . In what case Recv will not be called? In verbs case, Send only is only called once, but Recv is called multiple times.
 		",27.0,shamoya,2017-08-09T13:39:02Z,"
 		If Recv was called before the SEND ?
 It doesn't have to be called multiple times (only first tensor appearance)
 		",28.0,shamoya,2017-08-09T14:31:56Z,"
 		If the first Recv is called before Send, then the waiting table does not have the tensor yet, Recv will put a callback in the table and return. Now, if a second Recv is called and still before Send, I think there is an issue since we allow multiple Recvs. In this case, <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L246>an empty tensor will be received</denchmark-link>
 . We will have trouble in the downstream. Is this what you are referring to?
 		",29.0,shamoya,2017-08-09T19:48:02Z,"
 		No <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
 , you didn't follow me, I'll try to be more clear.
 Let's say RECV is called before SEND, and puts <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L678>this</denchmark-link>
  callback in the table as u said. And let's say there's already a buffer for this tensor, meaning this callback will be called once (trigger TENSOR_WRITE).
 now SEND flow performs <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L209>this</denchmark-link>
  Ref() before calling the callback, since it assumes the RECV  be duplicated and called again.
 But in this case it doesn't, and no one will do <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L264>this</denchmark-link>
  Unref.
 When we are stuck in the bug, I noticed there's a huge diff between Refs and Unref on the GPU that is ""stuck"" (hundernds).
 Now, I do see <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L323>this</denchmark-link>
  Unref, in the Item destructor, which might do the missing Unref, but I need to check if it happens.
 		",30.0,shamoya,2017-08-09T21:51:43Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  Thanks for the explanation. A Recv-Send sequence does increase the reference count by one for send_dev_context. This can be a problem. I tried adding Unref right after <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/core/framework/rendezvous.cc#L219>this</denchmark-link>
 , it crashed right away.
 <denchmark-link:https://github.com/katyakats>@katyakats</denchmark-link>
  mentioned hang at . I see similar issue. Here is a <denchmark-link:https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1>stack trace</denchmark-link>
  I captured with gdb when the program hung. It was stuck while trying to lock a mutex. Frame <denchmark-link:https://github.com/tensorflow/tensorflow/issues/2>#2</denchmark-link>
  is  and Frame <denchmark-link:https://github.com/tensorflow/tensorflow/issues/20>#20</denchmark-link>
  is . Note both of them call , which uses the same lock. So a nesting call like this tries to acquire a already obtained lock, a recipe for hang. Not sure what caused this situation.
 		",31.0,shamoya,2017-08-10T11:42:11Z,"
 		Wowww <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
 , looks like this is it !
 I've ran the test with inter/intra_op_parallelism_threads = 500 and it passed (multiple times).
 It workaround the problem by reducing the probability to have this scenario (Schedule of the threadpool more likely to choose an idle thread).
 It also explains why it happens when working with real data (queue readers also use the same threads)!
 <denchmark-link:https://github.com/poxvoculi>@poxvoculi</denchmark-link>
 
 Really not clear to me how Schedule is possible on a thread which holds a mutex.
 Can't understand also where the context switch happens in this thread while it performs ThenExecute.
 Thanks <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  !
 		",32.0,shamoya,2017-08-10T13:29:50Z,"
 		By reducing the number of threads to 1 inter_thread I managed to reproduce the bug in a system with 2 GPUs per host and 2 hosts only  (2ps + 2 workers). I believe its the same issue because it occurs only when using verbs and I am getting the same backtrace as <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  did. so I think we are close.
 But I don't think ThenExecute's lock is the problem.
 I've added VLOG prints before and after the critical section, And counted them afterward. No one seems to be 'stuck' inside (locks and unlocks were equal).
 		",33.0,shamoya,2017-08-10T13:38:46Z,"
 		<denchmark-link:https://github.com/shamoya>@shamoya</denchmark-link>
  Glad we are making some progress.
 <denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
   Thanks for your help. This <denchmark-link:https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1#file-gistfile1-txt-L2>line</denchmark-link>
  seems to be waiting for a mutex. We need to identify the mutex inside .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.0,shamoya,2017-08-10T13:50:16Z,"
 		This <denchmark-link:https://gist.github.com/junshi15/af3063e5bd22c911c30fcaafee060fa1#file-gistfile1-txt-L37>line</denchmark-link>
  is not  itself. It looks like a lambda function (callback?) inside .
 		",35.0,shamoya,2017-08-10T13:51:18Z,"
 		I think this line relate to the condition variable of ""Notification n"", which requires a mutex. (for waiting).
 If I got this right The condition variable is waiting for the callback, that was scheduled after the stream in ThenExecute.
 		",36.0,shamoya,2017-08-14T12:18:13Z,"
 		I think the problem rises because the Sync deviceToDevice operation blocks the thread, preventing the earlier Async Device to Device operation from finishing- which, for some reason, blocks the later operation.
 What I did to check this was change the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L724>call to the wrapper sync function</denchmark-link>
  to a call to the async function, and inserted the rest of the code (that which is executed after the sync wrapper) into then lambda callback function (""done""). With this setting, the application no longer hangs (Tested multipile times, few of which in 8 gpus system).
 <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
  , is there an additional motivation for the usage of the sync wrappers, beside the pending operations we execute after the operation is completed?
 		",37.0,shamoya,2017-08-14T20:30:58Z,"
 		<denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
  Thanks for looking into it. There is another sync wrapper function, <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/rdma.cc#L730>SetProtoFromGPUSync</denchmark-link>
 . We should convert that back to async as well.
 The motivation for using sync wrapper is mainly for simplicity. Otherwise, you end up with chained call-backs, which is actually fairly common in tensorflow code.
 Please submit a PR if you have a fix.
 		",38.0,shamoya,2017-08-15T11:48:05Z,"
 		Great job <denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
  , looks like this is it.
 Waiting for the PR with the fix.
 Looking now in the gRPC code, they are doing this flow aysnc, this is probably why we didn't see it when running with gRPC.
 		",39.0,shamoya,2017-08-15T12:49:08Z,"
 		Thanks to all to solve it! Great job!  Go to check it !
 Need to put it in r1.3 and master.
 		",40.0,shamoya,2017-08-17T18:01:29Z,"
 		<denchmark-link:https://github.com/poxvoculi>@poxvoculi</denchmark-link>
  We found some of <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/verbs_util.cc#L23-L70>the wrapper functions</denchmark-link>
  caused hang. Do you know whether we are not supposed to use the sync version of these functions? I know CPU/GPU transfer can be slow,  but I did not expect it to hang. Thanks.
 		",41.0,shamoya,2017-08-17T18:11:21Z,"
 		I would avoid using any sync version of a function where an async version with callback is available.  The sync versions suspend a thread which could lead to a deadlock if you run out of threads.  It may be that by blocking you've also introduced a deadlock some other way, where the callback is waiting on something whose execution would have been triggered after the async invocation returned.
 		",42.0,shamoya,2017-08-17T20:00:04Z,"
 		<denchmark-link:https://github.com/poxvoculi>@poxvoculi</denchmark-link>
  What we saw matches your description. Hang happens when 4+ GPUs are used. It goes away when we increase the number of threads. <denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
  is working on a <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12361>fix</denchmark-link>
 .
 		",43.0,shamoya,2017-08-24T05:21:42Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/pull/12361>#12361</denchmark-link>
  merged to master
 		",44.0,shamoya,2018-10-19T23:59:26Z,"
 		
 @poxvoculi What we saw matches your description. Hang happens when 4+ GPUs are used. It goes away when we increase the number of threads. @yanivbl6 is working on a fix.
 
 <denchmark-link:https://github.com/junshi15>@junshi15</denchmark-link>
   Cool thanks, I hit a very similar problem and got stuck during the warm up. As you suggested, I increased  to work around the problem.
 before changing the number of threads, I couldn't get this far.
 2018-10-20 01:54:02.736996: I tensorflow/stream_executor/dso_loader.cc:151] successfully opened CUDA library libcupti.so.9.2 locally
 		",45.0,shamoya,2018-10-20T12:46:14Z,"
 		<denchmark-link:https://github.com/nicolefinnie>@nicolefinnie</denchmark-link>
  - did you experience an hang with grpc+verbs?
 This issue was (At least, to my knowledge) fixed already.
 What TF version are you experiencing this issue with?
 		",46.0,shamoya,2018-10-20T18:51:58Z,"
 		<denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
   Thanks for your response. It's tensorflow  and I didn't specify the server protocol so it's  by default.
 		",47.0,shamoya,2018-10-21T02:17:21Z,"
 		<denchmark-link:https://github.com/nicolefinnie>@nicolefinnie</denchmark-link>
  The thread is about an issue related to grpc+verbs, and it has been fixed by <denchmark-link:https://github.com/yanivbl6>@yanivbl6</denchmark-link>
 .
 I do not know why you are experiencing the issue with grpc only.
 		",48.0,shamoya,2018-10-21T04:38:19Z,"
 		I experienced an hang with the master branch and vanilla grpc about a week ago, and it was already fixed in master. I would suggest trying to pull the latest commit.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11829,patrikerdes,2017-07-27T22:21:25Z,2017-09-17T20:38:41Z,Slow to import tensorflow.contrib with Python 3 because inspect.stack is slow,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Sierra 10.12.5
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): v1.2.0-2420-g2b4a0f9a4 1.3.0-rc0
 Python version: 3.6.2
 Bazel version (if compiling from source): 0.5.2-homebrew
 CUDA/cuDNN version: CPU only build
 GPU model and memory: CPU only build
 Exact command to reproduce: time python3 -c ""import tensorflow.contrib""
 
 <denchmark-h:h3>The problem</denchmark-h>
 
 Doing import tensorflow.contrib take 7.5 seconds on my machine when doing it with Python 3.6.2. With Python 2.7.13 it takes 3.2 seconds.
 Investigating this revealed that a lot of time is spent in _inspect.stack() in the function make_decorator in  python/util/tf_decorator.py. The stack is inspected to find the name of the caller of the function. With Python2 inspect.stack() is fast, but with Python 3 each call to inspect.stack() take approximately 0.2 seconds and there are 23 calls made, which account for the difference in time between Python 2 and 3.
 <denchmark-h:h3>References</denchmark-h>
 
 Keras by default imports tensorflow.contrib when the Tensorflow backend is used. Therefore Keras is slow to import when using Python 3: <denchmark-link:https://github.com/keras-team/keras/issues/7408>keras-team/keras#7408</denchmark-link>
 
 There is a stackoverflow question referencing this issue: <denchmark-link:https://stackoverflow.com/questions/45093653/import-tensorflow-contrib-module-is-slow-in-tensorflow-1-2-1>https://stackoverflow.com/questions/45093653/import-tensorflow-contrib-module-is-slow-in-tensorflow-1-2-1</denchmark-link>
 
 	",1.0,patrikerdes,2017-07-31T22:32:52Z,"
 		Is this PR also useful?  <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11919>#11919</denchmark-link>
 
 Waiting on this PR: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/11830>#11830</denchmark-link>
 
 		",2.0,patrikerdes,2017-10-09T10:51:28Z,"
 		I use TensorFlow and PythonTeX to make reproducible dynamic reports. Every time I update the code chunk that uses tf.contrib I have to wait about 10 seconds till the module is imported. It's so annoying. Fix it, please or suggest temporary workaround to speed up this import.
 There were some patches. So which version is improved?
 P.S. My Python version is 3.5.2, TensorFlow 1.3.0. And I wait from 15 to 30 seconds!
 		",3.0,patrikerdes,2017-10-09T16:50:07Z,"
 		<denchmark-link:https://github.com/konstunn>@konstunn</denchmark-link>
  did you sync past <denchmark-link:https://github.com/tensorflow/tensorflow/issues/11829>#11829</denchmark-link>
 ?
 		",d42ca5a1462e75e80536aa9c46c6834bd9455f2b,Patrik Erdes,2017-09-17 13:38:11-07:00,MODIFY,0,tensorflow\python\util\tf_decorator.py,tensorflow\python\util\tf_decorator.py,0.0,"86,87",86,,,,,4.0,patrikerdes,2017-10-09T17:03:53Z,"
 		<denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  not sure. Should I install from sources? I used to install TensorFlow from PyPI.
 		",5.0,patrikerdes,2017-10-09T17:12:03Z,"
 		Ah, yes, you have to install from source and pull from head. Alternatively, you can use a binary that we distribute. We're releasing release candidates for 1.4, which is the next version. Closer to cutting edge are (assuming you have a GPU): <denchmark-link:https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/>https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-linux-gpu/</denchmark-link>
 
 There's a corresponding CPU version.
 		",6.0,patrikerdes,2017-10-11T12:52:18Z,"
 		Thanks, I've simply done: $ sudo -H pip3 install tf-nightly to install a nightly build from PyPI. Now import tensorflow.contrib takes 1.4 seconds to run. An order of magnitude better! Good job!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,patrikerdes,2017-10-11T17:11:30Z,"
 		Woohoo!
 <denchmark-link:#>…</denchmark-link>
 
 
 On Wed, Oct 11, 2017, 5:55 AM Konstantin Gorbunov ***@***.***> wrote:
  Thanks, I've simply done: $ sudo -H pip3 install tf-nightly to install a
  nightly build from PyPI. Now import tensorflow.contrib takes 1.4 seconds
  to run. An order of magnitude better! Good job!
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#11829 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AT_SbTGUVikgzkec_uEPtayawvxsycULks5srLrcgaJpZM4Ol6ZN>
  .
 
 
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11948,riklopfer,2017-08-01T18:52:55Z,2018-02-03T01:35:39Z,Memory leak in Java API when using GPU,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Custom code: https://github.com/riklopfer/TensorflowJavaGpuMemoryTest
 OS: CentOS 7
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): n/a
 Python version: n/a
 Bazel version (if compiling from source): n/a
 CUDA/cuDNN version: 8.0
 GPU model and memory: GeForce GTX 1080
 Exact command to reproduce: see https://github.com/riklopfer/TensorflowJavaGpuMemoryTest
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Main memory on the machine is continuously consumed when running on the GPU. Memory consumption hovers around 600M when running on the CPU.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 see: <denchmark-link:https://github.com/riklopfer/TensorflowJavaGpuMemoryTest>https://github.com/riklopfer/TensorflowJavaGpuMemoryTest</denchmark-link>
 
 	",1.0,riklopfer,2017-08-02T18:00:17Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  could you please take a look at this.
 		",2.0,riklopfer,2017-08-29T19:10:22Z,"
 		Sample log output fwiw,
 <denchmark-code>2017-08-29 14:30:27.963729: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-29 14:30:27.963779: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-29 14:30:27.963788: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-29 14:30:27.963795: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-29 14:30:27.963802: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-29 14:30:29.569904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
 name: GeForce GTX 1080
 major: 6 minor: 1 memoryClockRate (GHz) 1.7335
 pciBusID 0000:01:00.0
 Total memory: 7.92GiB
 Free memory: 7.81GiB
 2017-08-29 14:30:29.569957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
 2017-08-29 14:30:29.569965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
 2017-08-29 14:30:29.569981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0)
 </denchmark-code>
 
 		",3.0,riklopfer,2017-08-30T16:04:36Z,"
 		I've added <denchmark-link:http://valgrind.org/info/tools.html#memcheck>valgrind</denchmark-link>
  output to the test repository: <denchmark-link:https://github.com/riklopfer/TensorflowJavaGpuMemoryTest/blob/master/valgrind.out>https://github.com/riklopfer/TensorflowJavaGpuMemoryTest/blob/master/valgrind.out</denchmark-link>
 
 I'm not really familiar with this tool, but it seems like it would be useful. The summary makes me think that there definitely is a leak somewhere
 <denchmark-code>==28997== LEAK SUMMARY:
 ==28997==    definitely lost: 257,022 bytes in 1,028 blocks
 ==28997==    indirectly lost: 6,840 bytes in 15 blocks
 ==28997==      possibly lost: 61,716,234 bytes in 14,975 blocks
 ==28997==    still reachable: 397,427,506 bytes in 261,680 blocks
 ==28997==                       of which reachable via heuristic:
 ==28997==                         stdstring          : 2,034,837 bytes in 43,856 blocks
 ==28997==                         newarray           : 22,536 bytes in 1 blocks
 ==28997==         suppressed: 0 bytes in 0 blocks
 ==28997== Reachable blocks (those to which a pointer was found) are not shown.
 ==28997== To see them, rerun with: --leak-check=full --show-leak-kinds=all
 ==28997== 
 ==28997== For counts of detected and suppressed errors, rerun with: -v
 ==28997== ERROR SUMMARY: 1011246 errors from 460 contexts (suppressed: 0 from 0)
 </denchmark-code>
 
 		",03d310cc61a864600d24977f73138e643659986c,Asim Shankar,2017-08-30 12:35:06-07:00,MODIFY,4,tensorflow\java\src\main\native\operation_builder_jni.cc,tensorflow\java\src\main\native\operation_builder_jni.cc,1.0,"78,81",,,,,,4.0,riklopfer,2017-08-30T16:45:52Z,"
 		<denchmark-link:https://github.com/riklopfer>@riklopfer</denchmark-link>
  : Thanks very much for getting that information across. Unfortunately not a lot struck out to me.
 I did see 32 bytes of leaks from graph construction, which I will fix, but that happens once - not in a loop so won't explain the increasing usage over time.
 		",5.0,riklopfer,2017-08-30T23:12:20Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  thanks for the fixes. Were you able to reproduce the issue of ever-increasing memory consumption? Any idea what the next steps might be?
 		",6.0,riklopfer,2017-08-31T19:01:20Z,"
 		Updating CUDA and Nvidia drivers seems to have greatly mitigated the problem for me. I added <denchmark-link:https://github.com/riklopfer/TensorflowJavaGpuMemoryTest/blob/master/updated-valgrind.out>updated valgrind output</denchmark-link>
  to the test repo.
 		",,,,,,,,,,,,,,,,,,,,,,Java_org_tensorflow_OperationBuilder_finish,"env,clazz,handle",71,83,1.0,240,,Java_org_tensorflow_OperationBuilder_setAttrTensorList,"env,clazz,handle,name,tensor_handles",220,242,1.0,"266,267,275,276,280,281","270,271,275",Java_org_tensorflow_OperationBuilder_setAttrStringList,"env,object,handle,name,values",265,292,1.0,216,,Java_org_tensorflow_OperationBuilder_setAttrTensor,"env,clazz,handle,name,tensor_handle",205,218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,riklopfer,2017-09-01T17:06:40Z,"
 		Thanks for the update <denchmark-link:https://github.com/riklopfer>@riklopfer</denchmark-link>
 
 Sampling the latest output, I'm not sure if there are false positives or actual leaks (e.g., many leaks are reported in , which IIUC has nothing to do with TensorFlow, it's just JVM initialization.
 When you say ""greatly mitigated"", are you still seeing a monotonic increase in memory usage over time, or does it stabilize?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,riklopfer,2017-09-06T15:01:08Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  I no longer see monotonic increase in memory consumption when running my the small test  in the linked repo. However, when I run a longer, more complicated graph on the GPU, it is killed by the OOM killer. I wasn't able to get a valgrind dump for that process. When I have time, I will try increasing the complexity of the test graph until it shows the problem again (or not).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,riklopfer,2017-12-20T01:17:05Z,"
 		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.
 		",10.0,riklopfer,2017-12-20T15:55:08Z,"
 		Running with 1.4.0, I still see a slow, monotonic increase in memory consumption. I haven't had a chance to attempt to minimally reproduce the issue.
 		",11.0,riklopfer,2018-01-04T19:05:47Z,"
 		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.
 		",12.0,riklopfer,2018-01-23T23:18:11Z,"
 		The original poster has replied to this issue after the stat:awaiting response label was applied.
 		",13.0,riklopfer,2018-02-03T01:35:39Z,"
 		Closing since the original issue has been fixed. Please file another ticket with a repro if you can. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1198,alquraishi,2016-02-19T15:47:34Z,2016-03-09T19:18:07Z,reverse_sequence's inability to accept int32 can break bidirectional_rnn,"
 In the latest releases bidirectional_rnn has been changed to accept int32 tensors for the sequence_length argument, but tf.reverse_sequence only accepts int64 tensors, and this is currently causing an error when an int32 tensor is passed to bidirectional_rnn.
 	",1.0,alquraishi,2016-03-08T01:08:58Z,"
 		<denchmark-link:https://github.com/ludimagister>@ludimagister</denchmark-link>
 : Is this problematic to fix?
 		",2.0,alquraishi,2016-03-09T19:18:07Z,"
 		Sorry for the delay, this is fixed at head now.
 Mike
 		",3.0,alquraishi,2016-03-09T19:19:24Z,"
 		<denchmark-link:https://github.com/ludimagister>@ludimagister</denchmark-link>
 : For future use: if you include ""Fixes #"" in the commit description the bug will be automatically closed on push.
 		",484a80ce99998b59cf9f606a7c2a9ad1c14ea29a,A. Unique TensorFlower,2016-03-08 17:19:13-08:00,MODIFY,1,tensorflow\python\ops\rnn.py,tensorflow\python\ops\rnn.py,1.0,"272,273,274,275",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_reverse_seq,"input_seq,lengths",249,282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11985,snnn,2017-08-03T03:24:45Z,2017-08-08T18:40:10Z,windows bazel build failed: undeclared inclusion,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Windows 10
 TensorFlow installed from (source or binary):
 source
 TensorFlow version (use command below):
 2ab9cb2
 Python version:
 3.5.3
 Bazel version (if compiling from source):
 0.5.3
 CUDA/cuDNN version:
 None
 GPU model and memory:
 None
 Exact command to reproduce:
 bazel --output_base C:\t  build  //tensorflow/tools/pip_package:build_pip_package
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 ERROR: C:/os/tensorflow/tensorflow/core/BUILD:1271:1: undeclared inclusion(s) in rule '//tensorflow/core:lib_internal':
 this rule is missing dependency declarations for the following files included by 'tensorflow/core/framework/variant_tensor_data.cc':
 'C:/os/tensorflow/tensorflow/core/framework/tensor.h'
 'C:/os/tensorflow/tensorflow/core/framework/allocator.h'
 'C:/os/tensorflow/tensorflow/core/framework/numeric_types.h'
 'C:/os/tensorflow/tensorflow/core/framework/type_traits.h'
 'C:/os/tensorflow/tensorflow/core/framework/variant.h'
 'C:/os/tensorflow/tensorflow/core/framework/type_index.h'
 'C:/os/tensorflow/tensorflow/core/framework/tensor_shape.h'
 'C:/os/tensorflow/tensorflow/core/framework/tensor_types.h'
 'C:/os/tensorflow/tensorflow/core/framework/types.h'
 'C:/os/tensorflow/tensorflow/core/framework/bfloat16.h'
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 	",1.0,snnn,2017-08-03T03:25:06Z,"
 		If no one is working on this, I'll take it.
 		",2.0,snnn,2017-08-03T09:19:50Z,"
 		Culprit is <denchmark-link:https://github.com/tensorflow/tensorflow/commit/7ed4beea18c2aecaa0a1f600427e8452933df7b3>7ed4bee</denchmark-link>
 
 I've commented on the internal change.
 		",3.0,snnn,2017-08-03T21:07:15Z,"
 		Looks like fix is on its way.  Assigning to <denchmark-link:https://github.com/gunan>@gunan</denchmark-link>
  for tracking.
 		",6e7f1dac288acda411a21949a5720b2ca2f1a3eb,Changming Sun,2017-08-08 11:40:09-07:00,MODIFY,0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,"1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1340,1341,1376,1377,1401,1402,1578,1579","1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12205,facaiy,2017-08-11T08:21:10Z,2017-08-21T23:56:32Z,BUG: TypeError in DNNClassifier.eval() when using same name for feature in feature_engineering_fn,"
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 If we use the  same key to replace a feature, tensorflow might throw TypeError when evaluating:
 eg:
 def feature_engineering_fn(features, label):
   features[""x""] = some_func(features[""x""])
 When  is , it is a mutable object. Hence the bug is caused by  method which runs  again, see <denchmark-link:https://github.com/facaiy/tensorflow/blob/c7b80d51da4fb6d51ea54a0bdf2601afa379d60c/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L1165>code</denchmark-link>
 .
 I'll open a PR later.
 	",,,,,,,,,,,,,f47c3ad964f42121cbab02afed52e2df367ea9ef,Yan Facai (颜发才),2017-08-21 16:56:32-07:00,MODIFY,3,tensorflow\contrib\learn\python\learn\estimators\estimator.py,tensorflow\contrib\learn\python\learn\estimators\estimator.py,1.0,"1136,1143,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183","1136,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175",MODIFY,4.0,tensorflow\contrib\learn\python\learn\estimators\estimators_test.py,tensorflow\contrib\learn\python\learn\estimators\estimators_test.py,,,,,,,,,,,,,1.0,"81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117",,testFeatureEngineeringFnWithSameName,self,81,117,,,,,,,,,,,,,,,_call_model_fn,"self,features,labels,mode,metrics",1136,1183,1.0,1227,"1219,1220,1221,1222,1223,1224,1225",_get_eval_ops,"self,features,labels,metrics",1201,1232,1.0,"1136,1143,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175","1136,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175",_call_model_fn,"self,features,labels,mode",1136,1175,,,,,,,,1.0,"83,84,85,86,87,88",,testFeatureEngineeringFnWithSameName.input_fn,,83,88,1.0,"90,91,92,93,94,95",,testFeatureEngineeringFnWithSameName.feature_engineering_fn,"features,labels",90,95,1.0,"97,98,99,100,101,102,103,104",,testFeatureEngineeringFnWithSameName.model_fn,"features,labels",97,104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12249,ngc92,2017-08-13T12:49:10Z,2017-12-11T18:00:03Z,tf.estimator.Estimator breaks when using python 3.5 type annotations,"
 Minimal example:
 <denchmark-code>import tensorflow as tf
 
 def model_fn(features: dict, labels: tf.Tensor, mode: str):
     pass
 
 estimator = tf.estimator.Estimator(model_fn)
 </denchmark-code>
 
 results in
 <denchmark-code>File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 173, in __init__
     _verify_model_fn_args(model_fn, params)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 742, in _verify_model_fn_args
     args = set(_model_fn_args(model_fn))
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py"", line 737, in _model_fn_args
     return tuple(tf_inspect.getargspec(fn).args)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_inspect.py"", line 45, in getargspec
     if d.decorator_argspec is not None), _inspect.getargspec(target))
   File ""/usr/lib/python3.5/inspect.py"", line 1045, in getargspec
     raise ValueError(""Function has keyword-only arguments or annotations""
 ValueError: Function has keyword-only arguments or annotations, use getfullargspec() API which can support them
 </denchmark-code>
 
 <denchmark-h:h3>System information</denchmark-h>
 
 
 OS Platform and Distribution: Linux Mint 17.2
 TensorFlow version: v1.2.0-0-g12f033d 1.2.0
 Python version: 3.5
 
 	",1.0,ngc92,2017-08-14T15:17:04Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  can you comment or redirect? Thanks.
 		",2.0,ngc92,2017-08-14T16:46:52Z,"
 		Yes, we're using getargspec, which doesn't support type annotations. This is worthwhile to fix. If someone wants to take it on, the code should be relatively isolated in a single function which is checking arguments.
 		",3.0,ngc92,2017-08-14T19:08:12Z,"
 		Added a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12276>#12276</denchmark-link>
  for this issue.
 		",93a652ef5b635ffbd678d3992767c4862bffeb15,Yong Tang,2017-12-11 10:00:02-08:00,MODIFY,1,tensorflow\python\estimator\util.py,tensorflow\python\estimator\util.py,1.0,55,55,MODIFY,1.0,tensorflow\python\util\tf_inspect.py,tensorflow\python\util\tf_inspect.py,,,,,,,,,,,,,1.0,"48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65",,getfullargspec,obj,48,65,,,,,,,,,,,,,,,fn_args,fn,36,58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12436,EFanZh,2017-08-21T03:15:45Z,2017-09-17T18:11:34Z,zeros_like doesn't fully respect the optimize argument,"
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1463>The definition of zeros_like</denchmark-link>
  is:
 def zeros_like(tensor, dtype=None, name=None, optimize=True):
   with ops.name_scope(name, ""zeros_like"", [tensor]) as name:
     tensor = ops.convert_to_tensor(tensor, name=""tensor"")
 
     if tensor.shape.is_fully_defined():
       # We can produce a zeros tensor independent of the value of 'tensor',
       # since the shape is known statically.
       return zeros(tensor.shape, dtype=dtype or tensor.dtype, name=name)
 
     if dtype is not None and dtype != tensor.dtype:
       return zeros(
           shape_internal(tensor, optimize=optimize), dtype=dtype, name=name)
     else:
       return gen_array_ops._zeros_like(tensor, name=name)
 We can see that if the shape of  is already known, the  parameter is ignored, which is inconsistent with the <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/zeros_like>documented behavior</denchmark-link>
 .
 	",1.0,EFanZh,2017-08-21T21:50:52Z,"
 		I think  flag should be respected. Added a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12459>#12459</denchmark-link>
  for it.
 		",2.0,EFanZh,2017-08-28T04:40:04Z,"
 		<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>
 , could you please update this issue upon concluding review of <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12459>#12459</denchmark-link>
 ? Thanks
 		",,,,,9ba48146942219a2d97e9a2110f88f20d62c4cb6,Yong Tang,2017-09-17 11:11:33-07:00,MODIFY,1,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,1.0,"1484,1485",1484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,zeros_like,"tensor,dtype,name,optimize",1446,1494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12569,malsulaimi,2017-08-24T21:04:12Z,2018-09-09T16:44:11Z,missing Documentation of the method AttentionWrapper.zero_state(...),"
 Hello ,
 I have noticed that the method AttentionWrapper.zero_state( batch_size,dtype) does not have any description of its functionality in the  documentation website , below is a reference link :
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper>https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/AttentionWrapper</denchmark-link>
 
 I really hope that this gets fixed , I have spent a couple of days trying to debug a code that I have written until I realized that I was misusing the method .
 thank you
 	",1.0,malsulaimi,2017-08-24T23:18:58Z,"
 		Can you describe more about how you were misusing the method? (adding <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  ...)
 		",2.0,malsulaimi,2017-08-25T11:19:37Z,"
 		I did not know that this would return a AttenionWrapperState , I though that this would return a normal initial state  , and thus I was using it as the below :
 <denchmark-code>def decoding_layer(dec_input, encoder_state,
                    target_sequence_length, max_target_sequence_length,
                    rnn_size,
                    num_layers, target_vocab_to_int, target_vocab_size,
                    batch_size, keep_prob, decoding_embedding_size , encoder_outputs):
     """"""
     Create decoding layer
     :param dec_input: Decoder input
     :param encoder_state: Encoder state
     :param target_sequence_length: The lengths of each sequence in the target batch
     :param max_target_sequence_length: Maximum length of target sequences
     :param rnn_size: RNN Size
     :param num_layers: Number of layers
     :param target_vocab_to_int: Dictionary to go from the target words to an id
     :param target_vocab_size: Size of target vocabulary
     :param batch_size: The size of the batch
     :param keep_prob: Dropout keep probability
     :param decoding_embedding_size: Decoding embedding size
     :return: Tuple of (Training BasicDecoderOutput, Inference BasicDecoderOutput)
     """"""
     # 1. Decoder Embedding
     dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))
     dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)
 
     # 2. Construct the decoder cell
     def create_cell(rnn_size):
         lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,
                                             initializer=tf.random_uniform_initializer(-0.1,0.1,seed=2))
         drop = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)
         return drop
 
 
     dec_cell = tf.contrib.rnn.MultiRNNCell([create_cell(rnn_size) for _ in range(num_layers)])
     #dec_cell = tf.contrib.rnn.MultiRNNCell(cells_a)  
 
     #attention details 
         attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units=rnn_size, memory=encoder_outputs) 
 
 attn_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism , attention_layer_size=rnn_size/2)
 
 attn_zero = attn_cell.zero_state(batch_size , tf.float32 )
 
 attn_zero = attn_zero.clone(cell_state = encoder_state)
 
 new_state = tf.contrib.seq2seq.AttentionWrapperState(cell_state = encoder_state, attention = attn_zero  , time = 0 ,alignments=None , alignment_history=())
 
 """"""out_cell = tf.contrib.rnn.OutputProjectionWrapper(
             attn_cell, target_vocab_size, reuse=True
         )""""""
 
     #end of attention 
 
     output_layer = Dense(target_vocab_size,
                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))
 
     with tf.variable_scope(""decode""):
         train_decoder_out = decoding_layer_train(new_state, attn_cell, dec_embed_input, 
                          target_sequence_length, max_target_sequence_length, output_layer, keep_prob)
 
     with tf.variable_scope(""decode"", reuse=True):
         infer_decoder_out = decoding_layer_infer(new_state, attn_cell, dec_embeddings, 
                              target_vocab_to_int['<GO>'], target_vocab_to_int['<EOS>'], max_target_sequence_length, 
                              target_vocab_size, output_layer, batch_size, keep_prob)
 
     return (train_decoder_out, infer_decoder_out)
 
 """"""
 DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE
 """"""
 #tests.test_decoding_layer(decoding_layer)
 </denchmark-code>
 
 		",3.0,malsulaimi,2017-08-25T18:35:53Z,"
 		OK, we'll mark it for adding documentation. FWIW we provide source code, and so you can also see the return type for that function (though its not a substitute for documentation, you can always go read the code as well)...
 		",aae34fa7e35d9c3931cae49bfc20384dd20dffec,Eugene Brevdo,2017-09-29 17:36:01-07:00,MODIFY,2,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py,1.0,"1191,1192,1193,1194,1195",,MODIFY,0.0,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,4.0,malsulaimi,2017-08-25T19:06:14Z,"
 		Thats great . Thanks
 		",5.0,malsulaimi,2017-09-29T23:10:33Z,"
 		Added better documentation and an example to the docstrings for both BeamSearchDecoder and AttentionWrapper.__init__ and AttentionWrapper.zero_state.
 		",6.0,malsulaimi,2017-09-29T23:10:43Z,"
 		Should show up in a day or two.
 		",0.0,"133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,176","133,144",,,,,,,,,,,,,,,,,,,state_size,self,1190,1203,1.0,"1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224",,zero_state,"self,batch_size,dtype",1205,1255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,malsulaimi,2018-09-08T18:36:05Z,"
 		Nagging Assignee <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 : It has been 342 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,malsulaimi,2018-09-09T16:44:07Z,"
 		Fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/aae34fa7e35d9c3931cae49bfc20384dd20dffec>aae34fa</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12608,Timeroot,2017-08-26T00:19:52Z,2018-01-05T20:53:31Z,gather_nd bounds checking not working,"
 When using gather_nd, sometimes out-of-bounds indices lead to errors (bounds checking -- the expected behavior) and sometimes it seems to just read zeros. I expect it reading just other memory from the GPU, but I've never observed anything other than zeros so I'm not sure. When I run on the CPU the bounds seem to be appropriately checked i.e. I get the errors desired. Here's some example code:
 import tensorflow as tf
 sess = tf.Session()
 print(sess.run(tf.gather_nd(tf.zeros([5,5,5]) + 1, [[6,6,6]])))
 print(sess.run(tf.gather_nd(tf.zeros([1,5]) + 1, [-50000000000000000])))
 print(sess.run(tf.gather_nd(tf.reshape(tf.range(5*5*5), [5,5,5]), [[6,6,6]])))
 The first two print statements execute successfully, which is a bug: the indices are clearly out of range, and the arrays are clearly all 1's; but instead it returns an appropriately shaped array of 0's. (The +1 is not necessary to trigger the bug, but highlights that it's drawing from incorrect memory). The third line, for some reason, has the bounds checking operate correctly, and says that -- yes -- the index [6,6,6] is not in the bounds. It appears to something based on what the previous op is, maybe? Where some ops, such as stack, allow me to go outside the bounds, while others such as reshape don't. Here's an example interactive session to show the output I get.
 Python 3.5.2 (default, Nov 17 2016, 17:05:23) 
 [GCC 5.4.0 20160609] on linux
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 >>> import tensorflow as tf
 >>> sess = tf.Session()
 2017-08-25 17:10:43.788433: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-25 17:10:43.788455: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-25 17:10:43.788463: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-25 17:10:43.788466: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-25 17:10:43.788470: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 2017-08-25 17:10:43.919384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2017-08-25 17:10:43.919779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
 name: GeForce GTX 1070
 major: 6 minor: 1 memoryClockRate (GHz) 1.645
 pciBusID 0000:01:00.0
 Total memory: 7.92GiB
 Free memory: 7.59GiB
 2017-08-25 17:10:43.919795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
 2017-08-25 17:10:43.919801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
 2017-08-25 17:10:43.919814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
 >>> sess.run(tf.gather_nd(tf.zeros([5,5,5]) + 1, [[6,6,6]]))
 array([ 0.], dtype=float32)
 >>> sess.run(tf.gather_nd(tf.zeros([1,5]) + 1, [-50000000000000000]))
 array([ 0.,  0.,  0.,  0.,  0.], dtype=float32)
 >>> sess.run(tf.gather_nd(tf.reshape(tf.range(5*5*5), [5,5,5]), [[6,6,6]]))
 Traceback (most recent call last):
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
     return fn(*args)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1306, in _run_fn
     status, run_metadata)
   File ""/usr/lib/python3.5/contextlib.py"", line 66, in __exit__
     next(self.gen)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 466, in raise_exception_on_not_ok_status
     pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
 	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 895, in run
     run_metadata_ptr)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1124, in _run
     feed_dict_tensor, options, run_metadata)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1321, in _do_run
     options, run_metadata)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1340, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
 	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]
 
 Caused by op 'GatherNd_2', defined at:
   File ""<stdin>"", line 1, in <module>
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 1338, in gather_nd
     name=name)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 767, in apply_op
     op_def=op_def)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2630, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 1204, in __init__
     self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access
 
 InvalidArgumentError (see above for traceback): flat indices[0, :] = [6, 6, 6] does not index into param (shape: [5,5,5]).
 	 [[Node: GatherNd_2 = GatherNd[Tindices=DT_INT32, Tparams=DT_INT32, _device=""/job:localhost/replica:0/task:0/cpu:0""](Reshape/_7, GatherNd_2/indices)]]
 Version info:
 Linux Mint 4.4.0-53-generic x86_64
 Python version 3.5.2
 CUDA version release 8.0, V8.0.61
 cuDNN version 6.0.21
 Tensorflow version v1.3.0-rc2-20-g0787eee 1.3.0
 nvidia drivers version 375
 	",1.0,Timeroot,2017-08-26T08:38:24Z,"
 		Related: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12405>#12405</denchmark-link>
 
 		",2.0,Timeroot,2017-08-28T04:28:41Z,"
 		<denchmark-link:https://github.com/langmore>@langmore</denchmark-link>
 , are you able to take a look?
 		",3.0,Timeroot,2017-08-28T17:25:58Z,"
 		Any array bounds checking issues would involve digging into the C++, or maybe even the GPU kernel.  That's beyond what I can do.
 		",5386775e64aac0bb5020974122645da900bc312a,Yong Tang,2018-01-05 15:53:30-05:00,MODIFY,0,tensorflow\core\api_def\base_api\api_def_GatherNd.pbtxt,tensorflow\core\api_def\base_api\api_def_GatherNd.pbtxt,0.0,"46,47,48,49",,MODIFY,0.0,tensorflow\core\api_def\base_api\api_def_GatherV2.pbtxt,tensorflow\core\api_def\base_api\api_def_GatherV2.pbtxt,4.0,Timeroot,2017-08-28T21:42:31Z,"
 		It looks like relevant bounds-checking code for the CPU is in 
 
 
 tensorflow/tensorflow/core/kernels/gather_nd_op_cpu_impl.h
 
 
          Line 57
       in
       dff1062
 
 
 
 
 
 
  bool out_of_bounds = false; 
 
 
 
 
  The 'out_of_bounds' variable gets stored in 'error_loc' and then passed out as 'bad_i' in 
 
 
 tensorflow/tensorflow/core/kernels/gather_nd_op.cc
 
 
          Line 126
       in
       e073322
 
 
 
 
 
 
  bad_i = func(c->eigen_device<Device>(), slice_size, scratch_scalar, \ 
 
 
 
 
 
 But based on L144 in gather_nd_op.cc, it seems that the GPU kernel is currently not designed to do the bounds checking.
 
 
 
 tensorflow/tensorflow/core/kernels/gather_nd_op_gpu.cu.cc
 
 
          Line 56
       in
       e073322
 
 
 
 
 
 
  out[i] = (out_of_bounds) ? T(0) : ldg(params + offset + loc_offset); 
 
 
 
 
  shows that it deliberately returns an all-zero tensors when the bounds check fails, but doesn't report the error. L91 says something about it requiring CPU/GPU communication that I honestly don't understand. I figure this will be a #dontfix for now then? :|
 Tagging <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  since it looks like his code (many thanks, though, for this useful function!)
 		",5.0,Timeroot,2017-12-20T01:18:50Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",6.0,Timeroot,2017-12-22T01:19:46Z,"
 		The issue and <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12405>#12405</denchmark-link>
  seems both similar with <denchmark-link:https://github.com/tensorflow/tensorflow/issues/13687>#13687</denchmark-link>
 , which requests a bound checking for GPU . cc <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 .
 
 It's hard to efficiently check an error condition coming off the GPU
 gather_nd does in a similar case. Want to send a PR to update the documentation for those two
 
 		",0.0,"53,54,55,56",,,,,,MODIFY,0.0,tensorflow\core\api_def\base_api\api_def_ScatterNd.pbtxt,tensorflow\core\api_def\base_api\api_def_ScatterNd.pbtxt,0.0,"101,102,103",,MODIFY,0.0,tensorflow\core\ops\array_ops.cc,tensorflow\core\ops\array_ops.cc,0.0,"1563,1564,1565,1566,1636,1637,1638,1639,5424,5425,5426",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,Timeroot,2018-01-04T20:26:16Z,"
 		Added a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/15857>#15857</denchmark-link>
  to update the docs.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12641,quaeler,2017-08-28T04:58:44Z,2018-02-08T00:46:32Z,Improve all-in-memory file copy architecture (Python at least),"
 Current file copy (at least via Python  (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py#L22>gfile.py</denchmark-link>
 → <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py#L371>file_io.py</denchmark-link>
  → <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.i#L113>file_io.i</denchmark-link>
 )) involves copying the source contents into memory, and then writing memory to the destination. For scenarios like <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12630>#12630</denchmark-link>
  which is working with an 11GB asset, this is unacceptable design.
 file_system.h's WritableFile is not stubbed to allow anything like a streaming, though its RandomAccessFile is. (not entirely, entirely, true - i suppose WriteableFile.Append(const StringPiece& data) could be employed in a streamable fashion -ish.)
 To cull the Python low hanging fruit, at least, please implement <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.i#L113>file_io.i</denchmark-link>
  using a regular streaming design instead of the above described current design.
 	",1.0,quaeler,2017-08-28T20:40:29Z,"
 		Created a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12658>#12658</denchmark-link>
  for that.
 		",2.0,quaeler,2017-08-30T21:25:41Z,"
 		Slurping the whole file into memory and writing it back out isn't exactly optimal. Streaming would be great. What would be even better is the <denchmark-link:http://man7.org/linux/man-pages/man2/sendfile.2.html>sendfile</denchmark-link>
  system call, which does it entirely in kernel space. If would be terrific if <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  was able to implement that when src and dst are Unix FS, and do streaming otherwise.
 		",3.0,quaeler,2017-08-31T03:31:17Z,"
 		The implementation of CopyFile in PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12658>#12658</denchmark-link>
  has been updated with calling  in Linux for posix file systems. Please take a look.
 		",b1f5f433959406c7aad634c05e85ccd62fd06e87,Yong Tang,2018-02-07 16:46:31-08:00,MODIFY,2,tensorflow\core\platform\env.cc,tensorflow\core\platform\env.cc,1.0,"284,285,286,287,288,289,290,291,292,293",,MODIFY,0.0,tensorflow\core\platform\env.h,tensorflow\core\platform\env.h,4.0,quaeler,2017-12-20T01:18:55Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",5.0,quaeler,2018-01-03T19:11:44Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",6.0,quaeler,2018-01-18T19:13:55Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",0.0,"217,218,219,387,388,389,390,391",,,,,,MODIFY,1.0,tensorflow\core\platform\file_system.cc,tensorflow\core\platform\file_system.cc,1.0,"268,269,270",,MODIFY,0.0,tensorflow\core\platform\file_system.h,tensorflow\core\platform\file_system.h,0.0,"192,193,194",,tensorflow::Env::CopyFile,"src,target",284,293,1.0,"423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444",,tensorflow::FileSystemCopyFile,"src_fs,src,target_fs,target",423,444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,quaeler,2018-02-06T07:42:08Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",tensorflow::FileSystem::CopyFile,"src,target",268,270,,,,,MODIFY,1.0,tensorflow\core\platform\posix\posix_file_system.cc,tensorflow\core\platform\posix\posix_file_system.cc,1.0,"285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349",,tensorflow::PosixFileSystem::CopyFile,"src,target",285,349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\platform\posix\posix_file_system.h,tensorflow\core\platform\posix\posix_file_system.h,0.0,"59,60",,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\lib\io\file_io.i,tensorflow\python\lib\io\file_io.i,0.0,"113,115,116,120,121","113,115,116,120,121,122,123,124,125,126,127",,,,,8.0,quaeler,2018-02-06T15:49:13Z,"
 		There's a PR somewhere adding in-kernel sendfile copy support I spent a lot of time helping to review a while back. I don't know what the status of that is, but I hope the author is generous enough to bring it to completion, because it'd be a great notch in his belt because this isn't an easy problem to solve.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,quaeler,2018-02-06T16:04:32Z,"
 		It's the PR cited above - <denchmark-link:https://github.com/tensorflow/tensorflow/pull/12658>#12658</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12902,Dorokhov,2017-09-08T09:15:10Z,2017-09-26T18:54:32Z,Change TanhGrad() operation definition with respect to documentation,"
 Hello,
 TanhGrad() documentation says: ""Specifically, grad = dy * (1 - y*y), where y = tanh(x), and dy
 is the corresponding input gradient."" 
 
 
 tensorflow/tensorflow/core/ops/math_ops.cc
 
 
          Line 323
       in
       bab2db4
 
 
 
 
 
 
  Specifically, `grad = dy * (1 - y*y)`, where `y = tanh(x)`, and `dy` 
 
 
 
 
  which is correct and looks good.
 But operation has following declaration of inputs:
 Input(""x: T"")
 .Input(""y: T"")
 
 
 
 tensorflow/tensorflow/core/ops/math_ops.cc
 
 
          Line 200
       in
       bab2db4
 
 
 
 
 
 
  Input(""x: T"")                                                \ 
 
 
 
 
 
 what doesn't correlate with the documentated formula: grad = dy * (1 - y*y).
 Could you please rename inputs with respect to documentation like this:
 Input(""y: T"")
 .Input(""dy: T"")
 Thanks.
 	",1.0,Dorokhov,2017-09-10T22:22:21Z,"
 		This means modifying UNARY_GRADIENT_COMPLEX macro and changing x to y and y to dy for every op that uses it, are you sure that wouldn't break things/contradict documentation for those other ops? BTW, you can press ""y"" in github to generate a permanent link with line number reference
 		",2.0,Dorokhov,2017-09-11T07:29:56Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
 , it doesn't contradict documentation for other operations which use UNARY_GRADIENT_COMPLEX macros, because the other ops use 'y' and 'dy' names in docs.
 Thanks for your suggestion, I updated the issue with permanent links.
 		",3.0,Dorokhov,2017-09-11T15:55:57Z,"
 		<denchmark-link:https://github.com/Dorokhov>@Dorokhov</denchmark-link>
  do you want to do a PR to fix it? If you do, make sure to run the tests locally first
 		",9b5bf2b6786a58df679b4be2249da8a235b9f4fd,Dorokhov,2017-09-26 11:54:32-07:00,MODIFY,0,tensorflow\core\ops\math_ops.cc,tensorflow\core\ops\math_ops.cc,0.0,"200,201","200,201",,,,,4.0,Dorokhov,2017-09-11T17:02:42Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  right, I will do
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13202,yaroslavvb,2017-09-21T03:44:15Z,2018-04-02T04:20:16Z,tf.InteractiveSession leaks sessions,"
 The following works fine with tf.Session() but will fail to release resources in tf.InteractiveSession
 <denchmark-code>sess = tf.InteractiveSession()
 # do stuff  
 sess.close()
 del sess
 </denchmark-code>
 
 The reason is that interactive session enters a context using __enter()__ and never quits it, leaving a reference from a DefaultStack object. I found this when debugging why my notebook was hogging all GPU RAM.
 The two work-arounds:
 
 Force C_API to close the session using sess.__del__()
 Get rid of the dangling reference
 
 <denchmark-code>    sess._default_session.__exit__(None, None, None)
     del sess
     import gc
     gc.collect()
 </denchmark-code>
 
 I think a better solution would be to have sess.close() call both TF_CloseSession and TF_DeleteSession, or have a method that will reset all sessions like session_lib.Reset
 	",1.0,yaroslavvb,2017-09-21T15:31:16Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  do you know if anyone's planning to make  work for local sessions?
 		",2.0,yaroslavvb,2017-09-21T16:39:15Z,"
 		Derek, could you please take a look. Thanks!
 		",3.0,yaroslavvb,2017-09-21T16:44:55Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  I'm confused by your diagnosis, because the  method <denchmark-link:https://github.com/tensorflow/tensorflow/blob/e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c/tensorflow/python/client/session.py#L1627>explicitly calls __exit__()</denchmark-link>
  on the context managers it enters in the constructor. Where is the dangling reference?
  is <denchmark-link:https://github.com/tensorflow/tensorflow/blob/e9d5ee1ebffba25cef65f1f354b9e4ca9bcea10c/tensorflow/core/common_runtime/direct_session.cc#L183>implemented</denchmark-link>
  for local sessions, but it's not clear what you think it should do....
 		",0f508d4de379e800ad7f990de08959bbd6fcabb5,Derek Murray,2018-03-15 12:37:47-07:00,MODIFY,2,tensorflow\python\client\session.py,tensorflow\python\client\session.py,1.0,"1659,1660,1661,1662,1663,1664,1665,1666,1667",,MODIFY,2.0,tensorflow\python\client\session_test.py,tensorflow\python\client\session_test.py,4.0,yaroslavvb,2017-09-21T16:47:26Z,"
 		I'm experiencing the same with the normal tf.Session and also while using the C API directly. The following code keeps the memory on the GPU allocated until it exits.
 <denchmark-code>#include <tensorflow/c/c_api.h>
 #include <unistd.h>  // usleep
 
 
 int main() {
     TF_Graph* graph = TF_NewGraph();
     TF_SessionOptions* options = TF_NewSessionOptions();
     TF_Status* status = TF_NewStatus();
     TF_Session* sess = TF_NewSession(graph, options, status);
     TF_CloseSession(sess, status);
     TF_DeleteSession(sess, status);
     TF_DeleteSessionOptions(options);
     TF_DeleteGraph(graph);
     TF_DeleteStatus(status);
     usleep(5000000);  // gpu resources are still allocated here
 }
 </denchmark-code>
 
 I'm running Ubuntu 16.04, cudnn 6.0, cuda 8.0, tensorflow-master build is 2 hours old.
 		",5.0,yaroslavvb,2017-09-21T17:04:15Z,"
 		<denchmark-link:https://github.com/PhilJd>@PhilJd</denchmark-link>
  that's a different issue -- the memory allocator is global to the process, so it will keep the memory allocated even if the session gets closed.
 <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  indeed that method gets called, but somehow session doesn't get garbage collected. I assumed DefaultStack was to blame by looking at 
 CPython is supposed to call __del__ immediately after ref-count gets decremented to 0, and somehow it doesn't. Adding import gc; gc.collect() after __exit__ seems to remove the leak as well
 Repro: <denchmark-link:https://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report.py>https://github.com/yaroslavvb/stuff/blob/master/resnet_leak_report.py</denchmark-link>
 
 When I run it, I see
 <denchmark-code>Run 0, GB's in use 2.1
 Run 1, GB's in use 3.4
 Run 2, GB's in use 5.0
 <OOM crash>
 
 Calling __del__
 Calling __del__
 Calling __del__
 </denchmark-code>
 
 I'll update this issue if I find why CPython doesn't call del
 		",6.0,yaroslavvb,2017-09-21T18:58:48Z,"
 		OK, looks like  isn't called because there's a cycle and ref count never goes to zero.
  points back to sess (args are filled in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/03619fab3f4dd6f28b67418455a953b0fccdd9bf/tensorflow/python/framework/ops.py#L4507>here</denchmark-link>
 ), but there's no such cycle when using 
 Running gc.collect detects cycles and cleans them up.
 		",1.0,"70,71,72",,setUp,self,70,72,,,,,,,,,,,,,,,__init__,"self,target,graph,config",1631,1675,1.0,"1680,1681,1682,1683,1684,1685",,close,self,1677,1688,,,,,,,,,,,,,,,1.0,"1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223",,testMultipleInteractiveSessionsWarning,self,1199,1223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,yaroslavvb,2017-09-21T19:25:04Z,"
 		Ironically InteractiveSession only causes this problem during interactive use, when you have a notebook open and create new graphs/sessions without restarting the process.
 The underlying issue is that sess needs to keep a reference to the default context manager (otherwise it gets garbage collected and closed), whereas default context manager must link back to session. This seems hard to fix this issue without big refactoring of context manager logic. Perhaps it should be a docs issue and mentioned in docs of InteractiveSession that gc.collect must be called to reclaim resources
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,yaroslavvb,2017-09-25T21:50:44Z,"
 		Would it make sense to make tf.InteractiveSession() a singleton and add a warnings.warn('Use tf.Session() instead if you intend to have multiple sessions') upon the second call? Singletons are typically bad, but maybe very fitting in this particular case.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,yaroslavvb,2017-09-25T21:56:26Z,"
 		I think that would be better than status quo which is ""1. create multiple interactive sessions by accident. 2. run out of GPU memory 3. restart notebook server""
 		",10.0,yaroslavvb,2017-12-20T01:13:14Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",11.0,yaroslavvb,2018-01-03T19:15:24Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",12.0,yaroslavvb,2018-01-18T19:16:00Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",13.0,yaroslavvb,2018-01-18T20:49:23Z,"
 		Fix in progress...
 		",14.0,yaroslavvb,2018-02-06T07:47:49Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",15.0,yaroslavvb,2018-02-20T19:40:31Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",16.0,yaroslavvb,2018-03-07T13:18:22Z,"
 		Nagging Assignee <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",17.0,yaroslavvb,2018-03-15T16:21:46Z,"
 		So after implementing a couple of fixes for this, it seems there's no way to stop the leak without breaking an existing use case. The ""fix"" will be a warning, as <denchmark-link:https://github.com/carlthome>@carlthome</denchmark-link>
  suggested, and we will investigate a more principled fix for TensorFlow 2.0.
 		",18.0,yaroslavvb,2018-03-16T23:15:53Z,"
 		PS, the work-around is to do the following
 <denchmark-code>sess = tf.InteractiveSession()
 import gc; gc.collect()
 </denchmark-code>
 
 That will run cyclic garbage collector and remove all the dangling sessions
 		",19.0,yaroslavvb,2018-03-16T23:25:50Z,"
 		Note that we've had to roll back <denchmark-link:https://github.com/tensorflow/tensorflow/commit/0f508d4de379e800ad7f990de08959bbd6fcabb5>0f508d4</denchmark-link>
  internally, because it has a bug that produces spurious warnings. We will fix it after the weekend, but in the mean time the nightly build will have the spurious messages.
 		",20.0,yaroslavvb,2018-03-16T23:31:18Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  maybe you can just do  in the first call to  of InteractiveSession
 		",21.0,yaroslavvb,2018-03-16T23:41:00Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  The  trick doesn't solve the problem for me.  Not sure why yet.
 		",22.0,yaroslavvb,2018-03-31T18:25:56Z,"
 		Nagging Assignee <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",23.0,yaroslavvb,2018-04-02T04:20:16Z,"
 		The warning was re-added in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/a80fb2b1cad1bb9c868222b8c25f162d69a509e6>a80fb2b</denchmark-link>
  so I'm marking this as closed.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13431,Utumno,2017-10-01T13:44:27Z,2017-10-04T00:03:08Z,Windows nightly build Dataset.from_generator fails with pyfunc error,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows-7
 TensorFlow installed from (source or binary):pip
 TensorFlow version:1.4.0-dev20170929
 Python version: 3.5.2
 Bazel version (if compiling from source):-
 CUDA/cuDNN version:-
 GPU model and memory:-
 Exact command to reproduce:see below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 As described in the SO question <denchmark-link:https://stackoverflow.com/q/46511328/281545>https://stackoverflow.com/q/46511328/281545</denchmark-link>
  the code:
 import tensorflow as tf
 
 Dataset = tf.contrib.data.Dataset
 it2 = Dataset.range(5).make_one_shot_iterator()
 
 # Dataset.from_generator need tensorflow > 1.3 !
 with tf.Session() as sess:
     print(tf.__version__)
     def _dataset_generator():
         while True:
             try:
                 yield sess.run(it2.get_next())
             except tf.errors.OutOfRangeError:
                 return
     das_dataset = Dataset.from_generator(_dataset_generator, tf.int64)
     das_dataset_it = das_dataset.make_one_shot_iterator()
     while True:
         try:
             print(sess.run(das_dataset_it.get_next()))
         except tf.errors.OutOfRangeError:
             break
 fails with:
 <denchmark-code>C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\Scripts\python.exe C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py
 1.4.0-dev20170929
 2017-10-01 16:41:41.978576: W C:\tf_jenkins\home\workspace\tf-nightly-windows\M\windows\PY\35\tensorflow\core\framework\op_kernel.cc:1192] Invalid argument: 0-th value returned by pyfunc_0 is int32, but expects int64
 	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
 Traceback (most recent call last):
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1323, in _do_call
     return fn(*args)
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1302, in _run_fn
     status, run_metadata)
   File ""C:\_\Python35\lib\contextlib.py"", line 66, in __exit__
     next(self.gen)
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 467, in raise_exception_on_not_ok_status
     c_api.TF_GetCode(status.status))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64
 	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
 	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator_1)]]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""C:/Dropbox/eclipse_workspaces/python/zebra/so_46511328_from_generator.py"", line 19, in <module>
     print(sess.run(das_dataset_it.get_next()))
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 889, in run
     run_metadata_ptr)
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1120, in _run
     feed_dict_tensor, options, run_metadata)
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1317, in _do_run
     options, run_metadata)
   File ""C:\Dropbox\_\PyCharmVirtual\TF-Nigthly-2\lib\site-packages\tensorflow\python\client\session.py"", line 1336, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: 0-th value returned by pyfunc_0 is int32, but expects int64
 	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_INT64], token=""pyfunc_0""]()]]
 	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[<unknown>], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/cpu:0""](OneShotIterator_1)]]
 
 Process finished with exit code 1
 </denchmark-code>
 
 That's a problem in windows nightly - installing the nightly on an Ubuntu machine works:
 <denchmark-code>$ pipenv run python3 so_46511328_from_generator.py
 2017-10-01 13:34:21.840423: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
 1.4.0-dev20170929
 0
 1
 2
 3
 4
 2017-10-01 13:34:21.903201: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: StopIteration: Iteration finished.
 </denchmark-code>
 
  Maybe related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/8196>#8196</denchmark-link>
  ?
 	",1.0,Utumno,2017-10-02T19:55:56Z,"
 		Yes, I think is a result of  returning a different array type (when  is a Python ) on Windows and Linux. If I remember correctly, on Windows the array will have type  and on Linux the array will have type . This behavior in  is inherited from , which performs the NumPy conversion automatically (as discussed in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/8196>#8196</denchmark-link>
 ). To write platform independent code that handles this case, you should explicitly wrap the return value in a NumPy array.
 		",2.0,Utumno,2017-10-02T20:15:54Z,"
 		On second thoughts... I think this is really easy to fix. I'm working on a patch.
 		",,,,,9b027db459ff771c246a266ac3ec40cfbb4a63ce,Derek Murray,2017-10-02 16:30:57-07:00,MODIFY,3,tensorflow\contrib\data\python\kernel_tests\dataset_constructor_op_test.py,tensorflow\contrib\data\python\kernel_tests\dataset_constructor_op_test.py,1.0,"438,439,440,441",,MODIFY,3.0,tensorflow\contrib\data\python\ops\dataset_ops.py,tensorflow\contrib\data\python\ops\dataset_ops.py,,,,,,,,,,,,,1.0,"194,195,196","194,195",from_generator,"generator,output_types,output_shapes",100,254,MODIFY,3.0,tensorflow\python\data\ops\dataset_ops.py,tensorflow\python\data\ops\dataset_ops.py,1.0,"309,310,311","309,310",MODIFY,3.0,tensorflow\python\kernel_tests\dataset_constructor_op_test.py,tensorflow\python\kernel_tests\dataset_constructor_op_test.py,1.0,"437,438,439,440",,testFromGeneratorImplicitConversion.generator,,438,441,1.0,478,,testFromGeneratorTypeError,self,461,482,1.0,"437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459",454,testFromGeneratorImplicitConversion,self,437,459,,,,,,,,1.0,"194,195,196","194,195",from_generator.generator_map_fn,iterator_id_t,167,226,1.0,"194,195,196","194,195",from_generator.from_generator.generator_map_fn.generator_py_func,iterator_id,181,215,,,,,,,,,,,,,,,,,,,,,,,,,,from_generator.from_generator.generator_map_fn.generator_py_func,iterator_id,296,330,testFromGeneratorImplicitConversion.generator,,437,440,MODIFY,2.0,tensorflow\python\ops\script_ops.py,tensorflow\python\ops\script_ops.py,1.0,"60,72,77","60,76",_convert,"value,dtype",60,86,1.0,"60,72,77","60,76",_convert,value,60,85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"309,310,311","309,310",from_generator,"generator,output_types,output_shapes",215,369,1.0,"309,310,311","309,310",from_generator.generator_map_fn,iterator_id_t,282,341,,,,,,,,1.0,477,,testFromGeneratorTypeError,self,460,481,1.0,"436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458",453,testFromGeneratorImplicitConversion,self,436,458,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13506,kcolford,2017-10-05T18:06:45Z,2017-10-17T21:35:52Z,tf.image.pad_to_bounding_box crashes when passed bounds with dtype int64,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 TensorFlow installed from (source or binary):from pip in virtualenv
 TensorFlow version (use command below):v1.3.0-rc2-20-g0787eee 1.3.0
 Python version: '3.5.2 (default, Nov 17 2016, 17:05:23) \n[GCC 5.4.0 20160609]'
 Bazel version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 Exact command to reproduce:
 
 <denchmark-h:h2>Description</denchmark-h>
 
 Passing arguments of type int64 to tf.image.pad_to_bounding_box triggers a crash of the python interpreter. This is a bug because the type required by tf.image.pad_to_bounding_box not documented anywhere and just causes a crash with a cryptic error message.
 <denchmark-h:h2>Sources/Logs</denchmark-h>
 
 The following snippet crashes the whole python interpreter with a core dump.
 <denchmark-code>import tensorflow as tf
 i = tf.constant([0, 0, 3, 3], dtype=tf.int64)
 img = tf.ones([1,1,1], dtype=tf.float32)
 sess = tf.Session()
 sess.run(tf.image.pad_to_bounding_box(img, i[0], i[1], i[2], i[3]))
 </denchmark-code>
 
 And leaves the following
 <denchmark-code>2017-10-05 13:51:24.789715: F tensorflow/core/framework/tensor.cc:493] Check failed: dtype() == expected_dtype (9 vs. 3)
 </denchmark-code>
 
 	",1.0,kcolford,2017-10-05T21:54:21Z,"
 		this is bad, it shouldn't crash Python even if type is wrong
 		",2.0,kcolford,2017-10-06T05:23:31Z,"
 		The crash is caused by the fact that int64 was never registered in the kernel of tf.image.pad_to_bounding_box. Instead, the int64 of the Tpadding is directly used as int32 without appropriate processing.
 A PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/13517>#13517</denchmark-link>
  has been created to address the crash.
 		",3.0,kcolford,2017-10-09T07:26:04Z,"
 		Thanks for debugging and the PR <denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
  !
 Marking as ""Contributions Welcome"", though it seems the contribution has already been sent :)
 		",cbb705f10149a11b8d17182343ef12ab2dbfd7a8,Yong Tang,2017-10-17 14:35:51-07:00,MODIFY,3,tensorflow\core\kernels\pad_op.cc,tensorflow\core\kernels\pad_op.cc,1.0,"85,87,88,89","85,87,88",MODIFY,4.0,tensorflow\core\kernels\pad_op.h,tensorflow\core\kernels\pad_op.h,,,,,,,,,,,,,1.0,34,34,tensorflow::functor::Pad::operator ( ),"d,output,input,paddings,pad_value",32,42,MODIFY,0.0,tensorflow\core\kernels\pad_op_gpu.cu.cc,tensorflow\core\kernels\pad_op_gpu.cu.cc,0.0,"29,30,31,32,33,34,35,36,37,38,39,40","29,30,31,32,33,34,35,36",MODIFY,1.0,tensorflow\python\kernel_tests\pad_op_test.py,tensorflow\python\kernel_tests\pad_op_test.py,1.0,"287,288,289,290,291,292,293,294,295",,tensorflow::PadOp::Compute,context,50,140,1.0,"146,150,154","145,149,153",tensorflow::PadOp::Operate,"context,input,paddings,pad_value,output",144,157,1.0,"146,150,154","145,149,153",tensorflow::PadOp::Operate,"context,input,paddings,pad_value,output",143,156,,,,,,,,1.0,50,50,"tensorflow::functor::Pad<Device,T,0>::operator ( )","d,output,input,array,T",48,52,1.0,34,34,tensorflow::functor::Pad::operator ( ),"d,output,input,paddings,pad_value",32,42,1.0,50,50,"tensorflow::functor::Pad<Device,T,Tpadding,0>::operator ( )","d,output,input,array,T",48,52,,,,,,,,,,,,,,,,,,,,,,,testPadTypes,self,287,295,MODIFY,1.0,tensorflow\python\ops\image_ops_test.py,tensorflow\python\ops\image_ops_test.py,1.0,"1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394",,testInt64,self,1377,1394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13526,tsoernes,2017-10-06T15:00:17Z,2017-10-06T17:27:49Z,Importing TF in Python yields 'cannot import name 'build_info',"
 <denchmark-h:h3>System information</denchmark-h>
 
 Fedora 26 x64 (4.13.4-200.fc26.x86_64)
 Tensorflow installed from source:
 <denchmark-code>tf.VERSION = 1.3.0
 tf.GIT_VERSION = b'v1.3.0-rc1-3011-gd86448938'
 tf.COMPILER_VERSION = b'v1.3.0-rc1-3011-gd86448938'
 </denchmark-code>
 
 Python version 3.6.2 (Anaconda)
 Bazel installed from their Fedora/COPR repositories, version 0.6.0- (@non-git)
 No CUDA (or compatible GPU)
 Intel MKL 2018.0.128
 c++ (GCC) 7.2.1 20170915 (Red Hat 7.2.1-2)
 bazel build -c opt --config=mkl //tensorflow/tools/pip_package:build_pip_package
 Notice the mkl flag in the bazel build
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Configuration and bazel build finished without error. When attempting to import tensorflow in Python, I get this:
 <denchmark-code>>>> import tensorflow as tf
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/home/torstein/progs/tensorflow/tensorflow/__init__.py"", line 24, in <module>
     from tensorflow.python import *
   File ""/home/torstein/progs/tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
     from tensorflow.python import pywrap_tensorflow
   File ""/home/torstein/progs/tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
     from tensorflow.python.platform import self_check
   File ""/home/torstein/progs/tensorflow/tensorflow/python/platform/self_check.py"", line 24, in <module>
     from tensorflow.python.platform import build_info
 ImportError: cannot import name 'build_info'
 </denchmark-code>
 
 	",1.0,tsoernes,2017-10-06T15:38:35Z,"
 		Does this only affect --config=mkl builds?
 		",2.0,tsoernes,2017-10-06T15:43:02Z,"
 		Actually, is /home/torstein/progs/tensorflow the directory in which you built TensorFlow?
 		",3.0,tsoernes,2017-10-06T15:51:04Z,"
 		Yes, that is correct
 
 2017-10-06 17:50 GMT+02:00 Derek Murray <notifications@github.com>:
 <denchmark-link:#>…</denchmark-link>
 
 
  Actually, is /home/torstein/progs/tensorflow the directory in which you
  built TensorFlow?
 
  —
  You are receiving this because you authored the thread.
  Reply to this email directly, view it on GitHub
  <#13526 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AGd9xEM_oDuRzi8cXHvsR866ewHp1C_fks5spkwvgaJpZM4PwnOJ>
  .
 
 
 
 		",251a1e70dc04b10fb25e8013d1ad1f27d5eda30b,Derek Murray,2017-10-06 10:27:49-07:00,MODIFY,0,tensorflow\python\platform\self_check.py,tensorflow\python\platform\self_check.py,0.0,"24,25,26,27,28,29,30",24,,,,,4.0,tsoernes,2017-10-06T15:54:07Z,"
 		OK, I think it should work if you cd out of that directory. build_info.py is a generated file and from the stack trace it looks like Python is trying to import tensorflow from the source tree, which doesn't contain any generated files.
 This has tripped us up before, and  recently became the first generated file that we attempt to import. I've sent PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/13528>#13528</denchmark-link>
  to add a better error message.
 		",5.0,tsoernes,2017-10-06T19:28:03Z,"
 		You were right. Starting Python from another directory does not result in the error.
 Thank you
 		",6.0,tsoernes,2017-10-06T19:42:37Z,"
 		Thanks for confirming (and for raising the issue in the first place... now we have a much better error message in place)!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,tsoernes,2017-12-13T12:44:49Z,"
 		Hi,
 can anyone tell me how can fix this error  i am runnig manage.py file but i am getting this error
 C:\Users\Dhanesh\Desktop\Check it Please\ChatterBot-master\examples\django_app>python manage.py
 Traceback (most recent call last):
 File ""manage.py"", line 10, in 
 execute_from_command_line(sys.argv)
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\django\core\management_init_.py"", line 350, in execute_from_command_line
 utility.execute()
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\django\core\management_init_.py"", line 324, in execute
 django.setup()
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\django_init_.py"", line 18, in setup
 apps.populate(settings.INSTALLED_APPS)
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\django\apps\registry.py"", line 85, in populate
 app_config = AppConfig.create(entry)
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\django\apps\config.py"", line 116, in create
 mod = import_module(mod_path)
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\importlib_init_.py"", line 126, in import_module
 return _bootstrap._gcd_import(name[level:], package, level)
 File """", line 986, in _gcd_import
 File """", line 969, in _find_and_load
 File """", line 944, in _find_and_load_unlocked
 File """", line 222, in _call_with_frames_removed
 File """", line 986, in _gcd_import
 File """", line 969, in _find_and_load
 File """", line 958, in _find_and_load_unlocked
 File """", line 664, in load_unlocked
 File """", line 634, in load_backward_compatible
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\chatterbot-0.7.6-py3.5.egg\chatterbot_init.py"", line 4, in 
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\chatterbot-0.7.6-py3.5.egg\chatterbot\chatterbot.py"", line 4, in 
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\chatterbot-0.7.6-py3.5.egg\chatterbot\input_init.py"", line 2, in 
 File ""C:\Users\Dhanesh\AppData\Local\Programs\Python\Python35\lib\site-packages\chatterbot-0.7.6-py3.5.egg\chatterbot\input\microsoft.py"", line 4, in 
 ImportError: cannot import name 'Statement'
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13536,bdaskalov,2017-10-06T20:53:52Z,2017-10-18T05:45:06Z,BeamSearchDecoder incorrectly truncates results when used with dynamic_decode,"
 <denchmark-h:h3>System information (irrelevant for this bug)</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04/Any
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0
 Python version: Python 3.5.2 :: Continuum Analytics, Inc.
 Bazel version (if compiling from source): N/A
 CUDA/cuDNN version: irrelevant
 GPU model and memory: irrelevant
 Exact command to reproduce: irrelevant
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 tf.contrib.seq2seq.BeamSearchDecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step.
 The root of the problem is that the while_loop body in dynamic_decode assumes that sequences are independent and will finish only once. In the same time BeamSearchDecoder creates a tree-like structure where a beam index can be reused in a later step for a state that originates from a different parent index.  This causes the decoding loop to sometimes record the wrong sequence length for a beam member. Then this wrong sequence length is passed to BeamSearchDecoder.finalize which returns a truncated sequence.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 I use the following code to workaround the problem. This causes the right sequence to be returned but still the length returned by dynamic_decode is wrong.
 class FixedBeamSearchDecoder(seq2seq.BeamSearchDecoder):
     def finalize(self, outputs, final_state, sequence_lengths):
         # BeamSearchDecoder does not follow the correct semantics of the the finished flag
         # which results in taking wrong length here and getting wrong decoded string.
         # We substitute the sequence length recorded by dynamic_decoder (which is wrong because
         # of the wrong finished flag returned by BeamSearchDecoder.step) with the length
         # recorded in BeamSearchState which is correct.
         return super().finalize(outputs, final_state, final_state.lengths)
 	",1.0,bdaskalov,2017-10-06T20:55:47Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  Can you take a look? I see that you wrote the seq2seq library. I wanted to submit a fix but I don't see how to correct this problem without changing some of the library's public inteface.
 		",2.0,bdaskalov,2017-10-06T21:47:37Z,"
 		Seems ok to update the BeamSearchDecoder.finalize to use final_state.lengths instead of sequence_lengths -- looks like this fixes a couple of other open issues.
 We could consider having finalize return new updated sequence lengths to decode_dynamic as well.
 		",3.0,bdaskalov,2017-10-06T21:48:02Z,"
 		Thanks for catching this!  Could you send a PR with the fix and a unit test that catches it?
 		",18f89c81d288f191abd56501ec6f86fe29265bdd,Eugene Brevdo,2017-10-17 08:56:25-07:00,MODIFY,3,tensorflow\contrib\seq2seq\kernels\beam_search_ops.cc,tensorflow\contrib\seq2seq\kernels\beam_search_ops.cc,1.0,"108,109,110,111,112,113,114,117,118,122,123,124,127,128,129,130,141,142,143,144","102,103,104,105,106,107,108,111,115,116,119,120,121,122,140",MODIFY,0.0,tensorflow\contrib\seq2seq\kernels\beam_search_ops.h,tensorflow\contrib\seq2seq\kernels\beam_search_ops.h,4.0,bdaskalov,2017-10-13T17:14:14Z,"
 		Will look into submitting a fix.
 		",5.0,bdaskalov,2017-10-15T02:17:02Z,"
 		Sorry, I've been meaning to make a PR last week but never got to it.
 		",6.0,bdaskalov,2017-10-15T02:19:07Z,"
 		No problem. We're evaluating your change internally.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Sat, Oct 14, 2017, 7:17 PM bdaskalov ***@***.***> wrote:
  Sorry, I've been meaning to make a PR last week but never got to it.
 
  —
  You are receiving this because you were assigned.
  Reply to this email directly, view it on GitHub
  <#13536 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/ABtimxRoy93pElky6ZtzF-ctLOu6Khocks5ssWs8gaJpZM4PxBUM>
  .
 
 
 
 		",0.0,"34,35","34,35",,,,,MODIFY,4.0,tensorflow\contrib\seq2seq\kernels\beam_search_ops_gpu.cu.cc,tensorflow\contrib\seq2seq\kernels\beam_search_ops_gpu.cu.cc,1.0,"32,33,38,39,40,41,42,46,49","32,37,38,42,45",MODIFY,0.0,tensorflow\contrib\seq2seq\ops\beam_search_ops.cc,tensorflow\contrib\seq2seq\ops\beam_search_ops.cc,0.0,"28,29,33,37,40,41,43,45,46,47,48,64,65","28,32,36,39,40,41,42,43,46,47,48,64","tensorflow::functor::GatherTree<CPUDevice,int32>::operator ( )","ctx,d,step_ids,parent_ids,sequence_length,beams",101,144,1.0,"108,109,110,111,112,113,114,117,118,122,123,124,127,128,129,130,141,142,143,144,145,146,147,148,156","107,108,111,115,116,119,120,121,122,140,151,152,153,154,155,156,157,158","tensorflow::functor::GatherTree<CPUDevice,int32>::operator ( )","ctx,d,step_ids,parent_ids,max_sequence_lengths,end_token,beams",107,160,1.0,"52,53,59,61,62,63,64,65,66,73,74,75,76,77,78,79,80,85,86,87,89,91","52,58,59,60,61,62,63,64,65,66,67,69,70,71,82,85",tensorflow::GatherTreeOp::Compute,ctx,48,92,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,bdaskalov,2017-10-16T19:26:44Z,"
 		The problem is deeper and the solution requires some additional changes.  I'll try to submit something in the next couple days.
 		",tensorflow::functor::GatherTreeOpKernel,"batch_size,max_time,beam_width,step_ids,parent_ids,sequence_length,beams",29,58,,,,,MODIFY,1.0,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_decoder_test.py,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_decoder_test.py,1.0,"57,58,65,66,67,68","57,58,65",test_gather_tree,self,44,73,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,4.0,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_ops_test.py,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_ops_test.py,1.0,"43,47,48,49,50","41,45,46",testGatherTreeOne,self,37,52,1.0,"84,89,90,91,92","78,83,84,89,90,93,94",testBadParentValuesOnGPU,self,73,94,MODIFY,2.0,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,tensorflow\contrib\seq2seq\python\ops\beam_search_decoder.py,1.0,"320,321,324,326,328,329,330,331,333,334,335,336","313,314",finalize,"self,outputs,final_state,sequence_lengths",311,339,8.0,bdaskalov,2018-04-30T03:37:35Z,"
 		Could anyone tell me when this bug was fixed. I couldn't find it in the release notes. Thank you! <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 
 		",,,,,,,,,,,,,,,1.0,"62,63,64,65,66,67,68,78,79","65,66,78","tensorflow::functor::GatherTree<GPUDevice,T>::operator ( )","ctx,d,step_ids,parent_ids,sequence_length,beams",62,81,1.0,"78,79,91,92,93,94",78,"tensorflow::functor::GatherTree<GPUDevice,T>::operator ( )","ctx,d,step_ids,parent_ids,max_sequence_length,end_token,beams",75,97,1.0,"32,33,38,39,40,41,42,46,49,60,61,62,63,64,65,66,67,68","32,37,38,42,45,65,66",tensorflow::functor::GatherTreeOpKernel,"batch_size,max_time,beam_width,step_ids,parent_ids,max_sequence_lengths,end_token,beams",29,71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"61,64,65,66,67","57,60,61",testBadParentValuesOnCPU,self,54,71,1.0,"257,258,259,260,261,262,263,264,265,266,267,268",,tracks_own_finished,self,257,268,,,,,,,,,,,,,,,MODIFY,3.0,tensorflow\contrib\seq2seq\python\ops\decoder.py,tensorflow\contrib\seq2seq\python\ops\decoder.py,1.0,"255,256,257,258",,,,,,,,,9.0,bdaskalov,2018-04-30T15:21:24Z,"
 		It was first released in TensorFlow 1.5.
 		",10.0,bdaskalov,2018-04-30T17:16:08Z,"
 		<denchmark-link:https://github.com/guillaumekln>@guillaumekln</denchmark-link>
  Thank you for the info!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"97,98,99,100,101,104,105,106,107,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135","96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135",testGatherTreeBatch,self,96,135,body,"time,outputs_ta,state,inputs,finished,sequence_lengths",237,299,1.0,"103,104,105,106","103,104,105,106",step,"self,time,inputs,state,name",90,108,1.0,"114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131",,tracks_own_finished,self,114,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13558,yaroslavvb,2017-10-07T23:40:22Z,2018-01-03T19:31:07Z,segfaults in GPU tf.matrix_inverse,"
 I'm running into segfaults in tf.matrix_inverse
 I'm adding identity*0.001 so matrices should be invertible, and same procedure works fine in numpy and in TensorFlow CPU version.
 <denchmark-link:https://github.com/yaroslavvb/stuff/blob/master/inverse_segfault.py>https://github.com/yaroslavvb/stuff/blob/master/inverse_segfault.py</denchmark-link>
 
 
 This non-deterministically crashes after 1-2 seconds with various backtraces.
 IE
 <denchmark-code>#0  0x0000000000000001 in ?? ()
 #1  0x00007fe90ed9c652 in tensorflow::Tensor::TotalBytes() const ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 #2  0x00007fe90ed9c7d6 in tensorflow::Tensor::tensor_data() const ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 #3  0x00007fe9137adda3 in bool tensorflow::internal::TransposeUsingTile<unsigned int>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #4  0x00007fe9137a696c in tensorflow::Status tensorflow::DoTranspose<Eigen::GpuDevice>(Eigen::GpuDevice const&, tensorflow::Tensor const&, tensorflow::gtl::ArraySlice<int>, tensorflow::Tensor*) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #5  0x00007fe911ada0fd in tensorflow::SvdOpGpu<float>::PerformSVD_MgeqN(tensorflow::OpKernelContext*, std::function<void ()>, long long, long long, long long, tensorflow::gtl::ArraySlice<int> const&, tensorflow::Tensor const&, tensorflow::Tensor*, tensorflow::Tensor*, tensorflow::Tensor*) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #6  0x00007fe911ade897 in tensorflow::SvdOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #7  0x00007fe90f20790b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 #8  0x00007fe90f23cf37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)
     ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 </denchmark-code>
 
 or this
 <denchmark-code>#0  0x00007fa89090686a in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
 #1  0x00007fa89091b074 in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
 #2  0x00007fa890826e2c in ?? () from /usr/lib/x86_64-linux-gnu/libcuda.so.1
 #3  0x00007fa890978880 in cuLaunchKernel ()
    from /usr/lib/x86_64-linux-gnu/libcuda.so.1
 #4  0x00007fa891bf1dc1 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #5  0x00007fa891c0f9cd in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #6  0x00007fa891aa1132 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #7  0x00007fa891aa2b72 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #8  0x00007fa891aa32e3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #9  0x00007fa891aa36fa in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #10 0x00007fa89190f5f3 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #11 0x00007fa891912375 in ?? () from /usr/local/cuda/lib64/libcusolver.so.8.0
 #12 0x00007fa89aa82c50 in tensorflow::Status tensorflow::CudaSolver::Getrf<float>(int, int, float*, int, int*, int*) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #13 0x00007fa89a55f5d6 in tensorflow::MatrixInverseOpGpu<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
 #14 0x00007fa897dce90b in tensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 #15 0x00007fa897e03f37 in tensorflow::(anonymous namespace)::ExecutorState::Process(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)
     ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 #16 0x00007fa897df1ec5 in std::_Function_handler<void (), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> (tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)> >::_M_invoke(std::_Any_data const&) ()
    from /home/yaroslav/anaconda3/envs/oct10/lib/python3.5/site-packages/tensorflow/python/../libtensorflow_framework.so
 
 </denchmark-code>
 
 TensorFlow commit: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/22a886b>22a886b</denchmark-link>
 
 NVIDIA-SMI 381.09
 libcudart.so.8.0.44
 libcudnn.so.6.0.21
 Nvidia GTX 1080
 	",1.0,yaroslavvb,2017-10-09T07:09:42Z,"
 		<denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  : Would you have some cycles to look into this?
 		",2.0,yaroslavvb,2017-12-20T01:11:21Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",3.0,yaroslavvb,2018-01-03T19:04:31Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",3629fc4e98254c37e614ac3f77fa250b75c70f8d,codrut3,2017-12-28 17:38:27-08:00,MODIFY,1,tensorflow\core\kernels\matrix_inverse_op.cc,tensorflow\core\kernels\matrix_inverse_op.cc,1.0,213,213,,,,,4.0,yaroslavvb,2018-01-03T19:31:07Z,"
 		From the comment on the PR, it looks like the mutex was added.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::MatrixInverseOpGpu::ComputeAsync,"context,done",98,263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13576,zishuaiz,2017-10-09T04:53:41Z,2017-10-10T21:17:27Z,sparse_softmax_cross_entropy_with_logits wrong annotation,"
 
 
 
 tensorflow/tensorflow/python/ops/nn_ops.py
 
 
          Line 1661
       in
       107cc77
 
 
 
 
 
 
        of the labels is not equal to the rank of the labels minus one. 
 
 
 
 
 
 It should be If logits are scalars (need to have rank >= 1) or if the rank of the labels is not equal to the rank of the logits minus one.
 	",1.0,zishuaiz,2017-10-09T07:05:14Z,"
 		Thanks for pointing that out, will send a fix.
 		",,,,,,,,,edfb9bb100f9814bf1bbcff2e8a32f12f049bfcc,Asim Shankar,2017-10-09 09:00:18-07:00,MODIFY,0,tensorflow\python\ops\nn_ops.py,tensorflow\python\ops\nn_ops.py,0.0,1847,1847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13764,vishvananda,2017-10-17T00:17:31Z,2017-11-04T03:09:38Z,Failure in TestNewTensor when running go test,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 TensorFlow installed from (source or binary): source (branch 1.4)
 TensorFlow version (use command below): 1.4.0-dev
 Python version: 3.5
 Bazel version (if compiling from source): 5.4.0
 CUDA/cuDNN version: 8.0/6.0
 GPU model and memory: nVidia 1080Ti 11G
 Exact command to reproduce: go test -v github.com/tensorflow/tensorflow/tensorflow/go
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 I'm trying to use the go bindings to the tensorflow c library. When I run the tests, I get a nil pointer dereference and a segfault. The details are below. Note that I've built the c library from source using the following options:
 bazel build -c opt --config=cuda --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 -c opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow:libtensorflow.so
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 When I run go test -v github.com/tensorflow/tensorflow/tensorflow/go I get the following error:
 <denchmark-code>2017-10-16 17:12:30.568054: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
 2017-10-16 17:12:30.568065: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
 --- FAIL: TestNewTensor (0.00s)
 panic: runtime error: invalid memory address or nil pointer dereference [recovered]
         panic: runtime error: invalid memory address or nil pointer dereference
 [signal SIGSEGV: segmentation violation code=0x1 addr=0x0 pc=0x536098]
 
 goroutine 168 [running]:
 testing.tRunner.func1(0xc42059c4e0)
         /usr/lib/go-1.8/src/testing/testing.go:622 +0x29d
 panic(0x6a0b80, 0xa18e80)
         /usr/lib/go-1.8/src/runtime/panic.go:489 +0x2cf
 github.com/tensorflow/tensorflow/tensorflow/go.tensorData(0x7fa8f40195b0, 0xc420595900, 0x688a80, 0x6ffb90)
         /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:209 +0x48
 github.com/tensorflow/tensorflow/tensorflow/go.NewTensor(0x683d20, 0xc4205945e0, 0xc42004d9a0, 0x2, 0x2)
         /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor.go:92 +0x221
 github.com/tensorflow/tensorflow/tensorflow/go.TestNewTensor(0xc42059c4e0)
         /home/vishvananda/go/src/github.com/tensorflow/tensorflow/tensorflow/go/tensor_test.go:92 +0x2526
 testing.tRunner(0xc42059c4e0, 0x6ffbd0)
         /usr/lib/go-1.8/src/testing/testing.go:657 +0x96
 created by testing.(*T).Run
         /usr/lib/go-1.8/src/testing/testing.go:697 +0x2ca
 exit status 2
 FAIL    github.com/tensorflow/tensorflow/tensorflow/go  0.443s
 </denchmark-code>
 
 Adding some debugging, it turns out that the TestNewTensor test fails when attempting to create the following tensor {[]int64{2, 0}, [][]int64{{}, {}}}. If I comment out that line, the tests pass.
 	",1.0,vishvananda,2017-10-17T00:40:19Z,"
 		It looks like this is due to the fact that TF_TensorData returns nil if no data is allocated. Assuming this is correct behavior and nil needs to be checked for on the go side then the following patch fixes the problem:
 <denchmark-code>diff --git a/tensorflow/go/tensor.go b/tensorflow/go/tensor.go
 index e8fa21a..6cbf759 100644
 --- a/tensorflow/go/tensor.go
 +++ b/tensorflow/go/tensor.go
 @@ -205,6 +205,9 @@ func (t *Tensor) WriteContentsTo(w io.Writer) (int64, error) {
  func tensorData(c *C.TF_Tensor) []byte {
         // See: https://github.com/golang/go/wiki/cgo#turning-c-arrays-into-go-slices
         cbytes := C.TF_TensorData(c)
 +       if cbytes == nil {
 +               return nil
 +       }
         length := int(C.TF_TensorByteSize(c))
         slice := (*[1 << 30]byte)(unsafe.Pointer(cbytes))[:length:length]
         return slice
 </denchmark-code>
 
 		",2.0,vishvananda,2017-10-17T01:38:26Z,"
 		Thanks for the report <denchmark-link:https://github.com/vishvananda>@vishvananda</denchmark-link>
 . I'm unable to reproduce the problem using the 1.3.0 release binary, or <denchmark-link:https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-gpu-linux-x86_64-1.3.0.tar.gz>1.4.0-rc0 release binary</denchmark-link>
  (will try rebuilding from source using the exact flags you're using). Do you see the same problem when using the release binaries of the C API?
 Which version of go are you using? Also, is it possible that LD_LIBRARY_PATH is somehow bringing in an older version of the C API libraries that your go program ends up using?
 It should be okay for TF_TensorData to return nil.
 Any additional information in reproducing the environment will be helpful. (I'll try to dig into this a bit more by rebuilding from source using the command you provided above)
 		",3.0,vishvananda,2017-10-17T04:41:14Z,"
 		Fascinating, both the 1.3.0 and the 1.4.0-rc0 release binary return zero length from TF_TensorData but they return a pointer to an actual buffer instead of nil. I'm attempting my flags on the 1.3 branch to see if it is the flags that are causing it to return nil. Next, I'll try removing the extra flags one at a time to see if I can narrow it down. I suspect AllocateTensor ends up with a nil buffer in certain cases. In the successful versions I don't see this error message:
 <denchmark-code>2017-10-16 21:31:57.797656: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
 2017-10-16 21:31:57.797690: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
 </denchmark-code>
 
 In any case the nil check is probably good to have anyway.
 		",db10718b38b2884cb5ed46d33c135c079f649d16,Vish (Ishaya) Abrams,2017-11-03 20:09:38-07:00,MODIFY,1,tensorflow\go\tensor.go,tensorflow\go\tensor.go,1.0,"210,211,212",,,,,,4.0,vishvananda,2017-10-17T05:42:38Z,"
 		Ok, I think I've tracked down the issue to building with MKL. If I build without --config=mkl the tests pass fine. The issue is that building with MKL uses the bfc_allocator to allocate memory. That allocator explicitly returns nil when an allocation of zero bytes is performed (As a side note, this probably should not be an error or warning if we expect it to happen when we request a zero length tensor). In the case of running without MKL, the allocation eventually calls malloc (or jemalloc or alloc_aligned). The man tells me that malloc(0) is implementation defined and our version returns a non-nil pointer so the code does not throw a null-pointer exception. I suggest using something like the patch I included above for the go side, and maybe downgrading the error and warning messages in bfc_allocator.cc and allocator_retry.cc to something a bit less scary (maybe Info?).
 		",5.0,vishvananda,2017-10-17T06:07:44Z,"
 		Thanks for the detailed trackdown <denchmark-link:https://github.com/vishvananda>@vishvananda</denchmark-link>
 , much appreciated.
 Yes, what you said makes sense. For the Go side, would you like to contribute a pull request to make the fix? If not, let me know and I'm happy to make the change as well.
 Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,tensorData,TF_Tensor,207,216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13827,Ouwen,2017-10-19T03:47:14Z,2017-12-21T15:19:12Z,"Tensorflow 1.3: tf.constant with dtype=[float32, float64, float16] may have inconsistent behavior.","
 <denchmark-h:h3>System information</denchmark-h>
 
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Ubuntu 16.04 with docker running gcr.io/tensorflow/tensorflow:latest
 
 
 TensorFlow installed from (source or binary): NA
 
 
 TensorFlow version (use command below): ('v1.3.0-rc2-20-g0787eee', '1.3.0')
 
 
 Python version: 2.7
 
 
 Bazel version (if compiling from source): NA
 
 
 CUDA/cuDNN version: NA
 
 
 GPU model and memory: NA
 
 
 Exact command to reproduce:
 
 
 <denchmark-code>// works
 test = numpy.array([1,2,3,4,5,6, None], dtype=numpy.float32)
 sess = tf.Session()
 print(sess.run(tf.constant(test, dtype=tf.float32)))
 </denchmark-code>
 
 <denchmark-code>// works
 sess = tf.Session()
 print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, None], dtype=tf.float16)))
 </denchmark-code>
 
 <denchmark-code>// returns error
 sess = tf.Session()
 print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, None], dtype=tf.float32)))
 // TypeError: Expected float32, got None of type '_Message' instead.
 </denchmark-code>
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 A tensorflow constant with None in array with dtype float32, float64 seem to throw an error. However, if they are first wrapped by a numpy array, none is accepted and turned into NaN. This behavior seems inconsistent.
 	",1.0,Ouwen,2017-10-19T04:00:38Z,"
 		/CC <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
 
 That's unfortunate. Would you like to contribute a fix?
 		",2.0,Ouwen,2017-10-19T04:32:16Z,"
 		I'd love to take a stab at contributing. From a brief glance i'm wondering if the convert_to_tensor function may be culprit. Glad to take any pointers as well.
 		",3.0,Ouwen,2017-10-19T07:44:51Z,"
 		So here is what I've found after looking through the code. This error is propagated from tensorflow version 0.5.0 to 1.4.0-rc0
   values pass without any problems because they are not checked in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/tensor_util.py#L273-L291>_TF_TO_IS_OK</denchmark-link>
 .  values are thus not stopped by the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/tensor_util.py#L376>_AssertCompatible</denchmark-link>
  call, where they continue onward to be transformed into  values by <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/tensor_util.py#L377>np.array</denchmark-link>
 
 For , and   types,  values are caught by the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/tensor_util.py#L294-L303>_AssertCompatible</denchmark-link>
 , and since they are not an instance of <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/tensor_util.py#L243>compat.real_types</denchmark-link>
   a  is raised.
 So while the following produces an error:
 <denchmark-code>sess = tf.Session()
 print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, None], dtype=tf.float32)))
 // TypeError: Expected float32, got None of type '_Message' instead.
 </denchmark-code>
 
 Using explicit nan will not cause an error
 <denchmark-code>sess = tf.Session()
 print(sess.run(tf.constant([1, 2, 3, 4, 5, 6, float('nan')], dtype=tf.float32)))
 </denchmark-code>
 
 Since the  abstraction states the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.3/tensorflow/python/framework/ops.py#L575-L578>following are equivalent</denchmark-link>
 
 <denchmark-code>value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))
 value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])
 value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))
 </denchmark-code>
 
 I believe that the below should be equivalent as well
 <denchmark-code>value_a = my_func(tf.constant([[1.0, 2.0], [3.0, None]], dtype=tf.float32))
 value_b = my_func(np.array([[1.0, 2.0], [3.0, None]], dtype=np.float32))
 </denchmark-code>
 
 Given what has been found, I would like to
 
 
 Update _FilterFloat to allow for None values.
 
 
 Add tf.float16 into the  _TF_TO_IS_OK dictionary with the _FilterFloat function
 
 
 Add a test into ops_test.py named testConvertToTensorFloatNoneValue
 
 
 Please let me know if this is a good plan assuming nothing breaks.
 		",c43d777b56a17832f7de288d0fe966bf537ffeb7,Ouwen Huang,2017-11-08 22:49:31-08:00,MODIFY,1,tensorflow\python\framework\ops.py,tensorflow\python\framework\ops.py,1.0,"866,867,868,869",,MODIFY,0.0,tensorflow\python\framework\tensor_util.py,tensorflow\python\framework\tensor_util.py,4.0,Ouwen,2017-10-19T16:26:37Z,"
 		Thanks Ouwen! This is the best kind of bug report.
 I'm not an expert on this part of TensorFlow, but this all looks pretty reasonable. Please send a PR after you fix that missing (self).
 		",5.0,Ouwen,2017-10-19T16:48:35Z,"
 		Appreciate it <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
 , here is the PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/13834>#13834</denchmark-link>
 
 		",6.0,Ouwen,2017-12-20T19:24:06Z,"
 		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue? Please update the label and/or status accordingly.
 		",0.0,289,,,,,,,,,,,,,,,,,,,,convert_to_tensor,"value,dtype,name,preferred_dtype",840,894,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,Ouwen,2017-12-21T15:19:12Z,"
 		This is fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/pull/13834>#13834</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13885,amirj,2017-10-21T17:39:20Z,2017-12-21T15:27:25Z,tf.reduce_mean is not compatible with np.mean,"
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/reduce_mean>tf.reduce_mean</denchmark-link>
  emphasized that this function is compatible with numpy:
 
 Equivalent to np.mean
 
 But it doesn't in the output type. Consider the following code for example:
 <denchmark-code>import tensorflow as tf
 x = tf.Variable([1, 0, 1, 0])
 init = tf.global_variables_initializer()
 sess = tf.Session()
 sess.run(init)
 print(sess.run(tf.reduce_mean(x)))
 
 </denchmark-code>
 
 The output is zero. It seems that tf.reduce_mean infer the output type from the input tensor because casting the input tensor to float values, solve the problem. This attribute is not compatible to np.mean:
 <denchmark-code>import numpy as np
 print(np.mean([1,0,0,1]))
 </denchmark-code>
 
 <denchmark-h:h3>System information</denchmark-h>
 
 
 OS Platform and Distribution: Linux Ubuntu 16.04
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): 1.3
 Python version: 3.6
 
 	",1.0,amirj,2017-10-21T18:10:49Z,"
 		In numpy,  has a  that could be used to specify the output type:
 <denchmark-link:https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html>https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.mean.html</denchmark-link>
 
 By default this is dtype=float64 for integer types and same type as input for non-integer types.
 It might be possible to add a dtype in tf.reduce_mean though a type cast is always needed I assume.
 		",2.0,amirj,2017-10-21T19:22:42Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  should we update the numpy compat docstring? Or encourage a contribution?
 /CC <denchmark-link:https://github.com/aselle>@aselle</denchmark-link>
 
 		",3.0,amirj,2017-10-21T21:07:58Z,"
 		<denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
   is an optional parameter in both TF and Numpy. I think it should be clear in the document or adapt to Numpy. It's my honer to contribute in both cases.
 		",a43f911e103aa5910d4e2405d77bdee8f9314fac,Amir H. Jadidinejad,2017-11-08 17:14:50-08:00,MODIFY,0,tensorflow\python\ops\math_ops.py,tensorflow\python\ops\math_ops.py,0.0,"1436,1437,1438,1439,1440,1441,1442,1443,1444,1445",,,,,,4.0,amirj,2017-10-23T13:59:24Z,"
 		Sure, if you want to update the @compatability(numpy) note we'd welcome the PR.
 But I don't think this is enough of a bug that we can break backwards compatibility to fix it.
 		",5.0,amirj,2017-10-24T06:30:45Z,"
 		<denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  I'd like to send a PR to add the  argument for  funciton.
 		",6.0,amirj,2017-10-24T18:06:45Z,"
 		<denchmark-link:https://github.com/DjangoPeng>@DjangoPeng</denchmark-link>
  sounds good, it'll have to go to api review. Thank you!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,amirj,2017-12-20T19:23:32Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,amirj,2017-12-21T15:27:24Z,"
 		Fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/pull/13941>#13941</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14292,ghendrymsft,2017-11-06T15:10:05Z,2018-04-26T18:29:20Z,Can't import contrib.boosted_trees,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 == cat /etc/issue ===============================================
 Linux 5508912-0913 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
 VERSION=""16.04.3 LTS (Xenial Xerus)""
 VERSION_ID=""16.04""
 VERSION_CODENAME=xenial
 
 == are we in docker =============================================
 No
 == compiler =====================================================
 c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
 Copyright (C) 2015 Free Software Foundation, Inc.
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 == uname -a =====================================================
 Linux 5508912-0913 4.4.0-43-Microsoft <denchmark-link:https://github.com/tensorflow/tensorflow/issues/1>#1</denchmark-link>
 -Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
 == check pips ===================================================
 numpy (1.13.3)
 protobuf (3.4.0)
 tensorflow (1.4.0)
 tensorflow-tensorboard (0.4.0rc1)
 == check for virtualenv =========================================
 False
 == tensorflow import ============================================
 tf.VERSION = 1.4.0
 tf.GIT_VERSION = v1.4.0-0-gd752244
 tf.COMPILER_VERSION = v1.4.0-0-gd752244
 Sanity check: array([1], dtype=int32)
 == env ==========================================================
 LD_LIBRARY_PATH is unset
 DYLD_LIBRARY_PATH is unset
 == nvidia-smi ===================================================
 ../../tf_env_collect.sh: line 105: nvidia-smi: command not found
 == cuda libs  ===================================================
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Can't import the boosted_trees module.
 Boosted_trees isn't properly listed in contrib/init.py, so I get:
 
 
 
 import tensorflow as tf
 est = tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeClassifier()
 Traceback (most recent call last):
 File """", line 1, in 
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py"", line 54, in getattr
 return getattr(module, item)
 AttributeError: 'module' object has no attribute 'boosted_trees'
 
 
 
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 See above.
 	",1.0,ghendrymsft,2017-11-06T15:59:50Z,"
 		<denchmark-link:https://github.com/yk5>@yk5</denchmark-link>
  <denchmark-link:https://github.com/tkoeppe>@tkoeppe</denchmark-link>
  Any particular reason why boosted_trees wouldn't be listed in ?
 		",2.0,ghendrymsft,2017-11-06T16:12:57Z,"
 		ask thomaswc@, nponomoreva@, or soroush@.  There may still be some loose ends left in the migration from learning/lib to third_party.
 		",3.0,ghendrymsft,2017-11-06T17:22:47Z,"
 		You can import the classifier the way in the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/examples/boston.py#L42>examples</denchmark-link>
  like:
 from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor
 But I'm asking the team about whether there's any reason of being made not accessible directly.
 		",e52706d1696faa2ab926c2d91a0d85ec99dac314,Younghee Kwon,2018-04-25 14:28:03-07:00,MODIFY,0,tensorflow\contrib\cmake\python_modules.txt,tensorflow\contrib\cmake\python_modules.txt,0.0,"132,134,135,136",,,,,,4.0,ghendrymsft,2017-11-06T17:51:41Z,"
 		Understood.  It's worth pointing out that using the 'nightly-gpu' container from the <denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow/tags/>docker hub</denchmark-link>
 , I can't import the GBDTClassifier or the estimator python module:
 Traceback (most recent call last):
 File ""/mnt/batch/tasks/shared/LS_root/mounts/azfileshare/scripts/train.py"", line 9, in 
 import models
 File ""/mnt/batch/tasks/shared/LS_root/mounts/azfileshare/scripts/models.py"", line 4, in 
 from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeClassifier
 ImportError: No module named estimator
 Using the '1.4.0-rc1' image works fine. I don't know if this is indicative of a problem introduced since the 1.4.0-rc1 image was cooked or not.  It says it was updated 4 days ago.
 		",5.0,ghendrymsft,2017-11-06T18:16:01Z,"
 		I don't have proper GPU/docker settings, so cannot test out that nightly, but can you try 1.4.0-gpu instead of rc1?
 1.4.0 is released last week, and the import works fine with non-gpu pip installation (not the docker).
 		",6.0,ghendrymsft,2017-11-06T21:28:17Z,"
 		I think the issue is that with version 1.5 some of the python files are not included in the release. We have to investigate and add the proper dependencies. (Probably from boosted_trees:init_py to estimator_batch/estimator).
 I just tried the nightly docker image and verified that in estimator_batch directory it only has:
 __init__.py  __init__.pyc  custom_export_strategy.py  custom_export_strategy.pyc  trainer_hooks.py  trainer_hooks.pyc
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ghendrymsft,2017-12-20T19:19:15Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,ghendrymsft,2018-01-04T19:11:04Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,ghendrymsft,2018-01-19T19:41:34Z,"
 		Well, I wrote a response but it seems lost so writing again (please forgive if there appears a dupe).
 I confirmed that the command I put on November 6 works for both tensorflow-1.4.1 and tensorflow-gpu-1.4.1, which are the latest versions.
 from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor
 Maybe the nightly would have been funky at the moment as I mentioned.
 Closing the issue.
 		",10.0,ghendrymsft,2018-01-19T19:50:43Z,"
 		That wasn't what was asked.  I wanted:
 import tensorflow as tf
 est = tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeRegressor()
 I realize there isn't a big difference, it's just more consistent to not have to say:
 
 Ok, I can import other estimators with tf.contrib....
 Huh, boosted_trees isn't found in tf.contrib, I wonder why
 
 
 Oh, now I realize that I have to explicitly import it, unlike all the other estimators.
 
 		",11.0,ghendrymsft,2018-01-19T20:06:58Z,"
 		Yes, my last comment is to your response (Nov 6 9:51AM PST) : ""in
 nightly-gpu, I can't even explicitly import it"" which sounds like a big
 issue.
 I confirmed that it's not the issue of the released tensorflow-gpu (nor
 tensorflow) package.
 
 For another issue you raised, many contrib libraries are not preloaded,
 which is obvious for stability, and boosted_trees is one of them.
 The reason of excluding boosted_trees had been Mac test failures, as I
 know. I think the failures are resolved but it might take time to put it to
 the list of preloaded packages.
 
 If you want to keep the issue open for that reason, that's fine, and the
 issue becomes
  ""a feature request to include contrib.boosted_trees as a preloaded
 package"".
 <denchmark-link:#>…</denchmark-link>
 
 
 On Fri, Jan 19, 2018 at 11:53 AM Gilbert Hendry ***@***.***> wrote:
  That wasn't what was asked. I wanted:
  import tensorflow as tf
  est =
  tf.contrib.boosted_trees.estimator_batch.estimator.GradientBoostedDecisionTreeRegressor()
 
  I realize there isn't a big difference, it's just more consistent to not
  have to say:
 
     1. Ok, I can import other estimators with tf.contrib....
     2. Huh, boosted_trees isn't found in tf.contrib, I wonder why
     3.
     4. Oh, now I realize that I have to explicitly import it, unlike all
     the other estimators.
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#14292 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AXihKvVL2BVT2eoi4gpLlhJabO9YiwIfks5tMPKvgaJpZM4QTYbC>
  .
 
 
 
 		",12.0,ghendrymsft,2018-01-26T07:50:32Z,"
 		the command ""from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor"" also couldn't work in Windows.
 it raised error:
 ImportError: No module named 'tensorflow.contrib.boosted_trees.python.training'
 could it be fixed?
 		",13.0,ghendrymsft,2018-02-09T19:13:59Z,"
 		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",14.0,ghendrymsft,2018-02-23T18:44:41Z,"
 		
 the command ""from tensorflow.contrib.boosted_trees.estimator_batch.estimator import GradientBoostedDecisionTreeRegressor"" also couldn't work in Windows.
 
 even using ubuntu bash on windows doesn't work
 		",15.0,ghendrymsft,2018-03-10T13:09:22Z,"
 		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",16.0,ghendrymsft,2018-03-13T00:54:56Z,"
 		<denchmark-link:https://github.com/sshrdp>@sshrdp</denchmark-link>
  Soroush, I found that the problem got worse since 1.5.0.
 Not even Linux python 2.7 can import boosted_trees.
 I installed 1.4.1 and 1.5.0 in different virtualenv, and checked the diffs.
 $ (cd tf-1.4/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch && ls *.py)
 custom_export_strategy.py  custom_loss_head.py  estimator.py  .py  model.py  trainer_hooks.py
 $ (cd tf-1.5/lib/python2.7/site-packages/tensorflow/contrib/boosted_trees/estimator_batch && ls *.py)
 custom_export_strategy.py  .py  trainer_hooks.py
 Many files are taken out in tf-1.5.0, even though I cannot find significant changes regarding BUILD or init.py under boosted_trees between the two versions.
 Could it be due to some BUILD file changes like <denchmark-link:https://github.com/tensorflow/tensorflow/commit/66b1615b6e2783c9ddce27e7b084fcc230c3a594#diff-133d898664bdd5d74926bf66659adb53>this commit</denchmark-link>
  or <denchmark-link:https://github.com/tensorflow/tensorflow/commit/966016b7f2382658e7c84baae0596d35f0a49bae#diff-133d898664bdd5d74926bf66659adb53>another</denchmark-link>
 ..?
 		",17.0,ghendrymsft,2018-03-25T02:45:13Z,"
 		<denchmark-link:https://github.com/yk5>@yk5</denchmark-link>
 : Yes indeed, the removal of the deps from  had the effect of removing the files from the PIP package. I think you can restore functionality by adding them back in (in ):
 <denchmark-code>py_library(                                                                                                                                                                                                        
     name = ""init_py"",                                                                                                                                                                                              
     srcs = [""__init__.py""],                                                                                                                                                                                        
     srcs_version = ""PY2AND3"",                                                                                                                                                                                      
     deps = [                                                                                                                                                                                                       
         ""custom_export_strategy"",                                                                                                                                                                                  
         "":custom_loss_head"",                                                                                                                                                                                       
         "":estimator"",                                                                                                                                                                                              
         "":model"",                                                                                                                                                                                                  
         "":trainer_hooks"",                                                                                                                                                                                          
     ],                                                                                                                                                                                                             
 )
 </denchmark-code>
 
 		",18.0,ghendrymsft,2018-03-26T20:31:14Z,"
 		<denchmark-link:https://github.com/ghendrymsft>@ghendrymsft</denchmark-link>
 : Could you perhaps try to add  to the dependencies of ? (On top of fixing up what I said in the previous comment, which are already committed at head.)
 		",19.0,ghendrymsft,2018-04-04T02:48:37Z,"
 		I'm away this week, but will continue to investigate on it later.
 At this moment, other platforms should be okay (confirmed on linux with
 nightly), but I suspect cmake as the culprit for windows problem.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Tue, Apr 3, 2018, 6:58 PM Rohan Jain ***@***.***> wrote:
  Assigned #14292 <#14292>
  to @yk5 <https://github.com/yk5>.
 
  —
  You are receiving this because you were assigned.
 
 
  Reply to this email directly, view it on GitHub
  <#14292 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AXihKljaQ9d8qO2sTNbwvDOYNvNtgLZ7ks5tlCjXgaJpZM4QTYbC>
  .
 
 
 
 		",20.0,ghendrymsft,2018-04-18T12:35:39Z,"
 		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
 		",21.0,ghendrymsft,2018-04-24T11:44:52Z,"
 		any work around?I need this GradientBoostedDecisionTreeClassifier and could not import it.
 ps. I am using tensorflow 1.5.0
 		",22.0,ghendrymsft,2018-04-25T16:38:16Z,"
 		It should be back in 1.8.0 for Linux/Mac, and you can test it out with 1.8.0rc1.
 For Windows, we have some package problem, but it's not easy to find what's the cause. We placed several patches but not successful. I have another, hopefully would more likely fix it this time.
 		",23.0,ghendrymsft,2018-05-01T23:28:20Z,"
 		Despite the latest fix, I'm afraid TF on Windows will get contrib.boosted_trees in near future, after looking at the CMake documentation.
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake#current-known-limitations-1>Known limitation</denchmark-link>
  section explains that CMake doesn't support tf.load_op_library(), which we use to load custom ops for contrib.boosted_trees (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/boosted_trees/python/ops/boosted_trees_ops_loader.py#L27>link</denchmark-link>
 )
 Until that functionality is implemented in CMake, it might not be possible to load contrib.boosted_trees packages properly due to lack of ops.
 OTOH, we started to migrate contrib boosted_trees into core, and v0 implementation is pushed to 1.8.0 (e.g. <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/estimator/BoostedTreesClassifier>BoostedTreesClassifier</denchmark-link>
 ), so try it out.
 Current limitation is that it only accepts bucketized_columns, however other features are being actively developed.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14455,cfehr247,2017-11-10T15:53:55Z,2017-11-30T01:12:59Z,Tensorflow cannot be installed with default Windows Python 3.5 stack,"
 After installing Python 3.5.0 using the Windows 64 bit installer (which includes pip in the install):
 pip3 install --upgrade tensorflow
 Collecting tensorflow
 Could not find a version that satisfies the requirement tensorflow (from versions: )
 No matching distribution found for tensorflow
 You are using pip version 7.1.2, however version 9.0.1 is available.
 You should consider upgrading via the 'python -m pip install --upgrade pip' command.
 I tried on a different machine that worked, and found the only difference to be the pip version.  Updating to pip 9.0.1 solved the issue.
 It's not explicitly stated anywhere that you need a newer version of pip.  When an old version of something is required to run something, people tend to ignore the messages indicating there is a newer version of it because that's exactly what they are expecting: ""yeah I know there's a newer version, I meant to do this"".
 If this cannot be resolved for older version of pip (specifically, versions included with the required Python versions), could you please state this in the documentation.
 	",1.0,cfehr247,2017-11-10T21:31:00Z,"
 		In the PR above, I added a link on the Install Windows docs to include a link to a stack overflow page that covers this.
 		",,,,,,,,,2ae9c6c7a20dbd8f05e4b60e921e60986e2968bf,Austin Anderson,2017-11-13 11:19:44-08:00,MODIFY,0,tensorflow\docs_src\install\install_windows.md,tensorflow\docs_src\install\install_windows.md,0.0,"87,221,222,223,224,225,226","87,101,224",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14542,SnowWalkerJ,2017-11-14T10:16:22Z,2017-11-17T18:34:20Z,'Model' object has no attribute 'container_nodes',"
 <denchmark-h:h2>Problem</denchmark-h>
 
 model = tf.keras.models.Model()
 model.add(...)
 tf.keras.utils.plot_model(model, to_file=""model.png"")
 Output:
 <denchmark-code>Traceback (most recent call last):
   File ""model.py"", line 36, in <module>
     K.utils.plot_model(model, to_file=""model.png"")
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 148, in plot_model
     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir)
   File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/utils/vis_utils.py"", line 123, in model_to_dot
     if node_key in model.container_nodes:
 AttributeError: 'Model' object has no attribute 'container_nodes'
 </denchmark-code>
 
 <denchmark-h:h2>Environment</denchmark-h>
 
 -System: Ubuntu 16.04
 -Tensorflow-gpu bin v1.4.0-rc1-11-g130a514 1.4.0
 	",1.0,SnowWalkerJ,2017-11-14T15:54:12Z,"
 		 does not work since commit  <denchmark-link:https://github.com/tensorflow/tensorflow/commit/3599fd44d6bfcb16f45e763608a0e5da4e9072f5>3599fd4</denchmark-link>
 . A minor fix seems works.
 		",,,,,,,,,3e53570d3bf518ec2b6cfeed4b5fd57d11370289,CSJY,2017-11-17 10:34:19-08:00,MODIFY,1,tensorflow\python\keras\_impl\keras\utils\vis_utils.py,tensorflow\python\keras\_impl\keras\utils\vis_utils.py,1.0,123,123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,model_to_dot,"model,show_shapes,show_layer_names,rankdir",53,128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14739,malmaud,2017-11-21T02:31:54Z,2017-12-02T09:39:46Z,Eager: Warn with invalid policy,"
 If a user accidentally writes tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_WARN) instead of the correct tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_WARN), they won't get an error until later in their program.
 For example, tfe.num_gpus() after  the incorrect enable call produces
 <denchmark-code>---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-8-71d6509178f5> in <module>()
 ----> 1 tfe.num_gpus()
 
 ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus()
     458     The number of available GPU devices.
     459   """"""
 --> 460   return context().num_gpus()
 
 ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in num_gpus(self)
     286   def num_gpus(self):
     287     """"""The number of GPUs available to execute operations.""""""
 --> 288     self._initialize_handle_and_devices()
     289     return self._num_gpus
     290 
 
 ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/context.py in _initialize_handle_and_devices(self)
     121         with errors.raise_exception_on_not_ok_status() as status:
     122           if self._config is not None:
 --> 123             config_str = self._config.SerializeToString()
     124             pywrap_tensorflow.TFE_ContextOptionsSetConfig(
     125                 opts, config_str, len(config_str), status)
 
 AttributeError: 'int' object has no attribute 'SerializeToString'
 </denchmark-code>
 
 I'd think it makes more sense to throw an error immediately after the incorrect enable_eager_execution.
 This is on  (<denchmark-link:https://github.com/tensorflow/tensorflow/commit/ab00df9b0b74910ca738e6ee850982f62ad42e55>ab00df9</denchmark-link>
 ).
 	",1.0,malmaud,2017-11-25T06:31:40Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  could you please take a look into this.
 		",2.0,malmaud,2017-11-28T01:10:15Z,"
 		Sounds like a fair request :), will send out a change to address this.
 		",,,,,ba87a8030aa30f24c354cf705e79734658bb0a8b,Asim Shankar,2017-11-28 12:22:09-08:00,MODIFY,1,tensorflow\python\framework\ops.py,tensorflow\python\framework\ops.py,1.0,"4798,4799,4800,4801,4802,4803,4804,4805,4806,4807",,MODIFY,1.0,tensorflow\python\framework\ops_test.py,tensorflow\python\framework\ops_test.py,,,,,,,,,,,,,1.0,"2398,2399,2400,2401,2402,2403",,testBadArgumentsToEnableEagerExecution,self,2398,2403,,,,,,,,,,,,,,,enable_eager_execution,"config,device_policy",4764,4831,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14776,droidicus,2017-11-22T00:35:51Z,2018-03-07T01:17:02Z,tf.keras.estimator.estimator_from_model does not respect options set in RunConfig,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tf.VERSION = 1.4.0 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
 Python version: 2.7.12
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: 8.0.61/6.0.21
 GPU model and memory: NVIDIA Tesla M60 8 GB
 Exact command to reproduce: See Below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 When trying to use an estimator that is derived from tf.keras.estimator.estimator_from_model() and training with tf.estimator.train_and_evaluate(), setting gpu_options in the session_config of tf.estimator.RunConfig does not cause the settings to be respected when passed to the estimator_from_model function. For example setting per_process_gpu_memory_fraction=0.5 does not decrease the memory allocated to the process on the GPU, similarly setting allow_growth=True continues to allocate all of the memory and does not allow memory growth.
 I also tested this with the canned estimator tf.estimator.DNNRegressor, and the settings were applied as expected when the RunConfig was passed to the estimator.
 Below is code to demonstrate this issue.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Minimal example, runs to completion and trains successfully. But, changing the GPUOptions settings does not cause the GPU memory to be utilized as expected:
 import os
 import numpy as np
 import tensorflow as tf
 
 tf.logging.set_verbosity(tf.logging.INFO)
 
 # Neither of these GPUOptions are respected
 gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
 #gpu_options = tf.GPUOptions(allow_growth=True)
 sess_config = tf.ConfigProto(gpu_options=gpu_options)
 run_config = tf.estimator.RunConfig(session_config=sess_config)
 
 inputs = tf.keras.layers.Input(shape=(10,))
 outputs = tf.keras.layers.Dense(10)(inputs)
 model = tf.keras.models.Model(inputs, outputs)
 model.compile(optimizer='sgd', loss='mse')
 est_keras = tf.keras.estimator.model_to_estimator(keras_model=model, config=run_config)
 
 input_name = model.input_names[0]
 data = np.random.rand(1000,10).astype(np.float32)
 train_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)
 
 train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100000)
 eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)
 tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)
 	",1.0,droidicus,2017-11-22T00:37:22Z,"
 		<denchmark-link:https://github.com/fchollet>@fchollet</denchmark-link>
  <denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
  May be related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/14504>#14504</denchmark-link>
 
 		",2.0,droidicus,2017-11-27T19:47:08Z,"
 		<denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
  could you please take a look.
 		",3.0,droidicus,2017-12-01T00:10:31Z,"
 		<denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
  any idea? The run_config is passed in directly when creating the keras version of the Estimator. Do we need to pass these configurations anywhere else?
 		",355fb5e14b325a1d106c4046f478da4bda350205,Yifei Feng,2018-03-05 13:52:51-08:00,MODIFY,0,tensorflow\python\keras\_impl\keras\estimator.py,tensorflow\python\keras\_impl\keras\estimator.py,0.0,"302,303,304,305,306",299,MODIFY,1.0,tensorflow\python\keras\_impl\keras\estimator_test.py,tensorflow\python\keras\_impl\keras\estimator_test.py,4.0,droidicus,2017-12-01T19:11:22Z,"
 		It should not be related with Keras code. It should work since it is handled within Estimator.
 <denchmark-link:https://github.com/droidicus>@droidicus</denchmark-link>
  could you please print est_keras.config and est_keras.config.cluster_spec.as_dict?
 		",5.0,droidicus,2017-12-03T17:00:01Z,"
 		Sure thing, here is the output:
 *********** est_keras.config *************************************
 <tensorflow.python.estimator.run_config.RunConfig object at 0x7f7694423fd0>
 *********** est_keras.config.cluster_spec.as_dict()  *************
 {}
 ******************************************************************
 Code to reproduce (same as the source in the origional issue above, but with print statements after the creation of the keras estimator), and full log output are avaliable here: <denchmark-link:https://gist.github.com/droidicus/146532eacf88ed57538bb41a8fc7da4b>https://gist.github.com/droidicus/146532eacf88ed57538bb41a8fc7da4b</denchmark-link>
 
 		",6.0,droidicus,2017-12-20T01:29:06Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",1.0,"381,382,383,384,385,386,387,388,389,390,391,392,393,394,395",,test_gpu_config,self,381,395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,droidicus,2018-01-02T20:05:46Z,"
 		Gentle ping, this is still an issue for me.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,droidicus,2018-01-09T23:38:09Z,"
 		Hi <denchmark-link:https://github.com/shivaniag>@shivaniag</denchmark-link>
 ,
 Estimator sends the given session_config directly to the Session constructor. Could you please assign somebody who is more familiar with tf.Session and it's handling of GPU settings?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,droidicus,2018-01-09T23:39:52Z,"
 		FYI, I've checked the keras.model_to_estimator. It's sending the config properly to tf.estimator.Estimator.
 tf.estimator.Estimator sends that config to tf.train.SessionManager calls which uses it as a constructor argument to tf.Session.
 		",10.0,droidicus,2018-01-10T00:04:09Z,"
 		Just as an FYI, while this is still a problem in TFv1.5rc0 we were able to do the following as a workaround for now, by setting the default session manually the memory fraction is respected:
 import os
 import numpy as np
 import tensorflow as tf
 
 tf.logging.set_verbosity(tf.logging.INFO)
 
 gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)
 sess_config = tf.ConfigProto(gpu_options=gpu_options)
 # Manually set the default session instead
 tf.Session(config=sess_config).as_default()
 #run_config = tf.estimator.RunConfig(session_config=sess_config)
 
 inputs = tf.keras.layers.Input(shape=(10,))
 outputs = tf.keras.layers.Dense(10)(inputs)
 model = tf.keras.models.Model(inputs, outputs)
 model.compile(optimizer='sgd', loss='mse')
 est_keras = tf.keras.estimator.model_to_estimator(keras_model=model)#, config=run_config)
 
 input_name = model.input_names[0]
 data = np.random.rand(1000,10).astype(np.float32)
 train_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)
 
 train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100000)
 eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)
 tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)
 		",11.0,droidicus,2018-01-10T00:14:24Z,"
 		<denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
 , <denchmark-link:https://github.com/fchollet>@fchollet</denchmark-link>
   is there any place within underlying Keras code that uses default session while building the graph, train_op, ...?
 		",12.0,droidicus,2018-01-23T22:57:11Z,"
 		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
 		",13.0,droidicus,2018-02-07T13:43:59Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",14.0,droidicus,2018-02-14T00:34:53Z,"
 		Sorry for the delay. <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/estimator.py#L147>This</denchmark-link>
  is how model_to_estimator create its model_fn. <denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
  anything you see with session that should be done differently? Thanks!
 Also tried print out estimator.config.__dict__ and estimator.config.cluster_spec.__dict__ for canned estimator, custom estimator and keras converted estimator, and I'm seeing the same results:
 `
 *********** est_keras.config.dict *************************************
 {'_save_checkpoints_secs': 600, '_session_config': gpu_options {
 per_process_gpu_memory_fraction: 0.5
 }
 , '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e3ebcabd0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/tmp/tmp8jvdNt', '_save_summary_steps': 100}
 *********** est_keras.config.cluster_spec.dict  *************
 {'_cluster_def': , '_cluster_spec': {}}
 <denchmark-h:hr></denchmark-h>
 
 `
 		",15.0,droidicus,2018-03-03T08:02:50Z,"
 		Nagging Assignees <denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
 , <denchmark-link:https://github.com/shivaniag>@shivaniag</denchmark-link>
 , <denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",16.0,droidicus,2018-03-06T06:52:01Z,"
 		A fix has been submitted internally and should make to master tomorrow. Thanks!
 		",17.0,droidicus,2018-03-06T19:34:10Z,"
 		Fantastic, thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14800,orpillar,2017-11-22T16:13:41Z,2017-12-11T01:44:54Z,Potential memory leak from deleting array and closing file handler,"
 Here are couple of minor memory leak for review.
 
 
 
 
 
 tensorflow/tensorflow/c/c_api.cc
 
 
         Lines 569 to 593
       in
       6c95675
 
 
 
 
 
 
  char* base = new char[size]; 
 
 
 
  char* data_start = base + sizeof(tensorflow::uint64) * srcarray.size(); 
 
 
 
  char* dst = data_start;  // Where next string is encoded. 
 
 
 
  size_t dst_len = size - static_cast<size_t>(data_start - base); 
 
 
 
  tensorflow::uint64* offsets = reinterpret_cast<tensorflow::uint64*>(base); 
 
 
 
  for (int i = 0; i < srcarray.size(); ++i) { 
 
 
 
    *offsets = (dst - data_start); 
 
 
 
    offsets++; 
 
 
 
  const string& s = srcarray(i); 
 
 
 
  size_t consumed = TF_StringEncode(s.data(), s.size(), dst, dst_len, status); 
 
 
 
  if (!status->status.ok()) { 
 
 
 
      status->status = InvalidArgument( 
 
 
 
  ""invalid string tensor encoding (string #"", i, "" of "", 
 
 
 
          srcarray.size(), ""): "", status->status.error_message()); 
 
 
 
  return nullptr; 
 
 
 
    } 
 
 
 
    dst += consumed; 
 
 
 
    dst_len -= consumed; 
 
 
 
  } 
 
 
 
  if (dst != base + size) { 
 
 
 
    status->status = InvalidArgument( 
 
 
 
  ""invalid string tensor encoding (decoded "", (dst - base), 
 
 
 
  "" bytes, but the tensor is encoded in "", size, "" bytes""); 
 
 
 
  return nullptr; 
 
 
 
  } 
 
 
 
 
  ""delete []base;"" looks missing.
 
 
 
 
 
 tensorflow/tensorflow/core/lib/io/snappy/snappy_outputbuffer.cc
 
 
         Lines 164 to 173
       in
       6c95675
 
 
 
 
 
 
  char* compressed_length_array = new char[4]; 
 
 
 
  std::fill(compressed_length_array, compressed_length_array + 4, 0); 
 
 
 
  for (int i = 0; i < 4; i++) { 
 
 
 
  // Little endian. 
 
 
 
    compressed_length_array[i] = output.size() >> (8 * (3 - i)); 
 
 
 
  } 
 
 
 
  TF_RETURN_IF_ERROR(AddToOutputBuffer(compressed_length_array, 4)); 
 
 
 
  
 
 
 
  // Write compressed output to buffer. 
 
 
 
  TF_RETURN_IF_ERROR(AddToOutputBuffer(output.data(), output.size())); 
 
 
 
 
   ""delete []compressed_length_array;"" looks missing when macro TF_RETURN_IF_ERROR() fails.
 
 
 
 
 
 tensorflow/tensorflow/core/platform/profile_utils/android_armv7a_cpu_utils_helper.cc
 
 
         Lines 113 to 123
       in
       6c95675
 
 
 
 
 
 
  FILE *fp = fopen(file_path.c_str(), ""r""); 
 
 
 
  if (fp == nullptr) { 
 
 
 
  return INVALID_CPU_FREQUENCY; 
 
 
 
  } 
 
 
 
  int64 freq_in_khz = INVALID_CPU_FREQUENCY; 
 
 
 
  const int retval = fscanf(fp, ""%lld"", &freq_in_khz); 
 
 
 
  if (retval < 0) { 
 
 
 
  LOG(WARNING) << ""Failed to \"""" << file_path << ""\""""; 
 
 
 
  return INVALID_CPU_FREQUENCY; 
 
 
 
  } 
 
 
 
  pclose(fp); 
 
 
 
 
  Two potential problems:
 a. There is no ""fclose()"" being called after fscanf() fails
 b. ""fclose()"" could be called instead of ""pclose()""
 
 
 
 
 
 tensorflow/tensorflow/tools/proto_text/gen_proto_text_functions.cc
 
 
         Lines 132 to 137
       in
       6c95675
 
 
 
 
 
 
  FILE* f = fopen(path.c_str(), ""w""); 
 
 
 
  if (f == nullptr) return -1; 
 
 
 
  if (fwrite(data.c_str(), 1, data.size(), f) != data.size()) { 
 
 
 
  return -1; 
 
 
 
  } 
 
 
 
  if (fclose(f) != 0) { 
 
 
 
 
  When ""fwrite() fails"", ""fclose()"" could be called before ""return -1"".
 
 
 PS: I don't have handy working environment setup yet, currently browsing code may be better fit for me.
 	",1.0,orpillar,2017-11-22T16:42:05Z,"
 		Could you edit your post and wrap code in three backticks (Markdown code highlighting), please?
 int main() {
   // This is easier to read.
 }
 <denchmark-link:https://user-images.githubusercontent.com/1595907/33139045-6ca77b38-cfac-11e7-8a29-233e0918a72d.png></denchmark-link>
 
 		",2.0,orpillar,2017-11-22T16:44:50Z,"
 		Or better yet, paste links to the relevant lines and GitHub will insert code snippets for you:
 
 
 
 tensorflow/tensorflow/c/c_api.cc
 
 
         Lines 580 to 583
       in
       6c95675
 
 
 
 
 
 
  status->status = InvalidArgument( 
 
 
 
  ""invalid string tensor encoding (string #"", i, "" of "", 
 
 
 
      srcarray.size(), ""): "", status->status.error_message()); 
 
 
 
  return nullptr; 
 
 
 
 
 
 		",3.0,orpillar,2017-11-22T21:45:47Z,"
 		<denchmark-link:https://github.com/carlthome>@carlthome</denchmark-link>
 , thanks for the tip. Now I updated the description.
 		",e17ae378063b46c894a8c193823f029d7d87de81,Yong Tang,2017-12-10 20:44:54-05:00,MODIFY,1,tensorflow\c\c_api.cc,tensorflow\c\c_api.cc,1.0,"582,592",,MODIFY,1.0,tensorflow\core\lib\io\snappy\snappy_outputbuffer.cc,tensorflow\core\lib\io\snappy\snappy_outputbuffer.cc,4.0,orpillar,2017-11-22T22:57:43Z,"
 		<denchmark-link:https://github.com/orpillar>@orpillar</denchmark-link>
  I think those issues are true.  In ,  Update: Actually there are only 4 bytes so it could be placed into the stack instead.
 Would you like to create a PR for that? Otherwise I could help create a PR for you.
 		",5.0,orpillar,2017-11-23T01:59:37Z,"
 		<denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
 , thanks for looking into the issues. You are right about snappy_outputbuffer.cc.
 I am new to open source community, just want to see there is any easy things I could contribute.
 Please feel free to help create a PR. Thanks,
 		",6.0,orpillar,2017-11-23T02:30:10Z,"
 		<denchmark-link:https://github.com/orpillar>@orpillar</denchmark-link>
  Created PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/14816>#14816</denchmark-link>
  for the fix. Thanks for your contribution to TensorFlow community! 
 		",1.0,164,"164,176",tensorflow::io::SnappyOutputBuffer::Deflate,,154,178,MODIFY,1.0,tensorflow\core\platform\profile_utils\android_armv7a_cpu_utils_helper.cc,tensorflow\core\platform\profile_utils\android_armv7a_cpu_utils_helper.cc,1.0,"121,124",123,MODIFY,1.0,tensorflow\tools\proto_text\gen_proto_text_functions.cc,tensorflow\tools\proto_text\gen_proto_text_functions.cc,1.0,135,,tensorflow::TF_TensorFromTensor,"src,status",523,606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,orpillar,2017-11-26T05:21:07Z,"
 		<denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
 . Thanks for the PR. It looks the sanity build had time out.
 		",tensorflow::profile_utils::AndroidArmV7ACpuUtilsHelper::ReadCpuFrequencyFile,"cpu_id,type",109,126,tensorflow::MainImpl,"argc,argv",70,144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,orpillar,2017-11-27T02:10:14Z,"
 		<denchmark-link:https://github.com/orpillar>@orpillar</denchmark-link>
  bumped the build. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14819,bri-jones,2017-11-23T04:24:01Z,2017-12-11T02:44:45Z,Keras Dropout support_masking gets reset to False,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.12.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0
 Python version: 3.6.1
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 Exact command to reproduce: see below
 
 You can collect some of this information using our environment capture script:
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</denchmark-link>
 
 You can obtain the TensorFlow version with
 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
 The Keras Dropout layer constructor (tensorflow/python/keras/_impl/keras/layers/core.py) sets support_masking=True and then calls its super constructor, which sets it back to False. Other layers defined in that module appear to set support_masking=True after the super constructor call.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
 <denchmark-code>from tensorflow.contrib.keras.api.keras.models import Sequential
 from tensorflow.contrib.keras.api.keras.layers import Dropout, InputLayer, LSTM, Masking 
 
 if __name__ == '__main__':
 
     test1 = True
 
     def model1():
         model = Sequential()
         model.add(InputLayer([8, 64]))
         model.add(Masking())
         model.add(Dropout(0.5))
 
     def model2():
         model = Sequential()
         model.add(InputLayer([8, 64]))
         model.add(Masking())
         model.add(LSTM(128, return_sequences=True))
         model.add(Dropout(0.5))
 
     if test1:
         model1()
     else:
         model2()
 </denchmark-code>
 
 <denchmark-code>Traceback (most recent call last):
   File ""expose_dropout_bug.py"", line 16, in <module>
     model.add(Dropout(0.5))
   File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 501, in add
     output_tensor = layer(self.outputs[0])
   File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 252, in __call__
     output = super(Layer, self).__call__(inputs, **kwargs)
   File ""/.venv/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 594, in __call__
     output_mask = self.compute_mask(inputs, previous_mask)
   File ""/.venv/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 308, in compute_mask
     'but was passed an input_mask: ' + str(mask))
 TypeError: Layer dropout_1 does not support masking, but was passed an input_mask: Tensor(""masking/Any_1:0"", shape=(?, 8), dtype=bool)
 </denchmark-code>
 
 	",1.0,bri-jones,2017-11-23T07:29:21Z,"
 		Sounds like a mistake, cc <denchmark-link:https://github.com/fchollet>@fchollet</denchmark-link>
 
 
 
 
 tensorflow/tensorflow/python/keras/_impl/keras/layers/core.py
 
 
         Lines 107 to 113
       in
       ab0fcac
 
 
 
 
 
 
  self.supports_masking = True 
 
 
 
  # Inheritance call order: 
 
 
 
  # 1) tf.layers.Dropout, 2) keras.layers.Layer, 3) tf.layers.Layer 
 
 
 
  super(Dropout, self).__init__(rate=rate, 
 
 
 
  noise_shape=noise_shape, 
 
 
 
  seed=seed, 
 
 
 
  **kwargs) 
 
 
 
 
 
 		",2.0,bri-jones,2017-11-23T07:38:19Z,"
 		Correct, that's a bug, self.supports_masking = True should be after the call to the parent's constructor.
 		",3.0,bri-jones,2017-11-23T07:42:37Z,"
 		I can work on it. I'll fix it later.
 		",fd1263fb9b9a81b4c8d7e7922308146b4f57428d,Zhengsheng Wei,2017-12-10 21:44:44-05:00,MODIFY,1,tensorflow\python\keras\_impl\keras\layers\core.py,tensorflow\python\keras\_impl\keras\layers\core.py,1.0,113,107,MODIFY,1.0,tensorflow\python\keras\_impl\keras\layers\core_test.py,tensorflow\python\keras\_impl\keras\layers\core_test.py,4.0,bri-jones,2017-11-23T07:47:19Z,"
 		Fine. It would be better if you can add a corresponding test case here:
 
 
 
 tensorflow/tensorflow/python/keras/_impl/keras/layers/core_test.py
 
 
          Line 38
       in
       ab0fcac
 
 
 
 
 
 
  def test_dropout(self): 
 
 
 
 
 
 		",5.0,bri-jones,2017-11-23T07:48:09Z,"
 		okay
 		",6.0,bri-jones,2017-11-23T10:22:43Z,"
 		<denchmark-link:https://github.com/facaiy>@facaiy</denchmark-link>
  please check if unit test added is valid. Thanks!
 		",1.0,"50,51,52,53,54",,test_dropout,self,38,83,,,,,,,,,,,,,,,__init__,"self,rate,noise_shape,seed,kwargs",106,113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14942,WenmuZhou,2017-11-28T14:01:08Z,2018-02-20T18:40:09Z,tensorflow 1.4 is 8 times slower than tensorflow 1.3 when read data,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 TensorFlow installed from (source or binary): python wheel
 TensorFlow version (use command below): 1.4 and 1.3
 Python version: 3.6.1
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: None
 GPU model and memory: None
 Exact command to reproduce:
 
 when I run tensorflow1.4 script using estimator, the script is 8 times slower than tensorflow 1.3
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 main script
 #!/usr/bin/env python
 __author__ = 'zj'
 
 import argparse
 import os
 import sys
 import numpy as np
 import time
 try:
     import better_exceptions
 except ImportError:
     pass
 import tensorflow as tf
 from src.model_ori import crnn_fn
 from src.data_handler import data_loader
 from src.config import Params, Alphabet
 from src.input_utils import input_fn
 
 
 def main(unused_argv):
     models_path = FLAGS.input_model_dir
     if not os.path.exists(models_path):
         assert FileNotFoundError
 
     models_list = [os.path.join(models_path, x[:-5]) for x in os.listdir(models_path) if x.endswith('.meta')]
 
     if not os.path.exists(FLAGS.output_model_dir):
         os.makedirs(FLAGS.output_model_dir)
 
     parameters = Params(eval_batch_size=128,
                         input_shape=(32, 304),
                         digits_only=False,
                         alphabet=Alphabet.CHINESECHAR_LETTERS_DIGITS_EXTENDED,
                         alphabet_decoding='same',
                         image_channels=1,
                         csv_delimiter=' ',
                         csv_files_eval=FLAGS.csv_files_eval,
                         output_model_dir=FLAGS.output_model_dir,
                         gpu=FLAGS.gpu
                         )
 
     model_params = {
         'Params': parameters,
     }
 
     os.environ['CUDA_VISIBLE_DEVICES'] = parameters.gpu
     config_sess = tf.ConfigProto()
     config_sess.gpu_options.per_process_gpu_memory_fraction = 0.6
 
     # Config estimator
     est_config = tf.estimator.RunConfig()
     est_config = est_config.replace(session_config=config_sess,
                                     save_summary_steps=100,
                                     model_dir=parameters.output_model_dir)
 
     estimator = tf.estimator.Estimator(model_fn=crnn_fn,
                                        params=model_params,
                                        config=est_config,
                                        model_dir=parameters.output_model_dir,
                                        )
     try:
         with open(FLAGS.output_file, encoding='utf-8', mode='w') as save_file:
             for model in models_list:
                 start = time.time()
                 
                 eval_results = estimator.evaluate(input_fn=data_loader(csv_filename=parameters.csv_files_eval,
                                                                        params=parameters,
                                                                        batch_size=parameters.eval_batch_size,
                                                                        num_epochs=1),
                                                   steps=3,
                                                   checkpoint_path=model)
                 print('time:',time.time() - start)
                 print('model: %s Evaluation results: %s' % (model, str(eval_results)))
                 save_file.write(model + ' ' + str(eval_results) + '\n')
 
     except KeyboardInterrupt:
         print('Interrupted')
 
 
 if __name__ == '__main__':
     parser = argparse.ArgumentParser()
     parser.add_argument('-fe', '--csv_files_eval', required=False, type=str, help='CSV filename for evaluation',
                         nargs='*', default=['E:/val1.csv'])
     parser.add_argument('-o', '--output_model_dir', required=False, type=str,
                         help='Directory for output', default='models_vgg_100K_no_eval')
     parser.add_argument('-m', '--input_model_dir', required=False, type=str,
                         help='Directory for output', default='model_test')
     parser.add_argument('-g', '--gpu', type=str, help=""GPU 0,1 or '' "", default='0')
     parser.add_argument('-of', '--output_file', required=False, type=str, default='123.txt', help=""the log output file"")
 
     tf.logging.set_verbosity(tf.logging.DEBUG)
     FLAGS, unparsed = parser.parse_known_args()
     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
 data_loader script
 #!/usr/bin/env python
 import tensorflow as tf
 import numpy as np
 from .config import Params, CONST
 from typing import Tuple
 
 
 def data_loader(csv_filename: str, params: Params, batch_size: int = 128, data_augmentation: bool = False,
                 num_epochs: int = None, image_summaries: bool = False):
     def input_fn():
         # Choose case one csv file or list of csv files
         if not isinstance(csv_filename, list):
             filename_queue = tf.train.string_input_producer([csv_filename], num_epochs=num_epochs,
                                                             name='filename_queue')
         elif isinstance(csv_filename, list):
             filename_queue = tf.train.string_input_producer(csv_filename, num_epochs=num_epochs, name='filename_queue')
 
         # Skip lines that have already been processed
         reader = tf.TextLineReader(name='CSV_Reader', skip_header_lines=0)
         key, value = reader.read(filename_queue, name='file_reading_op')
 
         default_line = [['None'], ['None']]
         path, label = tf.decode_csv(value, record_defaults=default_line, field_delim=params.csv_delimiter,
                                     name='csv_reading_op')
 
         image, img_width = image_reading(path, resized_size=params.input_shape, params=params,
                                          data_augmentation=data_augmentation, padding=True)
 
         to_batch = {'images': image, 'images_widths': img_width, 'filenames': path, 'labels': label}
         prepared_batch = tf.train.shuffle_batch(to_batch,
                                                 batch_size=batch_size,
                                                 min_after_dequeue=500,
                                                 num_threads=15, capacity=4000,
                                                 allow_smaller_final_batch=False,
                                                 name='prepared_batch_queue')
 
         if image_summaries:
             tf.summary.image('input/image', prepared_batch.get('images'), max_outputs=1)
         tf.summary.text('input/labels', prepared_batch.get('labels')[:10])
         tf.summary.text('input/widths', tf.as_string(prepared_batch.get('images_widths')))
 
         return prepared_batch, prepared_batch.get('labels')
 
     return input_fn
 
 
 def image_reading(path: str, params: Params, resized_size: Tuple[int, int] = None, data_augmentation: bool = False,
                   padding: bool = False) -> Tuple[tf.Tensor, tf.Tensor]:
     # Read image
     image_content = tf.read_file(path, name='image_reader')
     image = tf.cond(tf.equal(tf.string_split([path], '.').values[1], tf.constant('jpg', dtype=tf.string)),
                     true_fn=lambda: tf.image.decode_jpeg(image_content, channels=params.image_channels,
                                                          try_recover_truncated=True),  # TODO channels = 3 ?
                     false_fn=lambda: tf.image.decode_png(image_content, channels=params.image_channels),
                     name='image_decoding')
 
     # Data augmentation
     if data_augmentation:
         image = augment_data(image)
 
     # Padding
     if padding:
         with tf.name_scope('padding'):
             image, img_width = padding_inputs_width(image, resized_size, increment=CONST.DIMENSION_REDUCTION_W_POOLING)
     # Resize
     else:
         image = tf.image.resize_images(image, size=resized_size)
         img_width = tf.shape(image)[1]
 
     with tf.control_dependencies([tf.assert_equal(image.shape[:2], resized_size)]):
         return image, img_width
 
 
 def random_rotation(img: tf.Tensor, max_rotation: float = 0.1, crop: bool = True) -> tf.Tensor:  # from SeguinBe
     with tf.name_scope('RandomRotation'):
         rotation = tf.random_uniform([], -max_rotation, max_rotation)
         rotated_image = tf.contrib.image.rotate(img, rotation, interpolation='BILINEAR')
         if crop:
             rotation = tf.abs(rotation)
             original_shape = tf.shape(rotated_image)[:2]
             h, w = original_shape[0], original_shape[1]
             # see https://stackoverflow.com/questions/16702966/rotate-image-and-crop-out-black-borders for formulae
             old_l, old_s = tf.cond(h > w, lambda: [h, w], lambda: [w, h])
             old_l, old_s = tf.cast(old_l, tf.float32), tf.cast(old_s, tf.float32)
             new_l = (old_l * tf.cos(rotation) - old_s * tf.sin(rotation)) / tf.cos(2 * rotation)
             new_s = (old_s - tf.sin(rotation) * new_l) / tf.cos(rotation)
             new_h, new_w = tf.cond(h > w, lambda: [new_l, new_s], lambda: [new_s, new_l])
             new_h, new_w = tf.cast(new_h, tf.int32), tf.cast(new_w, tf.int32)
             bb_begin = tf.cast(tf.ceil((h - new_h) / 2), tf.int32), tf.cast(tf.ceil((w - new_w) / 2), tf.int32)
             rotated_image_crop = rotated_image[bb_begin[0]:h - bb_begin[0], bb_begin[1]:w - bb_begin[1], :]
 
             # If crop removes the entire image, keep the original image
             rotated_image = tf.cond(tf.equal(tf.size(rotated_image_crop), 0),
                                     true_fn=lambda: img,
                                     false_fn=lambda: rotated_image_crop)
 
         return rotated_image
 
 
 def random_padding(image: tf.Tensor, max_pad_w: int = 5, max_pad_h: int = 10) -> tf.Tensor:
     w_pad = list(np.random.randint(0, max_pad_w, size=[2]))
     h_pad = list(np.random.randint(0, max_pad_h, size=[2]))
     paddings = [h_pad, w_pad, [0, 0]]
 
     return tf.pad(image, paddings, mode='REFLECT', name='random_padding')
 
 
 def augment_data(image: tf.Tensor) -> tf.Tensor:
     with tf.name_scope('DataAugmentation'):
         # Random padding
         image = random_padding(image)
 
         image = tf.image.random_brightness(image, max_delta=0.1)
         image = tf.image.random_contrast(image, 0.5, 1.5)
         image = random_rotation(image, 0.05, crop=True)
 
         if image.shape[-1] >= 3:
             image = tf.image.random_hue(image, 0.2)
             image = tf.image.random_saturation(image, 0.5, 1.5)
 
         return image
 
 
 def padding_inputs_width(image: tf.Tensor, target_shape: Tuple[int, int], increment: int) -> Tuple[
     tf.Tensor, tf.Tensor]:
     target_ratio = target_shape[1] / target_shape[0]
     # Compute ratio to keep the same ratio in new image and get the size of padding
     # necessary to have the final desired shape
     shape = tf.shape(image)
     # 计算宽高比
     ratio = tf.divide(shape[1], shape[0], name='ratio')
 
     new_h = target_shape[0]
     new_w = tf.cast(tf.round((ratio * new_h) / increment) * increment, tf.int32)
     f1 = lambda: (new_w, ratio)
     f2 = lambda: (new_h, tf.constant(1.0, dtype=tf.float64))
     new_w, ratio = tf.case({tf.greater(new_w, 0): f1,
                             tf.less_equal(new_w, 0): f2},
                            default=f1, exclusive=True)
     target_w = target_shape[1]
 
     # Definitions for cases
     def pad_fn():
         with tf.name_scope('mirror_padding'):
             pad = tf.subtract(target_w, new_w)
 
             img_resized = tf.image.resize_images(image, [new_h, new_w])
 
             # Padding to have the desired width
             paddings = [[0, 0], [0, pad], [0, 0]]
             pad_image = tf.pad(img_resized, paddings, mode='SYMMETRIC', name=None)
 
             # Set manually the shape
             pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])
 
             return pad_image, (new_h, new_w)
 
     def replicate_fn():
         with tf.name_scope('replication_padding'):
             img_resized = tf.image.resize_images(image, [new_h, new_w])
 
             # If one symmetry is not enough to have a full width
             # Count number of replications needed
             n_replication = tf.cast(tf.ceil(target_shape[1] / new_w), tf.int32)
             img_replicated = tf.tile(img_resized, tf.stack([1, n_replication, 1]))
             pad_image = tf.image.crop_to_bounding_box(image=img_replicated, offset_height=0, offset_width=0,
                                                       target_height=target_shape[0], target_width=target_shape[1])
 
             # Set manually the shape
             pad_image.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])
 
             return pad_image, (new_h, new_w)
 
     def simple_resize():
         with tf.name_scope('simple_resize'):
             img_resized = tf.image.resize_images(image, target_shape)
 
             img_resized.set_shape([target_shape[0], target_shape[1], img_resized.get_shape()[2]])
 
             return img_resized, target_shape
 
     # 3 cases
     pad_image, (new_h, new_w) = tf.case(
         {  # case 1 : new_w >= target_w
             tf.logical_and(tf.greater_equal(ratio, target_ratio),
                            tf.greater_equal(new_w, target_w)): simple_resize,
             # case 2 : new_w >= target_w/2 & new_w < target_w & ratio < target_ratio
             tf.logical_and(tf.less(ratio, target_ratio),
                            tf.logical_and(tf.greater_equal(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)),
                                           tf.less(new_w, target_w))): pad_fn,
             # case 3 : new_w < target_w/2 & new_w < target_w & ratio < target_ratio
             tf.logical_and(tf.less(ratio, target_ratio),
                            tf.logical_and(tf.less(new_w, target_w),
                                           tf.less(new_w, tf.cast(tf.divide(target_w, 2), tf.int32)))): replicate_fn
         },
         default=simple_resize, exclusive=True)
 
     return pad_image, new_w  # new_w = image width used for computing sequence lengths
 
 
 def preprocess_image_for_prediction(fixed_height: int = 32, min_width: int = 8):
     """"""
     Input function to use when exporting the model for making predictions (see estimator.export_savedmodel)
     :param fixed_height: height of the input image after resizing
     :param min_width: minimum width of image after resizing
     :return:
     """"""
 
     def serving_input_fn():
         # define placeholder for input image
         image = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])
 
         shape = tf.shape(image)
         # Assert shape is h x w x c with c = 1
 
         ratio = tf.divide(shape[1], shape[0])
         increment = CONST.DIMENSION_REDUCTION_W_POOLING
         new_width = tf.cast(tf.round((ratio * fixed_height) / increment) * increment, tf.int32)
 
         resized_image = tf.cond(new_width < tf.constant(min_width, dtype=tf.int32),
                                 true_fn=lambda: tf.image.resize_images(image, size=(fixed_height, min_width)),
                                 false_fn=lambda: tf.image.resize_images(image, size=(fixed_height, new_width))
                                 )
 
         # Features to serve
         features = {'images': resized_image[None],  # cast to 1 x h x w x c
                     'images_widths': new_width[None]  # cast to tensor
                     }
 
         # Inputs received
         receiver_inputs = {'images': image}
 
         return tf.estimator.export.ServingInputReceiver(features, receiver_inputs)
 
     return serving_input_fn
 log
 tensorflow1.4
 INFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {
   per_process_gpu_memory_fraction: 0.6
 }
 , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002BAAA7A6780>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
 INFO:tensorflow:Starting evaluation at 2017-11-28-12:21:42
 INFO:tensorflow:Restoring parameters from model_test\model.ckpt-54692
 2017-11-28 20:22:04.720980: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.236689657]
 INFO:tensorflow:Evaluation [1/3]
 2017-11-28 20:28:32.360331: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.238805175]
 INFO:tensorflow:Evaluation [2/3]
 2017-11-28 20:35:41.020994: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.237995088]
 INFO:tensorflow:Evaluation [3/3]
 INFO:tensorflow:Finished evaluation at 2017-11-28-12:43:21
 INFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.0108218, eval/accuracy = 0.929688, global_step = 54692, loss = 0.23783
 time:1306.1133954524994
 model: model_test\model.ckpt-54692 Evaluation results: {'eval/CER': 0.01082176, 'eval/accuracy': 0.9296875, 'loss': 0.23782997, 'global_step': 54692}
 tensorflow 1.3
 INFO:tensorflow:Using config: {'_model_dir': 'models_vgg_100K_no_eval', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': gpu_options {
   per_process_gpu_memory_fraction: 0.6
 }
 , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}
 INFO:tensorflow:Starting evaluation at 2017-11-28-12:49:50
 INFO:tensorflow:Restoring parameters from model_test\model.ckpt-54692
 2017-11-28 20:50:12.841210: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.17519826]
 INFO:tensorflow:Evaluation [1/3]
 2017-11-28 20:51:03.366275: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.2987892]
 INFO:tensorflow:Evaluation [2/3]
 2017-11-28 20:51:49.843030: I C:\tf_jenkins\home\workspace\rel-win\M\windows\PY\36\tensorflow\core\kernels\logging_ops.cc:79] * Loss : [0.20660429]
 INFO:tensorflow:Evaluation [3/3]
 INFO:tensorflow:Finished evaluation at 2017-11-28-12:52:19
 INFO:tensorflow:Saving dict for global step 54692: eval/CER = 0.01188, eval/accuracy = 0.924479, global_step = 54692, loss = 0.226864
 time:157.26274514198303
 model: model_test\model.ckpt-54692 Evaluation results: {'eval/CER': 0.011879961, 'eval/accuracy': 0.92447919, 'loss': 0.22686392, 'global_step': 54692}
 	",1.0,WenmuZhou,2017-11-28T18:19:56Z,"
 		<denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
 , any ideas what might be causing this?
 		",2.0,WenmuZhou,2017-11-28T18:58:52Z,"
 		I'm not aware of any related change within estimator.
 		",3.0,WenmuZhou,2017-11-28T19:00:27Z,"
 		May be something related to data loader part? <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  do you mind to test time difference of input_fn between 1.3 and 1.4?
 		",2d4c29cd6a0627fdd71a752e6bd919204c7cb8bf,Mustafa Ispir,2017-12-07 14:39:27-08:00,MODIFY,1,tensorflow\python\training\server_lib.py,tensorflow\python\training\server_lib.py,1.0,"310,311,312,313,314",,MODIFY,1.0,tensorflow\python\training\server_lib_test.py,tensorflow\python\training\server_lib_test.py,4.0,WenmuZhou,2017-11-29T01:54:25Z,"
 		here is the time of input_fn between 1.3 and 1.4
 the script is
 # -*- coding: utf-8 -*-
 # @Time    : 2017/11/29 8:43
 # @Author  : zhoujun
 from src.data_handler import data_loader, input_fn
 from src.config import Params, Alphabet
 import tensorflow as tf
 import time
 
 if __name__ == '__main__':
     parameters = Params(eval_batch_size=128,
                         input_shape=(32, 304),
                         digits_only=False,
                         alphabet=Alphabet.CHINESECHAR_LETTERS_DIGITS_EXTENDED,
                         alphabet_decoding='same',
                         image_channels=1,
                         csv_delimiter=' ',
                         )
 
     featureBatch, labelBatch = input_fn(csv_filename='E:/val1.csv', params=parameters,
                                         batch_size=parameters.eval_batch_size,
                                         num_epochs=1)
 
     global_init = tf.global_variables_initializer()
     loacl_init = tf.local_variables_initializer()
     with tf.Session() as sess:
         sess.run(global_init)
         sess.run(loacl_init)
         coord = tf.train.Coordinator()
         threads = tf.train.start_queue_runners(sess=sess, coord=coord)
 
         start = time.time()
         example, label = sess.run([featureBatch, labelBatch])
         print('time: ',time.time()-start)
         print(len(label))
         coord.request_stop()
         coord.join(threads)
 input_fn is extracted from data_loader function
 def input_fn(csv_filename: str, params: Params, batch_size: int = 128, data_augmentation: bool = False,
                 num_epochs: int = None):
     # Choose case one csv file or list of csv files
     if not isinstance(csv_filename, list):
         filename_queue = tf.train.string_input_producer([csv_filename], num_epochs=num_epochs,
                                                         name='filename_queue')
     elif isinstance(csv_filename, list):
         filename_queue = tf.train.string_input_producer(csv_filename, num_epochs=num_epochs, name='filename_queue')
 
     # Skip lines that have already been processed
     reader = tf.TextLineReader(name='CSV_Reader', skip_header_lines=0)
     key, value = reader.read(filename_queue, name='file_reading_op')
 
     default_line = [['None'], ['None']]
     path, label = tf.decode_csv(value, record_defaults=default_line, field_delim=params.csv_delimiter,
                                 name='csv_reading_op')
 
     image, img_width = image_reading(path, resized_size=params.input_shape, params=params,
                                      data_augmentation=data_augmentation, padding=True)
 
     to_batch = {'images': image, 'images_widths': img_width, 'filenames': path, 'labels': label}
     prepared_batch = tf.train.shuffle_batch(to_batch,
                                             batch_size=batch_size,
                                             min_after_dequeue=500,
                                             num_threads=15, capacity=4000,
                                             allow_smaller_final_batch=False,
                                             name='prepared_batch_queue')
     return prepared_batch, prepared_batch.get('labels')
 tf 1.3 log
 time:  0.4531559944152832
 128
 tf 1.4 log
 time:  0.5000338554382324
 128
 		",5.0,WenmuZhou,2017-11-29T08:30:26Z,"
 		There seems to be a tiny difference in the accuracies, are the runs exactly the same?
 The config for 1.4 seems to include a clusterspec, are you running the 1.4 run locally as well?
 Also are either or both of them using a GPU?
 		",6.0,WenmuZhou,2017-11-30T01:41:53Z,"
 		both of them ara run using 1080TI，and everything is the same except for the tensorflow version
 		",1.0,"424,425,426,427,428,429,430,431,432,433",,testStringConversion,self,424,433,,,,,,,,,,,,,,,__str__,self,310,314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,WenmuZhou,2017-11-30T01:42:39Z,"
 		the tf1.4 log of input_fn is error and I have fixed it
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,WenmuZhou,2017-11-30T17:52:14Z,"
 		<denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  thanks for helping us to investigate this issue.
 To understand the issue better could you please give us following information:
 print est_config.cluster_spec
 print os.environ['TF_CONFIG']
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,WenmuZhou,2017-11-30T18:13:43Z,"
 		Another useful information can be get by using ProfilerHook as follows:
 <denchmark-code>from tensorflow.contrib.hooks.python.training import profiler_hook
 import os
 os.mkdir('/tmp/estimator_debug')
 estimator.evaluate(input_fn=test_input_fn, hooks=[profiler_hook.ProfilerHook(save_steps=1, output_dir='/tmp/estimator_debug')])
 </denchmark-code>
 
 You can check the output by using catapult as follows:
 <denchmark-code>git clone https://github.com/catapult-project/catapult
 catapult/tracing/bin/trace2html /tmp/estimator_debug/FILENAME.json --output=/tmp/estimator_debug/FILENAME.html
 </denchmark-code>
 
 Could you please let us know the differences between 1.3 and 1.4 in profiler output?
 		",10.0,WenmuZhou,2017-12-02T02:34:00Z,"
 		the est_config.cluster_spec is None and there are a error when run
 print os.environ['TF_CONFIG']
 log is
 1.3.0
 Traceback (most recent call last):
   File ""Z:/zhoujun/tf-crnn/test_model.py"", line 110, in <module>
     tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
   File ""C:\Anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 48, in run
     _sys.exit(main(_sys.argv[:1] + flags_passthrough))
   File ""Z:/zhoujun/tf-crnn/test_model.py"", line 57, in main
     print('os.environ[\'TF_CONFIG\']',os.environ['TF_CONFIG'])
   File ""C:\Anaconda3\lib\os.py"", line 669, in __getitem__
     raise KeyError(key) from None
 KeyError: 'TF_CONFIG'
 est_config.cluster_spec None
 and when I import profile_hook, there are a error
 >>> from tensorflow.contrib.hooks.python.training import profiler_hook
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""C:\Anaconda3\lib\site-packages\tensorflow\contrib\hooks\__init__.py"", line 25, in <module>
     from tensorflow.contrib.hooks.python.training import *
 ModuleNotFoundError: No module named 'tensorflow.contrib.hooks.python'
 		",11.0,WenmuZhou,2017-12-02T09:21:10Z,"
 		I've seen a similar issue:
 My code snippet to test on tf1.3:
 <denchmark-code>def parser(record, split_name, imagenet_mean):
     assert (split_name == 'train' or split_name == 'train_dev')
     keys_to_features = {
         'image/encoded': tf.FixedLenFeature((), tf.string, default_value=''),
         'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
         'image/class/class_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),
         'image/product_id': tf.FixedLenFeature([], tf.int64, default_value=tf.zeros([], dtype=tf.int64)),
     }
     parsed = tf.parse_single_example(record, keys_to_features)
     image = tf.image.decode_jpeg(parsed['image/encoded'])
     image.set_shape([180, 180, 3])
     image = tf.cast(image, tf.float32)
     image = tf.subtract(image, imagenet_mean)
     image = tf.expand_dims(image, axis=0)
     image = tf.image.resize_bicubic(image, [224, 224])
     image = tf.squeeze(image)
     if split_name == 'train':
         image = tf.image.random_flip_left_right(image)
     label = parsed['image/class/class_id']
     product_id = parsed['image/product_id']
     return image, label, product_id
 
 
 def get_dataset(file_patterns, split_name):
     assert (split_name == 'train' or split_name == 'train_dev')
     imagenet_mean = tf.constant([_R_MEAN, _G_MEAN, _B_MEAN])
     d = tf.contrib.data.Dataset.list_files(file_patterns)
     # We choose NUM_SHARDS as buffer_size to ensure that in each epoch we are seeing all the shard TFRecord files
     # with no duplicate or missing ones.
     d = d.shuffle(buffer_size=NUM_SHARDS)
     # cycle_length is set as NUM_SHARDS so in each cycle we will be able to see images from different shards.
     d = d.interleave(lambda x: tf.contrib.data.TFRecordDataset(filenames=x), cycle_length=NUM_SHARDS, block_length=1)
     d = d.map(lambda x: parser(x, split_name, imagenet_mean), num_threads=8192, output_buffer_size=BATCH_SIZE * 20)
     d = d.batch(BATCH_SIZE)
     return d
 
 
 def main():
     config = tf.ConfigProto()
     config.gpu_options.visible_device_list = '0'
     with tf.Graph().as_default() as g:
         with tf.device('/cpu:0'):
             train_set = get_dataset(TRAIN_ON_RAM, 'train')
             train_iter = train_set.make_one_shot_iterator()
             images, labels, product_ids = train_iter.get_next()
     with tf.Session(graph=g, config=config) as sess:
         for _ in tqdm(range(1000)):
             sess.run(images)
 </denchmark-code>
 
 For tf1.4, I simply changed tf.contrib.data to tf.data, num_threads into num_parallel_calls and output_buffer_size to prefetch. Then I've seen a very significant performance drop:
 On TF 1.3 I get:
 <denchmark-code>100%|███████████████████████████████████████| 1000/1000 [03:14<00:00,  5.14it/s]
 </denchmark-code>
 
 On TF 1.4 I get:
 <denchmark-code>100%|███████████████████████████████████████| 1000/1000 [05:28<00:00,  3.05it/s]
 </denchmark-code>
 
 		",12.0,WenmuZhou,2017-12-02T15:06:46Z,"
 		I did another test (using a trained models to predict the image) and also proved that tensorflow1.4 was slower than 1.3 <denchmark-link:https://github.com/tensorflow/tensorflow/issues/15057>#15057</denchmark-link>
 
 		",13.0,WenmuZhou,2017-12-02T20:45:17Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  and <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  may have better ideas about the differences in tf.data performance.
 		",14.0,WenmuZhou,2017-12-02T21:05:08Z,"
 		It’s not clear what caused the change in performance but num_threads=8192 (or num_parallel_calls=8192) is very unlikely to be optimal, because of the potential for contention from so many parallel work items. Try setting this to a much smaller value (e.g. the number of CPU cores in your test machine) to see if this speeds things up.
 		",15.0,WenmuZhou,2017-12-03T21:28:44Z,"
 		Thanks <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 . That code was a  benchmark script I was using when trying to tune the best parameter setting for input. Initially I was using something like 64 which is the number of cores, but I've increased the number of threads all the way from 64, 128, 256 to 8192 and the performance kept improving so that's why I was using 8192 in that script. I can change it back to smaller values and show more results.
 		",16.0,WenmuZhou,2017-12-04T17:09:06Z,"
 		Thank you <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  for adding a test with Predict. That test doesn't use Dataset or Estimator. <denchmark-link:https://github.com/aselle>@aselle</denchmark-link>
  who can help us here?
 		",17.0,WenmuZhou,2017-12-04T21:11:28Z,"
 		<denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  for your  experiment:
 
 
 Did you do the 64, 128, ..., 8192 scaling using TF 1.3 or TF 1.4? If TF 1.3, could you verify that you see a similar effect for TF 1.4?
 
 
 Could you also add timing information to the parser method to see how much time is spent in this method in TF 1.3 and TF 1.4? I am not aware of any tf.data changes between TF 1.3 and TF 1.4 to the transformations you are using that should result in a performance drop, so I am trying to see if perhaps the slowdown is due to a change in the image parsing logic.
 
 
 		",18.0,WenmuZhou,2017-12-05T00:22:35Z,"
 		<denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  re:ProfilerHook, it is available as  in TF1.4. But for 1.3 sorry to here it's not working. Could you please run it only on 1.4? It may give us some info.
 		",19.0,WenmuZhou,2017-12-05T22:13:59Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  - I think that's actually my experiment but I'm happy to add more data. :)
 		",20.0,WenmuZhou,2017-12-06T04:04:54Z,"
 		<denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
   for tf 1.4 the ouput of
 print(est_config.cluster_spec)
 is
 <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000226E91666A0>
 but, the output of tf 1.3 is None
 and my python verison is python3 ,can not run
 catapult/tracing/bin/trace2html /tmp/estimator_debug/FILENAME.json --output=/tmp/estimator_debug/FILENAME.html
 		",21.0,WenmuZhou,2017-12-06T17:02:39Z,"
 		<denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  please do print(est_config.cluster_spec.as_dict()) to get the content.
 re: catapult, did you clone it? I mean following: git clone https://github.com/catapult-project/catapult
 		",22.0,WenmuZhou,2017-12-07T01:53:15Z,"
 		<denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
  the output is a None dict
 {}
 I have clone the catapult
 		",23.0,WenmuZhou,2017-12-22T07:34:19Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",24.0,WenmuZhou,2018-01-05T19:07:02Z,"
 		Nagging Awaiting TensorFlower: It has been 14 days with no activityand the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",25.0,WenmuZhou,2018-01-06T12:56:03Z,"
 		the code I used is in this <denchmark-link:https://github.com/solivr/tf-crnn>https://github.com/solivr/tf-crnn</denchmark-link>
 
 		",26.0,WenmuZhou,2018-01-06T17:43:14Z,"
 		Thanks <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
 
 <denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
  could you please look to reproduce this, and then pull in whoever else is needed.
 		",27.0,WenmuZhou,2018-01-08T17:58:33Z,"
 		Thanks <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
 
 Could you also point us an example data set in format expected by your program?
 		",28.0,WenmuZhou,2018-01-08T20:48:27Z,"
 		I have a similar issue with v1.4.x. However, it goes away with v1.5rc0.
 		",29.0,WenmuZhou,2018-01-12T05:24:54Z,"
 		<denchmark-link:https://github.com/songgc>@songgc</denchmark-link>
  v1.5rc0 is slower than 1.3 in my test
 		",30.0,WenmuZhou,2018-01-12T16:51:26Z,"
 		Hi <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
  ,
 It would be great to have an example data set in format expected by your program and creates this issue. Would you mind to point us to it?
 		",31.0,WenmuZhou,2018-01-20T01:35:42Z,"
 		I have update the code to generate dataset
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/1648413/hlp.zip>hlp.zip</denchmark-link>
 
 		",32.0,WenmuZhou,2018-01-23T22:59:51Z,"
 		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
 		",33.0,WenmuZhou,2018-01-25T21:04:55Z,"
 		Hi <denchmark-link:https://github.com/WenmuZhou>@WenmuZhou</denchmark-link>
 
 We've created two environment with GPUS one with TF1.3 and one with TF1.4.
 We've run your data generation script (BTW, there is a small mismatch with between the generated data and what your model expects).
 We've run your model with following command as describe on your github:
 'python train.py -g 1 -ft ../data/10.csv -fe ../data/10.csv -o ./export_model_dir'.
 At the end we couldn't reproduce the issue. Could you please provide us a single script which we can run and which reproduces the slowness.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.0,WenmuZhou,2018-01-29T14:20:27Z,"
 		<denchmark-link:https://github.com/ispirmustafa>@ispirmustafa</denchmark-link>
  I have update a latest <denchmark-link:https://github.com/WenmuZhou/Segmentation-Free_OCR>code</denchmark-link>
 , the python version I used is python3.5.2, and I have test once and reproduce the issue.
 the environment is
 
 python 3.5.2
 tensorflow 1.3 or 1.4
 
 Some Chinese comments cause the code to no longer run under python2 and if you need to test in python2 you need to comment out these Chinese comments
 		",35.0,WenmuZhou,2018-02-15T13:14:41Z,"
 		Nagging Awaiting TensorFlower: It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",36.0,WenmuZhou,2018-02-20T18:40:09Z,"
 		I tried to reproduce your issue with the code you provided but I cannot see any slowdown switching from 1.3 to 1.4.
 The scripts you provided are very complex, many different issues inside and outside TensorFlow could contribute to a slowdown.
 Can you provide a small repro script that isolates the issue and which we can use for debugging?
 I will close this issue as not reproducible, but please reopen this with some more specific information.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
14985,balconychy,2017-11-30T04:20:21Z,2018-01-05T14:05:19Z,tf.nn.fractional_max_pool output have same batch size when feed with different input batch size,"
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 tf.nn.fractional_max_pool output have same batch size when feed with different input batch size.
 Attached is test code I write. 2 different input is feed in with different batch size , outputs get same batch size.
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/1516498/pool_test.py.txt>pool_test.py.txt</denchmark-link>
 
 ###code result
 shape of input_a (3, 32, 32, 3)
 shape of output_a (3, 21, 21, 3)
 shape of input_b (4, 32, 32, 3)
 shape of output_b (3, 21, 21, 3)
 <denchmark-h:h3>System information</denchmark-h>
 
 == cat /etc/issue ===============================================
 Linux c-1080u 4.10.0-40-generic <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44>#44</denchmark-link>
 ~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
 VERSION=""16.04.3 LTS (Xenial Xerus)""
 VERSION_ID=""16.04""
 VERSION_CODENAME=xenial
 == are we in docker =============================================
 No
 == compiler =====================================================
 c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
 Copyright (C) 2015 Free Software Foundation, Inc.
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 == uname -a =====================================================
 Linux c-1080u 4.10.0-40-generic <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44>#44</denchmark-link>
 ~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
 == check pips ===================================================
 numpy (1.13.3)
 numpydoc (0.7.0)
 == check for virtualenv =========================================
 False
 == tensorflow import ============================================
 Traceback (most recent call last):
 File """", line 1, in 
 ModuleNotFoundError: No module named 'tensorflow'
 == env ==========================================================
 LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
 DYLD_LIBRARY_PATH is unset
 == nvidia-smi ===================================================
 Thu Nov 30 11:55:40 2017
 +-----------------------------------------------------------------------------+
 | NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |===============================+======================+======================|
 |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
 |  0%   51C    P8    21W / 280W |    860MiB / 11169MiB |      9%      Default |
 +-------------------------------+----------------------+----------------------+
 +-----------------------------------------------------------------------------+
 | Processes:                                                       GPU Memory |
 |  GPU       PID   Type   Process name                             Usage      |
 |=============================================================================|
 |    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |
 |    0      1540      G   compiz                                       315MiB |
 +-----------------------------------------------------------------------------+
 == cuda libs  ===================================================
 /usr/local/cuda-8.0/doc/man/man7/libcudart.7
 /usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
 == cat /etc/issue ===============================================
 Linux c-1080u 4.10.0-40-generic <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44>#44</denchmark-link>
 ~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
 VERSION=""16.04.3 LTS (Xenial Xerus)""
 VERSION_ID=""16.04""
 VERSION_CODENAME=xenial
 == are we in docker =============================================
 No
 == compiler =====================================================
 c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
 Copyright (C) 2015 Free Software Foundation, Inc.
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 == uname -a =====================================================
 Linux c-1080u 4.10.0-40-generic <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44>#44</denchmark-link>
 ~16.04.1-Ubuntu SMP Thu Nov 9 15:37:44 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
 == check pips ===================================================
 numpy (1.13.3)
 protobuf (3.5.0.post1)
 tensorflow-gpu (1.4.0)
 tensorflow-tensorboard (0.4.0rc3)
 == check for virtualenv =========================================
 False
 == tensorflow import ============================================
 tf.VERSION = 1.4.0
 tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
 tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
 Sanity check: array([1], dtype=int32)
 == env ==========================================================
 LD_LIBRARY_PATH /usr/local/cuda-8.0/lib64:
 DYLD_LIBRARY_PATH is unset
 == nvidia-smi ===================================================
 Thu Nov 30 11:56:18 2017
 +-----------------------------------------------------------------------------+
 | NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |===============================+======================+======================|
 |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
 |  0%   51C    P0    80W / 280W |    860MiB / 11169MiB |      0%      Default |
 +-------------------------------+----------------------+----------------------+
 +-----------------------------------------------------------------------------+
 | Processes:                                                       GPU Memory |
 |  GPU       PID   Type   Process name                             Usage      |
 |=============================================================================|
 |    0      1060      G   /usr/lib/xorg/Xorg                           542MiB |
 |    0      1540      G   compiz                                       315MiB |
 +-----------------------------------------------------------------------------+
 == cuda libs  ===================================================
 /usr/local/cuda-8.0/doc/man/man7/libcudart.7
 /usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
 	",1.0,balconychy,2017-11-30T19:51:44Z,"
 		<denchmark-link:https://github.com/balconychy>@balconychy</denchmark-link>
  Thanks for the clear bug description!
 Looks like <denchmark-link:https://github.com/weiranzhao>@weiranzhao</denchmark-link>
  wrote the code, and <denchmark-link:https://github.com/josh11b>@josh11b</denchmark-link>
  might have reviewed it.  Can one of you take a look at this?
 For convenience, here's the repro program that <denchmark-link:https://github.com/balconychy>@balconychy</denchmark-link>
  created:
 <denchmark-code>import  cifar10
 import tensorflow as tf
 import tensorflow.contrib.slim as slim
 import numpy as np
 
 def check_unequal_size():
     input_holder = tf.placeholder(tf.float32, [None,32,32,3])
     out=tf.nn.fractional_max_pool(input_holder,[1,1.5,1.5,1],name=""low_fea_pool"")
     sess=tf.Session()
     (x_train, y_train), (x_test, y_test) = cifar10.load_data()
     x_train = x_train.astype('float32')
     input_a=x_train[1:4]
     input_b=x_train[1:5]
     output_a,_,_=sess.run(out,
                {input_holder:input_a
                 })
     output_b,_,_= sess.run(out,
                  {input_holder: input_b
                  })
     print(""shape of input_a"",input_a.shape)
     print(""shape of output_a"", output_a.shape)
     print(""shape of input_b"", input_b.shape)
     print(""shape of output_b"", output_b.shape)
     pass
 
 check_unequal_size()
 </denchmark-code>
 
 		",2.0,balconychy,2017-12-20T01:27:24Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",3.0,balconychy,2017-12-21T20:54:15Z,"
 		Thanks for the detailed example. I am working on a fix. Will update the thread when it is checked in to master branch.
 		",5f0d3395d4c61000cf0cfb3dc681177147be938d,A. Unique TensorFlower,2018-01-03 15:15:02-08:00,MODIFY,0,tensorflow\core\kernels\BUILD,tensorflow\core\kernels\BUILD,0.0,3373,,MODIFY,2.0,tensorflow\core\kernels\fractional_avg_pool_op.cc,tensorflow\core\kernels\fractional_avg_pool_op.cc,4.0,balconychy,2018-01-09T21:37:07Z,"
 		<denchmark-link:https://github.com/balconychy>@balconychy</denchmark-link>
 
 This should have been fixed. I checked this with nightly build at tf-nightly 1.6.0.dev20180109.
 		",,,,,,,,,1.0,"51,52,53,54,55,56,57,58,59,60,61,62,63,64","50,51,52",tensorflow::FractionalAvgPoolOp::FractionalAvgPoolOp,context,38,65,MODIFY,3.0,tensorflow\core\kernels\fractional_max_pool_op.cc,tensorflow\core\kernels\fractional_max_pool_op.cc,1.0,"82,83,85,91,92,93,99,100,101,102,103,104,108,109,110,111,112,125,126,128,129,151,152,153,165,174","71,77,78,79,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,108,109,110,111,112,125,126,127,129,130,152,153,154,166,175",MODIFY,0.0,tensorflow\python\kernel_tests\BUILD,tensorflow\python\kernel_tests\BUILD,0.0,"372,387",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"79,80,82,86,87,88,94,95,96,97,98,99,103,104,105,106,107,119,120,122,123,148,149,150,162,171","68,72,73,74,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,103,104,105,106,107,119,120,121,123,124,149,150,151,163,172",tensorflow::FractionalAvgPoolOp::Compute,context,67,181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::FractionalMaxPoolOp::Compute,context,70,182,,,,,MODIFY,1.0,tensorflow\python\kernel_tests\fractional_avg_pool_op_test.py,tensorflow\python\kernel_tests\fractional_avg_pool_op_test.py,1.0,"315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342",,testDifferentInputTensorShape,self,315,342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\kernel_tests\fractional_max_pool_op_test.py,tensorflow\python\kernel_tests\fractional_max_pool_op_test.py,1.0,"286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313",,testDifferentInputTensorShape,self,286,313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"54,55,56,57,58,59,60,61,62,63,64,65,66,67","53,54,55",tensorflow::FractionalMaxPoolOp::FractionalMaxPoolOp,context,38,68,1.0,"238,239,241,244","246,247,248,250,252,254",tensorflow::FractionalMaxPoolGradOp::Compute,context,215,357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15034,khanrc,2017-12-01T10:24:45Z,2018-01-31T23:02:26Z,Optimize graph & graph transform tools do not support NCHW,"
 I tried optimizing graph using both <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md>Graph transform tool</denchmark-link>
  and <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference.py>Optimize graph for inference</denchmark-link>
 . Both cases produced the same error because the fused batchnorm used not NCHW, but NHWC. I've got the error like this:
 <denchmark-code>InvalidArgumentError (see above for traceback): Must provide as many biases as the channel dimension of the input tensor: [256] vs. 19 in [1,256,19,19]
 	 [[Node: prefix/convblock/BatchNorm/FusedBatchNorm = BiasAdd[T=DT_FLOAT, data_format=""NHWC"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](prefix/convblock/Conv2D, prefix/convblock/Conv2D_bn_offset)]
 </denchmark-code>
 
 Although NCHW is faster than NHWC in GPU environment, why the tools do not support NCHW?
 	",1.0,khanrc,2017-12-01T17:47:50Z,"
 		Could you provide a reproducible test case of what exactly you tried to do? Generally speaking, I think a lot of the tooling after training requires NHWC, as that was the only format when those were written. If you could provide a reproducible test case, we could work to improve it. <denchmark-link:https://github.com/petewarden>@petewarden</denchmark-link>
 , do you have any other comments?
 		",2.0,khanrc,2017-12-04T03:04:48Z,"
 		I tried:
 <denchmark-code>python tensorflow/python/tools/optimize_for_inference.py \
 --input ./ckpt/frozen_model.pb \
 --output ./ckpt/optimized_model.pb \
 --frozen_graph true \
 --input_names Placeholder \
 --output_names policy_head/softmax,value_head/value/Tanh
 </denchmark-code>
 
 and
 <denchmark-code>tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
 --in_graph='./ckpt/frozen_model.pb' \
 --out_graph='./ckpt/transformed_model.pb' \
 --inputs='Placeholder' \
 --outputs='policy_head/softmax,value_head/value/Tanh' \
 --transforms='
 fold_constants(ignore_errors=true)
 fold_batch_norms
 fold_old_batch_norms
 fuse_pad_and_conv
 fuse_resize_and_conv
 fuse_resize_pad_and_conv
 '
 </denchmark-code>
 
 In both cases, the error occurred in fused batchnorm. The frozen model worked well, but the optimized model and transformed model emitted error.
 		",3.0,khanrc,2017-12-22T07:34:33Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",6afe900f543e0005ce69b3152330f1b7b16cb286,yegord,2018-01-31 15:02:25-08:00,MODIFY,1,tensorflow\python\tools\optimize_for_inference_lib.py,tensorflow\python\tools\optimize_for_inference_lib.py,1.0,352,,MODIFY,1.0,tensorflow\python\tools\optimize_for_inference_test.py,tensorflow\python\tools\optimize_for_inference_test.py,4.0,khanrc,2018-01-05T19:08:07Z,"
 		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",5.0,khanrc,2018-01-24T13:17:05Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,1.0,"176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222","176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217",testFoldFusedBatchNorms,self,175,222,,,,,,,,,,,,,,,fold_batch_norms,input_graph_def,201,365,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15239,cesarsouza,2017-12-09T20:09:30Z,2017-12-26T04:48:52Z,No gradient defined for op: Pow,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.4
 Bazel version: N/A
 Python version: None
 CUDA/cuDNN version: None
 GPU model and memory: None
 Exact command to reproduce: -
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 It seems there is no gradient defined for the Pow operation in the C++ API.
 I am actually transferring this issue from <denchmark-link:https://github.com/migueldeicaza/TensorFlowSharp/issues/187>migueldeicaza/TensorFlowSharp#187</denchmark-link>
 . Similar to the case of Select (<denchmark-link:https://github.com/tensorflow/tensorflow/issues/14845>#14845</denchmark-link>
 ), it seems there is also no gradient for the Pow operation in the C++ API.
 	",1.0,cesarsouza,2017-12-10T06:54:32Z,"
 		<denchmark-link:https://github.com/cesarsouza>@cesarsouza</denchmark-link>
  I have created a PR  <denchmark-link:https://github.com/tensorflow/tensorflow/pull/15245>#15245</denchmark-link>
  to resolve the issue.
 		",2.0,cesarsouza,2017-12-10T06:57:20Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Bazel version
 		",3.0,cesarsouza,2017-12-10T12:14:41Z,"
 		Huge thanks <denchmark-link:https://github.com/facaiy>@facaiy</denchmark-link>
 ! Also I've just noticed that I used the wrong title for the issue, probably it seemed confusing at first. Sorry about it, and thanks again for the extremely fast PR!
 		",e1ded7fa7cfacaeea43a903e738dd3fe2baabc57,Yan Facai (颜发才),2017-12-25 23:48:51-05:00,MODIFY,1,tensorflow\cc\gradients\math_grad.cc,tensorflow\cc\gradients\math_grad.cc,1.0,"476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508",,MODIFY,1.0,tensorflow\cc\gradients\math_grad_test.cc,tensorflow\cc\gradients\math_grad_test.cc,4.0,cesarsouza,2018-09-25T12:56:53Z,"
 		Is this issue fixed now. Because i get the same error with 1.9
 C++/C# Tensorflow is useles if you want to build your model.
 		",5.0,cesarsouza,2018-09-25T22:33:33Z,"
 		The PR has been merged, so I think the issue is fixed. Could you give an example?
 		",,,,,1.0,"846,847,848,849,850,851,852",,tensorflow::TEST_F,"NaryGradTest,Pow",846,852,,,,,,,,,,,,,,,tensorflow::ops::PowGrad,"scope,op,grad_inputs,grad_outputs",476,508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15345,ghost(ghost),2017-12-13T18:38:09Z,2017-12-13T22:58:31Z,Using wrong location for x86_64 android build,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 A: Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 A: OSX 10.13.1
 TensorFlow installed from (source or binary):
 A: Source
 TensorFlow version (use command below):
 A: 1.4.1
 Python version:
 A: 2.7
 Bazel version (if compiling from source):
 A: 0.8
 GCC/Compiler version (if compiling from source):
 A:
 
 <denchmark-code>Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
 Apple LLVM version 9.0.0 (clang-900.0.38)
 Target: x86_64-apple-darwin17.2.0
 Thread model: posix
 InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
 </denchmark-code>
 
 
 CUDA/cuDNN version:
 GPU model and memory:
 Exact command to reproduce:
 make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Android x86_64 build fails with Makefile using make -f tensorflow/contrib/makefile/Makefile TARGET=ANDROID ANDROID_ARCH=x86_64 because it cannot find the binary x86-64-linux-android-g++
 It can be fixed by changing the tensorflow/contrib/makefile/Makefile at line 303 from
 BIN_PREFIX := x86-64-linux-android to
 BIN_PREFIX := x86_64-linux-android
 	",1.0,ghost(ghost),2017-12-13T19:14:56Z,"
 		I've prepped a PR for this.
 		",2.0,ghost(ghost),2017-12-13T22:58:31Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/pull/15346>#15346</denchmark-link>
  fixes this!
 		",,,,,df9189cc4671facfecd3e8249c9e8b01b11c0df5,Austin Anderson,2017-12-13 12:41:38-08:00,MODIFY,0,tensorflow\contrib\makefile\Makefile,tensorflow\contrib\makefile\Makefile,0.0,303,303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15611,huaxz1986,2017-12-24T09:33:59Z,2017-12-31T06:33:44Z,'saved_model_cli.py' bug fix!,"
 In file python/tools/saved_model_cli.py  at function def _print_tensor_info(tensor_info):
 The first line should be:
   print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])
 Not be :  print('    dtype: ' + types_pb2.DataType.keyss()[tensor_info.dtype])
 because tensor_info.dtype  is an Integer which is the value of types(not the index of type values).
 	",1.0,huaxz1986,2017-12-24T15:34:24Z,"
 		It's not necessary to do that, types_pb2.DataType is not python dict. The order of EnumTypeWrapper.keys() is not arbitrary.
 See the definition of EnumTypeWrapper.keys():
   def keys(self):
     """"""Return a list of the string names in the enum.
     These are returned in the order they were defined in the .proto file.
     """"""
 
     return [value_descriptor.name
             for value_descriptor in self._enum_type.values]
 and self._enum_type.values is something like:
   values=[
     _descriptor.EnumValueDescriptor(
       name='DT_INVALID', index=0, number=0,
       options=None,
       type=None),
     _descriptor.EnumValueDescriptor(
       name='DT_FLOAT', index=1, number=1,
       options=None,
       type=None),
     _descriptor.EnumValueDescriptor(
       name='DT_DOUBLE', index=2, number=2,
       options=None,
       type=None),
       ...
   ]
 		",2.0,huaxz1986,2017-12-25T06:10:49Z,"
 		When I run tfdbg：
 <denchmark-code>graph = tf.get_default_graph()
 with  graph.as_default():
     v1 = tf.get_variable('v1', initializer=[1, 2, 3])
     v2 = tf.square(v1, name='v2')
     sess = tf.Session()
     sess = tf_dbg.LocalCLIDebugWrapperSession(sess)
     with sess.as_default():
       sess.run(tf.global_variables_initializer())
       _v2 = sess.run(v2)
       print(_v2)
 </denchmark-code>
 
 The type of v1 is DT_INT_REF，and saved_model_cli.py throws exception.
 		",3.0,huaxz1986,2017-12-26T02:02:32Z,"
 		Yes, my mistake, for REF type, the value is not the same as its index. Can you add a minimum repuducible code and its error log to better describe this problem?
 		",2e2715baa84720f786b38d1f9cb6887399020d6f,Yifei Feng,2017-12-28 14:01:06-08:00,MODIFY,0,tensorflow\python\tools\BUILD,tensorflow\python\tools\BUILD,0.0,254,,MODIFY,1.0,tensorflow\python\tools\saved_model_cli.py,tensorflow\python\tools\saved_model_cli.py,4.0,huaxz1986,2017-12-27T10:05:19Z,"
 		This is a simple  example to show the usage of saved_model_cli：
 <denchmark-code>export_dir = 'out'
 builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
 graph = tf.get_default_graph()
 with graph.as_default():
   v1 = tf.get_variable('v1',shape=[3],
                   initializer=tf.zeros_initializer)
   v2 = tf.square(v1,name='v2')
   with tf.Session(graph=graph) as sess:
     sess.run(v1.initializer)
     builder.add_meta_graph_and_variables(sess,['xxx'])
     predict_signature_def = tf.saved_model.\
     	signature_def_utils.build_signature_def(\
              inputs={
         		 'input_x': \
                   tf.saved_model.utils.build_tensor_info(v1),
              }, outputs={'pred_y':\
 				  tf.saved_model.utils.build_tensor_info(v2)},\
              method_name=tf.saved_model.\
  			 signature_constants.PREDICT_OUTPUTS)
     builder.add_meta_graph([tf.saved_model.\
              tag_constants.TRAINING,
              tf.saved_model.tag_constants.SERVING],\
              signature_def_map={'predict_graph': predict_signature_def})
     builder.save(as_text=True)
 </denchmark-code>
 
 Then run saved_model_cli in shell：
 <denchmark-code>saved_model_cli show --dir out/ --tag_set serve,train --signature_def predict_graph
 </denchmark-code>
 
 The result is :
 <denchmark-code>The given SavedModel SignatureDef contains the following input(s):
 inputs['input_x'] tensor_info:
 tensor type: 101
     dtype: DT_FLOAT_REF
     shape: (3)
     name: v1:0
 The given SavedModel SignatureDef contains the following output(s):
 outputs['pred_y'] tensor_info:
 tensor type: 1
     dtype: DT_FLOAT
     shape: (3)
     name: v2:0
 Method name is: outputs
 </denchmark-code>
 
 I change tensorflow/python/tools/saved_model_cli._print_tensor_info() like this:
 <denchmark-code>def _print_tensor_info(tensor_info):
   """"""Prints details of the given tensor_info.
 
   Args:
     tensor_info: TensorInfo object to be printed.
   """"""
 #####  this is what I do ####
   print('tensor type:',tensor_info.dtype)
   print('    dtype: ' + {value:key for (key,value) in types_pb2.DataType.items()}[tensor_info.dtype])
 ##########
   # Display shape as tuple.
   if tensor_info.tensor_shape.unknown_rank:
     shape = 'unknown_rank'
   else:
     dims = [str(dim.size) for dim in tensor_info.tensor_shape.dim]
     shape = ', '.join(dims)
     shape = '(' + shape + ')'
   print('    shape: ' + shape)
   print('    name: ' + tensor_info.name)
 </denchmark-code>
 
 By the way, I use python3.6+ubuntu 16.04+ tensorflow 1.4
 		",5.0,huaxz1986,2017-12-27T13:20:33Z,"
 		Thanks for the supplement. I am not sure who is responsible for this module, friendly ping <denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
  <denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
 
 		",6.0,huaxz1986,2017-12-27T21:18:48Z,"
 		/CC <denchmark-link:https://github.com/sukritiramesh>@sukritiramesh</denchmark-link>
 
 		",1.0,"155,156,157",155,_print_tensor_info,tensor_info,149,166,MODIFY,2.0,tensorflow\python\tools\saved_model_cli_test.py,tensorflow\python\tools\saved_model_cli_test.py,1.0,,220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,huaxz1986,2017-12-27T22:07:01Z,"
 		Thanks <denchmark-link:https://github.com/huaxz1986>@huaxz1986</denchmark-link>
 ! Will fix this internally!
 		",testInputPreProcessFileNames,self,216,223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"205,206,207,208,209,210,211",,testPrintREFTypeTensor,self,205,211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15766,matthew-z,2018-01-01T09:43:43Z,2018-01-05T14:05:19Z,tf.assert_equal raises incorrect traceback in Eager mode,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.1 LTS
 TensorFlow installed from (source or binary): pip binary
 TensorFlow version (use command below): 1.5.0-dev20171227
 Python version: 3.5.0
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: None
 GPU model and memory: None
 Exact command to reproduce: python main.py
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 In eager mode, tf.assert_equal only shows [] in traceback message when two inputs are different. However, in graph mode, it does show different values in the message.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 import tensorflow as tf
 import tensorflow.contrib.eager as tfe
 
 tfe.enable_eager_execution()
 
 x = tf.constant([1,2,3])
 y = tf.constant([3,2,1])
 
 with tf.control_dependencies([tf.assert_equal(x, y)]):
     output = tf.reduce_sum(x)
 Eager Mode Traceback:
 <denchmark-code>Traceback (most recent call last):
   File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
     with tf.control_dependencies([tf.assert_equal(x, y)]):
   File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 376, in assert_equal
     summary_msg)))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: 
 Condition x == y did not hold.
 Indices of first 0 different values:
 []
 Corresponding x values:
 []
 Corresponding y values:
 []
 </denchmark-code>
 
 Graph Mode Traceback:
 <denchmark-code>Traceback (most recent call last):
   File ""/Users/matt/PycharmProjects/scratch/main.py"", line 9, in <module>
     with tf.control_dependencies([tf.assert_equal(x, y)]):
   File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 391, in assert_equal
     _assert_static(condition_static, data)
   File ""/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/ops/check_ops.py"", line 104, in _assert_static
     message='\n'.join(data_static))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: 
 Condition x == y did not hold element-wise:
 x (Const:0) = 
 [1 2 3]
 y (Const_1:0) = 
 [3 2 1]
 </denchmark-code>
 
 	",1.0,matthew-z,2018-01-02T18:51:31Z,"
 		Thanks for the report. <denchmark-link:https://github.com/iganichev>@iganichev</denchmark-link>
  - could you take a look?
 		",2.0,matthew-z,2018-01-02T21:50:59Z,"
 		The fix is in review. In the meantime, you can use summarize=3 parameter to assert_equals.
 		",3.0,matthew-z,2018-01-03T05:07:48Z,"
 		Thank you very much!
 		",89cd0cd81ae829610fcbf4437597451ae5a59fe6,Igor Ganichev,2018-01-02 16:35:20-08:00,MODIFY,1,tensorflow\python\kernel_tests\check_ops_test.py,tensorflow\python\kernel_tests\check_ops_test.py,1.0,"120,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,169,170,171",120,MODIFY,1.0,tensorflow\python\ops\check_ops.py,tensorflow\python\ops\check_ops.py,,,,,,,,,,,,,1.0,"343,345,346,347,359,365,376,377,378","343,356,362,363,364,375,376,377",assert_equal,"x,y,data,summarize,message,name",302,395,,,,,,,,,,,,,,,test_error_message_eager,self,117,175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15882,jgefele,2018-01-05T15:34:48Z,2018-01-30T23:34:01Z,"tfdbg error ""Dump root directory does not exist"" with empty fetches","
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code: yes
 OS Platform and Distribution: Linux Ubuntu 16.04
 TensorFlow installed from: binary (pip install)
 TensorFlow version:
 == tensorflow import ============================================
 tf.VERSION = 1.4.1
 tf.GIT_VERSION = v1.4.0-19-ga52c8d9
 tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
 Sanity check: array([1], dtype=int32)
 Python version: 2.7.12
 CUDA/cuDNN version:
 == cuda libs  ===================================================
 /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
 /usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
 /usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
 /usr/local/cuda-9.0/doc/man/man7/libcudart.7
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
 /usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
 /usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
 /usr/local/cuda-8.0/doc/man/man7/libcudart.7
 GPU model and memory: GeForce GTX 1080, 8114MiB
 Exact command to reproduce: see code below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 LocalCLIDebugWrapperSession.run() does not behave like tf.Session.run() if there are no fetches. The dump directory will never be created and it crashes with an IOError. For me this issue occured in a situation like this:
 <denchmark-code>      session.run([var.initializer for var in not_initialized_from_checkpoint])
 </denchmark-code>
 
 where actually everything was restored from the checkpoint and  was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it's not too hard to fix, it would be nice to keep other users from the same pain (maybe - just speculating - <denchmark-link:https://github.com/tensorflow/tensorflow/issues/13604>#13604</denchmark-link>
  crashes for the same reason)
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>import tensorflow as tf
 from tensorflow.python import debug as tf_debug
 
 sess = tf.Session()
 dbg_sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())
 
 print sess.run([tf.constant(1.0)])     # [1.0]
 print sess.run([])                     # []
 print dbg_sess.run([tf.constant(1.0)]) # [1.0]
 print dbg_sess.run([])                 # IOError: Dump root directory /tmp/tfdbg_ai_aWv does not exist
 </denchmark-code>
 
 	",1.0,jgefele,2018-01-06T01:26:18Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Bazel version
 		",2.0,jgefele,2018-01-07T18:23:29Z,"
 		Dear <denchmark-link:https://github.com/tensorflowbutler>@tensorflowbutler</denchmark-link>
 , it's ""Bazel version: N/A"".
 		",3.0,jgefele,2018-01-23T23:06:44Z,"
 		The original poster has replied to this issue after the stat:awaiting response label was applied.
 		",1f26c65254268730b7409f517d1ed1b554d01e50,Shanqing Cai,2018-01-29 10:33:05-08:00,MODIFY,1,tensorflow\python\debug\wrappers\dumping_wrapper_test.py,tensorflow\python\debug\wrappers\dumping_wrapper_test.py,1.0,"392,393,394,395",,MODIFY,0.0,tensorflow\python\debug\wrappers\framework.py,tensorflow\python\debug\wrappers\framework.py,4.0,jgefele,2018-01-28T03:40:28Z,"
 		Thanks for reporting the issue, <denchmark-link:https://github.com/jgefele>@jgefele</denchmark-link>
  . We are working on a fix, which will let tfdbg's  wrappers bypass  calls with effectively empty fetches. Let me know if you have any further questions.
 		",,,,,,,,,0.0,"124,126,444,445,446,447,448,449",442,,,,,MODIFY,2.0,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,1.0,"674,675,676,677,678,679",,,,,,,,,testDumpingWrapperWithEmptyFetchWorks,self,392,395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testRunsWithEmptyNestedFetchWorks,self,674,679,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"667,668,669,670,671,672",,testRunsWithEmptyFetchWorks,self,667,672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
15891,ghost(ghost),2018-01-05T21:51:29Z,2018-02-06T18:29:02Z,Dependencies of tensors created within a tf.while_loop() might not be executed,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See test case below.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 'Sierra' Version 10.12.6 (16G1114)
 TensorFlow installed from (source or binary): Both. I have compiled TensorFlow at 136697e with my small change in PR #15823. I have also tried using the pip package.
 TensorFlow version (use command below): ('v1.4.0-19-ga52c8d9b01', '1.4.1') (pip package)
 Python version: 2.7.10
 Bazel version (if compiling from source): 0.9.0-homebrew
 GCC/Compiler version (if compiling from source): Apple LLVM version 8.1.0 (clang-802.0.42)
 CUDA/cuDNN version: CUDA 9.0.176_mac, cuDNN 9.0-osx-x64-v7
 GPU model and memory: NVIDIA GeForce GT 750M with 2048 MB device memory (CUDA Compute Capability 3.0)
 Exact command to reproduce:
 
 python repro.py
 .. where repro.py contains the test case to reproduce, listed below.
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Here is my test case:
 # Part I
 from __future__ import division, print_function
 import numpy as np
 import tensorflow as tf
 from tensorflow.python.ops import resource_variable_ops as rr
 
 rs = np.random.RandomState(seed = 2)
 A = rs.normal(size = (10, 10,))
 print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
 B = rs.normal(size = (10, 10,))
 print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))
 
 
 
 # Part II
 A_var = tf.Variable(B)
 init_A_var_op = tf.assign(A_var, A)
 A_dep = tf.constant(10, tf.int32)
 
 with tf.control_dependencies([init_A_var_op]):
     A_dep = A_dep + 1
 
 with tf.control_dependencies([A_dep]):
     var_s = tf.svd(A_var, compute_uv = False)
 with tf.Session() as session:
     session.run(tf.global_variables_initializer())
     computed_s, computed_A_dep = session.run([var_s, A_dep])
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 
 
 
 # Part III
 A_var = tf.Variable(B)
 init_A_var_op = tf.assign(A_var, A)
 A_dep = tf.constant(9, tf.int32)
 
 def loop_condition(j, A_dep):
     return j < 1
 def loop_body(j, A_dep):
     with tf.control_dependencies([init_A_var_op]):
         A_dep = A_dep + 1
     return j + 1, A_dep
 
 _, A_dep = tf.while_loop(loop_condition,
                          loop_body,
                          loop_vars = [tf.constant(0, tf.int32), A_dep],
                          parallel_iterations = 1,
                          back_prop = False)
 
 with tf.control_dependencies([A_dep]):
     var_s = tf.svd(A_var, compute_uv = False)
 with tf.Session() as session:
     session.run(tf.global_variables_initializer())
     computed_s, computed_A_dep = session.run([var_s, A_dep])
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 
 
 
 # Part IV
 A_var = rr.ResourceVariable(B)
 init_A_var_op = A_var.assign(A)
 A_dep = tf.constant(8, tf.int32)
 
 def loop_condition(j, A_dep):
     return j < 1
 def loop_body(j, A_dep):
     with tf.control_dependencies([init_A_var_op]):
         A_dep = A_dep + 1
     return j + 1, A_dep
 
 _, A_dep = tf.while_loop(loop_condition,
                          loop_body,
                          loop_vars = [tf.constant(0, tf.int32), A_dep],
                          parallel_iterations = 1,
                          back_prop = False)
 
 with tf.control_dependencies([A_dep]):
     var_s = tf.svd(A_var.read_value(), compute_uv = False)
 with tf.Session() as session:
     session.run(tf.global_variables_initializer())
     computed_s, computed_A_dep = session.run([var_s, A_dep])
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 Part I is basic setup. I create two random 10×10 matrices and compute their singular values:
 singular values of A: [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
   1.73387162  1.16000494  0.58836563  0.39101954]
 singular values of B: [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
   1.86004291  1.6626967   0.63884034  0.27131664]
 
 Part II shows usage of control_dependencies() to guarantee that A has been assigned to A_var before the singular values of A_var are computed. The output from this part is:
 computed_s = [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
   1.73387162  1.16000494  0.58836563  0.39101954], computed_A_dep = 11
 
 (This is the expected result for Part II.)
 In Part III, I have introduced use of a tf.while_loop(). Now, tf.svd() is returning the singular values of B:
 computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
   1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 10
 
 (This is not the expected result for Part III. I expect that the singular values of A would be printed.)
 In Part IV, based on reading <denchmark-link:https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336609536>#4663 (comment)</denchmark-link>
  , I switched to using . However, the output is still the same (the singular values of ):
 computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
   1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 9
 
 (This is not the expected result for Part IV. I expect that the singular values of A would be printed.)
 It appears the issue is that tf.control_dependencies() on tensors created by tf.while_loop() might not execute the tensors' own dependencies.
 This used to work okay (around TensorFlow 1.1, if I recall correctly).
 While searching for a previous report of this issue, I found <denchmark-link:https://github.com/tensorflow/tensorflow/issues/6087>#6087</denchmark-link>
  which appears related, in that the sample code there has a tf.while_loop() that creates tensors with dependencies. When I run the sample code, I consistently get result = 10. This is an unexpected result, in my opinion. What is happening is that  runs exactly once, so for each of the 5 loop iterations,  has the value 2.
 I tried rewriting the sample code to use a ResourceVariable, but the output is the same:
 from __future__ import division, print_function
 import tensorflow as tf
 from tensorflow.python.ops import resource_variable_ops as rr
 
 with tf.variable_scope('state'):
     x = rr.ResourceVariable(tf.constant(1, dtype=tf.float32))
     update_x = x.assign(x.read_value() + 1)
 
 def iter_fun(i, y):
     # comment the line below, the program will run without any error
     # but I need control_dependencies, or at least some way to replace it...
     with tf.control_dependencies([update_x]):
         y = y + tf.Print(x.read_value(), ['i = ', i, 'y = ', y, 'x = ', x.read_value()])
     return (i+1, y,)
 
 with tf.variable_scope('iteration'):
     num_iterations = 5
     initial_i = tf.constant(0, dtype=tf.int32)
     initial_y = tf.constant(0, dtype=tf.float32)
     _, result = tf.while_loop(
         cond=lambda i, *_: i < num_iterations,
         body=iter_fun,
         loop_vars=(initial_i, initial_y))
 
 init_op = tf.global_variables_initializer()
 
 with tf.Session() as sess:
     sess.run(init_op)
     print(sess.run(result))
 	",1.0,ghost(ghost),2018-01-11T21:54:17Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  Do you have any thoughts on this or know who would know it best?
 		",2.0,ghost(ghost),2018-01-11T22:03:58Z,"
 		Can you write a much smaller minimal failing test?
 		",3.0,ghost(ghost),2018-01-11T22:04:25Z,"
 		oh wait; i see you did!
 		",f8f921c828fb2c97da7c7b80c01390ccec90ae40,Alexandre Passos,2018-02-05 13:08:15-08:00,MODIFY,4,tensorflow\python\kernel_tests\control_flow_ops_py_test.py,tensorflow\python\kernel_tests\control_flow_ops_py_test.py,1.0,"707,708,709,710,711,712,713,714,715,716,717,718,719,720",,MODIFY,2.0,tensorflow\python\ops\control_flow_ops.py,tensorflow\python\ops\control_flow_ops.py,4.0,ghost(ghost),2018-01-11T22:05:22Z,"
 		Try passing parallel_iterations=1 to your while_loop call.
 		",5.0,ghost(ghost),2018-01-11T22:16:57Z,"
 		I just now tried adding  to my adapted <denchmark-link:https://github.com/tensorflow/tensorflow/issues/6087>#6087</denchmark-link>
  test case, but this didn't fix the problem.
 As for the first test case, you can take Parts I and III separately to reproduce the issue:
 from __future__ import division, print_function
 import numpy as np
 import tensorflow as tf
 from tensorflow.python.ops import resource_variable_ops as rr
 
 rs = np.random.RandomState(seed = 2)
 A = rs.normal(size = (10, 10,))
 print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
 B = rs.normal(size = (10, 10,))
 print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))
 
 A_var = tf.Variable(B)
 init_A_var_op = tf.assign(A_var, A)
 A_dep = tf.constant(9, tf.int32)
 
 def loop_condition(j, A_dep):
     return j < 1
 def loop_body(j, A_dep):
     with tf.control_dependencies([init_A_var_op]):
         A_dep = A_dep + 1
     return j + 1, A_dep
 
 _, A_dep = tf.while_loop(loop_condition,
                          loop_body,
                          loop_vars = [tf.constant(0, tf.int32), A_dep],
                          parallel_iterations = 1,
                          back_prop = False)
 
 with tf.control_dependencies([A_dep]):
     var_s = tf.svd(A_var, compute_uv = False)
 with tf.Session() as session:
     session.run(tf.global_variables_initializer())
     computed_s, computed_A_dep = session.run([var_s, A_dep])
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 (Alternatively, take Parts I and IV separately.)
 computed_s is the vector of singular values of B, whereas I am expecting that it will be the singular values of A.
 		",6.0,ghost(ghost),2018-01-23T23:06:34Z,"
 		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
 		",1.0,"1634,1636,1637,1640",1637,_RemoveExternalControlEdges,"self,op",1621,1640,,,,,,,,,,,,,,,testWhileExternalControlDependencies,self,707,720,1.0,"722,723,724,725,726,727,728,729,730,731,732,733,734,735",,testWhileExternalControlDependenciesNoInput,self,722,735,1.0,"713,714,715",,testWhileExternalControlDependencies.body_fn,i,713,715,1.0,"728,729,730",,testWhileExternalControlDependenciesNoInput.body_fn,unused_i,728,730,1.0,"2438,2439,2443,2456,2457,2458,2459,2465,2466,2467,2468,2469","2435,2436,2437,2438,2442,2455,2456,2457",_AddOpInternal,"self,op",2435,2476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ghost(ghost),2018-01-30T00:30:57Z,"
 		/CC <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  can you take a look?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,ghost(ghost),2018-01-30T00:38:47Z,"
 		Can you add a print(tf.get_default_graph().as_graph_def()) so I can understand how we're generating the wrong graph? (i.e. are control dependencies missing, mangled, or ignored because of weird variable stuff)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,ghost(ghost),2018-01-30T02:09:17Z,"
 		I created a gist for the graph def: <denchmark-link:https://gist.github.com/dtrebbien/f917cb2891e0b141b9fa6323a3c55239>https://gist.github.com/dtrebbien/f917cb2891e0b141b9fa6323a3c55239</denchmark-link>
 
 Here is the exact modified test case that I used to print the graph def:
 from __future__ import division, print_function
 import numpy as np
 import tensorflow as tf
 
 rs = np.random.RandomState(seed = 2)
 A = rs.normal(size = (10, 10,))
 print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
 B = rs.normal(size = (10, 10,))
 print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))
 
 graph = tf.Graph()
 with graph.as_default():
     A_var = tf.Variable(B, name = 'A_var')
     init_A_var_op = tf.assign(A_var, A, name = 'init_A_var_op')
     A_dep = tf.constant(9, tf.int32, name = 'initial_A_dep')
 
     def loop_condition(j, A_dep):
         return j < 1
     def loop_body(j, A_dep):
         with tf.control_dependencies([init_A_var_op]):
             A_dep = tf.add(A_dep, 1, name = 'increment_A_dep')
         return j + 1, A_dep
 
     _, A_dep = tf.while_loop(loop_condition,
                              loop_body,
                              loop_vars = [tf.constant(0, tf.int32), A_dep],
                              parallel_iterations = 1,
                              back_prop = False)
 
     with tf.control_dependencies([A_dep]):
         var_s = tf.svd(A_var, compute_uv = False)
 
 print(graph.as_graph_def())
 
 with tf.Session(graph = graph) as session:
     session.run(tf.global_variables_initializer())
     computed_A_dep, computed_s, computed_A_dep2 = session.run([A_dep, var_s, A_dep])
     assert computed_A_dep == computed_A_dep2
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 By the way, is there a utility that can graphically display this?
 		",10.0,ghost(ghost),2018-01-30T02:56:02Z,"
 		Looking at the graph def, it looks like no node has control input ""^init_A_var_op"".
 Contrasting that with the following working script—which does not use a tf.while_loop()—the ""increment_A_dep/y"" node corresponding to the const second argument to the ""increment_A_dep"" tf.add() op has control input ""^init_A_var_op"":
 from __future__ import division, print_function
 import numpy as np
 import tensorflow as tf
 
 rs = np.random.RandomState(seed = 2)
 A = rs.normal(size = (10, 10,))
 print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
 B = rs.normal(size = (10, 10,))
 print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))
 
 graph = tf.Graph()
 with graph.as_default():
     A_var = tf.Variable(B, name = 'A_var')
     init_A_var_op = tf.assign(A_var, A, name = 'init_A_var_op')
     A_dep = tf.constant(9, tf.int32, name = 'initial_A_dep')
 
     with tf.control_dependencies([init_A_var_op]):
         A_dep = tf.add(A_dep, 1, name = 'increment_A_dep')
 
     with tf.control_dependencies([A_dep]):
         var_s = tf.svd(A_var, compute_uv = False)
 
 print(graph.as_graph_def())
 
 with tf.Session(graph = graph) as session:
     session.run(tf.global_variables_initializer())
     computed_A_dep, computed_s, computed_A_dep2 = session.run([A_dep, var_s, A_dep])
     assert computed_A_dep == computed_A_dep2
 print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
 Here is an excerpt from the working script's graph def:
 <denchmark-code>node {
   name: ""initial_A_dep""
   op: ""Const""
   attr {
     key: ""dtype""
     value {
       type: DT_INT32
     }
   }
   attr {
     key: ""value""
     value {
       tensor {
         dtype: DT_INT32
         tensor_shape {
         }
         int_val: 9
       }
     }
   }
 }
 node {
   name: ""increment_A_dep/y""
   op: ""Const""
   input: ""^init_A_var_op""
   attr {
     key: ""dtype""
     value {
       type: DT_INT32
     }
   }
   attr {
     key: ""value""
     value {
       tensor {
         dtype: DT_INT32
         tensor_shape {
         }
         int_val: 1
       }
     }
   }
 }
 node {
   name: ""increment_A_dep""
   op: ""Add""
   input: ""initial_A_dep""
   input: ""increment_A_dep/y""
   attr {
     key: ""T""
     value {
       type: DT_INT32
     }
   }
 }
 node {
   name: ""Svd""
   op: ""Svd""
   input: ""A_var/read""
   input: ""^increment_A_dep""
   attr {
     key: ""T""
     value {
       type: DT_DOUBLE
     }
   }
   attr {
     key: ""compute_uv""
     value {
       b: false
     }
   }
   attr {
     key: ""full_matrices""
     value {
       b: false
     }
   }
 }
 </denchmark-code>
 
 The non-working graph's ""while/increment_A_dep/y"" node has control input ""^while/Identity"" but not ""^init_A_var_op"".
 		",11.0,ghost(ghost),2018-01-30T17:39:48Z,"
 		Ok, so this is a real bug. <denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  who do you think we should assign this to?
 There's a bug somewhere in the control dependency processing logic of WhileContext, somewhere around here most likely: 
 
 
 tensorflow/tensorflow/python/ops/control_flow_ops.py
 
 
          Line 2414
       in
       ba64f53
 
 
 
 
 
 
  def _MaybeAddControlDependency(self, op): 
 
 
 
 
 
 		",12.0,ghost(ghost),2018-01-30T19:28:41Z,"
 		<denchmark-link:https://github.com/skye>@skye</denchmark-link>
  was looking into something similar, I think.
 <denchmark-link:https://github.com/skye>@skye</denchmark-link>
  - let me know if I'm mistaken and will try to find someone else.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16100,progwolff,2018-01-13T12:20:41Z,2018-01-22T20:27:14Z,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
 OS Platform and Distribution:
 TensorFlow installed from: pip3 install --user tensorflow-gpu
 TensorFlow version: 1.4.1
 Python version: 3.5.2
 CUDA: 8.0
 GPU: NVidia Titan X
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Using a TimeFreqLSTMCell in a dynamic_rnn or static_rcnn without providing the optional parameter frequency_skip results in an exception:
 <denchmark-code>TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
 </denchmark-code>
 
 The line which throws this exception is 
 
 
 tensorflow/tensorflow/contrib/rnn/python/ops/rnn_cell.py
 
 
         Lines 474 to 475
       in
       8b78c23
 
 
 
 
 
 
  num_feats = int((input_size - self._feature_size) / ( 
 
 
 
  self._frequency_skip)) + 1 
 
 
 
 
 
 frequency_skip has it's default value None here.
 Maybe the default should be changed to 1?
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:
 <denchmark-code>lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                 
 stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                 
 lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
 </denchmark-code>
 
 	",1.0,progwolff,2018-01-13T15:02:35Z,"
 		After reading the code, I agree that this is a bug. Don't know who knows this most, friendly ping <denchmark-link:https://github.com/cy89>@cy89</denchmark-link>
 
 		",,,,,,,,,4b6abfd95254910d01e886123cd24c29f722e8d7,Julian Wolff,2018-01-22 12:27:14-08:00,MODIFY,2,tensorflow\contrib\rnn\python\ops\rnn_cell.py,tensorflow\contrib\rnn\python\ops\rnn_cell.py,1.0,332,332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,num_units,use_peepholes,cell_clip,initializer,num_unit_shards,forget_bias,feature_size,frequency_skip,reuse",329,333,1.0,332,332,__init__,"self,num_units,use_peepholes,cell_clip,initializer,num_unit_shards,forget_bias,feature_size,frequency_skip,reuse",329,333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16152,frexvahi,2018-01-16T09:41:53Z,2019-01-17T01:01:57Z,DeprecationWarning from `inspect.getargspec()`,"
  is deprecated in Python 3
 <denchmark-link:https://docs.python.org/3/library/inspect.html#inspect.getargspec>https://docs.python.org/3/library/inspect.html#inspect.getargspec</denchmark-link>
 
 I solved the problem in keras like this:
 <denchmark-link:https://github.com/keras-team/keras/pull/7035>keras-team/keras#7035</denchmark-link>
 
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Using tensorflow as a keras backend (keras 2.1.2)
 Linux Ubuntu 16.04
 installed from conda
 version 1.3.0
 python 3.6.4
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 We recently switched from theano to tensorflow and this warning message is filling up my test output.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>/home/<name>/.conda/envs/<env>/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning:
   
   inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
 </denchmark-code>
 
 	",1.0,frexvahi,2018-01-16T18:08:01Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
 , FYI
 <denchmark-link:https://github.com/frexvahi>@frexvahi</denchmark-link>
 , thanks for reporting this. Would you be willing to submit a PR to fix this, since you already didd this in Keras?
 		",2.0,frexvahi,2018-01-17T08:33:42Z,"
 		I could submit a PR, but getting someone to sign the corporate CLA would be harder.
 		",3.0,frexvahi,2018-01-23T23:03:23Z,"
 		The original poster has replied to this issue after the stat:awaiting response label was applied.
 		",1744b8c0519cec31764d205b813bd4fd6028cbf9,Yong Tang,2018-05-10 09:07:44-07:00,MODIFY,1,tensorflow\python\util\tf_inspect.py,tensorflow\python\util\tf_inspect.py,1.0,119,119,,,,,4.0,frexvahi,2018-02-15T01:18:36Z,"
 		I fixed it on my end. I will try to submit a pull request today or tomorrow with the fix.
 		",5.0,frexvahi,2018-02-15T03:20:32Z,"
 		Just a quick update: I submitted a pull request that should fix the issue.
 		",6.0,frexvahi,2018-02-21T20:58:16Z,"
 		Can you say ""Fixes <denchmark-link:https://github.com/tensorflow/tensorflow/issues/16152>#16152</denchmark-link>
 "" in the description for your PR? That way this issue will be closed when your PR is submitted.
 		",,,,,,,,,,,,,,,,,,,,,,getcallargs,"func,positional,named",103,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,frexvahi,2018-03-28T18:55:29Z,"
 		A quick update. I believe the issue has been fixed in r1.7 since ever since I started using it I haven't had any of the warnings.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,frexvahi,2018-04-01T05:33:34Z,"
 		Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,frexvahi,2018-04-27T08:48:43Z,"
 		I'm still hitting this, Tensorflow 1.7.0 and 1.8.0-rc1. Python 3.6.5. Afraid I don't have an easy reproducer to hand, will update if I get one.
 		",10.0,frexvahi,2018-04-27T09:56:26Z,"
 		Reproducer below tested on 1.8.0-rc1. Was hard to make a reproducer because the warning did not show up unless I had warnings.filterwarnings('error'), which I use to make it easier to find the sources of warnings.
 import tensorflow as tf
 
 import warnings
 warnings.filterwarnings('error')
 
 tf.reduce_sum(tf.placeholder(tf.float64))
 Full stack trace below.
 
 ---------------------------------------------------------------------------
 DeprecationWarning                        Traceback (most recent call last)
 <ipython-input-1-1de048b23827> in <module>()
       4 warnings.filterwarnings('error')
       5 
 ----> 6 tf.reduce_sum(tf.placeholder(tf.float64))
 
 ~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
     403       if is_in_graph_mode.IS_IN_GRAPH_MODE() and _PRINT_DEPRECATION_WARNINGS:
     404         invalid_args = []
 --> 405         named_args = tf_inspect.getcallargs(func, *args, **kwargs)
     406         for arg_name, spec in iter(deprecated_positions.items()):
     407           if (spec.position < len(args) and
 
 ~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/tf_inspect.py in getcallargs(func, *positional, **named)
     107   argspec will be used.
     108   """"""
 --> 109   argspec = getargspec(func)
     110   call_args = named.copy()
     111   this = getattr(func, 'im_self', None) or getattr(func, '__self__', None)
 
 ~/Library/Python/3.6/lib/python/site-packages/tensorflow/python/util/tf_inspect.py in getargspec(object)
      53   decorators, target = tf_decorator.unwrap(object)
      54   return next((d.decorator_argspec for d in decorators
 ---> 55                if d.decorator_argspec is not None), _inspect.getargspec(target))
      56 
      57 
 
 /usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/inspect.py in getargspec(func)
    1069     warnings.warn(""inspect.getargspec() is deprecated, ""
    1070                   ""use inspect.signature() or inspect.getfullargspec()"",
 -> 1071                   DeprecationWarning, stacklevel=2)
    1072     args, varargs, varkw, defaults, kwonlyargs, kwonlydefaults, ann = \
    1073         getfullargspec(func)
 
 DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
 
 		",11.0,frexvahi,2018-05-05T13:13:42Z,"
 		Tensorflow 1.9.0-dev20180427
 <denchmark-code>import tensorflow as tf
 miniconda3/envs/kaggle/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
   from ._conv import register_converters as _register_converters
 </denchmark-code>
 
 		",12.0,frexvahi,2018-05-10T14:33:11Z,"
 		Added PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/19199>#19199</denchmark-link>
  as an attempt for the fix.
 		",13.0,frexvahi,2018-09-25T10:55:25Z,"
 		I am still hitting this warning with the following versions:
 
 1.8.0
 1.9.0
 1.10.0
 1.10.1
 1.11.0-rc2
 
 		",14.0,frexvahi,2018-09-27T08:53:38Z,"
 		Had so many of these on 1.8 that i could not longer see my test output. Updated to 1.10 and now i only have a few dozen of these warnings left.
 		",15.0,frexvahi,2018-09-27T13:18:08Z,"
 		Yeah, still seeing this in 1.11.0-rc0:
 <denchmark-code>/Users/josh/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
   return _inspect.getargspec(target)
 /Users/josh/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
   return _inspect.getargspec(target)
 ...
 </denchmark-code>
 
 Can we reopen?
 		",16.0,frexvahi,2018-10-11T08:29:33Z,"
 		any updates?
 		",17.0,frexvahi,2018-10-11T15:21:14Z,"
 		I think the issue has been resolved in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/22517>#22517</denchmark-link>
  now.
 		",18.0,frexvahi,2018-10-18T22:45:01Z,"
 		Still getting this issue with 1.11.0 and Python 3.6
 		",19.0,frexvahi,2018-10-19T08:08:21Z,"
 		me too.
 		",20.0,frexvahi,2018-11-13T20:54:21Z,"
 		me too
 		",21.0,frexvahi,2018-11-17T01:34:41Z,"
 		I see it in 1.12 too on Python 3.6
 		",22.0,frexvahi,2018-11-23T01:15:48Z,"
 		
 I see it in 1.12 too on Python 3.6
 
 the same to you
 		",23.0,frexvahi,2019-01-17T00:55:34Z,"
 		Resolved in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/22517>#22517</denchmark-link>
 , so I believe it will appear in the next release (TF 1.13 or 2.0).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16163,velikodniy,2018-01-16T16:55:16Z,2018-02-23T05:43:34Z,Dataset.from_generator doesn't release memory after recreating the session,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.5.0-rc0
 Python version: Python 3.6
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: -
 GPU model and memory: -
 Exact command to reproduce: see below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 After closing the session and creating new one an iterator creates the generator instance but doesn't free the memory of the previous one.
 Every calling of the line session.run(x) (see below) increases memory consumption of the script:
 
 519 MiB after the first,
 600 MiB after the second,
 681 MiB after the third and so on.
 
 As you can see the delta is equal to 80 MiB = N * sizeof(data.dtype). (data.dtype is float64 here)
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 import numpy as np
 import tensorflow as tf
 
 N = 10 * 1024 * 1024
 
 def generate():
   data = np.random.rand(N)
   for k in range(N):
     yield data[k].copy()
 
 graph = tf.Graph()
 with graph.as_default():
   x = tf.data.Dataset\
     .from_generator(generate, tf.float32)\
     .make_one_shot_iterator()\
     .get_next()
 
 while True:
   session = tf.Session(graph=graph)
   session.run(x) # <--- PUT A BREAKPOINT HERE!
                  #  Be careful running the code without it!
   session.close()
 	",1.0,velikodniy,2018-01-16T17:28:40Z,"
 		Of course, if session.run consumes all the elements produced by the generator, it will be removed.
 		",2.0,velikodniy,2018-01-16T22:49:27Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 , could you comment on this please?
 		",3.0,velikodniy,2018-02-06T07:36:15Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",be862d5b91e9b9044f4e028dcdae0b6ad283e8b4,Derek Murray,2018-02-20 14:18:09-08:00,ADD,0,None,tensorflow\core\api_def\base_api\api_def_GeneratorDataset.pbtxt,,,,MODIFY,0.0,tensorflow\core\kernels\data\BUILD,tensorflow\core\kernels\data\BUILD,4.0,velikodniy,2018-02-07T22:46:27Z,"
 		Yes, this is a bug. I have a fix in preparation.
 		",5.0,velikodniy,2018-02-22T13:07:34Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",6.0,velikodniy,2018-02-22T15:34:37Z,"
 		The fix has been submitted internally, but not yet merged into the Git master branch. It should be coming in the next push!
 		",0.0,"212,213,214,215,216,217,218,219,220,221,222,223,224,535",,,,,,MODIFY,2.0,tensorflow\core\kernels\data\captured_function.cc,tensorflow\core\kernels\data\captured_function.cc,1.0,"259,260,261,262,263,264,265,266,267",,MODIFY,0.0,tensorflow\core\kernels\data\captured_function.h,tensorflow\core\kernels\data\captured_function.h,0.0,"67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,117",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::CapturedFunction::Instantiate,ctx,259,267,,,,,ADD,0.0,None,tensorflow\core\kernels\data\generator_dataset_op.cc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\ops\dataset_ops.cc,tensorflow\core\ops\dataset_ops.cc,0.0,"69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85",,,,,,,,,,,,,MODIFY,10.0,tensorflow\python\data\kernel_tests\dataset_from_generator_op_test.py,tensorflow\python\data\kernel_tests\dataset_from_generator_op_test.py,1.0,"338,339",,testFromGeneratorDestructorCalled.__next__,self,338,339,,,,,,,,,,,,,,,,,,,1.0,"269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313",,tensorflow::CapturedFunction::RunInstantiated,"args,rets",269,313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"310,311,312,313",,testFromGeneratorStopShort.generator,,310,313,1.0,"332,333",,testFromGeneratorDestructorCalled.__iter__,self,332,333,1.0,"369,370,371",,testGeneratorDatasetFinalizeFunctionCalled.testGeneratorDatasetFinalizeFunctionCalled.finalize_fn.finalize_py_func,,369,371,MODIFY,19.0,tensorflow\python\data\ops\dataset_ops.py,tensorflow\python\data\ops\dataset_ops.py,1.0,"1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,init_args,init_func,next_func,finalize_func",1051,1210,1.0,"1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207",,__init__.tf_finalize_func,args,1192,1207,1.0,"409,410,411,412,413,414,415,416,417,418,419,420,421,422","413,414,415,416,417,418,419,421",from_generator.finalize_fn,iterator_id_t,409,422,1.0,"341,342",,testFromGeneratorDestructorCalled.__del__,self,341,342,1.0,"326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358",,testFromGeneratorDestructorCalled,self,326,358,1.0,"368,369,370,371,372,373",,testGeneratorDatasetFinalizeFunctionCalled.finalize_fn,_,368,373,1.0,"308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324",,testFromGeneratorStopShort,self,308,324,1.0,"335,336",,testFromGeneratorDestructorCalled.next,self,335,336,1.0,"360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389",,testGeneratorDatasetFinalizeFunctionCalled,self,360,389,1.0,"1230,1231",,output_shapes,self,1230,1231,1.0,"366,367,368","366,367,368,369,370",from_generator.from_generator.generator_next_fn.generator_py_func,iterator_id,364,396,1.0,"334,337","334,337",from_generator.get_iterator_id_map_fn,unused_dummy,334,348,1.0,"350,366,367,368","350,366,367,368,369,370",from_generator.generator_next_fn,iterator_id_t,350,407,1.0,"426,427,428,429,430,432,433",429,from_generator.flat_map_fn,dummy_arg,426,433,1.0,"1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185",,__init__.tf_next_func,args,1138,1185,1.0,"1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223",,_as_variant_tensor,self,1212,1223,1.0,"366,367,368","366,367,368,369,370",from_generator.from_generator.generator_map_fn.generator_py_func,iterator_id,364,398,1.0,"1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126",,__init__.tf_init_func,args,1082,1126,1.0,"334,337,350,366,367,368,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,426,427,428,429,430,432,433,441","334,337,350,366,367,368,369,370,413,414,415,416,417,418,419,421,429",from_generator,"generator,output_types,output_shapes",266,449,1.0,"1234,1235",,output_types,self,1234,1235,1.0,"334,337","334,337",from_generator.get_iterator_id_fn,unused_dummy,334,348,1.0,"350,366,367,368,409","350,366,367,368,369,370",from_generator.generator_map_fn,iterator_id_t,350,409,1.0,"412,413,414,415,416,417,418,419","413,414,415,416,417,418,419",from_generator.from_generator.finalize_fn.finalize_py_func,iterator_id,412,419,1.0,"413,414,415,416,417,418,419,420,421","413,414,415,416,417,418,419,421",from_generator.flat_map_fn,iterator_id_t,413,421,1.0,"1226,1227",,output_classes,self,1226,1227,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16167,aidangomez,2018-01-16T19:28:06Z,2018-02-02T02:58:59Z,Documentation Method Templates Improvement,"
 <denchmark-h:h3>System information</denchmark-h>
 
 N/A
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.
 I.e. this is what we have at present (bad):
 <denchmark-link:https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png></denchmark-link>
 
 This is a more practical and copy/paste-friendly version:
 <denchmark-link:https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png></denchmark-link>
 
 I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 N/A
 	",1.0,aidangomez,2018-01-16T22:44:48Z,"
 		<denchmark-link:https://github.com/wolffg>@wolffg</denchmark-link>
 , could you consider this feature req. Thanks for the suggestion <denchmark-link:https://github.com/aidangomez>@aidangomez</denchmark-link>
 .
 		",2.0,aidangomez,2018-01-17T17:15:52Z,"
 		This is an easy fix, and clear win for regular functions.
 It's probably better not to change this for methods. Adding the path to the class, on each method signature  seems like unnecessary noise, since you rarely do a direct lookup of the method:
 get_collection(...)
 tf.Graph.get_collection(...)
 get_name_scope(...)
 tf.Graph.get_name_scope(...)
 WDYT?
 		",3.0,aidangomez,2018-01-17T17:29:11Z,"
 		Agreed. That's an important distinction.
 		",a1a34b1440c4c4792f945275529e6c5b3c7aa2ca,Mark Daoust,2018-02-01 17:09:08-08:00,MODIFY,3,tensorflow\tools\docs\pretty_docs.py,tensorflow\tools\docs\pretty_docs.py,1.0,"262,268,269,284,285,286,287,288","262,268,269,284",,,,,4.0,aidangomez,2018-01-18T13:33:12Z,"
 		Fix inflight.
 		",5.0,aidangomez,2018-01-23T23:03:02Z,"
 		A member of the TensorFlow organization has replied after the stat:awaiting tensorflower label was applied.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,_build_signature,"obj_info,use_full_name",262,291,1.0,"262,268,269,284,285,286,287","262,268,269,284",_build_signature,obj_info,262,287,1.0,165,165,_build_class_page,page_info,84,184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16238,namrata-ibm,2018-01-19T10:25:54Z,2018-01-26T05:31:42Z,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04  s390x
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): v1.4.1
 Python version: 2.7.12
 Bazel version (if compiling from source): 0.7.0
 GCC/Compiler version (if compiling from source): 5.4.0
 CUDA/cuDNN version: No GPU
 GPU model and memory: NA
 Exact command to reproduce: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 One of the sub-test test_stable_global_norm_unchanged fails on s390x with
 AssertionError: 110.709068 != 110.709084 +/- 0.000010
 Seems like a minor difference, so I tried changing the tolerance slightly as below:
 <denchmark-code>-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
 +        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
 </denchmark-code>
 
 with this the test is passing.
 Is it ok to create a PR with this change? Could you please share your thoughts on this.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>.......................F..................................................................................
 ======================================================================
 FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
 Test that preconditioning doesn't change global norm value.
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
     self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
   File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
     if msg is not None else """"))
 AssertionError: 110.709068 != 110.709084 +/- 0.000010
 
 ----------------------------------------------------------------------
 Ran 106 tests in 9.119s
 
 FAILED (failures=1)
 </denchmark-code>
 
 	",1.0,namrata-ibm,2018-01-19T21:00:09Z,"
 		<denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
 : any reason loosening this bound would not be acceptable?
 		",2.0,namrata-ibm,2018-01-24T00:17:32Z,"
 		redirect to <denchmark-link:https://github.com/joel-shor>@joel-shor</denchmark-link>
  who owns tf.contrib.gan. Joel do you mind taking a look? Thanks!
 		",,,,,4383f3d002ddb0712a7aac3303cde6e599de65eb,Joel Shor,2018-01-25 21:30:45-08:00,MODIFY,1,tensorflow\contrib\gan\python\losses\python\losses_impl_test.py,tensorflow\contrib\gan\python\losses\python\losses_impl_test.py,1.0,623,623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_stable_global_norm_unchanged,self,613,623,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16481,eaplatanios,2018-01-27T07:58:18Z,2018-02-02T02:58:59Z,Container localhost does not exist.,"
 Hi,
 I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:
 <denchmark-code>2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
 2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
 	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
 Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
 	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
 	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
 </denchmark-code>
 
 It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.
 Thanks!
 	",1.0,eaplatanios,2018-01-27T15:54:41Z,"
 		I think this could be a knock-on effect from <denchmark-link:https://github.com/tensorflow/tensorflow/commit/9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc>9f4118d</denchmark-link>
 , which modifies (and in principle simplifies) how DT_RESOURCE tensors are captured inside a  transformation (such as ).
 It sounds like there is some disagreement between the code that creates the lookup table and the op that performs the lookup itself. Could you share the code for creating the table and the Dataset.map()?
 /cc <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
 
 		",2.0,eaplatanios,2018-01-28T05:43:17Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I'd love to share the code but it's written in Scala using my API and it might be hard to get familiar with the codebase. I can do so if you think that can help. However, in order to give you a summary, the lookup table is created right before being used in the dataset map. It is also initialized prior to being used by calling . What kind of disagreement could cause such an error? Is there an easy way to debug? Also, is  the default container used when not providing and container?
 		",3.0,eaplatanios,2018-01-29T17:25:29Z,"
 		Hmm, what you're doing doesn't sound like it's wrong, because that's effectively what the Python version would be doing too. And the recent change should make it less likely to see a NotFound error, because it changes the function implementation to share the same ResourceMgr (which defines the namespace for resource names and containers), whereas before there was an explicit copying-and-rewriting step for captured resource handles.
 Could you possibly create a minimal example that exhibits the problem in both Python and Scala, and share the code for these? If you could also capture the text-format GraphDef, it might be possible to inspect the graphs and find the discrepancy.
 
 Also, is localhost the default container used when not providing and container?
 
 That's right. Are you using the container attr in your code? (It should still work, but it's possible there's a bug there....)
 		",77e6a452188e83ae4498cc3ae23e20e60061b367,Derek Murray,2018-02-01 17:15:09-08:00,MODIFY,2,tensorflow\core\kernels\data\iterator_ops.cc,tensorflow\core\kernels\data\iterator_ops.cc,1.0,"538,539,540,541,542,543,544,545,553",535,MODIFY,0.0,tensorflow\python\data\kernel_tests\BUILD,tensorflow\python\data\kernel_tests\BUILD,4.0,eaplatanios,2018-01-29T21:27:09Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I can look into creating a minimal example this week, but I have found some information that might be useful. It has to do with creating functions and capturing control dependencies of these functions. Currently, functions are created in a separate graphs and tensors that are used as inputs are captured and replaced with placeholders to be fed later. What about control dependencies that some of the new ops might have on ops defined outside the function? For example, I have a dataset map op that uses a lookup table, but I want the iterator initializer op for that dataset to have a control dependency on the lookup table initializer op of the outer graph. How should I go about handling that? Currently I get an error of the form , where  refers to the source of the control edge. I think that's what's causing the error because I cannot properly initialize the iterator.
 		",5.0,eaplatanios,2018-01-29T23:32:45Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Actually never mind, even if I work around this initialization issue and manage to run all the initializer ops fine, I still get the same error once I get to invoke . I'll try to create a minimal example for this, but in the meantime, do you have any idea of what could be causing the container to not be found? Shouldn't the  container always exist given that it's the default one?
 		",6.0,eaplatanios,2018-01-29T23:48:23Z,"
 		Note also that if I put a breakpoint in my Scala code, this error:
 <denchmark-code>W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
 </denchmark-code>
 
 keeps being printed over and over again indefinitely as if it keeps being thrown from within some loop running in another thread.
 		",0.0,"360,361,362",,,,,,MODIFY,1.0,tensorflow\python\data\kernel_tests\iterator_ops_cluster_test.py,tensorflow\python\data\kernel_tests\iterator_ops_cluster_test.py,1.0,"109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141",,,,,,,,,tensorflow::IteratorHandleOp::CreatePrivateFLR,"ctx,device_mgr,flib_def,pflr",534,554,1.0,"521,522,523,524,525,526,527,528,529,530,531,532","523,524,525,526,527",tensorflow::IteratorHandleOp::down_cast,f,521,532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,eaplatanios,2018-01-31T01:57:35Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Given that the lookup table initializer op and the iterator initializer op run without any issues, is there any chance the resource manager is cleared/reset afterwards in between the session runs? Is there any tips you might have on how to debug this sort of error?
 		",testCaptureHashTableInSharedIterator,self,109,141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,eaplatanios,2018-01-31T03:20:26Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I'm wondering if this has to do with me using the C API  method. I'm using it to create the function provided to the dataset map. The lookup table which is used by the function is replaced with a placeholder of type . The initialized lookup table resource tensor is then passed as input to the created function op. Could this be causing the problem? It was all working fine with version 1.5.0-rc1.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,eaplatanios,2018-01-31T05:52:01Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I finally resolved this. I was accidentally setting the shared name of the iterator I was creating. I'm not sure why this caused the problem, but without setting the shared name, everything works fine. Do you know why that could have happened? I'm actually curious.
 		",10.0,eaplatanios,2018-02-01T18:24:07Z,"
 		Thanks for tracking that down! I think this is a legitimate bug, introduced in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/9f4118d00fa9eb85f81a4eb3f96a5583ae5afcdc>9f4118d</denchmark-link>
 . That change modifies most iterators to use the same , , and  as the op that created them, which enables the resource-capturing logic to be simplified, because handles are valid in both the caller and the callee.
 However, when the shared_name is set, the iterator might outlive the FunctionLibraryRuntime used by the op that created it. The new code identifies this case and creates its own private FunctionLibrary, Device, and ResourceMgr that will outlive the caller. Unfortunately, this means that the handles from the caller and no longer valid in the callee, which manifests as a NotFoundError.
 Thanks for the promising lead on this bug! We'll try to get a fix in soon.
 		",11.0,eaplatanios,2018-02-03T22:38:28Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Thanks for tracking this down and fixing it! :)
 		",12.0,eaplatanios,2018-02-05T16:39:07Z,"
 		Thanks for digging into it and making it much easier to fix!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16747,thjashin,2018-02-04T06:48:59Z,2018-03-16T22:33:45Z,No documentation on the order of eigenvalues returned by tf.self_adjoint_eig,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
 TensorFlow installed from (source or binary):binary
 TensorFlow version (use command below):('v1.4.0-rc1-11-g130a514', '1.4.0')
 Python version: 2.7.14
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 From the documentation of tf.self_adjoint_eig I cannot see what the order of eigenvalues is. I tried with several examples and found they were sorted in ascending order. Does this always hold?
 	",1.0,thjashin,2018-02-05T04:29:05Z,"
 		TensorFlow doc doesn't tell much about that. But TensorFlow use Eigen to calculate the eigenvalues:
 
 
 
 tensorflow/tensorflow/core/kernels/self_adjoint_eig_op.cc
 
 
         Lines 58 to 65
       in
       ba032db
 
 
 
 
 
 
  Eigen::SelfAdjointEigenSolver< 
 
 
 
      Eigen::Matrix<Scalar, Eigen::Dynamic, Eigen::Dynamic, Eigen::RowMajor>> 
 
 
 
  es(inputs[0]); 
 
 
 
  OP_REQUIRES(context, es.info() == Eigen::Success, 
 
 
 
  errors::InvalidArgument(""Self Adjoint Eigen decomposition was"" 
 
 
 
  ""not successful. "" 
 
 
 
  ""The input might not be valid."")); 
 
 
 
  outputs->at(0).row(0) = es.eigenvalues().transpose(); 
 
 
 
 
 
 And document of Eigen says:
 
 The eigenvalues are repeated according to their algebraic multiplicity, so there are as many eigenvalues as rows in the matrix. The eigenvalues are sorted in increasing order.
 https://eigen.tuxfamily.org/dox/classEigen_1_1SelfAdjointEigenSolver.html#a3df8721abcc71132f7f02bf9dfe78e41
 
 So the order is guranteed by Eigen.
 For clarification, I think adding this to TensorFlow documents may be better.
 UPDATE:
 It also holds for GPU implementation. See CUDA doc:
 
 a real array of dimension n. The eigenvalue values of A, in ascending order ie, sorted so that W(i) <= W(i+1).
 http://docs.nvidia.com/cuda/cusolver/#cuds-lt-t-gt-syevd
 
 		",2.0,thjashin,2018-02-07T00:32:24Z,"
 		<denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  could you update the docs? I think <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  can help direct you to where they should be updated.
 		",3.0,thjashin,2018-02-07T00:51:15Z,"
 		These seem to be relevant files for SelfAdjointEig documentation:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEig.pbtxt>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEig.pbtxt</denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEigV2.pbtxt>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/api_def/base_api/api_def_SelfAdjointEigV2.pbtxt</denchmark-link>
 
 		",6f0dd0425c51360fe2be5a938a8f3fb39e420fa3,Jiongyan Zhang (张炯衍),2018-03-16 15:33:45-07:00,MODIFY,0,tensorflow\core\api_def\base_api\api_def_SelfAdjointEig.pbtxt,tensorflow\core\api_def\base_api\api_def_SelfAdjointEig.pbtxt,0.0,"22,23",22,MODIFY,0.0,tensorflow\core\api_def\base_api\api_def_SelfAdjointEigV2.pbtxt,tensorflow\core\api_def\base_api\api_def_SelfAdjointEigV2.pbtxt,4.0,thjashin,2018-02-09T17:43:51Z,"
 		I think <denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  has the flu, and <denchmark-link:https://github.com/qmick>@qmick</denchmark-link>
  seems to have discovered the answer, so I'm gonna mark contributions welcome for now. Please feel free to submit a PR!
 		",,,,,,,,,0.0,"34,35",34,,,,,MODIFY,1.0,tensorflow\python\ops\linalg_ops.py,tensorflow\python\ops\linalg_ops.py,1.0,344,344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,self_adjoint_eig,"tensor,name",331,349,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16829,x10000year,2018-02-07T11:27:24Z,2018-02-13T22:48:05Z,tf.contrib.estimator.replicate_model_fn fails when a trainable variable doesn't have gradient,"
 tf.contrib.estimator.replicate_model_fn fails when the gradient of a trainable variable is None. The error messages are:
 <denchmark-code>  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
     loss = self._train_model(input_fn, hooks, saving_listeners)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
     features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
     model_fn_results = self._model_fn(features=features, **kwargs)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 235, in replicated_model_fn
     local_ps_devices=ps_devices)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 558, in _get_loss_towers
     **optional_params)
   File ""model-60m-1280-2gpus-16-32-64-128-bn50000/net.py"", line 38, in model_fn
     train_op = optimizer.minimize(model.total_loss, global_step)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py"", line 353, in minimize
     name=name)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/estimator/python/estimator/replicate_model_fn.py"", line 317, in apply_gradients
     with ops_lib.control_dependencies(_extract_tensors(grads_and_vars)):
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4304, in control_dependencies
     return get_default_graph().control_dependencies(control_inputs)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 4017, in control_dependencies
     c = self.as_graph_element(c)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
     return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3124, in _as_graph_element_locked
     types_str))
 TypeError: Can not convert a NoneType into a Tensor or Operation.
 </denchmark-code>
 
 	",1.0,x10000year,2018-02-07T19:36:32Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		",2.0,x10000year,2018-02-12T06:38:49Z,"
 		<denchmark-link:https://github.com/isaprykin>@isaprykin</denchmark-link>
 
 		",3.0,x10000year,2018-02-13T18:40:28Z,"
 		<denchmark-link:https://github.com/x10000year>@x10000year</denchmark-link>
  Hi!  I fixed this yesterday and the fix is coming.  Thanks for reporting.
 I'll link the commit when it's available and then close the issue.  I hope you can re-built or take the nightly build to take advantage of the fix.
 		",96564330fb0508a50a0515be11c9202c64b0f5b7,Igor Saprykin,2018-02-12 16:28:14-08:00,MODIFY,1,tensorflow\contrib\estimator\python\estimator\replicate_model_fn.py,tensorflow\contrib\estimator\python\estimator\replicate_model_fn.py,1.0,793,793,MODIFY,2.0,tensorflow\contrib\estimator\python\estimator\replicate_model_fn_test.py,tensorflow\contrib\estimator\python\estimator\replicate_model_fn_test.py,4.0,x10000year,2018-02-25T13:34:05Z,"
 		Thank you!
 		",,,,,,,,,1.0,"243,244,245,246,247,248,249",,test_train_with_mean_reduction,self,238,267,,,,,,,,,,,,,,,_extract_tensors,tensors_and_vars,787,795,,,,,,,,,,,,,,,,,,,,,,1.0,,"1122,1123",test_sparse_tensor_can_be_split_unevenly_repeated_row,self,1108,1135,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
16954,jlysiak,2018-02-12T20:20:58Z,2018-02-15T00:31:54Z,Iterator.get_next() documentation improvement request,"
 <denchmark-h:h3>System information</denchmark-h>
 
 N/A
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Recently I've written some code using Dataset API and I would like to request a problem with documentation (IMO).  Instead of hardcoding comments <denchmark-link:https://github.com/tensorflow/tensorflow/blob/3ee1721b46d0e61097d0ee72f01e3de9739f0b6f/tensorflow/python/data/ops/iterator_ops.py#L31>here</denchmark-link>
 , please, move <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 's annotation about  and  into  method documentation.
 I don't know why I didn't get that beautiful warning on my console output but I think I'm not the first person with funny thread-bomb running and consuming system resources. :) You know about that also (see comment).
 So... It would be great if you could move all critical annotation into main documentation. I'm thinking now about all ML newcomers rather than me (Yeah, I actually found solution by myself :))
 That's all.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 N/A
 	",,,,,,,,,,,,,620dc3f097d047346943c416823f5e370df9fe4b,Derek Murray,2018-02-14 13:52:38-08:00,MODIFY,1,tensorflow\python\data\ops\iterator_ops.py,tensorflow\python\data\ops\iterator_ops.py,1.0,"307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342",306,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get_next,"self,name",306,367,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17130,benmosher,2018-02-19T15:01:11Z,2018-02-23T05:43:34Z,Java: SIGSEGV when `Tensors.create`-ing from an uninitialized array,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 macOS 10.13.3
 JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
 TensorFlow installed from (source or binary): maven
 
     <dependency>
       <groupId>org.tensorflow</groupId>
       <artifactId>tensorflow</artifactId>
       <version>1.4.0</version>
     </dependency>
 
 TensorFlow version (use command below): 1.4.0
 Python version: N/A
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce:
 
     // JUnit test case
     public void testSigSegv() {
         byte[][] bb = new byte[3][];  // note: new byte[3][1] doesn't crash, presumably it is initialized by the compile
 
         bb[0] = new byte[] { 0 };
         bb[1] = new byte[] { 1 };
         // no bb[2]
 
         // next line sigsegv's
         Tensors.create(bb);
     }
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Using Tensors.create to get a Tensor<String> from an uninitialized 2D byte array (byte[][]) results in a SIGSEGV from JNI. See full log below.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 When running the above test case, I get the following output.
 <denchmark-code>#
 # A fatal error has been detected by the Java Runtime Environment:
 #
 #  SIGSEGV (0xb) at pc=0x000000010571f0b3, pid=30179, tid=0x0000000000001a03
 #
 # JRE version: Java(TM) SE Runtime Environment (8.0_121-b13) (build 1.8.0_121-b13)
 # Java VM: Java HotSpot(TM) 64-Bit Server VM (25.121-b13 mixed mode bsd-amd64 compressed oops)
 # Problematic frame:
 # V  [libjvm.dylib+0x31f0b3]
 #
 # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try ""ulimit -c unlimited"" before starting Java again
 #
 # An error report file with more information is saved as:
 # /Users/ben/AGLabs/nfl/JavaProjects/nlp/hs_err_pid30179.log
 #
 # If you would like to submit a bug report, please visit:
 #   http://bugreport.java.com/bugreport/crash.jsp
 #
 
 Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
 </denchmark-code>
 
 I'm not familiar enough with JNI in general to know if I'm expecting too much. I won't be surprised if you mark this wontfix, I was just surprised to be able to crash the process with a segfault given some bad data. Easy enough to work around (I will size/init my arrays more carefully) but thought you might want to know.
 Cheers! Thanks so much for this library!
 	",1.0,benmosher,2018-02-19T20:40:36Z,"
 		Thanks very much for the report! Yes, this is indeed a bug (it should not be possible to cause the JVM to segfault when using the Java API :).
 Will send a fix shortly.
 		",,,,,,,,,624a2e47329fefa1f17373954ac541b0e42a9fca,Asim Shankar,2018-02-20 13:40:07-08:00,MODIFY,3,tensorflow\java\src\main\native\tensor_jni.cc,tensorflow\java\src\main\native\tensor_jni.cc,1.0,453,,MODIFY,2.0,tensorflow\java\src\test\java\org\tensorflow\TensorTest.java,tensorflow\java\src\test\java\org\tensorflow\TensorTest.java,,,,,,,,,,,,,1.0,"541,542,543,544,545,546,547,548,549",,TensorTest::gracefullyFailCreationFromNullArrayForStringTensor,,541,549,,,,,,,,,,,,,,,Java_org_tensorflow_Tensor_allocateNonScalarBytes,"env,clazz,shape,value",436,474,1.0,"403,404,405,406,407,409",,nonScalarTF_STRINGTensorSize,"env,value,num_dims",391,412,1.0,431,424,fillNonScalarTF_STRINGTensorData,"env,value,num_dims,writer,status",414,433,,,,,,,,1.0,435,435,TensorTest::testCreateFromArrayOfBoxed,,430,437,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17246,yaroslavvb,2018-02-24T23:39:54Z,2019-03-07T00:10:47Z,Fetching value of Variable unnecessarily slow,"
 Doing sess.run(var) is about 5x slower than sess.run(var+1).
 python <denchmark-link:https://github.com/diux-dev/cluster/blob/26f8e01bd79e49fe2d1dac342dd90493f693b85c/yuxin_numpy/variable_fetch_bug_report.py>variable_fetch_bug_report.py</denchmark-link>
 
 100MB variable
 fetch_cpu_variable  : 2.5 GB/sec, min: 40.74, median: 41.33, mean: 42.08
 fetch_cpu_variable_add: 12.6 GB/sec, min: 7.96, median: 8.54, mean: 8.71
 fetch_cpu_variable_concat: 14.0 GB/sec, min: 7.12, median: 8.14, mean: 8.28
 TensorFlow version info:
 version: 1.7.0-dev20180221
 : v1.6.0-rc1-337-gd100729
 <denchmark-link:https://github.com/tensorflow/tensorflow/commit/d100729>d100729</denchmark-link>
 
 	",1.0,yaroslavvb,2018-02-26T19:54:38Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Any clue what's going on here?
 		",2.0,yaroslavvb,2018-02-26T20:31:16Z,"
 		Presumably it's copying when it doesn't (?) need to. <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  added the optimizations for the fetch path, so might know what could be going on here.
 		",3.0,yaroslavvb,2018-02-26T20:36:27Z,"
 		Does the problem also happen with resource variables? (tfe.Variable or tf.get_variable(..., use_resource=True)
 		",879fc3440495d9388754cb7d1878caf034d03d61,Eugene Zhulenev,2018-06-06 11:29:18-07:00,MODIFY,2,tensorflow\python\lib\core\ndarray_tensor.cc,tensorflow\python\lib\core\ndarray_tensor.cc,1.0,"316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347",,,,,,4.0,yaroslavvb,2018-02-26T22:10:03Z,"
 		Yes, same speed is with resource variables. I also get fast fetch speed if I turn off all optimizers and fetch var+0
 <denchmark-link:https://github.com/diux-dev/cluster/blob/37d069c20fae6aeac10a53e3f801d29aebc5d6b4/yuxin_numpy/tf_numpy_benchmark.py>tf_numpy_benchmark.py</denchmark-link>
 
 <denchmark-code>python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_add --size-mb=1024 --num-iters=31
 fetch_cpu_variable_add        :  21.0 GB/sec, min: 48.83, median: 56.08, mean: 55.69
 
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_resource_variable --num-iters=11 --size-mb=1024
 fetch_cpu_variable            :   2.6 GB/sec, min: 401.36, median: 404.02, mean: 403.63
 </denchmark-code>
 
 I suspect that fetching variable triggers a single threaded memcpy. Meanwhile fetching ""var+0"" uses multiple cores, so it's essentially a multi-threaded memory copy
 		",5.0,yaroslavvb,2018-02-26T22:58:33Z,"
 		The code which fetches tensors is <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/core/ndarray_tensor.cc#L331>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/core/ndarray_tensor.cc#L331</denchmark-link>
  which calls <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L227>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.cc#L227</denchmark-link>
  which triggers a copy if the refcount is not 1 (and it's never 1 for variables). It does the copy with normal memcpy.
 I remember people caring passionately about us not simply forwarding the memory when tensorflow still holds a reference to it as it can break some py_func use cases and some multithreaded use cases.
 So I guess we should use a faster memcpy?
 		",6.0,yaroslavvb,2018-02-27T00:58:35Z,"
 		Yes, faster memcpy would also resolve <denchmark-link:https://github.com/tensorflow/tensorflow/issues/17233>#17233</denchmark-link>
  , until there are better tools to create 64-byte aligned numpy arrays.
 (PS: I wonder if this alignment requirement is moot in a first place. In PyTorch, I can inititialize tensors from unaligned numpy arrays, with memory reuse, and add them at 20 GB/second on CPU, there's no benefit in starting with aligned memory.)
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::FastMemcpy,"dst,src,size",316,347,1.0,"399,400","365,366",tensorflow::TF_TensorToPyArray,"tensor,out_ndarray",353,407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,yaroslavvb,2018-03-06T01:43:42Z,"
 		Having faster memcpy in TF also could be later worked into open-source distributed TensorFlow. Right now sending messages locally in Open-Source distributed TF is done at speed of single-threaded memcpy, so TF process can't take advantage of faster network cards (ie, AWS instances have 25 Gbps)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,yaroslavvb,2018-03-20T17:48:18Z,"
 		BTW, here's an example of multi-threaded memcpy with some performance numbers on the same system. The time for 100MB chunk goes from 40ms to 5ms on the same system
 <denchmark-link:https://github.com/diux-dev/cluster/blob/master/psbench/memcpy.cc>https://github.com/diux-dev/cluster/blob/master/psbench/memcpy.cc</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,yaroslavvb,2018-03-20T18:03:12Z,"
 		On dual XeonV4
 <denchmark-code>wget https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy.cc
 g++ -std=c++0x memcpy.cc -pthread -march=native -O6
 ./a.out 1000 32
 
 Stream copy 32 threads: 4.8 ms, 20.98 GB/sec
 Stream copy 32 threads: 4.9 ms, 20.53 GB/sec
 Stream copy 32 threads: 4.8 ms, 20.87 GB/sec
 Stream copy 32 threads: 4.9 ms, 20.36 GB/sec
 Stream copy 32 threads: 4.8 ms, 20.74 GB/sec
 Stream copy 32 threads: 4.7 ms, 21.39 GB/sec
 
 </denchmark-code>
 
 		",10.0,yaroslavvb,2018-03-20T18:14:53Z,"
 		A single threaded call to the system memcpy should be able to hit the memory bandwidth of the system except in two cases:
 
 
 there are many small copies (not the case here)
 
 
 A single large copy will normally hit the memcpy path that avoids polluting the cache (the copies will be done using streaming memory instructions), this is known to decrease absolute performance but avoids cache pollution on multiprocessor machines.  This is hardcoded behavior of glibc.  The latest versions of glibc should expose this parameter as a tunable value (I put them there, see https://www.gnu.org/software/libc/manual/html_node/Hardware-Capability-Tunables.html#Hardware-Capability-Tunables).   Can you try making the non-temporal threshold larger than your copy size?
 
 
 I suspect what's happening is that when you shard the copy 32 times it drops below the hard-coded non temporal threshold and you're hitting the faster path (but blowing out the caches).
 		",11.0,yaroslavvb,2018-03-20T19:54:30Z,"
 		That benchmark was done using a loop with calls to _mm256_stream_load_si256 and _mm256_stream_si256 rather than memcpy.
 I tried using <denchmark-link:https://github.com/diux-dev/cluster/blob/master/psbench/memcpy_classic.cc>regular memcpy</denchmark-link>
  and the performance is very close, so I think stock memcpy is already using stream version.
 However, single threaded performance is quite far from multi-threaded performance. Here's an experiment on AWS c5.18xlarge instance (dual 18-core Skylake), it runs 6x faster than single-threaded memcpy, copying 100MB array in 2.7ms. Skylakes <denchmark-link:https://www.anandtech.com/show/11544/intel-skylake-ep-vs-amd-epyc-7000-cpu-battle-of-the-decade/12>supposedly have</denchmark-link>
  200 GB/second memory bandwidth
 <denchmark-code>wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_fast.cc
 g++ -std=c++0x memcpy_fast.cc -pthread -march=native -O6 -o memcpy_fast
 numactl --cpunodebind 0 --membind 0 ./memcpy_fast 100 16
 Stream copy 16 threads: 2.7 ms, 37.33 GB/sec
 Stream copy 16 threads: 2.7 ms, 37.41 GB/sec
 Stream copy 16 threads: 2.7 ms, 37.19 GB/sec
 Stream copy 16 threads: 2.7 ms, 37.27 GB/sec
 </denchmark-code>
 
 This <denchmark-link:http://web.archive.org/web/20131223174037/http://software.intel.com/en-us/articles/memcpy-performance/>article</denchmark-link>
  talks limitations of default . Since it's precompiled, it doesn't use the latest instructions (ie, Skylakes have AVX512). Apparently  replaces  with machine optimized version
 		",12.0,yaroslavvb,2018-03-20T20:05:22Z,"
 		That tunable didn't seem to make any difference for me, maybe glibc is too old (version 2.23)
 <denchmark-code>export GLIBC_TUNABLES=glibc.tune.x86_non_temporal_threshold=100000000000
 wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_classic.cc
 g++ -std=c++0x memcpy_classic.cc -pthread -march=native -O6 -o memcpy_classic
 ./memcpy_classic 100 
 memcpy: 23.1 ms, 4.34 GB/sec
 memcpy: 23.1 ms, 4.34 GB/sec
 memcpy: 23.0 ms, 4.34 GB/sec
 memcpy: 23.0 ms, 4.34 GB/sec
 </denchmark-code>
 
 		",13.0,yaroslavvb,2018-03-20T20:40:47Z,"
 		Yes that's before it went in; 2.25 or 2.26 I believe.
 That article is nearly a decade old and out of date.  The memcpy maintainer in glibc works for Intel.  There are assembly routines for every specialized architecture variant including AVX 512 in the latest versions (but AVX2 is preferred because of the large down clocks associated with using AVX 512).
 I think your bandwidth calculation is off by a factor of 2, it only includes bytes read.
 That being said, it does seem like I was incorrect and multi-threading does provide a benefit, especially on skylake.  However, 2.5 GB/sec from single threaded system memcpy seems way too low (even lowly skylake should get 10 GB/sec), so I don't know that we completely understand what's going on here.
 		",14.0,yaroslavvb,2018-03-20T20:55:50Z,"
 		I see.
 To summarize, currently tensorflow takes 40ms to fetch a 100MB variable on CPU, whereas copying 100MB on same machine is possible to do in 2.7ms.
 This delay is an issue when integrating TensorFlow with other systems. For instance resnet-50 backprop is 120ms, if you use something like Ray to synchronize parameter values between machines, this extra 40ms delay is significant
 		",15.0,yaroslavvb,2018-03-31T21:00:21Z,"
 		cc <denchmark-link:https://github.com/tatianashp>@tatianashp</denchmark-link>
 
 		",16.0,yaroslavvb,2018-06-07T21:06:12Z,"
 		I believe that <denchmark-link:https://github.com/tensorflow/tensorflow/commit/879fc3440495d9388754cb7d1878caf034d03d61>879fc34</denchmark-link>
  should solve this issue (or at least make it slightly better). Though solution is CPU/platform/glibc dependent, on my CPU (Intel Broadwell with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:15MB) I see ~1.7x improvement.
 		",17.0,yaroslavvb,2018-06-12T17:12:38Z,"
 		Weird, is memmove doing multi-threaded copy?
 BTW, here's the code that does 100MB copy in 2.7 ms on Skylake. Besides multi-threading, it uses stream instructions to turn off cache, cache just slows things down on large copies
 <denchmark-code>wget -N https://raw.githubusercontent.com/diux-dev/cluster/master/psbench/memcpy_fast.cc
 g++ -std=c++0x memcpy_fast.cc -pthread -march=native -O6 -o memcpy_fast
 numactl --cpunodebind 0 --membind 0 ./memcpy_fast 100 16
 Stream copy 16 threads: 2.7 ms, 37.33 GB/sec
 Stream copy 16 threads: 2.7 ms, 37.41 GB/sec
 </denchmark-code>
 
 		",18.0,yaroslavvb,2018-06-12T18:04:37Z,"
 		Nope, they both are single threaded, but memmove is using sse3 instruction to copy memory, while memcpy is using sse2 (that's what I see in perf). This ""fix"" is more like an ugly workaround for the old glibc versions.
 There is a discussion in Eigen (<denchmark-link:https://bitbucket.org/eigen/eigen/pull-requests/292/adds-a-fast-memcpy-function-to-eigen/diff>https://bitbucket.org/eigen/eigen/pull-requests/292/adds-a-fast-memcpy-function-to-eigen/diff</denchmark-link>
 ), but that change was later rolled back because it's not  guaranteed to be faster.
 		",19.0,yaroslavvb,2018-06-28T17:30:55Z,"
 		I'm closing this issue now since I think <denchmark-link:https://github.com/ezhulenev>@ezhulenev</denchmark-link>
  's change fixed the problem. Please reopen if that's not the case.
 		",20.0,yaroslavvb,2018-06-28T17:41:46Z,"
 		From the discussion in the eigen thread (by <denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  ), looks like the above fix is only a workaround for suckiness in certain version of glibc (where memmove is even faster than memcpy).
 		",21.0,yaroslavvb,2018-07-16T11:00:01Z,"
 		The change improves single threaded copy, but it's still much faster to fetch a+0 instead of a on Skylake, because the former does multi-threaded
 		",22.0,yaroslavvb,2018-07-17T17:50:45Z,"
 		BTW, I just did a benchmark with TF on DLAMI v11 (with MKL) on Skylake 18 core, and I'm seeing 10x improvement if I fetch ""var+0"" instead of var. Could try with nightly because of <denchmark-link:https://github.com/tensorflow/tensorflow/issues/20887>#20887</denchmark-link>
 
 <denchmark-code># fetching variable is slow from TF, because its single threaded
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable --num-iters=200
 fetch_cpu_variable            :   2.0 GB/sec, min: 50.75, median: 50.89, mean: 50.94
 
 # however, there's a trick, adding 0 to variable makes it multithreaded 10x faster
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0 --num-iters=200
 fetch_cpu_variable            :  33.1 GB/sec, min:  3.02, median:  3.21, mean:  3.22
 
 </denchmark-code>
 
 		",23.0,yaroslavvb,2018-07-17T19:03:13Z,"
 		Still present in tf_nightly (""b'v1.9.0-rc2-572-geadcdf91aa'"")
 		",24.0,yaroslavvb,2018-07-18T03:36:20Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  Mkl has it's own kernel for Add, if it's easy for you to do the same test with default Tensorflow, I'd be very interested to know if it's any significant difference between MKL and Eigen for such simple kernel
 		",25.0,yaroslavvb,2018-07-18T05:45:14Z,"
 		Doing  I get about 8x speed-up instead of 10x by using +0 trick (tfnightly points to this commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/eadcdf91aa>eadcdf9</denchmark-link>
 )
 <denchmark-code># regular fetching
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable
 fetch_cpu_variable            :   2.0 GB/sec, min: 49.50, median: 49.62, mean: 49.68
 
 # stock tensorflow
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0
 fetch_cpu_variable_plus0      :  16.2 GB/sec, min:  6.19, median:  6.94, mean:  6.94
 
 # DLAMI version
 python tf_numpy_benchmark.py --benchmark=fetch_cpu_variable_plus0
 fetch_cpu_variable_plus0      :  25.8 GB/sec, min:  3.87, median:  4.03, mean:  4.04
 </denchmark-code>
 
 		",26.0,yaroslavvb,2019-03-03T17:13:32Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  Please let us know if this this still an issue with latest TF or can we close this issue? Thanks!
 		",27.0,yaroslavvb,2019-03-07T00:10:46Z,"
 		Closing due to lack of recent activity. Please open a new ticket when new information becomes available. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17284,rightaditya,2018-02-26T21:15:35Z,2018-03-02T20:05:14Z,tensor-valued seeds in tf.data API can result in nondeterministic results,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): v1.5.0-11-g4588350f20 1.5.0
 Python version: 3.6.3
 Bazel version (if compiling from source): 0.11.0
 GCC/Compiler version (if compiling from source): 7.2.0
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce: See code below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 The tf.data API allows/requires seeds to be provided that are tf.tensors. This is an issue when the graph-level seed has been set to 0, and the provided op-level seed tensor takes on a value of 0. As noted in the comments in the code for tf.get_seed, a (0, 0) seed is problematic because the C++ ops assume this means nondeterminism. Of course, when a user is specifying these seeds, they're expecting deterministic behaviour. Unfortunately, tf.get_seed only checks for this issue for the case where the seeds are ints, not tensors. See the code below for an example.
 I would have been happy to submit a PR for this, but I have no idea where the fix should be for this bug. As I'm not especially familiar with the code base, it's not apparent whether it's even possible to have the code for tf.get_seed to check the value of a tensor seed. If not, I'm guessing the tf.data API would need to provide the checks.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 The following code reproduces the bug for me:
 import tensorflow as tf
 
 tf.set_random_seed(0)
 seed_tensor = tf.placeholder(tf.int64, shape=[], name='data_seed')
 data = tf.data.Dataset.range(10).shuffle(10, seed=seed_tensor)
 iterator = tf.data.Iterator.from_structure(tf.int64, tf.TensorShape([]))
 
 init = iterator.make_initializer(data)
 value = iterator.get_next()
 
 print('First run: ', end='')
 with tf.Session() as sess1:
     sess1.run(init, feed_dict={seed_tensor: 0})
     values = []
     while True:
         try:
             values.append(str(sess1.run(value)))
         except tf.errors.OutOfRangeError:
             break
 
     print(', '.join(values))
 
 print('Second run: ', end='')
 with tf.Session() as sess2:
     sess2.run(init, feed_dict={seed_tensor: 0})
     values = []
     while True:
         try:
             values.append(str(sess2.run(value)))
         except tf.errors.OutOfRangeError:
             break
 
     print(', '.join(values))
 The result I get is:
 <denchmark-code>First run: 8, 4, 6, 9, 1, 0, 5, 7, 3, 2
 Second run: 1, 6, 3, 0, 7, 5, 2, 9, 8, 4
 </denchmark-code>
 
 I would expect the first and second runs to produce the exact same sequence, though the particular sequence might differ from environment to environment.
 If I change the feed_dict to provide a value of 0 for seed_tensor for both runs, I get the expected result:
 <denchmark-code>First run: 5, 2, 0, 1, 7, 4, 9, 3, 8, 6
 Second run: 5, 2, 0, 1, 7, 4, 9, 3, 8, 6
 </denchmark-code>
 
 If I change the shuffle() call to take a value of 0 directly (i.e., `...shuffle(10, seed=0)), I get the expected result as well:
 <denchmark-code>First run: 4, 8, 7, 1, 3, 2, 6, 9, 0, 5
 Second run: 4, 8, 7, 1, 3, 2, 6, 9, 0, 5
 </denchmark-code>
 
 For the record, I encountered this issue because I'm using an Iterator via Iterator.from_structure, and when I use that iterator on a Dataset.shuffle() I get the same order for each epoch. To get around this, I provided the epoch number as a seed to Dataset.shuffle(), with the first epoch being epoch 0. In my case I can avoid this bug by just starting the epoch count at 1, but it took me a while to track down, and there may be other cases where the API is used in a similar way that would be problematic for those expecting deterministic results.
 	",1.0,rightaditya,2018-02-26T23:12:09Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Can you look at this?
 		",2.0,rightaditya,2018-02-26T23:34:03Z,"
 		I'm a bit confused by the code example. Why would you expect the shuffled order to be the same when you feed two different values for seed_tensor?
 It might make sense to rewrite the seed, as tf.get_seed() does, when the runtime value is 0. Is that what you're suggesting?
 		",3.0,rightaditya,2018-02-27T00:24:45Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Ugh, sorry, that's my mistake. Both fed values should be 0. I copied-and-pasted the code I was using to test it and had changed one of them but not the other. I've edited the code to reflect my intention.
 
 It might make sense to rewrite the seed, as tf.get_seed() does, when the runtime value is 0. Is that what you're suggesting?
 
 Yes, exactly. the tf.get_seed() check is insufficient if tensor-valued seeds are allowed, as it only does an identity check against 0.
 		",f6bda409206dc642d7a6f02842e76b0be7234491,Derek Murray,2018-02-27 19:17:13-08:00,MODIFY,1,tensorflow\contrib\data\python\ops\random_ops.py,tensorflow\contrib\data\python\ops\random_ops.py,1.0,36,,MODIFY,0.0,tensorflow\contrib\data\python\ops\shuffle_ops.py,tensorflow\contrib\data\python\ops\shuffle_ops.py,4.0,rightaditya,2018-02-27T01:45:00Z,"
 		OK, that does seem like a bug, and the fix isn't too difficult. The gist is to set seed = tf.where(seed == 0, tf.constant(2 ** 31 - 1, dtype=tf.int64), seed) when seed is a tf.Tensor.
 Watch this space for a fix in the next few days.
 		",5.0,rightaditya,2018-02-27T22:28:28Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I'd be willing to submit a PR for this if no one's already working on it.
 		",6.0,rightaditya,2018-02-27T22:33:20Z,"
 		Thanks for offering! I've already got a fix under review, so it should be merged soon.
 		",0.0,"22,48","26,48,49,50,51,52,53,54,55,56,57,58",,,,,MODIFY,1.0,tensorflow\python\data\kernel_tests\shuffle_dataset_op_test.py,tensorflow\python\data\kernel_tests\shuffle_dataset_op_test.py,1.0,"135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160",,MODIFY,0.0,tensorflow\python\data\ops\BUILD,tensorflow\python\data\ops\BUILD,0.0,26,,__init__,"self,seed",33,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testSeedZero,self,135,160,,,,,MODIFY,0.0,tensorflow\python\data\ops\dataset_ops.py,tensorflow\python\data\ops\dataset_ops.py,0.0,"29,1487","35,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\data\util\BUILD,tensorflow\python\data\util\BUILD,0.0,"89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112",,,,,,,,,,,,,ADD,0.0,None,tensorflow\python\data\util\random_seed.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\python\data\util\random_seed_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17360,kbsriram,2018-03-01T16:26:37Z,2018-03-07T01:17:02Z,C++ api: use of op::Attrs methods in gradients,"
 The generated op::Attrs struct returns new instances on its chainable methods, and doesn't change the original object.
 
 
 
 tensorflow/tensorflow/cc/framework/cc_op_gen.cc
 
 
          Line 701
       in
       d7d7f4e
 
 
 
 
 
 
  strings::StrAppend(&setters, ""      Attrs ret = *this;\n""); 
 
 
 
 
 
 There are a few related issues e.g. 
 
 
 tensorflow/tensorflow/cc/gradients/nn_grad.cc
 
 
          Line 164
       in
       6fdb9ad
 
 
 
 
 
 
  grad_attrs.DataFormat(data_format); 
 
 
 
 
  where the code assumes the underlying object is being mutated and the parameters don't actually pass through.
 I guess there might be a couple of ways forward, depending on how Tensorflow prefers the C++ API:
 
 Decide the Attrs chaining methods mutate the underlying object and fix the code generation.
 Decide the Attrs chaining methods return new instances, and fix the uses.
 
 Suggestions?
 Fwiw if option 2, it might be nice to add TF_MUST_USE_RESULT to the generated API. (Unfortunately a <denchmark-link:https://gcc.gnu.org/bugzilla/show_bug.cgi?id=38172>long-standing bug in gcc</denchmark-link>
  means this may be unreliable as an actual error across versions of gcc that contributors may use.)
 /cc <denchmark-link:https://github.com/suharshs>@suharshs</denchmark-link>
  <denchmark-link:https://github.com/keveman>@keveman</denchmark-link>
 
 	",1.0,kbsriram,2018-03-01T16:44:30Z,"
 		<denchmark-link:https://github.com/kbsriram>@kbsriram</denchmark-link>
  thanks for pointing out the bug. I am not a fan of mutable state, so my natural inclination is towards option 2. Adding  to the generated API would simply point out the existing erroneous uses as compiler errors.
 		",2.0,kbsriram,2018-03-01T20:25:50Z,"
 		<denchmark-link:https://github.com/keveman>@keveman</denchmark-link>
  thanks for the note! Sgtm - don't have a strong opinion on this. But would like to make progress on fixes and (for option 2 as you note) add a bit of a compile-time safety net for new code.
 Tentatively predicated on option 2, suggestions on next steps? If someone is already on it, awesome - otherwise, I can sign up for a pull request on this.
 		",,,,,5279cf29cea96b3ec50df506bb51d8ffabdabac9,A. Unique TensorFlower,2018-03-05 14:49:23-08:00,MODIFY,1,tensorflow\cc\framework\cc_op_gen.cc,tensorflow\cc\framework\cc_op_gen.cc,1.0,"700,701",700,MODIFY,6.0,tensorflow\cc\gradients\nn_grad.cc,tensorflow\cc\gradients\nn_grad.cc,,,,,,,,,,,,,1.0,"112,113","110,113,114",tensorflow::ops::BiasAddGradHelper,"scope,op,grad_inputs,grad_outputs",106,117,,,,,,,,,,,,,,,tensorflow::OpInfo::GetOpAttrStruct,,667,726,,,,,,,,,,,,,,,,,,,,,,1.0,"187,188","185,186,187,188,189,190,191",tensorflow::ops::LRNGradHelper,"scope,op,grad_inputs,grad_outputs",185,191,1.0,"51,52","51,52",tensorflow::ops::LogSoftmaxGrad,"scope,op,grad_inputs,grad_outputs",50,59,1.0,"175,176,177","167,168,169",tensorflow::ops::MaxPoolGradV2Helper,"scope,op,grad_inputs,grad_outputs",167,182,1.0,"132,133,134,135,137,138,139,140,141","133,134,135,136,137,138,140,141,142,143,144",tensorflow::ops::Conv2DGrad,"scope,op,grad_inputs,grad_outputs",120,144,1.0,"159,160,161","163,164",tensorflow::ops::MaxPoolGradHelper,"scope,op,grad_inputs,grad_outputs",147,164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17374,mixalturek,2018-03-02T08:55:38Z,2020-01-24T21:27:29Z,Potential resource leaks caused by unclear Java examples,"
 <denchmark-h:h3>System information</denchmark-h>
 
 Java examples at <denchmark-link:https://www.tensorflow.org/>https://www.tensorflow.org/</denchmark-link>
  for tensorflow 1.6.0.
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
  returns list of closables, Javadoc clearly states that the caller is responsible to free all of them. None of the Java examples I found at <denchmark-link:https://www.tensorflow.org/>https://www.tensorflow.org/</denchmark-link>
  highlights that, I realized it by happy accident during in-depth reading implementation in Session.java quite long time after I wrote my code that uses TensorFlow.
 <denchmark-code>    /**
      * Execute the graph fragments necessary to compute all requested fetches.
      *
      * <p><b>WARNING:</b> The caller assumes ownership of all returned {@link Tensor}s, i.e., the
      * caller must call {@link Tensor#close()} on all elements of the returned list to free up
      * resources.
      *
      * ...
      */
     public List<Tensor<?>> run() {
       return runHelper(false).outputs;
     }
 </denchmark-code>
 
 I'm not sure if the examples them-self contain any resource leak or not, they free only the first element of the list, but there may be more of them (in general). I would expect an explicit loop properly freeing all the returned resources there.
 Such examples for beginners should be as explicit as possible, 100% clear and understandable for anyone. A lot of people (like me) base core of their code on them which may easily introduce significant resource leak bugs to their applications.
 
 https://www.tensorflow.org/install/install_java, HelloTF example
 https://www.tensorflow.org/install/install_java, referenced advanced example LabelImage
 https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java
 
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 None.
 	",1.0,mixalturek,2018-03-03T00:39:15Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  Can you take a look at this?
 		",2.0,mixalturek,2018-03-03T00:52:24Z,"
 		<denchmark-link:https://github.com/mixalturek>@mixalturek</denchmark-link>
  : Thanks for the note and yes we'd like to make documentation and examples as clear as possible.
 The examples referenced only fetch a single tensor, hence the list returned necessarily has a single element (and since that is referenced in the try-with-resources blocks, there is no leak). When the example is explicitly requesting a single tensor, the loop seems unnecessary.
 Happy to hear any suggestions for improvement though.
 		",3.0,mixalturek,2018-03-04T19:16:42Z,"
 		Yes, I guessed there is only a single one so the code is correct. So what about to add a comment with an explicit info.
 <denchmark-code>      // Execute the ""MyConst"" operation in a Session.
       try (Session s = new Session(g);
            // Generally, there may be multiple output tensors, all of them must be closed to prevent resource leaks.
            Tensor output = s.runner().fetch(""MyConst"").run().get(0)) {
         System.out.println(new String(output.bytesValue(), ""UTF-8""));
       }
 </denchmark-code>
 
 		",de72c8cccef2ee77667c041b68a34be6fb61ea65,Michal Turek,2018-04-11 18:32:10-07:00,MODIFY,0,tensorflow\docs_src\install\install_java.md,tensorflow\docs_src\install\install_java.md,0.0,"96,211",,MODIFY,2.0,tensorflow\java\src\main\java\org\tensorflow\examples\LabelImage.java,tensorflow\java\src\main\java\org\tensorflow\examples\LabelImage.java,4.0,mixalturek,2018-03-19T06:59:55Z,"
 		Sure. A PR would be welcome :)
 		",5.0,mixalturek,2020-01-24T21:27:29Z,"
 		The fix PR is merged, so I think we can close this.
 		",,,,,1.0,104,,LabelImage::constructAndExecuteGraphToNormalizeImage,imageBytes,75,108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,114,,LabelImage::executeInceptionGraph,"graphDef,image",110,128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1748,elanmart,2016-04-02T15:05:39Z,2016-06-06T20:51:06Z,Optimizers incompatible with sampling -- missing docs?,"
 Hi,
 perhaps Tensorflow docs should mention that 5 out of 7 available optimizers will not work with sampling losses? For now, they fail with mysterious messages.
 The following script will fail for Momentum, AdaGrad, AdaDelta, RMSProp and FTRL.
 Also, where should I look if I'd like to implement my own optimizers for GPU?
 import tensorflow as tf
 import numpy.random as nr
 import numpy as np
 
 # config 
 num_classes = 10000
 num_sampled = 512
 num_true = 32
 activation_dim = 512
 batch_sz = 16
 
 # ""model"" setup
 activations = tf.placeholder(tf.float32, shape=(None, activation_dim))
 labels = tf.placeholder(tf.int64, shape=(None, num_true))
 
 nce_W = tf.Variable(tf.truncated_normal((num_classes, activation_dim)))
 nce_b = tf.Variable(tf.truncated_normal((num_classes, )))
 
 nce_loss = tf.reduce_mean(
                 tf.nn.nce_loss(nce_W, nce_b, 
                                activations, labels, 
                                num_sampled, num_classes, num_true))
 
 # optimizer setup
 global_step   = tf.Variable(1)
 initial_alpha = tf.Variable(0.1)
 alpha         = tf.Variable(0.01)
 
 optimizer = tf.train.FtrlOptimizer(alpha)
 
 # fetches
 step = optimizer.minimize(nce_loss, global_step=global_step, name='sgd_step') 
 init = tf.initialize_all_variables()
 
 # synthetic data
 X = nr.randn(batch_sz, activation_dim).astype(np.float32)
 y = nr.randint(0, num_classes, (batch_sz, num_true)).astype(np.int64)
 
 with tf.Session() as sess:
     sess.run(init)
     sess.run(step, feed_dict={activations:X, labels:y})
 The error message is:
 <denchmark-code>---------------------------------------------------------------------------
 StatusNotOK                               Traceback (most recent call last)
 StatusNotOK: Invalid argument: Cannot assign a device to node 'Variable_1/read': Could not satisfy explicit device specification '' because the node was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/GPU:0'
      [[Node: Variable_1/read = Identity[T=DT_FLOAT, _class=[""loc:@Variable_1""]](Variable_1)]]
 </denchmark-code>
 
 	",1.0,elanmart,2016-04-07T01:53:30Z,"
 		Does pinning the variables to CPU make this work? i.e.
 with tf.device(""/cpu:0""):
   nce_W = …
   nce_b = …
 		",2.0,elanmart,2016-06-06T20:51:06Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  I assume this is fixed now, in that we don't do improper placement, but I'm not sure all sparse optimizers are supported on GPU, which is perhaps another bug which I believe we already have an issue for.
 		",,,,,c34a0d7ea6b557588a7a0c9f9c4e60d59f593af7,Vijay Vasudevan,2016-05-12 16:31:33-07:00,MODIFY,0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,1368,,MODIFY,8.0,tensorflow\core\common_runtime\direct_session.cc,tensorflow\core\common_runtime\direct_session.cc,,,,,,,,,,,,,1.0,"656,657,658,659,660,664,668,703,704,705","685,687,688,692,727,728,729",tensorflow::DirectSession::GetOrCreateExecutors,"inputs,outputs,target_nodes,executors_and_keys,run_state_args",618,779,MODIFY,1.0,tensorflow\core\common_runtime\direct_session.h,tensorflow\core\common_runtime\direct_session.h,1.0,,122,MODIFY,2.0,tensorflow\core\common_runtime\direct_session_test.cc,tensorflow\core\common_runtime\direct_session_test.cc,1.0,"243,244,245,259","243,257",,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"214,215,216,217,219,220,221","213,214,215,216,217,218,219,220,221,222,223,224,225",tensorflow::DirectSession::ExtendLocked,graph,213,225,1.0,"805,806,807,810,811,812","805,806,807,808,809,812",tensorflow::DirectSession::SaveStatefulNodes,graph,805,812,1.0,"814,815,816,817,818,819,820,821,822,823,824","814,815,816,817,818,819,820,823,824",tensorflow::DirectSession::RestoreStatefulNodes,graph,814,824,1.0,"826,827,831,833,837,838,856,857,858,898,903","826,827,828,829,830,831,832,833,834,835,836,837,838,839,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,865,880,920,925,943",tensorflow::DirectSession::CreateGraphs,"feeds,fetches,target_nodes,func_defs,outputs,run_state_args",826,945,1.0,"781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,802,803,804,805,806,807,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,831,833,837,838,856,857,858,898,903","805,806,807,808,809,812,814,815,816,817,818,819,820,823,824,826,827,828,829,830,831,832,833,834,835,836,837,838,839,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,865,880,920",tensorflow::DirectSession::CreateGraphs,"options,outputs,run_state_args",781,922,,,,,tensorflow::DirectSession::ExecutorsAndKeys::~ExecutorsAndKeys,,117,125,tensorflow::TEST_F,"DirectSessionMinusAXTest,InvalidDevice",225,262,MODIFY,6.0,tensorflow\core\common_runtime\simple_graph_execution_state.cc,tensorflow\core\common_runtime\simple_graph_execution_state.cc,1.0,"205,211,212,213,214,215,216,217,218",,tensorflow::SimpleGraphExecutionState::BuildGraph,"options,out",199,234,1.0,"154,155,156,157,158,159,160,161",,tensorflow::SimpleGraphExecutionState::SaveStatefulNodes,graph,154,161,1.0,"113,114,115,116,117,118,119,120,122,123",113,tensorflow::SimpleGraphExecutionState::InitBaseGraph,,113,123,1.0,"175,176,181,182,183,184,185,186,187,188,189,190,194",,tensorflow::SimpleGraphExecutionState::InitBaseGraph,options,175,197,1.0,"163,164,165,166,167,168,169,170,171,172,173",,tensorflow::SimpleGraphExecutionState::RestoreStatefulNodes,graph,163,173,1.0,"91,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,122,123,124,125,126,127,128,129,130,131,132,133,136,145","94,113,131,137,138,139,140,141",tensorflow::SimpleGraphExecutionState::Extend,"extension_def,out",68,152,MODIFY,4.0,tensorflow\core\common_runtime\simple_graph_execution_state.h,tensorflow\core\common_runtime\simple_graph_execution_state.h,1.0,"119,120,121,122",120,tensorflow::SimpleGraphExecutionState::full_graph,,119,122,1.0,126,,tensorflow::SimpleGraphExecutionState::original_graph_def,,126,126,MODIFY,2.0,tensorflow\core\common_runtime\simple_placer.cc,tensorflow\core\common_runtime\simple_placer.cc,1.0,,662,tensorflow::SimplePlacer::Run,,548,672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478",,tensorflow::TEST,"DirectSessionTest,PlacePrunedGraph",437,478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"137,138,139,140",,tensorflow::SimpleGraphExecutionState::SetStatefulPlacements,sp,137,140,1.0,,472,tensorflow::ColocationGraph::InitializeMember,"node,member",413,487,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\protobuf\config.proto,tensorflow\core\protobuf\config.proto,0.0,"83,84,85,86,87,88,89,90,91",,MODIFY,1.0,tensorflow\python\client\session.py,tensorflow\python\client\session.py,1.0,"901,902,903,904,905",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\python\client\session_test.py,tensorflow\python\client\session_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"130,131,132,133",,tensorflow::SimpleGraphExecutionState::GetStatefulPlacements,,130,133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"180,181,182,183",,tensorflow::DirectSession::~DirectSession,,166,183,1.0,"185,186,187,188,189,190,191,192,193,194,195,196",,tensorflow::DirectSession::MaybeInitializeExecutionState,graph,185,197,__init__,"self,target,graph,config",883,912,1.0,"731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750",,testDefaultSessionPlacePrunedGraph,self,731,750,1.0,"710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729",,testInteractivePlacePrunedGraph,self,710,729,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17614,GingoBang,2018-03-10T08:25:36Z,2018-03-11T05:40:26Z,Error command in installation guild,"
 Please go to Stack Overflow for help and support:
 <denchmark-link:https://stackoverflow.com/questions/tagged/tensorflow>https://stackoverflow.com/questions/tagged/tensorflow</denchmark-link>
 
 There is an small error in the <denchmark-link:https://www.tensorflow.org/install/install_mac#determine_which_tensorflow_to_install>installation guild</denchmark-link>
 
 which is 7th step under the  section.
 The site give an example command of installing TensorFlow in the active Virtualenv for macOS, python which is actually for py3 with pip3 command.
 <denchmark-code> $ pip3 install --upgrade \
  https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl
 </denchmark-code>
 
 	",1.0,GingoBang,2018-03-10T20:08:00Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		",2.0,GingoBang,2018-03-10T20:13:38Z,"
 		Added a PR <denchmark-link:https://github.com/tensorflow/tensorflow/issues/17614>#17614</denchmark-link>
  for the doc fix.
 		",3.0,GingoBang,2018-03-10T20:18:48Z,"
 		I believe this was fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f4e70be18b104fbb2efeefeb83bea190aec12727>f4e70be</denchmark-link>
 
 but an update to the site hasn't been pushed since.
 <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  : Can we push an update?
 		",df2b8447dc026d1402e3c0cbf7c0071ad5c67178,Yong Tang,2018-03-10 21:40:25-08:00,MODIFY,0,tensorflow\docs_src\install\install_mac.md,tensorflow\docs_src\install\install_mac.md,0.0,"121,122,244,245","121,122,244,245",,,,,4.0,GingoBang,2018-03-10T20:45:06Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  It looks like the issue was fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f4e70be18b104fbb2efeefeb83bea190aec12727>f4e70be</denchmark-link>
 , but later it has been overridden by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/9dae88dace281c0c5dc3b9e15521a6c46feb6d75>9dae88d</denchmark-link>
 . I created <denchmark-link:https://github.com/tensorflow/tensorflow/pull/17617>#17617</denchmark-link>
  to get the fix back.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17802,darthdeus,2018-03-18T03:24:59Z,2018-03-20T22:20:23Z,Importing a meta graph which contains a SummaryWriter doesn't work,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 17.10
 TensorFlow installed from (source or binary): binary via pip
 TensorFlow version (use command below): v1.6.0-0-gd2e24b6039 1.6.0
 Python version: 3.6.4
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: 9.0/7.0
 GPU model and memory: GTX 1080ti 11G
 Exact command to reproduce:
 
 First run this:
 import tensorflow as tf
 v1 = tf.placeholder(tf.float32, name=""v1"")
 v2 = tf.placeholder(tf.float32, name=""v2"")
 v3 = v1 * v2
 vx = tf.Variable(10.0, name=""vx"")
 v4 = v3 * vx
 writer = tf.contrib.summary.create_file_writer(""foo"")
 saver = tf.train.Saver([vx])
 sess = tf.Session()
 sess.run(tf.initialize_all_variables())
 sess.run(vx.assign(tf.add(vx, vx)))
 result = sess.run(v4, feed_dict={v1:12.0, v2:3.3})
 print(result)
 saver.save(sess, ""./model_ex1"")
 Then in a different Python instance (it works if done right after the first snippet within the same instance)
 import tensorflow as tf
 saver = tf.train.import_meta_graph(""./model_ex1.meta"")
 sess = tf.Session()
 saver.restore(sess, ""./model_ex1"")
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Trying to restore the meta graph via import_meta_graph does not work if the graph contains a SummaryWriter as shown in the example above. The example works if import_meta_graph is called within the same instance of Python, or if the tf.contrib.summary.create_file_writer(""foo"") call is removed from the graph.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>---------------------------------------------------------------------------
 KeyError                                  Traceback (most recent call last)
 <ipython-input-1-1661c33bc0e5> in <module>()
       1 import tensorflow as tf
 ----> 2 saver = tf.train.import_meta_graph(""./model_ex1.meta"")
       3 sess = tf.Session()
       4 saver.restore(sess, ""./model_ex1"")
 
 ~/.miniconda/lib/python3.6/site-packages/tensorflow/python/training/saver.py in import_meta_graph(meta_graph_or_file, clear_devices, import_scope, **kwargs)
    1907                                       clear_devices=clear_devices,
    1908                                       import_scope=import_scope,
 -> 1909                                       **kwargs)
    1910   if meta_graph_def.HasField(""saver_def""):
    1911     return Saver(saver_def=meta_graph_def.saver_def, name=import_scope)
 
 ~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate)
     735     importer.import_graph_def(
     736         input_graph_def, name=(import_scope or """"), input_map=input_map,
 --> 737         producer_op_list=producer_op_list)
     738
     739     # Restores all the other collections.
 
 ~/.miniconda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
     430                 'in a future version' if date is None else ('after %s' % date),
     431                 instructions)
 --> 432       return func(*args, **kwargs)
     433     return tf_decorator.make_decorator(func, new_func, 'deprecated',
     434                                        _add_deprecated_arg_notice_to_docstring(
 
 ~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in import_graph_def(graph_def, input_map, return_elements, name, op_dict, producer_op_list)
     429   if producer_op_list is not None:
     430     # TODO(skyewm): make a copy of graph_def so we're not mutating the argument?
 --> 431     _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
     432
     433   graph = ops.get_default_graph()
 
 ~/.miniconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py in _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)
     209     # Remove any default attr values that aren't in op_def.
     210     if node.op in producer_op_dict:
 --> 211       op_def = op_dict[node.op]
     212       producer_op_def = producer_op_dict[node.op]
     213       # We make a copy of node.attr to iterate through since we may modify
 
 KeyError: 'SummaryWriter'
 </denchmark-code>
 
 	",1.0,darthdeus,2018-03-19T19:54:40Z,"
 		Thanks for the report.
 <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  : Do we need to recreate the resource ops on import from the  or something?
 		",2.0,darthdeus,2018-03-19T19:59:19Z,"
 		This looks like the metagraph is saved from a newer version of tf than it is loaded in. It's failing to find the SummaryWriter op's registration, which doesn't make sense to me.
 <denchmark-link:https://github.com/darthdeus>@darthdeus</denchmark-link>
  is this reproducible if you use tf-nightly?
 		",3.0,darthdeus,2018-03-19T20:02:10Z,"
 		Ah I just checked and I think I know what's going on. The python code for the generated ops is not getting imported unless you use tf.contrib.summary (because of lazy contrib imports) so we're not registering these summary ops on the separate process which loads the metagraph.
 		",88334807a5beb8b61a967d21e534ed238e7916c0,Alexandre Passos,2018-03-19 20:25:38-07:00,MODIFY,0,tensorflow\contrib\cmake\tf_python.cmake,tensorflow\contrib\cmake\tf_python.cmake,0.0,351,"422,423",MODIFY,0.0,tensorflow\contrib\summary\BUILD,tensorflow\contrib\summary\BUILD,4.0,darthdeus,2018-03-19T20:22:34Z,"
 		I'm submitting a fix, but a temporary workaround is to have a line like ""tf.contrib.summary"" somewhere on your program before you try to import the metagraph.
 		",,,,,,,,,0.0,68,"13,14,15,16,17,18,64",,,,,MODIFY,0.0,tensorflow\contrib\summary\summary_ops.py,tensorflow\contrib\summary\summary_ops.py,0.0,37,29,MODIFY,0.0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,725,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_CloseSummaryWriter.pbtxt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_CreateSummaryDbWriter.pbtxt,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_CreateSummaryFileWriter.pbtxt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_FlushSummaryWriter.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_ImportEvent.pbtxt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_SummaryWriter.pbtxt,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteAudioSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteGraphSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteHistogramSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteImageSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteScalarSummary.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\base_api\api_def_WriteSummary.pbtxt,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_CloseSummaryWriter.pbtxt,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_CreateSummaryDbWriter.pbtxt,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_CreateSummaryFileWriter.pbtxt,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_FlushSummaryWriter.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_ImportEvent.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_SummaryWriter.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteAudioSummary.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteGraphSummary.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteHistogramSummary.pbtxt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteImageSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteScalarSummary.pbtxt,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_WriteSummary.pbtxt,MODIFY,0.0,tensorflow\core\ops\summary_ops.cc,tensorflow\core\ops\summary_ops.cc,0.0,"25,33,41,45,49,58,63,71,79,89,98,104","25,26,27,28,29,30,31,32,33,41,42,43,44,45,46,47,48,49,50,51,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,82,83,84,85,86,87,91,92,93,94,95,96,97,98,99,108,109,110,111,112,113,114,115,116,117,118,123,124,125,126,127,128,129,130,131,132,140,141,142,143,144,145,146,147,148,149,150,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,264,265,266,267,268,269,270,271",MODIFY,0.0,tensorflow\python\BUILD,tensorflow\python\BUILD,0.0,"1361,1362,1363,1364,1365,1366,4119",,MODIFY,0.0,tensorflow\python\summary\summary.py,tensorflow\python\summary\summary.py,0.0,"51,56",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17932,lhlmgr,2018-03-22T19:22:53Z,2018-03-26T18:20:41Z,tf.contrib.data.bucket_by_sequence_length fails for nested Dataset element,"
 Hello everyone,
 I just tried the new function to group variable length inputs for the dataset API, namely: tf.contrib.data.bucket_by_sequence_length, for a small Estimator-Model.
 I implemented the  such that it returns a dataset, where each element is a tuple <denchmark-link:https://www.tensorflow.org/get_started/premade_estimators#create_input_functions>(feature-dict, label)</denchmark-link>
 . However, when I run it, I get following exception:
 
 Traceback (most recent call last):
 ...
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 960, in apply
 dataset = transformation_func(self)
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py"", line 198, in _apply_fn
 window_size_func=window_size_fn))
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 960, in apply
 dataset = transformation_func(self)
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py"", line 90, in _apply_fn
 window_size_func)
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py"", line 239, in init
 self._make_key_func(key_func, input_dataset)
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py"", line 289, in _make_key_func
 self._key_func.add_to_graph(ops.get_default_graph())
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 488, in add_to_graph
 self._create_definition_if_needed()
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 321, in _create_definition_if_needed
 self._create_definition_if_needed_impl()
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 338, in _create_definition_if_needed_impl
 outputs = self._func(*inputs)
 File ""/home/leo/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/data/python/ops/grouping.py"", line 279, in tf_key_func
 ret = key_func(*nested_args)
 TypeError: element_to_bucket_id() takes 1 positional argument but 2 were given
 
 Here is a <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/data/python/ops/grouping.py#L143>link</denchmark-link>
  to the function.
 Here is a code snipped to reproduce the error:
 import tensorflow as tf
 
 def input_fn():
   def generator():
     text = [[1, 2, 3],
             [3, 4, 5, 6, 7],
             [1, 2],
             [8, 9, 0, 2, 3]]
     label = [1, 2, 1, 2]
 
     for x, y in zip(text, label):
       yield (x, y)
 
   dataset = tf.data.Dataset.from_generator(generator=generator,
                                            output_shapes=(tf.TensorShape([None]), tf.TensorShape([])),
                                            output_types=(tf.int32, tf.int32))
 
   dataset = dataset.map(parse_example)
   dataset = dataset.apply(tf.contrib.data.bucket_by_sequence_length(element_length_func=element_length_fn,
                                                                     bucket_batch_sizes=[2, 2, 2],
                                                                     bucket_boundaries=[0, 8],
                                                                     pad_to_bucket_boundary=False))
 
   return dataset
 
 def parse_example(x, y):
   return dict(
     x=x
   ), y
 
 def element_length_fn(element):
   features, label = element
   return tf.shape(features[""x""])[0]
 
 if __name__ == '__main__':
   with tf.Session() as sess:
     dataset = input_fn()
     iter = dataset.make_one_shot_iterator()
 
     print(sess.run(iter.get_next()))
 My Env-Specs are logged in: <denchmark-link:https://github.com/tensorflow/tensorflow/files/1838947/tf_env.txt>tf_env.txt</denchmark-link>
 
 Thanks in advance!
 	",1.0,lhlmgr,2018-03-23T00:59:03Z,"
 		<denchmark-link:https://github.com/rsepassi>@rsepassi</denchmark-link>
  Can you take a look, please?
 /cc <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  FYI.
 		",2.0,lhlmgr,2018-03-23T11:37:59Z,"
 		Yes, looking into it.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Thu, Mar 22, 2018 at 9:02 PM Derek Murray ***@***.***> wrote:
  @rsepassi <https://github.com/rsepassi> Can you take a look, please?
 
  /cc @ebrevdo <https://github.com/ebrevdo> FYI.
 
  —
  You are receiving this because you were mentioned.
 
 
  Reply to this email directly, view it on GitHub
  <#17932 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/ABEGW-jtSsBu9OcEFUz8J3yEIi4AdEo5ks5thEmIgaJpZM4S3plo>
  .
 
 
 
 		",3.0,lhlmgr,2018-03-23T15:59:21Z,"
 		This is consistent with the behavior of all callables in  with  inputs. The function <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/data/ops/dataset_ops.py#L1769>_should_unpack_args</denchmark-link>
  is used to determine whether inputs to a user-supplied callable should be unpacked (i.e. called as ) and is  if the inputs are a . If the elements of a  are tuples, as they are in the example in this thread, then callables must expect the arguments to be passed unpacked.
 For example, here's a Dataset where the elements are tuples and a call to map fails when the map_fn expects only 1 element.
 <denchmark-code>    def data_gen():
       text = [[1, 2, 3], [3, 4, 5, 6, 7], [1, 2], [8, 9, 0, 2, 3]]
       label = [1, 2, 1, 2]
       for x, y in zip(text, label):
         yield (x, y)
 
     dataset = tf.data.Dataset.from_generator(
         generator=data_gen,
         output_shapes=(tensor_shape.TensorShape([None]), tensor_shape.TensorShape([])),
         output_types=(dtypes.int32, dtypes.int32))
 
     # Fails
     dataset.map(lambda el: el)
 </denchmark-code>
 
 We may want to update this behavior but that's where it stands right now.
 Fix right now would be to use a dict going in and apply a map to turn into a tuple afterwards.
 <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 , should we look into updating the behavior of Dataset callables?
 		",2219b88a3d5154b9158a1902b061cad6cae2d0a8,A. Unique TensorFlower,2018-03-25 03:00:00-07:00,MODIFY,3,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py,1.0,"473,474,475,476,477",,MODIFY,2.0,tensorflow\contrib\data\python\ops\grouping.py,tensorflow\contrib\data\python\ops\grouping.py,4.0,lhlmgr,2018-03-23T16:12:19Z,"
 		<denchmark-link:https://github.com/rsepassi>@rsepassi</denchmark-link>
  Right, that example should fail, and we can't change this even if we wanted to. It works if you instead do .
 I think there might be a missing  in the  code, because making the equivalent change to <denchmark-link:https://github.com/lhlmgr>@lhlmgr</denchmark-link>
 's program doesn't fix things, i.e.:
 def element_length_fn(features, label):
   return tf.shape(features[""x""])[0]
 ...still yields the same error:
 
 TypeError: element_to_bucket_id() takes exactly 1 argument (2 given)
 
 		",5.0,lhlmgr,2018-03-23T19:28:56Z,"
 		Yup, found the issue. Have a fix out. bucket_by_sequence_length was not correctly handling tupleized elements.
 		",,,,,1.0,"143,145","143,145",element_to_bucket_id,args,143,155,,,,,,,,,,,,,,,testTupleElements.elements_gen,,473,477,1.0,"479,480,481",,testTupleElements.element_length_fn,"x,y",479,481,1.0,"471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494",,testTupleElements,self,471,494,,,,,,,,1.0,"143,145","143,145",element_to_bucket_id,element,143,155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17946,Conceptron,2018-03-23T09:20:00Z,2018-04-06T00:14:30Z,Formatting issue in tf debugger documentation,"
 <denchmark-link:https://user-images.githubusercontent.com/35289454/37821263-29a87d1c-2ea9-11e8-90b5-7f3bb5f42d49.png></denchmark-link>
 
 In the <denchmark-link:https://www.tensorflow.org/programmers_guide/debugger#frequently_asked_questions>FAQ section</denchmark-link>
 , there seem to be some markdown formatting problems.
 	",1.0,Conceptron,2018-03-25T00:47:15Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		",2.0,Conceptron,2018-03-30T01:29:55Z,"
 		<denchmark-link:https://github.com/caisq>@caisq</denchmark-link>
  I don't understand the markup details, but it looks like the source has a ""```python"" that's indented rather than flush-left around the place where things go wrong. PTAL.
 		",3.0,Conceptron,2018-03-30T03:15:52Z,"
 		<denchmark-link:https://github.com/Conceptron>@Conceptron</denchmark-link>
  <denchmark-link:https://github.com/cy89>@cy89</denchmark-link>
  thanks for reporting this issue. I believe this has to do with some subtle differences among the two markdown renderers that we have:
 
 The one on GitHub
 The one on the tensorflow.org website.
 
 This file renders correctly on GitHub:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/debugger.md>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/docs_src/programmers_guide/debugger.md</denchmark-link>
 
 cc <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  for any possible insight.
 		",cde06a39592a849a2bc0ba022e858e6366c87cc5,Billy Lamberta,2018-04-05 17:14:29-07:00,MODIFY,0,tensorflow\docs_src\programmers_guide\debugger.md,tensorflow\docs_src\programmers_guide\debugger.md,0.0,"7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,750,751,752,754,755,756,758,759","7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,751,752,753,754,755,756,757,760",,,,,4.0,Conceptron,2018-04-05T01:21:03Z,"
 		The site source (<denchmark-link:https://www.tensorflow.org/versions/r1.3/programmers_guide/debugger#debugging_model_training_with_tfdbg>https://www.tensorflow.org/versions/r1.3/programmers_guide/debugger#debugging_model_training_with_tfdbg</denchmark-link>
 ) looks like:
 <denchmark-code><li>The constructors of <code>LocalCLIDebugWrapperSession</code> and <code>LocalCLIDebugHook</code>
    provide a keyword argument, <code>dump_root</code>, to specify the path
    to which tfdbg dumps the debug data. You can use it to let tfdbg dump the
    debug data on a disk with larger free space. For example:</li>
 </ul>
 <p>``` python
    # For LocalCLIDebugWrapperSession
    sess = tf_debug.LocalCLIDebugWrapperSession(dump_root=""/with/lots/of/space"")</p>
 <p># For LocalCLIDebugHook
    hooks = [tf_debug.LocalCLIDebugHook(dump_root=""/with/lots/of/space"")]
    <code>``
    Make sure that the directory pointed to by dump_root is empty or nonexistent.
    tfdbg cleans up the dump directories before exiting.
 </denchmark-code>
 
 All other instances of code markdown on the guide are wrapped with <pre class=""prettyprint lang-python""><code>python goes here </code></pre>, but this one instance is not.
 To fix the documentation on the tensorflow.org website, you should just need to replace the appropriate section with:
 <denchmark-code><li>The constructors of <code>LocalCLIDebugWrapperSession</code> and <code>LocalCLIDebugHook</code>
    provide a keyword argument, <code>dump_root</code>, to specify the path
    to which tfdbg dumps the debug data. You can use it to let tfdbg dump the
    debug data on a disk with larger free space. For example:</li>
 </ul>
 <pre class=""prettyprint lang-none""><code>   # For LocalCLIDebugWrapperSession
    sess = tf_debug.LocalCLIDebugWrapperSession(dump_root=""/with/lots/of/space"")
 
    # For LocalCLIDebugHook
    hooks = [tf_debug.LocalCLIDebugHook(dump_root=""/with/lots/of/space"")]
 </code></pre>
    Make sure that the directory pointed to by dump_root is empty or nonexistent.
    tfdbg cleans up the dump directories before exiting.
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17949,denkuzin,2018-03-23T11:54:29Z,2018-08-20T19:44:47Z,incorrect description of num_sampled parameter in tf.nn.nce_loss() function: num_sampled is number of negative examples per 1 positive example (NOT per batch),"
 Hi all,
 Looks like parameter  in   function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss>https://www.tensorflow.org/api_docs/python/tf/nn/nce_loss</denchmark-link>
 )
 (see the next code)
 <denchmark-code>import tensorflow as tf
 import numpy as np
 # `_compute_sampled_logits` is invoked in nce_loss to generate negative sample and calculate logits
 from tensorflow.python.ops.nn_impl import _compute_sampled_logits
 
 embedding_size = 10
 words_number = 300
 batch_size = 3
 num_sampled = 3
 
 graph = tf.Graph()
 with graph.as_default():
     # Input data.
     train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
     train_labels = tf.placeholder(tf.int64, shape=[batch_size, 1])
 
     with tf.device('/cpu:0'):
         embeddings = tf.Variable(
                 tf.random_uniform([words_number, embedding_size], -4., 4.))
         embed = tf.nn.embedding_lookup(embeddings, train_inputs)
         nce_weights = tf.Variable(
                 tf.random_uniform([words_number, embedding_size], -4., 4.))
         nce_biases = tf.Variable(tf.zeros([words_number]))
         
     logits, labels = _compute_sampled_logits(
                        weights=nce_weights,
                        biases=nce_biases,
                        inputs=embed,
                        labels=train_labels,
                        num_true=1,
                        num_sampled=num_sampled,
                        num_classes=words_number,
                        remove_accidental_hits = False)
     init = tf.global_variables_initializer()
 
 
 session = tf.InteractiveSession(graph=graph)
 init.run(session=session)
 
 batch_inputs = np.array([0,1,2], dtype=np.int32)
 batch_labels = np.array([[3],[4],[5]], dtype=np.int32)
 
 feed_dict = {train_inputs : batch_inputs, train_labels : batch_labels}
 logits_val, labels_val = session.run([logits, labels], feed_dict=feed_dict)
 
 print (""logits_val = {}"".format(logits_val))
 print (""labels_val = {}"".format(labels_val))
 
 </denchmark-code>
 
 As a result, _compute_sampled_logits function generated num_sampled examples per 1 positive example:
 <denchmark-code>logits_val = [[ -8.18727493   2.02518415  14.18676853   0.51900673]
  [ -8.97232056   5.60003376   4.52866602   3.68161726]
  [ -0.36226368  -5.84330416  -3.39891291   5.58423615]]
 labels_val = [[ 1.  0.  0.  0.]
  [ 1.  0.  0.  0.]
  [ 1.  0.  0.  0.]]
 </denchmark-code>
 
 	",1.0,denkuzin,2018-03-23T12:02:03Z,"
 		Maybe it's not a bug but very significant typo in the documentation
 		",2.0,denkuzin,2018-06-29T18:31:52Z,"
 		From my limited understanding, I think the documentation is correct. The first logit in your output is computed from all true samples, not just from 1 true sample. The rest 3 logits are for the 3 random samples.
 		",3.0,denkuzin,2018-07-21T00:30:23Z,"
 		<denchmark-link:https://github.com/denkuzin>@denkuzin</denchmark-link>
  thank you for pointing this out!
 <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  would you PTAL?
 		",600caf99897e82cd0db8665acca5e7630ec1a292,Mark Daoust,2018-08-20 12:42:36-07:00,MODIFY,0,tensorflow\python\ops\nn_impl.py,tensorflow\python\ops\nn_impl.py,0.0,"1213,1214,1215",1213,,,,,4.0,denkuzin,2018-08-19T18:47:35Z,"
 		Nagging Assignee <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
 : It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",5.0,denkuzin,2018-08-20T19:51:01Z,"
 		Hi denkuzin@,
 Thanks for the report. Here's a fix that I think clarifies what's actually happening here.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18094,zmjjmz,2018-03-29T18:13:25Z,2018-04-20T23:10:42Z,"`tf.keras.estimator._create_ordered_io` casts everything to floatx, which breaks non-floatx inputs","
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 3.16.36
 TensorFlow installed from (source or binary): Installed via pip
 TensorFlow version (use command below): ('v1.6.0-0-gd2e24b6039', '1.6.0')
 Python version: 2.7.9
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 Exact command to reproduce: Requires significant code, let me know if necessary
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 This is kind of a simple issue with using Keras models as Tensorflow Estimators. I unfortunately need to do this awkward conversion in order to use SageMaker, which is even more awkwardly behind by two versions of Tensorflow. Which is fun.
 Basically, I have a  model that expects a  input , which is then passed through to a Lookup layer for some text embeddings. This works fine as a Keras model and works fine if I extract the input layers myself and connect them into an Estimator. However, if I go to create an estimator from the model using  I run into this code path: <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/keras/_impl/keras/estimator.py#L80>https://github.com/tensorflow/tensorflow/blob/r1.6/tensorflow/python/keras/_impl/keras/estimator.py#L80</denchmark-link>
 
 This conversion then causes the model to break further down the line. I'm not sure why this float cast occurs, but this commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/4c86ece040cb96ea689f5c0d084b6959274eab91#diff-69effda952f96b36c8015cc1a3462d65>4c86ece#diff-69effda952f96b36c8015cc1a3462d65</denchmark-link>
  seems to imply that Keras models are meant to only take floatx input, which doesn't really seem right.
 Would not doing this cast break anything? If so, is there a way to use a non-float32 input with Keras models that need to be converted to Estimators?
 Thanks!
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Here's the exact traceback for the issue:
 <denchmark-code>/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument 
 of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.          
   from ._conv import register_converters as _register_converters                                                                           
 WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp6Wogzk                                                               
 2018-03-29 14:12:41.586292: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow       
 binary was not compiled to use: AVX2 FMA                                                                                                   
 WARNING:tensorflow:Output ""final_representation"" missing from loss dictionary. We assume this was done on purpose, and we will not be      
 expecting any data to be passed to ""final_representation"" during training.                                                                 
 WARNING:tensorflow:Output ""oov_code"" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any    
 data to be passed to ""oov_code"" during training.                                                                                           
 Testing common_estimator_fns.py locally                                                                                                    
 Making estimator                                                                                                                           
 Model dir: /tmp/tmp6Wogzk                                                                                                                  
 Training estimator                                                                                                                         
 float64                                                                                                                                    
 Tensor(""random_shuffle_queue_DequeueMany:1"", shape=(32, 1), dtype=string, device=/device:CPU:0)                                            
 Traceback (most recent call last):                                                                                                         
   File ""common_estimator_fns.py"", line 423, in <module>                                                                                    
     hooks=[tf_debug.LocalCLIDebugHook()])                                                                                                  
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train 
     loss = self._train_model(input_fn, hooks, saving_listeners)                                                                            
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 812, in       
 _train_model                                                                                                                               
     features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)                                                                            
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 793, in       
 _call_model_fn                                                                                                                             
     model_fn_results = self._model_fn(features=features, **kwargs)                                                                         
   File ""common_estimator_fns.py"", line 381, in model_fn                                                                                    
     return keras_model_fn(features, labels, mode)                                                                                          
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py"", line 160,  
 in model_fn                                                                                                                                
     labels)                                                                                                                                
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/estimator.py"", line 109,  
 in _clone_and_build_model                                                                                                                  
     model = models.clone_model(keras_model, input_tensors=input_tensors)                                                                   
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 1557, in 
 clone_model                                                                                                                                
     return _clone_functional_model(model, input_tensors=input_tensors)                                                                     
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/models.py"", line 1451, in 
 _clone_functional_model                                                                                                                    
     output_tensors = topology._to_list(layer(computed_tensor, **kwargs))                                                                   
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py"", line 
 258, in __call__                                                                                                                           
     output = super(Layer, self).__call__(inputs, **kwargs)                                                                                 
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 673, in __call__      
     self._assert_input_compatibility(inputs)                                                                                               
   File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 1204, in              
 _assert_input_compatibility                                                                                                                
     ', found dtype=' + str(x.dtype))                                                                                                       
 ValueError: Input 0 of layer lookedup is incompatible with the layer: expected dtype=<dtype: 'string'>, found dtype=<dtype: 'float32'>     
 </denchmark-code>
 
 I can provide code if absolutely necessary, but it'd take some work to get to a minimal reproduction.
 	",1.0,zmjjmz,2018-03-29T18:21:19Z,"
 		Looking at the history of this file, using other types were never tested.
 We should use <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/is_numeric_tensor>https://www.tensorflow.org/api_docs/python/tf/is_numeric_tensor</denchmark-link>
  before casting
 		",2.0,zmjjmz,2018-04-03T17:05:21Z,"
 		<denchmark-link:https://github.com/fchollet>@fchollet</denchmark-link>
  Can you take a look at this?
 		",3.0,zmjjmz,2018-04-12T00:08:37Z,"
 		I confirm this bug. /cc <denchmark-link:https://github.com/lenlen>@lenlen</denchmark-link>
 
 		",3fa8795c511931b55a9703956bdf564fde817c2a,Frédéric Branchaud-Charron,2018-04-20 16:10:41-07:00,MODIFY,2,tensorflow\python\keras\_impl\keras\estimator.py,tensorflow\python\keras\_impl\keras\estimator.py,1.0,"60,61,62,63,64,65,66,67,68",,MODIFY,4.0,tensorflow\python\keras\_impl\keras\estimator_test.py,tensorflow\python\keras\_impl\keras\estimator_test.py,4.0,zmjjmz,2018-04-12T19:09:43Z,"
 		<denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
  what do you think about this issue? Could it be a simple fix?
 		",5.0,zmjjmz,2018-04-12T19:16:48Z,"
 		I opened a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/18104>#18104</denchmark-link>
  as a starting point.
 		",,,,,1.0,"360,361,362,363,364,365,366,373,374,379,380","361,366",test_multi_inputs_multi_outputs,self,347,391,,,,,,,,,,,,,,,_convert_tensor,x,60,68,1.0,"102,124,129","89,111,116",_create_ordered_io,"keras_model,estimator_io,is_input",84,129,,,,,,,,,,,,,,,1.0,"373,374",,test_multi_inputs_multi_outputs.train_input_fn,,372,376,1.0,"379,380",,test_multi_inputs_multi_outputs.eval_input_fn,,378,382,1.0,"148,150,152,153,154,156,159","145,151,154",multi_inputs_multi_outputs_model,,145,167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18180,asimshankar,2018-04-02T17:10:21Z,2018-04-05T20:14:32Z,Eager: tf.size() does not respect `out_type`,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux, Ubuntu 16.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.7.0
 Python version:  2.7.12
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce:
 
 import tensorflow as tf
 
 tf.enable_eager_execution()
 
 print(tf.size([1, 2, 3]).dtype)
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 As per the documentation of <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/size>tf.size</denchmark-link>
 , the  of the returned tensor should default to  and it can be optionally overridden by providing an  argument.
 However, in the snippet above,  returns a  tensor, and in a related StackOverflow question: <denchmark-link:https://stackoverflow.com/questions/49604969/gradient-error-occurred-when-calculate-two-embeddings-on-eager-mode>https://stackoverflow.com/questions/49604969/gradient-error-occurred-when-calculate-two-embeddings-on-eager-mode</denchmark-link>
  the  used by  is resulting in a  tensor.
 Long story short, this is a buggy discrepancy between eager execution and graph construction.
 (Likely introduced in commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/47ea851d3faf029d5b23ee70cb3b96bad0128324>47ea851</denchmark-link>
 )
 CC <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
 
 	",,,,,,,,,,,,,46df4a1afd50f69966e63245e7758cc0d5656c4e,Asim Shankar,2018-04-02 12:03:55-07:00,MODIFY,3,tensorflow\python\kernel_tests\array_ops_test.py,tensorflow\python\kernel_tests\array_ops_test.py,1.0,"1024,1025,1026,1027,1028,1029,1030,1031,1032,1033","1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033",MODIFY,1.0,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,,,,,,,,,,,,,1.0,"390,391,392,393",390,size_internal,"input,name,optimize,out_type",374,407,,,,,,,,,,,,,,,testSparseShape,self,1023,1033,1.0,"1036,1037,1038,1039,1040,1041",,testSizeDtype,self,1036,1041,1.0,"1012,1013,1014,1015,1017,1018,1019,1020","1011,1012,1013,1014,1015,1017,1018,1019,1020",testDenseShape,self,1011,1020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1853,yaroslavvb,2016-04-11T17:40:39Z,2017-06-13T23:58:22Z,shuffle_batch gives ZeroDivisionError when computing capacity stat,"
 <denchmark-code>import tensorflow as tf
 raw = tf.ones((2))
 tf.train.shuffle_batch([raw], batch_size=2, capacity=4, min_after_dequeue=4, seed=5)
 
 </denchmark-code>
 
 this fails with
 <denchmark-code>ZeroDivisionError                         Traceback (most recent call last)
 <ipython-input-11-86e4af7ca1ee> in <module>()
       1 import tensorflow as tf
       2 raw = tf.ones((2))
 ----> 3 tf.train.shuffle_batch([raw], batch_size=2, capacity=4, min_after_dequeue=4, seed=5)
 
 /Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/training/input.py in shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)
    1220       allow_smaller_final_batch=allow_smaller_final_batch,
    1221       shared_name=shared_name,
 -> 1222       name=name)
    1223 
    1224 
 
 /Users/yaroslav/anaconda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/training/input.py in _shuffle_batch(tensors, batch_size, capacity, min_after_dequeue, keep_input, num_threads, seed, enqueue_many, shapes, allow_smaller_final_batch, shared_name, name)
     779     full = (math_ops.cast(math_ops.maximum(0, queue.size() - min_after_dequeue),
     780                           dtypes.float32) *
 --> 781             (1. / (capacity - min_after_dequeue)))
     782     # Note that name contains a '/' at the end so we intentionally do not place
     783     # a '/' after %s below.
 
 </denchmark-code>
 
 Unlike TF tensors, Python doesn't handle float division by zero. One solution is to wrap ""capacity"" and ""min_after_dequeue"" into TF tensors so that you get ""inf"" as a result instead of RuntimeError
 	",,,,,,,,,,,,,70ade1b64f65d0a2275672d27129627ff116a997,Jing Jun Yin,2017-06-13 16:58:21-07:00,MODIFY,0,tensorflow\python\training\input.py,tensorflow\python\training\input.py,0.0,"765,766,767",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18769,rothn,2018-04-22T06:03:46Z,2019-05-08T02:17:48Z,"InvalidArgumentError for save/restore of variables (same version, same OS, same directory)","
 I get an InvalidArgumentError with no further information when I try to save and then restore parts of my model later to continue training it (due to needing my laptop for class).
 Initialization:
 saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})
 Save:
 saver.save(sess, model_checkpoint_path)
 Load:
 saver.restore(sess, model_checkpoint_path)
 <denchmark-code>2018-04-21 22:45:00.143245: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
   File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1327, in _do_call
     return fn(*args)
   File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _run_fn
     options, feed_dict, fetch_list, target_list, run_metadata)
   File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1420, in _call_tf_sessionrun
     status, run_metadata)
   File ""/Users/nroth/tf_python/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 516, in __exit__
     c_api.TF_GetCode(self.status.status))
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 
 ... <contains sensitive info> ...
 
 InvalidArgumentError (see above for traceback): /Users/nroth/Documents/****/trained_model/****embeddings.ckpt.data-00000-of-00001; Invalid argument
 	 [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 </denchmark-code>
 
 
 
 Yes, I modified this code (<denchmark-link:https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py>https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook/blob/master/Chapter%2007/doc2vec.py</denchmark-link>
 ) to work with TensorFlow 1.7 and to use the same embeddings variable for documents as for words with average instead of concatenation. I also updated the saved variables to include nce_weights and nce_biases so that training may be resumed.
 
 MacOS 10.13.4 (17E199)
 
 pip on VirtualEnv, according to instructions (<denchmark-link:https://www.tensorflow.org/install/install_mac>https://www.tensorflow.org/install/install_mac</denchmark-link>
 )
 
 1.7
 
 NA
 
 NA
 
 NA
 
 saver = tf.train.Saver({""embeddings"": embeddings, ""weights"": nce_weights, ""biases"": nce_biases})
 saver.restore(sess, ""../trained_model/saved_stuff"")
 	",1.0,rothn,2018-04-22T18:30:04Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		",2.0,rothn,2018-04-22T22:26:02Z,"
 		Ticket updated with requested information. Will use template in the future!
 		",3.0,rothn,2018-04-25T02:39:06Z,"
 		Possibly related to: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/18640>#18640</denchmark-link>
 
 		",f2be10a6d278f9a4546fa9cded94074959e67302,Allen Lavoie,2019-05-07 19:01:17-07:00,MODIFY,1,tensorflow\core\platform\posix\posix_file_system.cc,tensorflow\core\platform\posix\posix_file_system.cc,1.0,"66,67,68,69,70,71,72,73,74,75",65,,,,,4.0,rothn,2018-04-27T00:36:54Z,"
 		Update: I tried this on Windows as well, and I got rid of the stuff for loading the model graph and used the Saver to explicitly restore my variables. On Windows, it fails for the same reason (because I am loading an embedding larger than 2GB) but with a better error message:
 OutOfRangeError (see above for traceback): Read fewer bytes than requested [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]
 		",5.0,rothn,2018-05-13T00:31:41Z,"
 		<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  bump :)
 		",6.0,rothn,2018-06-06T12:36:16Z,"
 		I am using tensorflow-gpu 1.7.0, and meet exactly the same problem when restoring model in distributed tensorflow environment.
 when the model file is larger than 3GB, the error below occurs:
 InvalidArgumentError (see above for traceback): hdfs://xxxx/model.ckpt-5154361.data-00001-of-00008; Invalid argument
 [[Node: save_1/RestoreV2_1 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:ps/replica:0/task:0/device:CPU:0""](_recv_save_1/Const_0_S7, save_1/RestoreV2_1/tensor_names, save_1/RestoreV2_1/shape_and_slices)]]
 [[Node: save_1/restore_all/NoOp_S10 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:worker/replica:0/task:0/device:GPU:0"", send_device=""/job:ps/replica:0/task:0/device:GPU:0"", send_device_incarnation=1493071510865629599, tensor_name=""edge_147_save_1/restore_all/NoOp"", tensor_type=DT_FLOAT, _device=""/job:worker/replica:0/task:0/device:GPU:0""</denchmark-link>
 ]]
 when I decrease the model size, the error changed:
 tensorflow.python.framework.errors_impl.OutOfRangeError: Read less bytes than requested
 [[Node: save_1/RestoreV2_3 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT], _device=""/job:ps/replica:0/task:1/device:CPU:0""](_recv_save_1/Const_0_S1, save_1/RestoreV2_3/tensor_names, save_1/RestoreV2_3/shape_and_slices)]]
 List the smaller models:
 -rw-r--r--   3 root supergroup   11893084 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00000-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00001-of-00008
 -rw-r--r--   3 root supergroup  106483864 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00002-of-00008
 -rw-r--r--   3 root supergroup 5130000440 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.data-00003-of-00008
 -rw-r--r--   3 root supergroup     375688 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00004-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00005-of-00008
 -rw-r--r--   3 root supergroup  232909600 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00006-of-00008
 -rw-r--r--   3 root supergroup  330000440 2018-06-06 19:12 /tmp/xxx/model.ckpt-446327.data-00007-of-00008
 -rw-r--r--   3 root supergroup       1564 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.index
 -rw-r--r--   3 root supergroup     571996 2018-06-06 19:13 /tmp/xxx/model.ckpt-446327.meta
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::PosixRandomAccessFile::Read,"offset,n,result,scratch",61,90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,rothn,2018-06-08T17:18:28Z,"
 		Any updates, <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  ?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,rothn,2018-06-11T05:34:43Z,"
 		I think the root cause may be:
 in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc>https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/core/platform/hadoop/hadoop_file_system.cc</denchmark-link>
 
 line 213:
      tSize r = hdfs_->hdfsPread(fs_, file_, static_cast<tOffset>(offset), dst, static_cast<tSize>(n));
 the last param static_cast(n) means the number of bytes required. ie., the tensor's size.
 in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h>https://github.com/tensorflow/tensorflow/blob/r1.7/third_party/hadoop/hdfs.h</denchmark-link>
 
 line 75,
 typedef int32_t tSize;    /// size of data for read/write io ops
 tSize is int32, but tensor's size is int64, when the tensor is big enough, overflow occurs.
 My solution is to use partitioned variables for large tensor, then problem solved.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,rothn,2018-06-15T02:06:05Z,"
 		But I'm not using HDFS -- this is all on my local SSD.
 		",10.0,rothn,2018-07-18T14:14:04Z,"
 		I am having a very similar issue as well. Following <denchmark-link:https://www.tensorflow.org/tutorials/estimators/cnn>this tutorial</denchmark-link>
 , but working with larger images.
 		",11.0,rothn,2018-10-13T03:53:12Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  seems like we're having issues with large restore? I thought the V2 version of the save restore ops was supposed to fix this. Any ideas?
 		",12.0,rothn,2018-10-15T22:05:30Z,"
 		We can put >2GB in checkpoints now. <denchmark-link:https://github.com/tensorflow/tensorflow/commit/b8c86c3bbd8271ed968087f24e7fb704103bc733#diff-f4340b63dcfb03e060905682f7471faa>b8c86c3#diff-f4340b63dcfb03e060905682f7471faa</denchmark-link>
  fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From ""OP_REQUIRES failed at save_restore_v2_ops.cc:184"" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.
 Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:
 <denchmark-code>import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/some/network/path'
   io_ops.save_v2([path], [""a""], [""""], [large])
   restored = io_ops.restore_v2(path, [""a""], [""""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 
 </denchmark-code>
 
 Prints:
 tf.Tensor(5e+09, shape=(), dtype=float32)
 		",13.0,rothn,2018-11-08T13:29:12Z,"
 		
 We can put >2GB in checkpoints now. b8c86c3#diff-f4340b63dcfb03e060905682f7471faa fixes an int32 size issue for string dtypes. I don't see a similar issue for Tensors, and this error doesn't look like it's a length/checksum issue. From ""OP_REQUIRES failed at save_restore_v2_ops.cc:184"" and reading the exceptions in the function it's calling, it looks vaguely like this could be complaining about a dtype mismatch? But that could just be the first thing that doesn't match if the whole file is corrupted for some reason.
 Can someone distill their issue into a snippet I can run, e.g. with fill()? I ran the following and it seems to work:
 import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/some/network/path'
   io_ops.save_v2([path], [""a""], [""""], [large])
   restored = io_ops.restore_v2(path, [""a""], [""""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 
 Prints:
 tf.Tensor(5e+09, shape=(), dtype=float32)
 
 I've tried this snippet on Mac OS X 10.14.1 + TF 1.12.0
 Got such error:
 
 restored = io_ops.restore_v2(path, [""a""], [""""], [tf.float32])
 2018-11-08 17:27:05.767089: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
 File """", line 1, in 
 File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1486, in restore_v2
 ctx=_ctx)
 File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1511, in restore_v2_eager_fallback
 attrs=_attrs, ctx=_ctx, name=name)
 File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
 six.raise_from(core._status_to_exception(e.code, message), None)
 File """", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/alex/HDD/Develop/tmp.data-00000-of-00001; Invalid argument [Op:RestoreV2]
 
 		",14.0,rothn,2018-11-08T17:21:01Z,"
 		Interesting, thank you for trying that out. So maybe it is a Mac-specific issue.
 Just to check, do you have >20GB of disk and >20GB of RAM free before running that snippet? This could be a badly reported OOM or out-of-disk error I suppose.
 		",15.0,rothn,2018-11-09T11:38:08Z,"
 		I have about 400Gb free space on disk and total 16Gb of RAM (swapfile ON).
 Such issue occurs even if try to load 2.7Gb variable file.
 		",16.0,rothn,2018-11-10T00:32:07Z,"
 		Interesting. I'll find a Mac next week and see if I can reproduce on it.
 		",17.0,rothn,2018-11-22T04:24:27Z,"
 		Finally i found the way how to use save/load embeddings. Variable partitioning is the answer.
 With tf.variable_axis_size_partitioner and max_shard_bytes=2 ** 30 large embeddings loading works well.
 		",18.0,rothn,2019-01-21T05:57:20Z,"
 		The issue is still there
 		",19.0,rothn,2019-03-15T00:29:43Z,"
 		Problem still exist when I restore a 18GB model.
 My OS is Windows Server 2016
 Tensorflow 1.13.0 installed from pip
 512GB memory and hard disk
 Only run on CPU.
 Please advise.
 		",20.0,rothn,2019-03-16T08:21:04Z,"
 		I also put the below in my code
 tf.variable_axis_size_partitioner(
 max_shard_bytes=2**35,
 axis=0,
 bytes_per_string_element=16,
 max_shards=None
 )
 However it still shows below error when restoring the model:
 2019-03-16 08:12:32.430456: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Out of range: Read fewer bytes than requested
 		",21.0,rothn,2019-04-05T05:17:47Z,"
 		
 It has been 17 days with no activity and the awaiting response label was assigned. Is this still an issue?
 
 Yes, it is still an issue
 		",22.0,rothn,2019-05-04T12:41:11Z,"
 		It has been 29 days with no activity and the awaiting response label was assigned. Is this still an issue?
 		",23.0,rothn,2019-05-04T17:05:31Z,"
 		Yes it’s still an issue
 		",24.0,rothn,2019-05-08T00:14:27Z,"
 		Finally got a chance to debug this on a mac. Apparently the pread system call, despite taking an eight-byte size_t for its nbytes argument, returns EINVAL if ""The sum of the iov_len values in the iov array overflowed a 32-bit integer."" And presumably pread is implemented in terms of readv so they have the same limitation.
 I have a change out for review which just limits reads to INT32_MAX on every platform. Seems to work if we do that. I checked that the checkpoints themselves were identical to what gets written on Linux, so existing checkpoints will start working once that change is in.
 		",25.0,rothn,2019-05-08T02:17:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=18769>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=18769>No</denchmark-link>
 
 		",26.0,rothn,2019-05-22T11:25:04Z,"
 		Uh, this is still an issue? I built the latest version of tensorflow from source and it's still happening! I've tried practically everything. One of the most arcane and infuriating errors I have dealt with...
 		",27.0,rothn,2019-05-22T16:10:54Z,"
 		@Sam-DevZ operating system and version, TF git version, repro instructions? The <denchmark-link:https://github.com/tensorflow/tensorflow/issues/18769#issuecomment-430030868>repro I posted</denchmark-link>
  works for me on  (nightly) and macOS 10.14.2.
 		",28.0,rothn,2019-05-23T11:18:11Z,"
 		Hi! Your snippet doesn't work either. Code:
 import tensorflow as tf
 from tensorflow.python.ops import io_ops
 
 print('Tensorflow version: ' + tf.__version__)
 print('MacOS Version: ' + '10.14.5')
 
 def main(_):
   tf.enable_eager_execution()
   large = tf.fill([10] * 9 + [5], value=tf.constant(1., dtype=tf.float32))
   path = '/Users/clearlycoder/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN'
   io_ops.save_v2([path], [""a""], [""""], [large])
   restored = io_ops.restore_v2(path, [""a""], [""""], [tf.float32])
   print(tf.reduce_sum(restored))
 
 if __name__ == '__main__':
   tf.app.run()
 Output:
 <denchmark-code>Tensorflow version: 1.12.2
 MacOS Version: 10.14.5
 2019-05-23 07:16:10.724624: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Invalid argument: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument
 Traceback (most recent call last):
   File ""/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py"", line 16, in <module>
     tf.app.run()
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
     _sys.exit(main(argv))
   File ""/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py"", line 12, in main
     restored = io_ops.restore_v2(path, [""a""], [""""], [tf.float32])
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1486, in restore_v2
     ctx=_ctx)
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 1511, in restore_v2_eager_fallback
     attrs=_attrs, ctx=_ctx, name=name)
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""<string>"", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN.data-00000-of-00001; Invalid argument [Op:RestoreV2]
 [Finished in 80.3s with exit code 1]
 [cmd: ['/Library/Frameworks/Python.framework/Versions/3.6/bin/python3', '-u', '/Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN/tester.py']]
 [dir: /Users/samdevz/Desktop/Extra.nosync/Machine Learning.nosync/TensorCNN]
 [path: /usr/bin:/bin:/usr/sbin:/sbin]
 </denchmark-code>
 
 		",29.0,rothn,2019-05-23T11:18:24Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
 
 		",30.0,rothn,2019-05-23T11:21:02Z,"
 		Oh, er, I'm guessing version 0.12.2 is not recent enough. I need 1.14.1-dev20190522, right? Haha.
 		",31.0,rothn,2019-05-23T14:55:50Z,"
 		Sorry! It's definitely fixed. I ran:
 <denchmark-code>pip uninstall tensorflow
 pip install tf-nightly
 </denchmark-code>
 
 And now it works perfectly. Sorry about that!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19043,nikitaverbis,2018-05-03T01:09:21Z,2018-05-03T18:21:37Z,Code documetnantion. Probably misprint in function parameter description.,"
 
 
 
 tensorflow/tensorflow/contrib/slim/python/slim/learning.py
 
 
          Line 573
       in
       a44996a
 
 
 
 
 
 
      log_every_n_steps: The frequency, in terms of global steps, that the loss 
 
 
 
 
 
 I believe there was a typo in the description in the penultimate word (""and""), which is probably worth replacing with ""are"".
 Example:
 log_every_n_steps: The frequency, in terms of global steps, that the loss and global step are logged.
 	",1.0,nikitaverbis,2018-05-03T12:34:57Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 TensorFlow version
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 		",2.0,nikitaverbis,2018-05-03T16:48:14Z,"
 		That seems reasonable to me; seems like a good PR if you're interested in contributing.
 		",3.0,nikitaverbis,2018-05-03T17:31:14Z,"
 		Created a pull request Update Learning.py to resolve this issue.
 		",ebcde41d721ec554a7840cb18e4e8a7a489e424a,Aditya Yogi,2018-05-03 14:13:40-04:00,MODIFY,0,tensorflow\contrib\slim\python\slim\learning.py,tensorflow\contrib\slim\python\slim\learning.py,0.0,574,574,,,,,4.0,nikitaverbis,2018-05-03T18:21:36Z,"
 		Fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/19064>#19064</denchmark-link>
  . Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19360,girving,2018-05-17T20:28:10Z,2018-08-09T23:49:48Z,tf.split's -1 support doesn't handle zero dimensions,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
 TensorFlow installed from (source or binary): Colab
 TensorFlow version (use command below): ('unknown', '1.7.0') and 1.8.0
 Python version: 3.6.3
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 Exact command to reproduce: tf.split(tf.zeros([0]), [0, -1], axis=-1)
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 The variable size version of tf.split (SplitV in C++) allows one of the sizes to be -1.  The corresponding output will expand as necessary so that the total output size matches the input.
 Unfortunately, the -1 support currently assumes the -1 dimension corresponds to positive size.  It should handle zero as well.  E.g., this should work, but it doesn't:
 <denchmark-code>>>> tf.split(tf.zeros([0]), [0, -1], axis=-1)
 Traceback (most recent call last):
   File ""/Users/irving/anaconda/envs/openai/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1592, in _create_c_op
     c_op = c_api.TF_FinishOperation(op_desc)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Sum of output sizes must match the size of the original Tensor along the split dimension or the sum of the positive sizes must be less if it contains a -1 for 'split_3' (op: 'SplitV') with input shapes: [0], [2], [] and with computed input tensors: input[1] = <0 -1>, input[2] = <-1>.
 </denchmark-code>
 
 By comparison, the positive case works fine:
 <denchmark-code>>>> tf.split(tf.zeros([1]), [0, -1], axis=-1)
 [<tf.Tensor 'split_4:0' shape=(0,) dtype=float32>, <tf.Tensor 'split_4:1' shape=(?,) dtype=float32>]
 </denchmark-code>
 
 	",1.0,girving,2018-05-17T20:30:27Z,"
 		Here's a colab exhibiting the problem, though since the repro is a single line it's not really necessary:
 <denchmark-link:https://drive.google.com/file/d/1AyZLBPYkiB3HgUZYL9cWZx3IFu2OzcTI/view?usp=sharing>https://drive.google.com/file/d/1AyZLBPYkiB3HgUZYL9cWZx3IFu2OzcTI/view?usp=sharing</denchmark-link>
 
 		",2.0,girving,2018-05-17T20:35:57Z,"
 		<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>
 : Did you introduce this restriction?  Can't tell if <denchmark-link:https://github.com/tensorflow/tensorflow/commit/53f68459f18fd9c707183511e1e58d03e2f367db>53f6845</denchmark-link>
  is yours.
 		",3.0,girving,2018-05-18T17:23:32Z,"
 		<denchmark-link:https://github.com/ekelsen>@ekelsen</denchmark-link>
  might have more insight into that particular commit?
 		",88ba64b668044c5cb776fa27c8429e2c0e64e665,Geoffrey Irving,2018-08-09 09:44:59-07:00,MODIFY,0,tensorflow\core\ops\array_ops.cc,tensorflow\core\ops\array_ops.cc,0.0,"634,636,637,638,639,640,641,642,643,644,645,646,652,653,654,655,657,660,661,662,663,664,665,666","634,641,643,645,646,647,648,649,650,652,653,654,655,656,657,659,660,661,662,663,664,665",MODIFY,2.0,tensorflow\python\kernel_tests\split_op_test.py,tensorflow\python\kernel_tests\split_op_test.py,4.0,girving,2018-06-02T13:59:27Z,"
 		Nagging Assignee <denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",5.0,girving,2018-06-02T20:04:43Z,"
 		Bump for <denchmark-link:https://github.com/ekelsen>@ekelsen</denchmark-link>
  - do you have any insight here, or is someone else a better bet?
 		",6.0,girving,2018-06-17T18:41:09Z,"
 		Nagging Assignee <denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",1.0,"178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195",,testDegenerateVariable,self,178,195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"359,360,361,362,363,364,365,366,367",,testVariableShapeFunction,self,359,367,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,girving,2018-07-03T01:02:01Z,"
 		Nagging Assignee <denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
 : It has been 29 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,girving,2018-07-17T19:26:50Z,"
 		Nagging Assignee <denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
 : It has been 44 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,girving,2018-07-17T20:36:13Z,"
 		Bumping <denchmark-link:https://github.com/ekelsen>@ekelsen</denchmark-link>
  again, who may be back?
 		",10.0,girving,2018-08-08T18:57:21Z,"
 		Nagging Assignee <denchmark-link:https://github.com/girving>@girving</denchmark-link>
 : It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",11.0,girving,2018-08-08T20:21:01Z,"
 		I sent a PR fixing this two weeks ago, so nagged me is unnecessary.  Hopefully the PR will be reviewed at some point.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19496,arogozhnikov,2018-05-23T11:18:27Z,2018-05-30T18:17:18Z,Segfault with rpc ops in eager mode,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 TensorFlow installed from (source or binary): binary, pip
 TensorFlow version (use command below): 1.8
 Python version:  3.6
 Bazel version (if compiling from source): NA
 GCC/Compiler version (if compiling from source): NA
 CUDA/cuDNN version: NA
 GPU model and memory: NA
 Exact command to reproduce: see below
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Rpc ops (rpc, try_rpc)  are not working in eager mode. See minimal examples below.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Non-eager version works
 import tensorflow as tf
 from tensorflow.contrib.rpc.python.ops.gen_rpc_op import try_rpc
 
 with tf.Graph().as_default():
     response = try_rpc('localhost:80', '/Test', 'some simple message', protocol='grpc')
     session = tf.InteractiveSession()
     print( session.run([response], feed_dict={}) )
 Eager version does not (results in segfault):
 import tensorflow as tf
 from tensorflow.contrib.rpc.python.ops.gen_rpc_op import try_rpc
 tf.enable_eager_execution()
 response = try_rpc('localhost:80', '/Test', 'some simple message', protocol='grpc')
 	",1.0,arogozhnikov,2018-05-25T07:18:33Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  : Mind taking a look?
 		",2.0,arogozhnikov,2018-05-29T19:45:48Z,"
 		will do
 		",3.0,arogozhnikov,2018-05-30T18:17:18Z,"
 		This was just fixed internally and will be pushed to master in the next sync.
 		",2c75dbfd2d37a3c06d34cc4b12682a63a75503f7,Jiri Simsa,2018-05-29 18:12:55-07:00,MODIFY,1,tensorflow\core\util\rpc\call_container.h,tensorflow\core\util\rpc\call_container.h,1.0,"105,106,107,115,116,117,132,133,134,135,136,137,138,139","105,113,128,129,130,131,132",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::CallContainer<Call>::CallContainer,"ctx,num_calls,fail_fast,try_rpc,done,create_call_fn,start_call_fn",98,158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19499,LionelCons,2018-05-23T13:47:21Z,2018-05-30T21:59:33Z,tf.data.Dataset iterators are not cleaned when the loop ends with a break,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS7
 TensorFlow installed from (source or binary):binary
 TensorFlow version (use command below):1.8.0
 Python version: 2.7.5
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 Exact command to reproduce:
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 tf.data.Dataset iterators are not cleaned when the loop ends with a break.
 The code below opens one file per epoch. This eventually hits a system limit (maximum number of open files).
 Replacing the break by a continue works better since the files are closed. However, this is inefficient if we only need to iterate over a small fraction of the data
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>dataset = tf.data.TextLineDataset(fp)
 ...
 for epoch in xrange(epochs):
     ...
     batches = 0
     for (x, y) in dataset:
         batches += 1
         if batches > MAX_BATCHES:
             break
         ...
 </denchmark-code>
 
 	",1.0,LionelCons,2018-05-23T19:57:22Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  It looks like you've done something related to Eager resource garbage collection in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/309e340619ab922f1ecb51b8f142283e09bda07d>309e340</denchmark-link>
 , and this is still used in the current :
 
 
 
 tensorflow/tensorflow/python/data/ops/iterator_ops.py
 
 
         Lines 480 to 482
       in
       2c9d129
 
 
 
 
 
 
  # Delete the resource when this object is deleted 
 
 
 
  self._resource_deleter = resource_variable_ops.EagerResourceDeleter( 
 
 
 
  handle=self._resource, handle_device=""/device:CPU:0"") 
 
 
 
 
 
 Can you please take a look and see if there's a reference leak here?
 		",2.0,LionelCons,2018-05-24T00:13:30Z,"
 		There is a reference leak, but I don't think it's in Python. DestroyResourceOp gets run and Unrefs the resource, but it looks like IteratorHandleOp is <denchmark-link:https://github.com/tensorflow/tensorflow/blob/f8b74d642420dcf2f5cab763b41884a05777ea45/tensorflow/core/kernels/data/iterator_ops.cc#L510>keeping a reference to the resource in its OpKernel</denchmark-link>
 . AFAIK kernels are never deleted when executing eagerly, they just sit around in the kernel cache.
 I've verified that removing the reference from IteratorHandleOp fixes the ""files not closed"" issue (they get closed when DestroyResourceOp runs). I can think of horrible hacks to get this to happen only when executing eagerly, but maybe we should discuss tomorrow.
 		",3.0,LionelCons,2018-05-24T14:53:51Z,"
 		Ugh, yes, whatever we do to that kernel implementation, the current version will still leak the  object and related guff for each iterator. As a strawman, we could solve it with (i) a new version of  that creates the handle doesn't retain a resource, and (ii) some API for creating-and-running-but-not-caching a kernel in eager mode. (CCing <denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  since kernel lifetimes in eager is something we've talked about in the past.)
 <denchmark-link:https://github.com/LionelCons>@LionelCons</denchmark-link>
  In the meantime, here's a workaround that should alleviate the file handle leak:
 dataset = tf.data.TextLineDataset(fp)
 # ...
 for epoch in xrange(epochs):
   # ...
   for (x, y) in dataset.take(MAX_BATCHES):
     # ...
 		",70674b950ab48f913ed1c99e48c4162287595d46,Allen Lavoie,2018-05-29 10:36:51-07:00,ADD,0,None,tensorflow\core\api_def\base_api\api_def_AnonymousIterator.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_AnonymousIterator.pbtxt,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\core\kernels\data\iterator_ops.cc,tensorflow\core\kernels\data\iterator_ops.cc,1.0,"592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629",,MODIFY,0.0,tensorflow\core\ops\dataset_ops.cc,tensorflow\core\ops\dataset_ops.cc,0.0,"567,568,569,570,571,572",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::AnonymousIteratorHandleOp::Compute,context,592,629,,,,,MODIFY,1.0,tensorflow\python\data\kernel_tests\reader_dataset_ops_test.py,tensorflow\python\data\kernel_tests\reader_dataset_ops_test.py,1.0,"173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199",,testIteratorResourceCleanup,self,173,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\data\ops\iterator_ops.py,tensorflow\python\data\ops\iterator_ops.py,1.0,474,"474,475,476",__init__,"self,dataset",439,481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"586,587,588,589,590",,tensorflow::AnonymousIteratorHandleOp::AnonymousIteratorHandleOp,context,586,590,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
19551,ed-alertedh,2018-05-25T05:39:45Z,2018-08-17T19:05:29Z,Cannot use AdagradOptimizer with MirroredStrategy,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
 TensorFlow installed from (source or binary): Binary
 TensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0
 Python version: 3.6.3 64-bit
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 9.0 / 6.0
 GPU model and memory: Nvidia GTX 1080 8 GB
 Exact command to reproduce: python train_model.py
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 It took me a while to work out what was going on, but it seems that  tf.train.AdagradOptimizer has some specific implementation detail that causes an error when used with MirroredStrategy. I did a spot check with GradientDescentOptimizer and RMSPropOptimizer and they both appear to work in my environment. I'm happy to use a different optimizer as a workaround but I thought at the very least this might save others some time hunting down the cause of the error!
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 This is almost exactly copied from the example at <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute</denchmark-link>
  (except for the choice of optimizer)
 <denchmark-code>import tensorflow as tf
 
 def model_fn(features, labels, mode):
     layer = tf.layers.Dense(1)
     logits = layer(features)
 
     if mode == tf.estimator.ModeKeys.PREDICT:
         predictions = {""logits"": logits}
         return tf.estimator.EstimatorSpec(mode, predictions=predictions)
 
     loss = tf.losses.mean_squared_error(
             labels=labels, predictions=tf.reshape(logits, []))
 
     if mode == tf.estimator.ModeKeys.EVAL:
         return tf.estimator.EstimatorSpec(mode, loss=loss)
 
     if mode == tf.estimator.ModeKeys.TRAIN:
         train_op = tf.train.AdagradOptimizer(0.2).minimize(loss)
         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)
 
 def input_fn():
     features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)
     labels = tf.data.Dataset.from_tensors(1.).repeat(100)
     return tf.data.Dataset.zip((features, labels))
 
 distribution = tf.contrib.distribute.MirroredStrategy()
 config = tf.estimator.RunConfig(train_distribute=distribution)
 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)
 classifier.train(input_fn=input_fn)
 </denchmark-code>
 
 Log output:
 <denchmark-code>2018-05-25 15:30:12.300908: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2018-05-25 15:30:14.231174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 0 with properties: 
 name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898
 pciBusID: 0000:03:00.0
 totalMemory: 7.93GiB freeMemory: 7.81GiB
 2018-05-25 15:30:14.557081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1356] Found device 1 with properties: 
 name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.898
 pciBusID: 0000:81:00.0
 totalMemory: 7.93GiB freeMemory: 7.81GiB
 2018-05-25 15:30:14.557174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1
 2018-05-25 15:30:15.198082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
 2018-05-25 15:30:15.198134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 
 2018-05-25 15:30:15.198142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N 
 2018-05-25 15:30:15.198145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N 
 2018-05-25 15:30:15.198488: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
 2018-05-25 15:30:15.324510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)
 WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqglycjzk
 2018-05-25 15:30:15.455314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1
 2018-05-25 15:30:15.455414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
 2018-05-25 15:30:15.455423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 
 2018-05-25 15:30:15.455427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N N 
 2018-05-25 15:30:15.455431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   N N 
 2018-05-25 15:30:15.455615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 7542 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
 2018-05-25 15:30:15.455720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 7543 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:81:00.0, compute capability: 6.1)
 Traceback (most recent call last):
   File ""train_model.py"", line 62, in <module>
     classifier.train(input_fn=input_fn)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
     loss = self._train_model(input_fn, hooks, saving_listeners)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model
     return self._train_model_distributed(input_fn, hooks, saving_listeners)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 884, in _train_model_distributed
     self.config)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 756, in call_for_each_tower
     return self._call_for_each_tower(fn, *args, **kwargs)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 254, in _call_for_each_tower
     coord.join(threads)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
     six.reraise(*self._exc_info_to_raise)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/six.py"", line 693, in reraise
     raise value
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
     yield
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 248, in _call_for_each_tower
     self, *merge_args, **merge_kwargs)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 671, in _distributed_apply
     self._create_slots(var_list)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py"", line 66, in _create_slots
     with ops.colocate_with(v):
   File ""/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4186, in _colocate_with_for_gradient
     with self.colocate_with(op, ignore_existing):
   File ""/home/aeiq/.pyenv/versions/3.6.3/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4239, in colocate_with
     op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1262, in internal_convert_to_tensor_or_indexed_slices
     value, dtype=dtype, name=name, as_ref=as_ref)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   File ""/home/aeiq/.local/share/virtualenvs/echoiq-308-HW9bcZ4A/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 243, in _tensor_conversion
     assert not as_ref
 AssertionError
 </denchmark-code>
 
 	",1.0,ed-alertedh,2018-06-05T12:40:41Z,"
 		I got the same problem when using the MovingAverageOptimizer
 def model_fn(features, labels, mode):
     layer = tf.layers.Dense(1)
     logits = layer(features)
 
     if mode == tf.estimator.ModeKeys.PREDICT:
         predictions = {""logits"": logits}
         return tf.estimator.EstimatorSpec(mode, predictions=predictions)
 
     loss = tf.losses.mean_squared_error(
             labels=labels, predictions=tf.reshape(logits, []))
 
     if mode == tf.estimator.ModeKeys.EVAL:
         return tf.estimator.EstimatorSpec(mode, loss=loss)
 
     if mode == tf.estimator.ModeKeys.TRAIN:
         optimizer = tf.train.GradientDescentOptimizer(0.02)
         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)
         train_op = optimizer.minimize(loss)
         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)
 
 def input_fn():
     features = tf.data.Dataset.from_tensors([[1.]]).repeat(100)
     labels = tf.data.Dataset.from_tensors(1.).repeat(100)
     return tf.data.Dataset.zip((features, labels))
 
 distribution = tf.contrib.distribute.MirroredStrategy(num_gpus=2)
 config = tf.estimator.RunConfig(train_distribute=distribution)
 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)
 classifier.train(input_fn=input_fn)
 The error with some context
 <denchmark-code>AssertionError                            Traceback (most recent call last)
 <ipython-input-114-62e57e2880d4> in <module>()
      27 config = tf.estimator.RunConfig(train_distribute=distribution)
      28 classifier = tf.estimator.Estimator(model_fn=model_fn, config=config)
 ---> 29 classifier.train(input_fn=input_fn)
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
     361 
     362     saving_listeners = _check_listeners_type(saving_listeners)
 --> 363     loss = self._train_model(input_fn, hooks, saving_listeners)
     364     logging.info('Loss for final step: %s.', loss)
     365     return self
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
     839   def _train_model(self, input_fn, hooks, saving_listeners):
     840     if self._distribution:
 --> 841       return self._train_model_distributed(input_fn, hooks, saving_listeners)
     842     else:
     843       return self._train_model_default(input_fn, hooks, saving_listeners)
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _train_model_distributed(self, input_fn, hooks, saving_listeners)
     882             labels,  # although this will be None it seems
     883             model_fn_lib.ModeKeys.TRAIN,
 --> 884             self.config)
     885 
     886         # TODO(anjalisridhar): Figure out how to resolve the folowing scaffold
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/distribute.py in call_for_each_tower(self, fn, *args, **kwargs)
     754     """"""
     755     _require_cross_tower_context(self)
 --> 756     return self._call_for_each_tower(fn, *args, **kwargs)
     757 
     758   def _call_for_each_tower(self, fn, *args, **kwargs):
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in _call_for_each_tower(self, fn, *args, **kwargs)
     252       for t in threads:
     253         t.should_run.set()
 --> 254       coord.join(threads)
     255 
     256     return values.regroup({t.device: t.main_result for t in threads})
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in join(self, threads, stop_grace_period_secs, ignore_live_threads)
     387       self._registered_threads = set()
     388       if self._exc_info_to_raise:
 --> 389         six.reraise(*self._exc_info_to_raise)
     390       elif stragglers:
     391         if ignore_live_threads:
 
 /usr/local/lib/python3.5/dist-packages/six.py in reraise(tp, value, tb)
     691             if value.__traceback__ is not tb:
     692                 raise value.with_traceback(tb)
 --> 693             raise value
     694         finally:
     695             value = None
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py in stop_on_exception(self)
     295     """"""
     296     try:
 --> 297       yield
     298     except:  # pylint: disable=bare-except
     299       self.request_stop(ex=sys.exc_info())
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py in run(self)
     463                 self._captured_var_scope, reuse=self.tower_id > 0), \
     464             variable_scope.variable_creator_scope(self.variable_creator_fn):
 --> 465           self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
     466           self.done = True
     467       finally:
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
     829 
     830     logging.info('Calling model_fn.')
 --> 831     model_fn_results = self._model_fn(features=features, **kwargs)
     832     logging.info('Done calling model_fn.')
     833 
 
 <ipython-input-114-62e57e2880d4> in model_fn(features, labels, mode)
      16         optimizer = tf.train.GradientDescentOptimizer(0.02)
      17         optimizer = tf.contrib.opt.MovingAverageOptimizer(optimizer, 0.9)
 ---> 18         train_op = optimizer.minimize(loss)
      19         return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)
      20 
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
     422 
     423     return self.apply_gradients(grads_and_vars, global_step=global_step,
 --> 424                                 name=name)
     425 
     426   def compute_gradients(self, loss, var_list=None,
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/opt/python/training/moving_average_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
      97     if self._sequential_update:
      98       with ops.control_dependencies([train_op]):
 ---> 99         ma_op = self._ema.apply(var_list)
     100     else:
     101       ma_op = self._ema.apply(var_list)
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in apply(self, var_list)
     423         zero_debias = self._averages[var] in zero_debias_true
     424         updates.append(assign_moving_average(
 --> 425             self._averages[var], var, decay, zero_debias=zero_debias))
     426       return control_flow_ops.group(*updates, name=scope)
     427 
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py in assign_moving_average(variable, value, decay, zero_debias, name)
      82   with ops.name_scope(name, ""AssignMovingAvg"",
      83                       [variable, value, decay]) as scope:
 ---> 84     with ops.colocate_with(variable):
      85       decay = ops.convert_to_tensor(1.0 - decay, name=""decay"")
      86       if decay.dtype != variable.dtype.base_dtype:
 
 /usr/lib/python3.5/contextlib.py in __enter__(self)
      57     def __enter__(self):
      58         try:
 ---> 59             return next(self.gen)
      60         except StopIteration:
      61             raise RuntimeError(""generator didn't yield"") from None
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in _colocate_with_for_gradient(self, op, gradient_uid, ignore_existing)
    4184   def _colocate_with_for_gradient(self, op, gradient_uid,
    4185                                   ignore_existing=False):
 -> 4186     with self.colocate_with(op, ignore_existing):
    4187       if gradient_uid is not None and self._control_flow_context is not None:
    4188         try:
 
 /usr/lib/python3.5/contextlib.py in __enter__(self)
      57     def __enter__(self):
      58         try:
 ---> 59             return next(self.gen)
      60         except StopIteration:
      61             raise RuntimeError(""generator didn't yield"") from None
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in colocate_with(self, op, ignore_existing)
    4237     if op is not None and not isinstance(op, Operation):
    4238       # We always want to colocate with the reference op.
 -> 4239       op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
    4240 
    4241     # By default, colocate_with resets the device function stack,
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_indexed_slices(value, dtype, name, as_ref)
    1260   else:
    1261     return internal_convert_to_tensor(
 -> 1262         value, dtype=dtype, name=name, as_ref=as_ref)
    1263 
    1264 
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
    1102 
    1103     if ret is None:
 -> 1104       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    1105 
    1106     if ret is NotImplemented:
 
 /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py in _tensor_conversion(var, dtype, name, as_ref)
     241   # Try to avoid assignments to and other mutations of MirroredVariable
     242   # state except through a DistributionStrategy.update() call.
 --> 243   assert not as_ref
     244   return ops.internal_convert_to_tensor(
     245       var.get(), dtype=dtype, name=name, as_ref=as_ref)
 
 AssertionError: 
 
 </denchmark-code>
 
 		",2.0,ed-alertedh,2018-06-19T19:10:13Z,"
 		Thanks for bringing this to our attention. We will look into why AdagadOptimizer and MovingAverageOptimizer don't work.
 		",3.0,ed-alertedh,2018-07-30T12:07:57Z,"
 		Hi All,
 I'm getting this same Error with Adagrad (the default for pre-canned tf.estimarot.DNNClassifier)
 Here's my definitions:
 <denchmark-code># Setup MirroredStrategy
 distribution = tf.contrib.distribute.MirroredStrategy()
 run_config = tf.estimator.RunConfig(train_distribute=distribution)
 
 # Define Specs                                                                                                                                                                               
 train_spec_dnn = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))
 eval_spec_dnn = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )
 
 
 DNNClassifier = tf.estimator.DNNClassifier(
     feature_columns = [tf.feature_column.numeric_column(key='feats', dtype=tf.float64, shape=(nDims,))],                                                                    
     hidden_units = [256, 256, 256, 256],                                                                                                                                 
     n_classes = 200,
     model_dir = '/tmp/tf',
     config = run_config)
 
 </denchmark-code>
 
 Below is the ERROR message:
 <denchmark-code>2018-07-30 11:57:19.736726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3
 2018-07-30 11:57:19.736801: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
 2018-07-30 11:57:19.736818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 
 2018-07-30 11:57:19.736826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y 
 2018-07-30 11:57:19.736833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y 
 2018-07-30 11:57:19.736840: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y 
 2018-07-30 11:57:19.736846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N 
 2018-07-30 11:57:19.737320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)
 2018-07-30 11:57:19.737953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)
 2018-07-30 11:57:19.738083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)
 2018-07-30 11:57:19.738209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)
 WARNING:tensorflow:Partitioned variables are disabled when using DistributionStrategy.
 Traceback (most recent call last):
   File ""train_and_eval.py"", line 137, in <module>
     tf.estimator.train_and_evaluate(DNNClassifier, train_spec_dnn, eval_spec_dnn)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 439, in train_and_evaluate
     executor.run()
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 518, in run
     self.run_local()
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 650, in run_local
     hooks=train_hooks)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 363, in train
     loss = self._train_model(input_fn, hooks, saving_listeners)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 841, in _train_model
     return self._train_model_distributed(input_fn, hooks, saving_listeners)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 884, in _train_model_distributed
     self.config)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 756, in call_for_each_tower
     return self._call_for_each_tower(fn, *args, **kwargs)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 254, in _call_for_each_tower
     coord.join(threads)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
     six.reraise(*self._exc_info_to_raise)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/six.py"", line 693, in reraise
     raise value
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
     yield
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 248, in _call_for_each_tower
     self, *merge_args, **merge_kwargs)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 671, in _distributed_apply
     self._create_slots(var_list)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/adagrad.py"", line 66, in _create_slots
     with ops.colocate_with(v):
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4186, in _colocate_with_for_gradient
     with self.colocate_with(op, ignore_existing):
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4239, in colocate_with
     op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1262, in internal_convert_to_tensor_or_indexed_slices
     value, dtype=dtype, name=name, as_ref=as_ref)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1104, in internal_convert_to_tensor
     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   File ""/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 243, in _tensor_conversion
     assert not as_ref
 AssertionError
 </denchmark-code>
 
 		",d0b51d7d9d50dd73c46ba2f2daaa9d26cc0666d0,Anjali Sridhar,2018-08-17 12:03:13-07:00,MODIFY,0,tensorflow\python\ops\variable_scope.py,tensorflow\python\ops\variable_scope.py,0.0,"45,855,856,857,858,859,860,861,862,863,864,865","840,841,842,857,858",MODIFY,3.0,tensorflow\python\training\adagrad.py,tensorflow\python\training\adagrad.py,4.0,ed-alertedh,2018-08-15T02:56:29Z,"
 		Nagging Assignee <denchmark-link:https://github.com/anj-s>@anj-s</denchmark-link>
 : It has been 15 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",5.0,ed-alertedh,2018-12-29T06:54:37Z,"
 		<denchmark-code>optimizer1 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.tnet_lr)
 optimizer1 = tf.contrib.estimator.TowerOptimizer(optimizer1)
 optimizer2 = tf.train. GradientDescentOptimizer(learning_rate=FLAGS.mnet_lr)
 optimizer2 = tf.contrib.estimator.TowerOptimizer(optimizer2)
 tnet_variables = get_model_variables('tnet')
 mnet_variables = get_model_variables('mnet')
 
 train_op1 = slim.learning.create_train_op(loss,
                                           optimizer1,
                                           variables_to_train=tnet_variables,
                                           summarize_gradients=True)
 train_op2 = slim.learning.create_train_op(loss,
                                           optimizer2,
                                           variables_to_train=mnet_variables,
                                           summarize_gradients=True)
 train_op = tf.group(train_op1, train_op2)
 </denchmark-code>
 
 Above two GradientDescentOptimizers do not work either.
 		",,,,,1.0,"73,74,75,76,77,78","73,74,75,76,77,78,79,80",_create_slots,"self,var_list",71,80,MODIFY,1.0,tensorflow\python\training\adagrad_test.py,tensorflow\python\training\adagrad_test.py,1.0,"305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"83,84,85,86,87,88",83,_init_constant_op.init,,83,88,1.0,"82,83,84,85,86,87,88,89","82,83",_init_constant_op,"self,v,dtype",82,89,,,,,,,,,,,,,,,,,,,,,,,,,,testDynamicShapeVariableWithCallableInit,self,305,336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
20516,sleighsoft,2018-07-03T12:25:05Z,2018-07-04T00:45:38Z,Cannot restore variables with Checkpoint because keys do not align,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tf-nightly==1.10.0.dev20180609
 Python version: 3.6.5
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: -
 GPU model and memory: -
 Exact command to reproduce:
 
 I get the error that is thrown here:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633>https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/util.py#L633</denchmark-link>
 
 I cannot provide code to reproduce it. But basically what happens is I have a class that inherits from Checkpointable. It assigns all variables to itself to make them checkpointable. It also assigns an optimizer to itself. I then save the model and restore it. When calling .assert_consumed() on the load_status object of restore(dir, session) it throws an error because some key does not match. The variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from enumerate(self._checkpoint.object_graph_proto.nodes).
 This describes it as good as I can. Sorry for not being able to share the code. I tried to reproduce it but so far I cannot. I believe it is a bug, because I call ckpt.save() and immediately after it ckpt.restore() and I get the exception.
 Output of self._checkpoint.object_by_proto_id.keys() at line 631.
 You can see that some keys are missing (idk why) but theses are the ones I need to restore.
 <denchmark-code>[0, 1, 2, 3, 4, 5, 6, 7, 14, 19, 10, 17, 22, 11, 18, 23, 12, 13]
 </denchmark-code>
 
 Output of util._serialize_object_graph(self.checkpoint, None) after restore before assert_consumed
 nodes {
   children {
     node_id: 1
     local_name: ""model""
   }
   children {
     node_id: 2
     local_name: ""save_counter""
   }
 }
 nodes {
   children {
     node_id: 3
     local_name: ""_global_step_pretrain""
   }
   children {
     node_id: 4
     local_name: ""embedding""
   }
   children {
     node_id: 5
     local_name: ""cell""
   }
   children {
     node_id: 6
     local_name: ""dense""
   }
   children {
     node_id: 7
     local_name: ""_optimizer""
   }
   children {
     local_name: ""_checkpoint""
   }
 }
 nodes {
   attributes {
     name: ""VARIABLE_VALUE""
     full_name: ""initialize_or_restore/save_counter""
     checkpoint_key: ""save_counter/.ATTRIBUTES/VARIABLE_VALUE""
   }
 }
 nodes {
   attributes {
     name: ""VARIABLE_VALUE""
     full_name: ""ConditionedLSTMGenerator2/CONDITIONED_LSTM/pretrain/global_step""
     checkpoint_key: ""model/_global_step_pretrain/.ATTRIBUTES/VARIABLE_VALUE""
   }
 }
 nodes {
   attributes {
     name: ""VARIABLE_VALUE""
     full_name: ""ConditionedLSTMGenerator2/CONDITIONED_LSTM/embedding""
     checkpoint_key: ""model/embedding/.ATTRIBUTES/VARIABLE_VALUE""
   }
 }
 nodes {
   children {
     node_id: 8
     local_name: ""kernel""
   }
   children {
     node_id: 9
     local_name: ""bias""
   }
   attributes {
     name: ""OBJECT_CONFIG_JSON""
     checkpoint_key: ""model/cell/.ATTRIBUTES/OBJECT_CONFIG_JSON""
   }
 }
 nodes {
   children {
     node_id: 10
     local_name: ""kernel""
   }
   children {
     node_id: 11
     local_name: ""bias""
   }
   attributes {
     name: ""OBJECT_CONFIG_JSON""
     checkpoint_key: ""model/dense/.ATTRIBUTES/OBJECT_CONFIG_JSON""
   }
 }
 nodes {
   children {
     node_id: 12
     local_name: ""beta1_power""
   }
   children {
     node_id: 13
     local_name: ""beta2_power""
   }
   slot_variables {
     original_variable_node_id: 4
     slot_name: ""m""
     slot_variable_node_id: 14
   }
   slot_variables {
     original_variable_node_id: 8
     slot_name: ""m""
     slot_variable_node_id: 15
   }
   slot_variables {
     original_variable_node_id: 9
     slot_name: ""m""
     slot_variable_node_id: 16
   }
   slot_variables {
     original_variable_node_id: 10
     slot_name: ""m""
     slot_variable_node_id: 17
   }
   slot_variables {
     original_variable_node_id: 11
     slot_name: ""m""
     slot_variable_node_id: 18
   }
   slot_variables {
     original_variable_node_id: 4
     slot_name: ""v""
     slot_variable_node_id: 19
   }
   slot_variables {
     original_variable_node_id: 8
     slot_name: ""v""
     slot_variable_node_id: 20
   }
   slot_variables {
     original_variable_node_id: 9
     slot_name: ""v""
     slot_variable_node_id: 21
   }
   slot_variables {
     original_variable_node_id: 10
     slot_name: ""v""
     slot_variable_node_id: 22
   }
   slot_variables {
     original_variable_node_id: 11
     slot_name: ""v""
     slot_variable_node_id: 23
   }
 }
 	",1.0,sleighsoft,2018-07-03T12:43:33Z,"
 		I assume this is something for <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  ?
 		",2.0,sleighsoft,2018-07-03T13:15:10Z,"
 		I found the culprit. Double assigning a variable. Here BasicLSTMCell.
 import tensorflow as tf
 from tensorflow.python.training.checkpointable import base as checkpointable
 from tensorflow.python.training.checkpointable import util
 
 class Model(checkpointable.Checkpointable):
 
   def __init__(self):
     self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     # self.cell = tf.nn.rnn_cell.BasicLSTMCell(4)
     out = self.cell(tf.constant([[1.]]), self.cell.zero_state(1, tf.float32))
     self.optimizer = tf.train.AdamOptimizer()
     self.optimizer.minimize(tf.reduce_sum(out[0]))
     self.session = tf.Session()
     self.checkpoint = tf.train.Checkpoint(model=self)
 
   def init(self):
     print('Init')
     self.session.run(tf.global_variables_initializer())
 
   def save(self):
     print('Save')
     self.checkpoint.save('./tmp/', self.session)
 
   def restore(self):
     print('Restore')
     latest = tf.train.latest_checkpoint('./tmp/')
     load_status = self.checkpoint.restore(latest)
     print(util._serialize_object_graph(self.checkpoint, None))
     load_status.assert_consumed().run_restore_ops(self.session)
 
   def print(self):
     print(self.session.run(self.cell._kernel))
 
 
 m = Model()
 m.init()
 m.print()
 m.save()
 m.restore()
 m.print()
 		",3.0,sleighsoft,2018-07-03T13:35:35Z,"
 		It works if I double assign a  though. Seems like this <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/base.py#L593>https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/training/checkpointable/base.py#L593</denchmark-link>
  does not override/clear all previous state.
 Edit: Also does not work for tf.layers.Dense and probably others.
 		",f46627f9ed9cd41b5a1ad9cebbdd4c240846c4e0,Allen Lavoie,2018-07-03 11:32:16-07:00,MODIFY,1,tensorflow\python\training\checkpointable\base.py,tensorflow\python\training\checkpointable\base.py,1.0,636,634,MODIFY,1.0,tensorflow\python\training\checkpointable\tracking_test.py,tensorflow\python\training\checkpointable\tracking_test.py,4.0,sleighsoft,2018-07-03T16:27:24Z,"
 		Huh, good point, looks like the by-name lookup isn't being updated when the dependency is replaced. Thank you for the report!
 		",5.0,sleighsoft,2018-07-03T17:55:41Z,"
 		Happy to help
 		",,,,,1.0,"47,48,49",,testMultipleAssignment,self,36,49,MODIFY,1.0,tensorflow\python\training\checkpointable\util_test.py,tensorflow\python\training\checkpointable\util_test.py,1.0,105,105,,,,,,,,_track_checkpointable,"self,checkpointable,name,overwrite",585,637,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testAddVariable,self,79,136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
21277,nfergu,2018-07-31T13:23:01Z,2018-08-09T17:59:36Z,Using TensorFlow's Datasets API causes process to hang in session destructor,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 
 Yes
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 
 MacOS High Sierra (10.13.1), though we've also seen this happen on Linux as well, we believe.
 
 TensorFlow installed from (source or binary):
 
 Source (but happens with the binary version as well)
 
 TensorFlow version (use command below):
 
 v1.8.0-0-g93bc2e2072 1.8.0
 
 Python version:
 
 Python 3.6.1 (v3.6.1:69c0db5050, Mar 21 2017, 01:21:04)
 
 Bazel version (if compiling from source):
 
 0.10.1
 
 GCC/Compiler version (if compiling from source):
 
 Apple LLVM version 9.0.0 (clang-900.0.39.2)
 
 CUDA/cuDNN version:
 
 N/A
 
 GPU model and memory:
 
 N/A
 
 Exact command to reproduce:
 
 Unfortunately the issue isn't that easy to reproduce without running our application (I haven't managed to produce a smaller test case).
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Summary:
 We are using TensorFlow's Datasets API. More specifically, we're using tf.data.Dataset.from_generator to create a dataset based on a generator function.
 When Python comes to garbage collect our tf.Session object its destructor makes a call into TensorFlow to delete the session (tf_session.TF_DeleteSession). This call hangs because it's trying to execute a tf.py_func function, but cannot acquire Python's global interpreter lock. The function its trying to execute appears to be the ""finalize"" function from our dataset.
 This looks to me like a bug in TensorFlow, as (my understanding is) that we shouldn't be able to write code that causes this to happen. Although it's clearly a consequence of our specific use of TensorFlow I can't see that we're doing anything in our application that we shouldn't be.
 More Details:
 When our tf.Session object is garbage collected in Python, its destructor (__del__ method) hangs indefinitely. The problem appears to be this call in BaseSession:
 <denchmark-code>tf_session.TF_DeleteSession(self._session)
 </denchmark-code>
 
 Running lldb shows the following stack trace:
 <denchmark-code>* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
   * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x000000011279a63b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283
     frame #4: 0x0000000112796eb7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
     frame #5: 0x0000000112797621 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
     frame #6: 0x00000001090810e3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67
     frame #7: 0x0000000109d4d809 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649
     frame #8: 0x0000000109cffa21 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97
     frame #9: 0x0000000109cffb8e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #10: 0x0000000109cfd669 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105
     frame #11: 0x0000000109cfd6de _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43
     frame #13: 0x0000000109d0a579 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169
     frame #14: 0x0000000109d0a5fe _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14
     frame #15: 0x000000011226db4d libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301
     frame #16: 0x000000011226dd50 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192
     frame #17: 0x0000000109d0c558 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104
     frame #18: 0x0000000109d0c71e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14
     frame #19: 0x00000001122670ff libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63
     frame #20: 0x0000000112267ffd libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205
     frame #21: 0x000000010b880b42 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546
     frame #22: 0x000000010b88108e _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14
     frame #23: 0x000000010935dfd3 _pywrap_tensorflow_internal.so`TF_DeleteSession + 931
     frame #24: 0x0000000109006e5a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122
     frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #26: 0x00000001008443e4 Python`call_function + 612
     frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
     frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125
     frame #40: 0x00000001008992a4 Python`Py_Main + 1812
     frame #41: 0x0000000100000dfe Python
     frame #42: 0x0000000100000c34 Python
 </denchmark-code>
 
 It appears that the session's destructor is waiting for an op to complete. The culprit seems to be PyFuncOp, which doesn't get past this line:
 <denchmark-code>py_threadstate = PyGILState_Ensure();
 </denchmark-code>
 
 So it looks like this op is trying to acquire the GIL but can't. My assumption is that this py_func is the ""finalize"" function for the dataset (from _GeneratorDataset).
 My assumption is that when Python calls tf_session.TF_DeleteSession(self._session) that the GIL should be released, and so PyFuncOp should then be able to acquire it again. Indeed, when I write an isolated test to try and reproduce this I don't see this problem, and the GIL is acquired successfully.
 Unfortunately, as I mention, I have been unsuccessful in writing an isolated test case to reproduce the problem. The problem only seems to happen when we use our application in a particular scenario, but I haven't been able to isolate exactly what it is about this scenario that causes the problem.
 	",1.0,nfergu,2018-07-31T14:52:46Z,"
 		Thanks for the report and for diving into the details... this definitely sounds like a bug, although my understanding is the same as yours: tf_session.TF_DeleteSession(self._session) should release the GIL because of this SWIG code block here:
 
 
 
 tensorflow/tensorflow/python/client/tf_session.i
 
 
         Lines 106 to 111
       in
       2826d12
 
 
 
 
 
 
  // Release the Python GIL for the duration of most methods. 
 
 
 
  %exception { 
 
 
 
    Py_BEGIN_ALLOW_THREADS; 
 
 
 
    $action 
 
 
 
    Py_END_ALLOW_THREADS; 
 
 
 
  } 
 
 
 
 
 
 In terms of a reproduction, would you be able to capture a dump of all thread stacks when the problem occurs?  One thing I've found useful in debugging this kind of problem is to set config=tf.ConfigProto(inter_op_parallelism=1, intra_op_parallelism=1) when creating the session. This makes the set of threads less unwieldy, and can sometimes tease out deadlock bugs that are less likely to happen with larger threadpools.
 		",2.0,nfergu,2018-07-31T15:25:24Z,"
 		Sure, I've included a backtrace of all native threads below. This is with inter_op_parallelism_threads = 1 and intra_op_parallelism_threads = 1.
 I've also included a thread dump from Python as well, in case it's interesting. It doesn't look to me like any of the Python threads should be holding the GIL either. The only interesting one looks like 0x00007000025b4000, which is performing an IO operation (consuming from a multiprocessing queue) that I believe should also have caused the GIL to be released.
 Native threads:
 <denchmark-code>(lldb) thread backtrace all
 * thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
   * frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x00000001127b665b libtensorflow_framework.so`nsync::nsync_mu_semaphore_p_with_deadline(nsync::nsync_semaphore_s_*, timespec) + 283
     frame #4: 0x00000001127b2ed7 libtensorflow_framework.so`nsync::nsync_cv_wait_with_deadline_generic(nsync::nsync_cv_s_*, void*, void (*)(void*), void (*)(void*), timespec, nsync::nsync_note_s_*) + 423
     frame #5: 0x00000001127b3641 libtensorflow_framework.so`nsync::nsync_cv_wait(nsync::nsync_cv_s_*, nsync::nsync_mu_s_*) + 49
     frame #6: 0x000000010909d4b3 _pywrap_tensorflow_internal.so`tensorflow::Notification::WaitForNotification() + 67
     frame #7: 0x0000000109d69869 _pywrap_tensorflow_internal.so`tensorflow::CapturedFunction::RunInstantiated(std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> > const&, std::__1::vector<tensorflow::Tensor, std::__1::allocator<tensorflow::Tensor> >*) + 649
     frame #8: 0x0000000109d1ba81 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 97
     frame #9: 0x0000000109d1bbee _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::GeneratorDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #10: 0x0000000109d196c9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 105
     frame #11: 0x0000000109d1973e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::FlatMapDatasetOp::Dataset::Iterator::~Iterator() + 14
     frame #12: 0x00000001019e98fd libc++.1.dylib`std::__1::__shared_weak_count::__release_shared() + 43
     frame #13: 0x0000000109d265d9 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 169
     frame #14: 0x0000000109d2665e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::IteratorResource::~IteratorResource() + 14
     frame #15: 0x0000000112289cad libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 301
     frame #16: 0x0000000112289eb0 libtensorflow_framework.so`tensorflow::ResourceMgr::DoDelete(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::type_index, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 192
     frame #17: 0x0000000109d285b8 _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 104
     frame #18: 0x0000000109d2877e _pywrap_tensorflow_internal.so`tensorflow::(anonymous namespace)::OneShotIteratorOp::~OneShotIteratorOp() + 14
     frame #19: 0x000000011228325f libtensorflow_framework.so`tensorflow::OpSegment::Item::~Item() + 63
     frame #20: 0x000000011228415d libtensorflow_framework.so`tensorflow::OpSegment::RemoveHold(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) + 205
     frame #21: 0x000000010b89cba2 _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 546
     frame #22: 0x000000010b89d0ee _pywrap_tensorflow_internal.so`tensorflow::DirectSession::~DirectSession() + 14
     frame #23: 0x000000010937a0b2 _pywrap_tensorflow_internal.so`TF_DeleteSession + 290
     frame #24: 0x000000010902319a _pywrap_tensorflow_internal.so`_wrap_TF_DeleteSession(_object*, _object*) + 122
     frame #25: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #26: 0x00000001008443e4 Python`call_function + 612
     frame #27: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #28: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #29: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #30: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
     frame #39: 0x0000000100880a1d Python`PyRun_SimpleStringFlags + 125
     frame #40: 0x00000001008992a4 Python`Py_Main + 1812
     frame #41: 0x0000000100000dfe Python
     frame #42: 0x0000000100000c34 Python
   thread #2
     frame #0: 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10
     frame #1: 0x000000010188c06a libsystem_pthread.dylib`_pthread_wqthread + 1035
     frame #2: 0x000000010188bc4d libsystem_pthread.dylib`start_wqthread + 13
   thread #3
     frame #0: 0x0000000000000000
   thread #4
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x0000000100896738 Python`PyThread_acquire_lock_timed + 312
     frame #3: 0x000000010089ca89 Python`acquire_timed + 137
     frame #4: 0x000000010089cbdd Python`lock_PyThread_acquire_lock + 61
     frame #5: 0x00000001007bb545 Python`_PyCFunction_FastCallDict + 245
     frame #6: 0x00000001008443e4 Python`call_function + 612
     frame #7: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #8: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #9: 0x0000000100843fab Python`fast_function + 219
     frame #10: 0x00000001008443cb Python`call_function + 587
     frame #11: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #12: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #13: 0x0000000100843fab Python`fast_function + 219
     frame #14: 0x00000001008443cb Python`call_function + 587
     frame #15: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #16: 0x000000010084412e Python`fast_function + 606
     frame #17: 0x00000001008443cb Python`call_function + 587
     frame #18: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #19: 0x000000010084412e Python`fast_function + 606
     frame #20: 0x00000001008443cb Python`call_function + 587
     frame #21: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #22: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #23: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #24: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #25: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #26: 0x000000010089c587 Python`t_bootstrap + 71
     frame #27: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #28: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #29: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #5
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #6
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #7
     frame #0: 0x0000000101857592 libsystem_kernel.dylib`read + 10
     frame #1: 0x0000000100895402 Python`_Py_read + 82
     frame #2: 0x000000010089f9b9 Python`os_read + 89
     frame #3: 0x00000001007bb688 Python`_PyCFunction_FastCallDict + 568
     frame #4: 0x00000001008443e4 Python`call_function + 612
     frame #5: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #6: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #7: 0x0000000100843fab Python`fast_function + 219
     frame #8: 0x00000001008443cb Python`call_function + 587
     frame #9: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #10: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #11: 0x0000000100843fab Python`fast_function + 219
     frame #12: 0x00000001008443cb Python`call_function + 587
     frame #13: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #14: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #15: 0x0000000100843fab Python`fast_function + 219
     frame #16: 0x00000001008443cb Python`call_function + 587
     frame #17: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #18: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #19: 0x0000000100843fab Python`fast_function + 219
     frame #20: 0x00000001008443cb Python`call_function + 587
     frame #21: 0x0000000100848aa0 Python`_PyEval_EvalFrameDefault + 17056
     frame #22: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #23: 0x0000000100843fab Python`fast_function + 219
     frame #24: 0x00000001008443cb Python`call_function + 587
     frame #25: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #26: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #27: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #28: 0x000000010079344a Python`function_call + 186
     frame #29: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #30: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #31: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #32: 0x0000000100843fab Python`fast_function + 219
     frame #33: 0x00000001008443cb Python`call_function + 587
     frame #34: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #35: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #36: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #37: 0x000000010079344a Python`function_call + 186
     frame #38: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #39: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #40: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #41: 0x0000000100843fab Python`fast_function + 219
     frame #42: 0x00000001008443cb Python`call_function + 587
     frame #43: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #44: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #45: 0x00000001008438cf Python`PyEval_EvalCodeEx + 95
     frame #46: 0x000000010079344a Python`function_call + 186
     frame #47: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #48: 0x0000000100848c31 Python`_PyEval_EvalFrameDefault + 17457
     frame #49: 0x00000001008437a0 Python`_PyEval_EvalCodeWithName + 2720
     frame #50: 0x0000000100843fab Python`fast_function + 219
     frame #51: 0x00000001008443cb Python`call_function + 587
     frame #52: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #53: 0x000000010084412e Python`fast_function + 606
     frame #54: 0x00000001008443cb Python`call_function + 587
     frame #55: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #56: 0x000000010084412e Python`fast_function + 606
     frame #57: 0x00000001008443cb Python`call_function + 587
     frame #58: 0x0000000100849d84 Python`_PyEval_EvalFrameDefault + 21892
     frame #59: 0x00000001008447cc Python`_PyFunction_FastCallDict + 828
     frame #60: 0x000000010075f984 Python`_PyObject_FastCallDict + 356
     frame #61: 0x000000010075faa0 Python`_PyObject_Call_Prepend + 208
     frame #62: 0x000000010075f5b3 Python`PyObject_Call + 99
     frame #63: 0x000000010089c587 Python`t_bootstrap + 71
     frame #64: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #65: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #66: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #8
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
   thread #9
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x000000011239b5f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x000000011239b26c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x000000011239a898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568
     frame #6: 0x000000011239a55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47
     frame #7: 0x00000001123c12f0 libtensorflow_framework.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) + 48
     frame #8: 0x000000010188c6c1 libsystem_pthread.dylib`_pthread_body + 340
     frame #9: 0x000000010188c56d libsystem_pthread.dylib`_pthread_start + 377
     frame #10: 0x000000010188bc5d libsystem_pthread.dylib`thread_start + 13
 </denchmark-code>
 
 Python threads:
 <denchmark-code>Thread 0x0000700002637000 (most recent call first):
 
 Thread 0x00007000025b4000 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py"", line 379 in _recv
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py"", line 407 in _recv_bytes
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/connection.py"", line 216 in recv_bytes
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/queues.py"", line 94 in get
 <REDACTED>
 
 
 Thread 0x0000700001fab000 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 299 in wait
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 551 in wait
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 1180 in run
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 884 in _bootstrap
 
 Thread 0x0000700001aa8000 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 299 in wait
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 551 in wait
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 1180 in run
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 884 in _bootstrap
 
 Thread 0x0000700000f19000 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 299 in wait
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/queue.py"", line 173 in get
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py"", line 360 in dispatch_events
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/watchdog/observers/api.py"", line 199 in run
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 884 in _bootstrap
 
 Thread 0x0000700000a16000 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/selectors.py"", line 577 in select
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py"", line 1389 in _run_once
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/asyncio/base_events.py"", line 421 in run_forever
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 864 in run
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py"", line 884 in _bootstrap
 
 Current thread 0x0000000101895340 (most recent call first):
   File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 707 in __del__
 </denchmark-code>
 
 		",3.0,nfergu,2018-07-31T15:54:35Z,"
 		Hmm, I think the Python thread in multiprocessing/connection.py (0x00007000025b4000) maps to native thread 7, which is in _Py_read() and that drops the GIL as well.
 As far as I can tell though, none of the native threads is blocked trying to acquire the GIL:
 
 Thread 1 is waiting on a tensorflow::Notification for the end of the finialize function.
 Thread 2 isn't TF-related.
 Thread 3 is ???
 Thread 4 is a Python thread that's waiting on a Python lock. I don't think this is TF-related.
 Threads 5, 6, 8, and 9 are idle TF/Eigen threadpool threads that are waiting for work. I'm a little surprised to see 4 of them rather than inter_op_parallelism + intra_op_parallelism = 2 threads, but they don't seem to be doing anything concerning.
 Thread 7 is (probably) Python thread 0x00007000025b4000, blocked on a multiprocessing queue.
 
 It would be interesting to know if the PyFunc op in finalization actually started and/or finished. One way to do this is to set the environment variable TF_CPP_MIN_VLOG_LEVEL=1 (which triggers very verbose logging, including each op invocation and completion). Could you try that and capture the part of the log that is produced after the tf.Session destructor begins?
 		",8cd2d6fe9389e93a4182ae9287f2f8325913fe6c,Derek Murray,2018-08-01 14:31:15-07:00,MODIFY,1,tensorflow\python\lib\core\py_func.cc,tensorflow\python\lib\core\py_func.cc,1.0,"503,504,505,506,507",,,,,,4.0,nfergu,2018-07-31T16:06:53Z,"
 		Yes, I was also confused by the fact that none of the native threads appear to be waiting to acquire the GIL. I haven't tried TF_CPP_MIN_VLOG_LEVEL=1 yet, but if I surround the GIL lock acquisition with a couple of log statements as follows:
 <denchmark-code>neil-mac-2:tensorflow nferguson$ git diff
 diff --git a/tensorflow/python/lib/core/py_func.cc b/tensorflow/python/lib/core/py_func.cc
 index 22317a348c..d52b5e4a00 100644
 --- a/tensorflow/python/lib/core/py_func.cc
 +++ b/tensorflow/python/lib/core/py_func.cc
 @@ -469,7 +469,9 @@ class PyFuncOp : public OpKernel {
      }
 
      PyGILState_STATE py_threadstate;
 +    LOG(ERROR) << ""About to ensure GIL"";
      py_threadstate = PyGILState_Ensure();
 +    LOG(ERROR) << ""Got GIL!"";
      bool log_on_error;
      Status s = DoCallPyFunc(&call, &log_on_error);
 </denchmark-code>
 
 I can observe it printing:
 About to ensure GIL
 but not:
 Got GIL!
 Which is why I mentioned that it was getting to PyGILState_Ensure(), but no further.
 Anyway, I will try with TF_CPP_MIN_VLOG_LEVEL=1.
 		",5.0,nfergu,2018-07-31T16:24:19Z,"
 		I've set TF_CPP_MIN_VLOG_LEVEL=1 and attached the resulting log from after the call to TF_DeleteSession. Note that this includes my added log line ""About to ensure GIL"" at the end (see my previous comment). And then the program hangs.
 <denchmark-code>2018-07-31 17:22:11.229247: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229282: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229298: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229311: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229329: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229338: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229356: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229375: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229396: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229413: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229426: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229452: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229462: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229474: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229495: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229503: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229515: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229534: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229543: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229917: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229930: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229961: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229970: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229990: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.229998: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230006: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230014: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230022: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230030: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230066: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230074: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230093: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230101: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230109: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230138: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230155: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230219: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230336: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230346: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230354: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230362: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230370: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230384: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.230401: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.231726: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.231838: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorDeallocation { allocator_name: ""cpu"" }
 2018-07-31 17:22:11.232569: I tensorflow/core/common_runtime/function.cc:584] Graph Initial #nodes 5 #edges 6
 2018-07-31 17:22:11.232583: I tensorflow/core/common_runtime/function.cc:584] Graph Before #nodes 5 #edges 6
 2018-07-31 17:22:11.232599: I tensorflow/core/common_runtime/constant_folding.cc:571] No constant foldable nodes found
 2018-07-31 17:22:11.232644: I tensorflow/core/common_runtime/function.cc:584] Graph ReCopy #nodes 5 #edges 7
 2018-07-31 17:22:11.232673: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SOURCE = NoOp[]()
 2018-07-31 17:22:11.232701: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: _SINK = NoOp[]()
 2018-07-31 17:22:11.232724: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: arg0 = _Arg[T=DT_INT64, index=0]()
 2018-07-31 17:22:11.232763: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=""pyfunc_2""](arg0)
 2018-07-31 17:22:11.232802: I tensorflow/core/framework/op_kernel.cc:1157] Instantiating kernel for node: pyfunc_RetVal = _Retval[T=DT_INT64, index=0](PyFunc)
 2018-07-31 17:22:11.232861: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 0 step -6231738800936566896 _SOURCE = NoOp[]() is dead: 0
 2018-07-31 17:22:11.232880: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 2 step -6231738800936566896 arg0 = _Arg[T=DT_INT64, index=0]() is dead: 0
 2018-07-31 17:22:11.232906: I tensorflow/core/framework/log_memory.cc:35] __LOG_MEMORY__ MemoryLogTensorOutput { step_id: -6231738800936566896 kernel_name: ""arg0"" tensor { dtype: DT_INT64 shape { } allocation_description { requested_bytes: 8 allocator_name: ""cpu"" } } }
 2018-07-31 17:22:11.232926: I tensorflow/core/common_runtime/executor.cc:1578] Process node: 3 step -6231738800936566896 PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=""pyfunc_2""](arg0) is dead: 0
 2018-07-31 17:22:11.232938: E tensorflow/python/lib/core/py_func.cc:472] About to ensure GIL
 </denchmark-code>
 
 		",6.0,nfergu,2018-07-31T20:05:46Z,"
 		It definitely looks like it's failing to acquire the lock :). Is there some way you could log the thread ID on which the PyFunc op is trying to acquire the GIL so we can associate it with the stack trace?
 Also, the presence of multiprocessing is slightly suspicious, because depending on when things are forked the TensorFlow runtime might end up in an illegal state (essentially, the process isn't forkable once a tf.Session has been created). Is it possible to reproduce the problem in a process that doesn't use multiprocessing?
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::PyFuncOp::Compute,ctx,484,540,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,nfergu,2018-08-01T15:37:48Z,"
 		I've logged the thread that is executing the PyFunc op. Here's the output:
 <denchmark-code>2018-08-01 15:37:06.264162: E tensorflow/python/lib/core/py_func.cc:473] About to ensure GIL in thread: 0x700011a15000
 2018-08-01 15:37:06.264168: E tensorflow/python/lib/core/py_func.cc:476] Native thread ID is : 4923929
 </denchmark-code>
 
 The native thread ID is the one we're interested in (4923929). This corresponds to 0x4B2219 in hex.
 Here's what lldb tells us about our threads:
 <denchmark-code>(lldb) thread info all
 thread #1: tid = 0x4b1ea2, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10, queue = 'com.apple.main-thread', stop reason = signal SIGSTOP
 
 thread #5: tid = 0x4b1f91, 0x00000001018566da libsystem_kernel.dylib`__workq_kernreturn + 10
 
 thread #7: tid = 0x4b21c4, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #8: tid = 0x4b21c6, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #9: tid = 0x4b2214, 0x0000000101857592 libsystem_kernel.dylib`read + 10
 
 thread #10: tid = 0x4b2218, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 
 thread #12: tid = 0x4b232d, 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
 </denchmark-code>
 
 However, 0x4B2219 is not there! However, if we look at our threads just before the session is destroyed we can see that this thread did previously exist:
 thread #11: tid = 0x4b2219, 0x0000000101855e7e libsystem_kernel.dylib__psynch_cvwait + 10`
 As might be expected, this is one of the Eigen threads:
 <denchmark-code>thread #11
     frame #0: 0x0000000101855e7e libsystem_kernel.dylib`__psynch_cvwait + 10
     frame #1: 0x000000010188d662 libsystem_pthread.dylib`_pthread_cond_wait + 732
     frame #2: 0x00000001019b6cb0 libc++.1.dylib`std::__1::condition_variable::wait(std::__1::unique_lock<std::__1::mutex>&) + 18
     frame #3: 0x0000000111b605f6 libtensorflow_framework.so`Eigen::EventCount::CommitWait(Eigen::EventCount::Waiter*) + 278
     frame #4: 0x0000000111b6026c libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) + 828
     frame #5: 0x0000000111b5f898 libtensorflow_framework.so`Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) + 568
     frame #6: 0x0000000111b5f55f libtensorflow_framework.so`std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() + 47
 </denchmark-code>
 
 So it looks like this thread has tried to get the GIL, hasn't managed, and then has stopped or been killed.
 Is it possible that the thread encounters an error when getting the GIL (which is not logged for some reason), and then is killed?
 Regarding the multiprocessing stuff, we are using Python's ""spawn"" multiprocessing context (multiprocessing.get_context('spawn')), so as I understand it the limitations around spawning processes after sessions have been created do not apply (but in any case we don't spawn any processes after session creation anyway). Having said that, both of the scenarios where this happens in our application do use multiprocessing, so I'm not able to rule this out as a cause.
 I was also wondering why there are 4 Eigen threads (in fact there are 6 before session destruction time), so just to double-check our config I dumped it out. Here it is:
 <denchmark-code>intra_op_parallelism_threads: 1
 inter_op_parallelism_threads: 1
 gpu_options {
   per_process_gpu_memory_fraction: 1.0
   allow_growth: true
 }
 allow_soft_placement: true
 
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,nfergu,2018-08-01T17:41:27Z,"
 		It looks like an error may be occurring, which is killing the thread. Stepping through the PyGILState_Ensure function in lldb, I can see the following happening:
 <denchmark-code>Process 71453 stopped
 * thread #11, stop reason = instruction step over
     frame #0: 0x000000010087d794 Python`PyGILState_Ensure + 100
 Python`PyGILState_Ensure:
 ->  0x10087d794 <+100>: movl   $0x0, 0x88(%rbx)
     0x10087d79e <+110>: jmp    0x10087d75a               ; <+42>
     0x10087d7a0 <+112>: leaq   0x8e1d1(%rip), %rdi       ; ""Couldn't create thread-state for new thread""
     0x10087d7a7 <+119>: callq  0x10087a510               ; Py_FatalError
 </denchmark-code>
 
 This looks to me like the Py_FatalError function is being called with ""Couldn't create thread-state for new thread"". Soon after this happens the thread dies.
 This seems to correspond to these lines from  (looking at the code here: <denchmark-link:https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Python/pystate.c>https://github.com/python/cpython/blob/e42b705188271da108de42b55d9344642170aa2b/Python/pystate.c</denchmark-link>
 ):
 <denchmark-code>tcur = PyThreadState_New(_PyRuntime.gilstate.autoInterpreterState);
         if (tcur == NULL)
             Py_FatalError(""Couldn't create thread-state for new thread"");
 </denchmark-code>
 
 But it's not obvious to me why this would be failing.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,nfergu,2018-08-01T18:53:50Z,"
 		Is the process terminating when you see the hang? Looking the the PyGILState_Ensure() code, as far as I can tell the only situation in which we'd hit this path is if malloc() returned null. I can't think why that might be happening, but I'd be less surprised if we were in some rarely-hit exit path.
 		",10.0,nfergu,2018-08-01T19:47:58Z,"
 		Yes, the process is terminating when we see the hang. In fact we can see from the original thread dump that the destructor appears to be called as a consequence of a garbage collection which happens as part of shut-down:
 <denchmark-code>frame #31: 0x000000010075f8d4 Python`_PyObject_FastCallDict + 180
     frame #32: 0x00000001007d6579 Python`slot_tp_finalize + 121
     frame #33: 0x000000010089b18a Python`collect + 1418
     frame #34: 0x000000010089b8c3 Python`_PyGC_CollectIfEnabled + 99
     frame #35: 0x000000010087af57 Python`Py_FinalizeEx + 119
     frame #36: 0x000000010087b0e0 Python`Py_Exit + 16
     frame #37: 0x000000010087ef4c Python`handle_system_exit + 252
     frame #38: 0x000000010087f1a5 Python`PyErr_PrintEx + 437
 </denchmark-code>
 
 The docs for Py_FinalizeEx say ""Undo all initializations made by Py_Initialize() and subsequent use of Python/C API functions"" so this could well be why PyThreadState_New subsequently fails.
 I think this is may be why my simplified test case does not reproduce the problem: it looks like the destructor is called as part of a ""regular"" GC in this test case.
 		",11.0,nfergu,2018-08-01T20:42:08Z,"
 		Adding a call to gc.collect() just before the process starts shutting-down seems to fix the problem for us, so it does look like ""normal"" garbage collections are OK, but when the Python VM is shutting-down calling BaseSession's destructor is problematic.
 		",12.0,nfergu,2018-08-01T21:32:55Z,"
 		Aha, that makes sense. Can you try patching the fix in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/8cd2d6fe9389e93a4182ae9287f2f8325913fe6c>8cd2d6f</denchmark-link>
  and see if that fixes the problem without having to call ?
 		",13.0,nfergu,2018-08-02T16:04:36Z,"
 		Yes, that fixes the issue, and I get a log message saying:
 <denchmark-code>2018-08-02 16:58:17.950407: W tensorflow/core/kernels/data/generator_dataset_op.cc:129] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
 	 [[Node: PyFunc = PyFunc[Tin=[DT_INT64], Tout=[DT_INT64], token=""pyfunc_2""](arg0)]]
 </denchmark-code>
 
 Thanks very much for your assistance on this.
 		",14.0,nfergu,2018-08-02T16:12:03Z,"
 		Thank you very much for digging into the details and providing such a useful report!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22013,mpekalski,2018-09-02T21:09:33Z,2018-10-12T20:17:50Z,tf.scatter_nd_update - Segmentation fault (core dumped),"
 <denchmark-h:hr></denchmark-h>
 
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary):
 source
 TensorFlow version (use command below):
 
 TF checkpoint I have built
 <denchmark-code>/tmp/tensorflow# git log   
 commit 09792df012c22622324f085f46edde33006c7355
 Author: A. Unique TensorFlower <gardener@tensorflow.org>
 Date:   Sun Aug 26 02:07:11 2018 -0700
 
     compat: Update forward compatibility horizon to 2018-08-26
     
     PiperOrigin-RevId: 210266798
 </denchmark-code>
 
 <denchmark-code>== cat /etc/issue ===============================================
 Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
 VERSION=""16.04.5 LTS (Xenial Xerus)""
 VERSION_ID=""16.04""
 VERSION_CODENAME=xenial
 
 == are we in docker =============================================
 Yes
 
 == compiler =====================================================
 c++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609
 Copyright (C) 2015 Free Software Foundation, Inc.
 This is free software; see the source for copying conditions.  There is NO
 warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
 
 
 == uname -a =====================================================
 Linux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
 
 == check pips ===================================================
 numpy (1.14.5)
 protobuf (3.6.1)
 tensorflow (1.10.0)
 
 == check for virtualenv =========================================
 False
 
 == tensorflow import ============================================
 tf.VERSION = 1.10.0
 tf.GIT_VERSION = b'unknown'
 tf.COMPILER_VERSION = b'unknown'
 Sanity check: array([1], dtype=int32)
 
 == env ==========================================================
 LD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib
 DYLD_LIBRARY_PATH is unset
 
 == nvidia-smi ===================================================
 Wed Aug 29 19:57:14 2018       
 +-----------------------------------------------------------------------------+
 | NVIDIA-SMI 396.26                 Driver Version: 396.26                    |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |===============================+======================+======================|
 |   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |
 |  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |
 +-------------------------------+----------------------+----------------------+
                                                                                
 +-----------------------------------------------------------------------------+
 | Processes:                                                       GPU Memory |
 |  GPU       PID   Type   Process name                             Usage      |
 |=============================================================================|
 +-----------------------------------------------------------------------------+
 
 == cuda libs  ===================================================
 /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148
 /usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a
 
 </denchmark-code>
 
 Bazel version
 <denchmark-code>$ bazel version
 WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
 Build label: 0.16.0
 Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Tue Jul 31 17:01:24 2018 (1533056484)
 Build timestamp: 1533056484
 Build timestamp as int: 1533056484
 </denchmark-code>
 
 CUDNN version:
 <denchmark-code>$ nvcc --version
 nvcc: NVIDIA (R) Cuda compiler driver
 Copyright (c) 2005-2018 NVIDIA Corporation
 Built on Tue_Jun_12_23:07:04_CDT_2018
 Cuda compilation tools, release 9.2, V9.2.148
 </denchmark-code>
 
 GPU: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS
 
 Exact command to reproduce:
 
 <denchmark-code>from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
 
 def scope_1():
     print(""DS1 SCOPE ============="")
     with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
         x = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                                , trainable=False, use_resource=True)             
         print(""graph: {}"".format(x.graph))
         print(""scope: {}"".format(tf.get_variable_scope().name))
         print("" name: {}"".format(x.name))
         print(""  var: {}"".format(str(x)))
         current_scope = tf.get_variable_scope()       
         assign_one = tf.assign(x, 1.0, name=""x_is_one"")
     
     def scope_2(inputs, label):        
         print(""initial scope: {}"".format(tf.get_variable_scope().name))
         print(""DS1 SCOPE ============="")
         #with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
         with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
             y = tf.get_variable(""x"", initializer=lambda: tf.zeros(shape=[], dtype=tf.float32), dtype=tf.float32
                                    , trainable=False, use_resource=True)         
             print(""graph: {}"".format(y.graph))
             print(""scope: {}"".format(tf.get_variable_scope().name))
             print("" name: {}"".format(y.name))
             print(""  var: {}"".format(str(y)))
             print(""============="")
             print(y)
             print(inputs)
             #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=""inputs_plus_1"")
             assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))
             with tf.control_dependencies([assign_two]):
                 with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                     return y.read_value(), label
             #return x,label
     
     # test that original x is mutable
     with tf.control_dependencies([assign_one]):
         dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                     .map(scope_2)
                     .batch(1)
                     .repeat(1)        
                     )
     return dataset
     
                 
 with tf.variable_scope(""scope_0""):
         dataset_fn = scope_1()
 
 with tf.variable_scope(""iterator""):
     # Define iterator from_string_handle. In general it is useful to have
     # this kind of iterator if one wants to switch between train and validation
     # within the training loop.        
     iterator_t = dataset_fn.make_initializable_iterator()
     iterator_handle = tf.placeholder(tf.string, shape=[], name=""iterator_handle"")
     iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                 iterator_t.output_types,
                                                 iterator_t.output_shapes)
     
     def get_next_item():
         next_elem = iterator.get_next(name=""next_element"")
         x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
         return x, y    
 with tf.Session() as sess:
 
     sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
     handle_t = sess.run(iterator_t.string_handle())
     # Run data iterator initialisation
     sess.run(iterator_t.initializer)
     print(sess.graph.get_operations()) 
     while True:
         try:
             print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
         except tf.errors.OutOfRangeError:
                         print(""End of training dataset."")
                         break        
     print()
     print(""global vars: {}"".format(tf.global_variables()))
     print(""local vars: {}"".format(tf.local_variables()))
     print(tf.get_default_graph().get_name_scope())
 
 </denchmark-code>
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.
 The scoping may seem weird, but that is minimal example that shows the problem that I created from my project. After adding  I started to get segmentation fault.
 A minimal example with  instead of  worked, see <denchmark-link:https://github.com/tensorflow/tensorflow/issues/22009>#22009</denchmark-link>
 
 I tried disabling GPU with config=tf.ConfigProto(device_count={'GPU': 0}) and CUDA_VISIBLE_DEVICES="""" but the result was the same.
 I will recompile TF overnight (from the current master) with --copt=-g and try to provide a stacktrace
 <denchmark-code>TF_BUILD_INFO = {container_type: ""gpu"", command: ""bazel build --config=opt --config=cuda --copt=-march=native --copt=-mfpmath=both --copt=-mtune=native --copt=-g --verbose_failures --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --jobs=8 --config=mkl --action_env=LD_LIBRARY_PATH=/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib tensorflow/tools/pip_package:build_pip_package"", source_HEAD: ""201be3d514d7239aa19496dba4dd0c85303b03f1"", source_remote_origin: ""https://github.com/tensorflow/tensorflow"", OS: ""Linux"", kernel: ""4.13.0-38-generic"", architecture: ""x86_64"", processor: ""Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz"", processor_count: ""8"", memory_total: ""32877820 kB"", swap_total: ""69444596 kB"", Bazel_version: ""Build label: 0.16.0"", Java_version: ""1.8.0_181"", Python_version: ""3.6.2"", gpp_version: ""g++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609"", swig_version: """", NVIDIA_driver_version: ""396.26"", CUDA_device_count: ""1"", CUDA_device_names: ""GeForce GTX 1080 Ti   (*PrimaryCard),"", CUDA_toolkit_version: ""V9.2.148""}
 </denchmark-code>
 
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 <denchmark-code>DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x:0
   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
 initial scope: 
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7f0940f582b0>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x_1:0
   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 =============
 <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 Tensor(""arg0:0"", shape=(), dtype=int32)
 2018-09-02 20:52:39.456276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2018-09-02 20:52:39.456710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: 
 name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
 pciBusID: 0000:01:00.0
 totalMemory: 10.92GiB freeMemory: 10.23GiB
 2018-09-02 20:52:39.456728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0
 2018-09-02 20:52:39.665323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:
 2018-09-02 20:52:39.665362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 
 2018-09-02 20:52:39.665369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N 
 2018-09-02 20:52:39.665731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9887 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
 2018-09-02 20:52:39.756397: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
 Segmentation fault (core dumped)
 </denchmark-code>
 
 log of run on CPU
 <denchmark-code>CUDA_VISIBLE_DEVICES="""" python3 bug.py 
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x:0
   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
 initial scope: 
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7ffaa61de390>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x_1:0
   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 =============
 <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 Tensor(""arg0:0"", shape=(), dtype=int32)
 2018-09-02 20:54:41.271567: E tensorflow/stream_executor/cuda/cuda_driver.cc:300] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
 2018-09-02 20:54:41.271604: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] retrieving CUDA diagnostic information for host: 3bed2f328777
 2018-09-02 20:54:41.271614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:170] hostname: 3bed2f328777
 2018-09-02 20:54:41.271655: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:194] libcuda reported version is: 396.26.0
 2018-09-02 20:54:41.271686: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:198] kernel reported version is: 396.26.0
 2018-09-02 20:54:41.271694: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:305] kernel version seems to match DSO: 396.26.0
 2018-09-02 20:54:41.271962: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
 Segmentation fault (core dumped)
 </denchmark-code>
 
 	",1.0,mpekalski,2018-09-03T06:35:19Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 CUDA/cuDNN version
 GPU model and memory
 		",2.0,mpekalski,2018-09-03T07:44:02Z,"
 		When executing line by line everything works fine until it starts the session.
 I've tried running it on different environment, on kaggle.com, and the it also fails.
 		",3.0,mpekalski,2018-09-03T08:14:44Z,"
 		I tried it on one more environment. Where I have just installed TF from <denchmark-link:https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl>https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl</denchmark-link>
 
 I had to modify a bit lambda initializer in get_variable and specify the shape, but otherwise I got the same segmentation fault. This env does not have GPU, so it was run on a CPU.
 <denchmark-code># python3 tf_bug.py 
 /opt/conda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
   from ._conv import register_converters as _register_converters
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x:0
   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
 initial scope: 
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7fa7eb105a20>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x_1:0
   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 =============
 <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 Tensor(""arg0:0"", shape=(), dtype=int32)
 2018-09-03 08:12:16.809170: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 Segmentation fault (core dumped)
 </denchmark-code>
 
 		",85f4f6b7ced7afab7e77e65c2b21448cfbf2d6f2,Eugene Brevdo,2018-10-12 13:15:27-07:00,MODIFY,1,tensorflow\core\framework\common_shape_fns.cc,tensorflow\core\framework\common_shape_fns.cc,1.0,"1460,1461,1462,1463,1464,1521,1522","1460,1517",MODIFY,1.0,tensorflow\python\kernel_tests\scatter_nd_ops_test.py,tensorflow\python\kernel_tests\scatter_nd_ops_test.py,4.0,mpekalski,2018-09-03T09:04:06Z,"
 		I tried running the script in gdb but it says no stack. Any hints how to get it work?
 		",5.0,mpekalski,2018-09-04T08:33:16Z,"
 		I've managed to get gdb working, looks like the issue is with inferring shape in scatter_nd_update
 <denchmark-code>0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()
    from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
 </denchmark-code>
 
 Full log
 <denchmark-code># gdb python3
 GNU gdb (Ubuntu 7.11.1-0ubuntu1~16.5) 7.11.1                                              Copyright (C) 2016 Free Software Foundation, Inc.                                         License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html> py
 This is free software: you are free to change and redistribute it.
 There is NO WARRANTY, to the extent permitted by law.  Type ""show copying""gdb py
 and ""show warranty"" for details.
 This GDB was configured as ""x86_64-linux-gnu"".
 Type ""show configuration"" for configuration details.
 For bug reporting instructions, please see:
 <http://www.gnu.org/software/gdb/bugs/>.
 Find the GDB manual and other documentation resources online at:
 <http://www.gnu.org/software/gdb/documentation/>.
 For help, type ""help"".
 Type ""apropos word"" to search for commands related to ""word""...
 Reading symbols from python3...done.
 (gdb) run bug.py
 Starting program: /opt/conda/bin/python3 bug.py
 [Thread debugging using libthread_db enabled]
 Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
 [New Thread 0x7ffff31fa700 (LWP 606)]
 [New Thread 0x7ffff09f9700 (LWP 607)]
 [New Thread 0x7fffee1f8700 (LWP 608)]
 [New Thread 0x7fffeb9f7700 (LWP 609)]
 [New Thread 0x7fffe91f6700 (LWP 610)]
 [New Thread 0x7fffe69f5700 (LWP 611)]
 [New Thread 0x7fffe41f4700 (LWP 612)]
 [Thread 0x7fffe41f4700 (LWP 612) exited]
 [Thread 0x7fffe69f5700 (LWP 611) exited]
 [Thread 0x7fffe91f6700 (LWP 610) exited]
 [Thread 0x7fffeb9f7700 (LWP 609) exited]
 [Thread 0x7fffee1f8700 (LWP 608) exited]
 [Thread 0x7ffff09f9700 (LWP 607) exited]
 [Thread 0x7ffff31fa700 (LWP 606) exited]
 /opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
   return f(*args, **kwds)
 /opt/conda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
   return f(*args, **kwds)
 [New Thread 0x7fffe41f4700 (LWP 617)]
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x:0
   var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>
 [New Thread 0x7fffe69f5700 (LWP 618)]
 initial scope:
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7ffff6789160>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x_1:0
   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 =============
 <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>
 Tensor(""arg0:0"", shape=(), dtype=int32)
 [New Thread 0x7fffe91f6700 (LWP 619)]
 [New Thread 0x7fffeb9f7700 (LWP 620)]
 [New Thread 0x7fff871ff700 (LWP 621)]
 [New Thread 0x7fff869fe700 (LWP 622)]
 [New Thread 0x7fff861fd700 (LWP 623)]
 [New Thread 0x7fff859fc700 (LWP 624)]
 [New Thread 0x7fff851fb700 (LWP 625)]
 [New Thread 0x7fff849fa700 (LWP 626)]
 [New Thread 0x7fff5fdff700 (LWP 627)]
 [New Thread 0x7fff5f5fe700 (LWP 628)]
 [New Thread 0x7fff5edfd700 (LWP 629)]
 [New Thread 0x7fff5e5fc700 (LWP 630)]
 2018-09-04 08:28:02.088839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2018-09-04 08:28:02.089267: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties:
 name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721
 pciBusID: 0000:01:00.0
 totalMemory: 10.92GiB freeMemory: 10.20GiB
 2018-09-04 08:28:02.089287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0
 2018-09-04 08:28:02.089297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:
 2018-09-04 08:28:02.089303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0
 2018-09-04 08:28:02.089307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N
 2018-09-04 08:28:02.089464: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
 [New Thread 0x7fff5ddfb700 (LWP 631)]
 [New Thread 0x7fff5d5fa700 (LWP 632)]
 [New Thread 0x7fff5cdf9700 (LWP 633)]
 [Thread 0x7fff5cdf9700 (LWP 633) exited]
 [New Thread 0x7fff5cdf9700 (LWP 634)]
 [Thread 0x7fff5cdf9700 (LWP 634) exited]
 [New Thread 0x7fff5cdf9700 (LWP 635)]
 [Thread 0x7fff5cdf9700 (LWP 635) exited]
 [New Thread 0x7fff5cdf9700 (LWP 636)]
 [Thread 0x7fff5cdf9700 (LWP 636) exited]
 [New Thread 0x7fff5cdf9700 (LWP 637)]
 [Thread 0x7fff5cdf9700 (LWP 637) exited]
 [New Thread 0x7fff5cdf9700 (LWP 638)]
 [Thread 0x7fff5cdf9700 (LWP 638) exited]
 [New Thread 0x7fff5cdf9700 (LWP 639)]
 [Thread 0x7fff5cdf9700 (LWP 639) exited]
 [New Thread 0x7fff5cdf9700 (LWP 640)]
 [Thread 0x7fff5cdf9700 (LWP 640) exited]
 
 Thread 1 ""python3"" received signal SIGSEGV, Segmentation fault.
 0x00007fffcd5f912f in tensorflow::shape_inference::ScatterNdUpdateShape(tensorflow::shape_inference::InferenceContext*) ()
    from /opt/conda/lib/python3.6/site-packages/tensorflow/python/../libtensorflow_framework.so
 (gdb)
 </denchmark-code>
 
 		",6.0,mpekalski,2018-09-04T09:10:55Z,"
 		I've got the code running by explicitly specifying shape in tf.get_variable(), and then making sure that the corresponding tensors in assign ops had the same shape (so not assigning 1.0 but [1.0]). Still, I could not make it work with dynamic shapes, and segmentation fault should not take place but throw some error message.
 Here is the working code
 <denchmark-code>from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
 
 def scope_1():
     print(""DS1 SCOPE ============="")
     with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
         x = tf.get_variable(""x"", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32
                                , trainable=False, use_resource=True, shape=[1])             
         print(""graph: {}"".format(x.graph))
         print(""scope: {}"".format(tf.get_variable_scope().name))
         print("" name: {}"".format(x.name))
         print(""  var: {}"".format(str(x)))
         current_scope = tf.get_variable_scope()       
         assign_one = tf.assign(x, [1.0], name=""x_is_one"")
     
     def scope_2(inputs, label):        
         print(""initial scope: {}"".format(tf.get_variable_scope().name))
         print(""DS1 SCOPE ============="")
         #with tf.variable_scope(""scope_1"", reuse=tf.AUTO_REUSE):
         with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):
             y = tf.get_variable(""x"", initializer=lambda shape, dtype, partition_info: tf.zeros(shape=shape, dtype=dtype), dtype=tf.float32
                                    , trainable=False, use_resource=True, shape=[1])         
             print(""graph: {}"".format(y.graph))
             print(""scope: {}"".format(tf.get_variable_scope().name))
             print("" name: {}"".format(y.name))
             print(""  var: {}"".format(str(y)))
             print(""============="")
             print(y)
             print(inputs)
             #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=""inputs_plus_1"")
             assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), [1.0])))
             with tf.control_dependencies([assign_two]):
                 with tf.control_dependencies([tf.scatter_nd_update(y, [[0]], [0.22])]):
                     return y.read_value(), label
 
             #return x,label
     
     # test that original x is mutable
     with tf.control_dependencies([assign_one]):
         dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))
                     .map(scope_2)
                     .batch(1)
                     .repeat(1)        
                     )
     return dataset
     
                 
 with tf.variable_scope(""scope_0""):
         dataset_fn = scope_1()
 
 with tf.variable_scope(""iterator""):
     # Define iterator from_string_handle. In general it is useful to have
     # this kind of iterator if one wants to switch between train and validation
     # within the training loop.        
     iterator_t = dataset_fn.make_initializable_iterator()
     iterator_handle = tf.placeholder(tf.string, shape=[], name=""iterator_handle"")
     iterator = tf.data.Iterator.from_string_handle(iterator_handle, 
                                                 iterator_t.output_types,
                                                 iterator_t.output_shapes)
     
     def get_next_item():
         next_elem = iterator.get_next(name=""next_element"")
         x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)
         return x, y    
         
 with tf.Session() as sess:
 
     sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])
     handle_t = sess.run(iterator_t.string_handle())
     # Run data iterator initialisation
     sess.run(iterator_t.initializer)
     print(sess.graph.get_operations()) 
     while True:
         try:
             print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))
         except tf.errors.OutOfRangeError:
                         print(""End of training dataset."")
                         break        
     print()
     print(""global vars: {}"".format(tf.global_variables()))
     print(""local vars: {}"".format(tf.local_variables()))
     print(tf.get_default_graph().get_name_scope())
 </denchmark-code>
 
 and the output
 <denchmark-code>DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x:0
   var: <tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>
 initial scope: 
 DS1 SCOPE =============
 graph: <tensorflow.python.framework.ops.Graph object at 0x7f67cfcce940>
 scope: scope_0/scope_1
  name: scope_0/scope_1/x_1:0
   var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>
 =============
 <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>
 Tensor(""arg0:0"", shape=(), dtype=int32)
 [<tf.Operation 'Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0_1/scope_1/Placeholder' type=Placeholder>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/scope_1/Const' type=Const>, <tf.Operation 'scope_0_1/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0_1/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/tensors/component_0' type=Const>, <tf.Operation 'scope_0_1/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/shape_as_tensor' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/zeros' type=Fill>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0_1/batch_size' type=Const>, <tf.Operation 'scope_0_1/count' type=Const>, <tf.Operation 'iterator/Iterator' type=Iterator>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDataset' type=BatchDataset>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandle' type=IteratorFromStringHandle>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]
 (array([[0.22]], dtype=float32), array([-1], dtype=int32))
 (array([[0.22]], dtype=float32), array([-2], dtype=int32))
 (array([[0.22]], dtype=float32), array([-3], dtype=int32))
 (array([[0.22]], dtype=float32), array([-4], dtype=int32))
 (array([[0.22]], dtype=float32), array([-5], dtype=int32))
 End of training dataset.
 
 global vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=(1,) dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=(1,) dtype=float32>]
 local vars: []
 </denchmark-code>
 
 		",1.0,"288,289,290,291,292,293,294,295",,testResVarInvalidOutputShape,self,288,295,,,,,,,,,,,,,,,tensorflow::shape_inference::ScatterNdUpdateShape,c,1457,1526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,mpekalski,2018-09-04T09:14:25Z,"
 		<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  I have identified the reason for the segmentation fault. Please have a look at my comments above.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,mpekalski,2018-10-04T19:06:58Z,"
 		Nagging Assignee <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
 : It has been 30 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,mpekalski,2018-10-11T00:46:16Z,"
 		Looking at the documentation for scatter_nd_update (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update>https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update</denchmark-link>
 ) seems like indexing into a zero rank tensor is sort of invalid? Adding Eugene for more info.
 		",10.0,mpekalski,2018-10-11T19:35:54Z,"
 		Thanks for reporting.  I found the cause of the problem and will send a fix.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22039,gulingfengze,2018-09-04T03:49:40Z,2020-07-28T12:32:01Z,Session.run () takes a long time,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below):1.9.0
 Python version:3.6
 Bazel version (if compiling from source):N/A
 GCC/Compiler version (if compiling from source):6.4.0
 CUDA/cuDNN version:9.0 / 7.0
 GPU model and memory:GeForce GTX960
 Exact command to reproduce:N/A
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Each time tensorflow performs session.run() for target detection, the detection time of the first image is very long (including some initialization operations, of course), while the detection time of other images is normal. Suppose I detect the image under a certain path (for example, there are ten images), the time of detecting the first image is relatively long, and the remaining nine images are relatively short (basically consistent). However, my operation in practical application is as follows: the session.run() is called every once in a while to detect a picture. I hope that the detection time after the first one is normal except for the long time. However, through my test (guess), after exiting the loop logic of detection, tensorflow redid a series of initialization operations the next time the detection was done, which puzzled me.
 With other frameworks, such as mxnet, initial detection takes longer. And then the detection time is normal, as if it's not doing some initialization anymore. I thought, could tensorflow do the same thing?
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 2018-09-04 14:48:59.111510: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.57GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 time：5627.940655ms
 0.9999423027038574
 time：657.650471ms
 0.9996843338012695
 time：676.565170ms
 0.9722122550010681
 0.6320008635520935
 time：667.881966ms
 0.996504545211792
 	",1.0,gulingfengze,2018-09-05T00:44:11Z,"
 		I met a similar problem. It seems to be caused by some lazy operations. That means some operations will not be executed until the graph actually runs.
 		",2.0,gulingfengze,2018-09-06T02:39:19Z,"
 		<denchmark-link:https://github.com/bignamehyp>@bignamehyp</denchmark-link>
  Is there a way to solve this problem?
 		",3.0,gulingfengze,2019-02-13T21:54:31Z,"
 		<denchmark-link:https://github.com/gulingfengze>@gulingfengze</denchmark-link>
  is this still an issue? Could you load TF 1.12 (stable) or latest version and check whether the issue persists? It is will be good if you can share a simplified code to reproduce the issue. If it was solved by newer version, please close the issue. Thanks!
 		",59166604c26b8ffde742389fcd99c8090bf8ec04,Feng Liu,2019-12-19 14:36:01-08:00,MODIFY,1,tensorflow\compiler\mlir\lite\python\graphdef_to_tfl_flatbuffer.cc,tensorflow\compiler\mlir\lite\python\graphdef_to_tfl_flatbuffer.cc,1.0,,280,,,,,4.0,gulingfengze,2019-02-16T08:18:08Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  The latest version still has this problem, and my guess is that the program did some initialization like gpu at the beginning of execution.
 		",5.0,gulingfengze,2019-02-19T17:49:28Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  I work on XLA, so this is outside of my area of ownership.
 		",6.0,gulingfengze,2019-02-19T17:50:00Z,"
 		
 my guess is that the program did some initialization like gpu at the beginning of execution.
 
 Quite possibly.  To address this, when you run configure.py, you need to ensure that you tell TF to build for sm_XY corresponding to your machine's GPU.
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::ConvertGraphDefToTFLiteFlatBuffer,"model_flags,toco_flags,debug_info,input,result",169,289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,gulingfengze,2020-07-14T11:05:35Z,"
 		<denchmark-link:https://github.com/gulingfengze>@gulingfengze</denchmark-link>
 
 Please confirm if this is still an issue, or if possible use later stable versions of tf and let us know.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,gulingfengze,2020-07-21T11:52:04Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,gulingfengze,2020-07-28T12:31:58Z,"
 		Closing as stale. Please reopen if you'd like to work on this further.
 		",10.0,gulingfengze,2020-07-28T12:32:03Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22039>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/22039>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22438,jilljenn,2018-09-21T07:00:58Z,2019-08-08T18:03:58Z,InvalidArgumentError when SparseTensorValue is not ordered by row then col,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34 1.10.0
 Python version: 3.6.5
 Bazel version (if compiling from source): no
 GCC/Compiler version (if compiling from source): no
 CUDA/cuDNN version: no
 GPU model and memory: no
 Exact command to reproduce: python bug.py
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 If we feed a SparseTensorValue to tf.data.Dataset.from_tensor_slices such that its indices are not lexicographically sorted by row then col, then we will get a InvalidArgumentError.
 Maybe it could be said in the docs, or the error should provide a clearer message. It was hard to guess that indices[2] = [1,0] is out of order meant the indices were not provided in lexicographic order.
 I finally saw that it was explained in the <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/SparseTensor>SparseTensor docs</denchmark-link>
 , but I feel it should be said in the <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue>SparseTensorValue docs</denchmark-link>
  as well.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 from scipy.sparse import csr_matrix
 import tensorflow as tf
 import numpy as np
 
 
 M = np.array([[0, 1, 1], [1, 0, 0], [1, 0, 2], [0, 1, 1]])
 
 # First observation: these two slicing operations provide different orderings
 S = csr_matrix(M)[1:3].tocoo()
 print('1:3', S.row, S.col)
 S = csr_matrix(M)[[1, 2]].tocoo()
 print('1,2', S.row, S.col)
 
 entries = np.column_stack((S.row, S.col, S.data))
 ordering = np.arange(len(S.data))
 
 # Uncomment the following line to fix the error
 # ordering = np.lexsort((S.col, S.row))  # Sort by row then col
 
 X_train = tf.SparseTensorValue(indices=entries[ordering, :2],
                                values=entries[ordering, 2],
                                dense_shape=S.shape)
 
 dataset = tf.data.Dataset.from_tensor_slices(X_train)
 iterator = dataset.make_initializable_iterator()
 X_sample = iterator.get_next()
 
 with tf.Session() as sess:
     sess.run(iterator.initializer)
     print(sess.run(X_sample))
 <denchmark-code>Traceback (most recent call last):
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1278, in _do_call
     return fn(*args)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1263, in _run_fn
     options, feed_dict, fetch_list, target_list, run_metadata)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1350, in _call_tf_sessionrun
     run_metadata)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[2] = [1,0] is out of order
      [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""bug.py"", line 27, in <module>
     sess.run(iterator.initializer)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 877, in run
     run_metadata_ptr)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1100, in _run
     feed_dict_tensor, options, run_metadata)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1272, in _do_run
     run_metadata)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1291, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[2] = [1,0] is out of order
      [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]
 
 Caused by op 'SerializeManySparse', defined at:
   File ""bug.py"", line 22, in <module>
     dataset = tf.data.Dataset.from_tensor_slices(X_train)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 254, in from_tensor_slices
     return TensorSliceDataset(tensors)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1173, in __init__
     self._tensors = sparse.serialize_many_sparse_tensors(tensors)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/util/sparse.py"", line 132, in serialize_many_sparse_tensors
     for tensor in nest.flatten(tensors)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/data/util/sparse.py"", line 132, in <listcomp>
     for tensor in nest.flatten(tensors)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py"", line 1469, in serialize_many_sparse
     out_type=out_type)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_sparse_ops.py"", line 502, in serialize_many_sparse
     out_type=out_type, name=name)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
     op_def=op_def)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 454, in new_func
     return func(*args, **kwargs)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3155, in create_op
     op_def=op_def)
   File ""/Users/jilljenn/code/vae/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1717, in __init__
     self._traceback = tf_stack.extract_stack()
 
 InvalidArgumentError (see above for traceback): indices[2] = [1,0] is out of order
      [[Node: SerializeManySparse = SerializeManySparse[T=DT_INT64, out_type=DT_VARIANT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](tensors/SparseTensor/indices, tensors/SparseTensor/values, tensors/SparseTensor/dense_shape)]]
 </denchmark-code>
 
 	",1.0,jilljenn,2018-09-21T23:18:01Z,"
 		<denchmark-link:https://github.com/jilljenn>@jilljenn</denchmark-link>
  Good catch and agreed that more clarity on SparseTensorValue would help.
 <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  Can you take a look at this?
 		",2.0,jilljenn,2018-09-29T03:43:07Z,"
 		Closing this issue, we will add this to the future release.
 		",3.0,jilljenn,2019-08-08T12:16:29Z,"
 		I've also met this error and been confused this message.
 I hope you to implement some sorting function to avoid this error as well as to update the error message.
 
 Environment:
 
 macOS: 10.14.4
 docker: 19.03.1
 docker image: tensorflow/tensorflow:latest-py3
 
 
 
 Sorry, I missed to find the description and the function.
 
 https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor
 https://www.tensorflow.org/api_docs/python/tf/sparse/reorder
 
 		",eb625fb51a302dc812c97879697642db9aa8cfc1,Mark Daoust,2019-08-08 11:01:48-07:00,MODIFY,1,tensorflow\core\util\sparse\sparse_tensor.h,tensorflow\core\util\sparse\sparse_tensor.h,1.0,"315,316,317,318,319",315,MODIFY,1.0,tensorflow\core\util\sparse\sparse_tensor_test.cc,tensorflow\core\util\sparse\sparse_tensor_test.cc,4.0,jilljenn,2019-08-08T18:16:45Z,"
 		<denchmark-link:https://github.com/hiro-o918>@hiro-o918</denchmark-link>
  thanks for pinging this bug,
 I've improved the error message to point people in the right direction:
 <denchmark-code> InvalidArgumentError (see above for traceback): indices[2] = [1,0] is out of order. Many sparse ops require sorted indices.
     Use `tf.sparse.reorder` to create a correctly ordered copy.
 </denchmark-code>
 
 		",,,,,,,,,1.0,"194,195,196,197,198,199","194,195",tensorflow::sparse::TEST,"SparseTensorTest,SparseTensorConstruction",169,225,,,,,,,,,,,,,,,tensorflow::sparse::SparseTensor::IndexValid,"ix_t,n",286,326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23083,zachmayer,2018-10-18T20:26:27Z,2018-10-25T00:57:21Z,Documentation for learning_rate_power in the FTRL optimizer,"
 Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
 System information
 
 TensorFlow version: 1.6.0
 Doc Link: https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer
 
 
 This is probably a silly, small issue, but the docs for the FTRL optimizer do not describe what  and how it interacts with .  I tried to figure it out based on the source, but couldn't actually find <denchmark-link:https://github.com/tensorflow/tensorflow/blob/1c7bc899dbb86cec70a2c11207a9ce8acf30c13b/tensorflow/python/training/ftrl.py#L41>where learning_rate_power is used in the code.</denchmark-link>
 
 The l2_shrinkage_regularization_strength has a very detailed explanation and an equation.  Something similar for learning_rate_power would be nice.
 	",1.0,zachmayer,2018-10-18T23:13:17Z,"
 		There's a fix now in review.
 See section 3.1 in the <denchmark-link:https://www.eecs.tufts.edu/~dsculley/papers/ad-click-prediction.pdf>paper</denchmark-link>
 .
 		",2.0,zachmayer,2018-10-19T13:52:51Z,"
 		Thanks!
 		",3.0,zachmayer,2018-10-19T16:47:34Z,"
 		Thank you <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
 . Please post here once the fix is completed.
 		",5097def5847270353d7d8a37eacbd0f85e98f2a0,Mark Daoust,2018-10-24 17:55:03-07:00,MODIFY,0,tensorflow\python\training\ftrl.py,tensorflow\python\training\ftrl.py,0.0,"55,56,57",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23195,brianmartin,2018-10-23T20:21:54Z,2018-10-31T01:38:43Z,Segfault reading dataset more than once (`make_batched_features_dataset`),"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14 Mojave
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
 TensorFlow installed from (source or binary): Binary / pip
 TensorFlow version (use command below): ('v1.11.0-rc2-4-gc19e29306c', '1.11.0')
 Python version: 2.7.10
 Bazel version (if compiling from source): NA
 GCC/Compiler version (if compiling from source): NA
 CUDA/cuDNN version: NA
 GPU model and memory: NA
 
 <denchmark-link:https://gist.github.com/brianmartin/2b43dca69453478ed33b49f1029e03fe>Gist of full output of `./tools/tf_env_collect.sh</denchmark-link>
 
 Exact command to reproduce:
 <denchmark-code>$ wget https://gist.githubusercontent.com/brianmartin/4f221eb838dce8099342f829fb13253d/raw/00c2a1736b9a292f8a6fb88164d240a766468744/gistfile1.txt
 $ python gistfile1.txt
 </denchmark-code>
 
 Describe the current behavior
 Current behavior in 1.11.0, 1.12.0rcX is that the included script segfaults (11: SIGSEGV).
 Describe the expected behavior
 Expected behavior is no segfault. Version 1.10.1, 1.10.0, and 1.9.0 do not segfault. I have not checked lower versions.
 Code to reproduce the issue
 See a full script which reproduces this issue along two different code paths here: <denchmark-link:https://gist.github.com/brianmartin/4f221eb838dce8099342f829fb13253d>https://gist.github.com/brianmartin/4f221eb838dce8099342f829fb13253d</denchmark-link>
 
 The segfault occurs when reading the dataset a second time. The first read works as expected.
 For reference the two code samples which produce datasets which cause a segfault on second read are:
     dataset = make_batched_features_dataset(file_pattern=data_file,
                                             batch_size=1,
                                             features=feature_spec)
 I've also dug into make_batched_features_dataset and minified down to this repro:
     from tensorflow.contrib.data.python.ops import parsing_ops
     from tensorflow.python.data.ops import readers
 
     dataset = Dataset.from_tensor_slices([data_file]) \
                      .interleave(readers.TFRecordDataset, cycle_length=1) \
                      .batch(batch_size=1) \
                      .apply(parsing_ops.parse_example_dataset(feature_spec))
 <denchmark-h:hr></denchmark-h>
 
 Please let me know if there's anything else I can provide or help with! This is blocking us from upgrading to the latest Tensorflow version in <denchmark-link:https://github.com/spotify/spotify-tensorflow>spotify/spotify-tensorflow</denchmark-link>
 .
 	",1.0,brianmartin,2018-10-24T06:44:07Z,"
 		Thank you for your post. We noticed you have not filled out the following field in the issue template. Could you update them if they are relevant in your case, or leave them as N/A? Thanks.
 Have I written custom code
 OS Platform and Distribution
 TensorFlow installed from
 Bazel version
 CUDA/cuDNN version
 GPU model and memory
 Exact command to reproduce
 Mobile device
 		",2.0,brianmartin,2018-10-24T14:28:33Z,"
 		Updated!
 		",3.0,brianmartin,2018-10-26T01:24:06Z,"
 		<denchmark-link:https://github.com/shivaniag>@shivaniag</denchmark-link>
  As we discussed offline, can you please take a look at this issue, in case there’s a bug in ? Thanks!
 		",95de98b58a35aaac2804716a70979e68596f3dae,Shivani Agrawal,2018-10-30 18:36:11-07:00,MODIFY,1,tensorflow\core\kernels\data\parse_example_dataset_op.cc,tensorflow\core\kernels\data\parse_example_dataset_op.cc,1.0,"145,146,147,148,149","145,146,147,148,149",MODIFY,25.0,tensorflow\python\data\experimental\kernel_tests\parse_example_dataset_test.py,tensorflow\python\data\experimental\kernel_tests\parse_example_dataset_test.py,4.0,brianmartin,2018-10-30T14:56:30Z,"
 		I've dug in a bit more. For a FixedLenFeature, this test case still succeeds on the first read and fails on the second read -- no change there. But it fails with an exception rather than the segfault we see with VarLenFeature.
 I've updated the gist to reproduce in tensorflow>=1.11.0 here: <denchmark-link:https://gist.github.com/brianmartin/4f221eb838dce8099342f829fb13253d>https://gist.github.com/brianmartin/4f221eb838dce8099342f829fb13253d</denchmark-link>
 
 The exception on the second, failing read (after seeing the first read succeed):
 <denchmark-code>Attempting first read..
 2018-10-30 10:53:26.766551: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 {'f0': <tf.Tensor: id=26, shape=(1, 1), dtype=int64, numpy=array([[1]])>}
 Attempting second identical read..
 Traceback (most recent call last):
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/scratch.py"", line 109, in <module>
     main()
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/scratch.py"", line 104, in main
     run(make_it_fail, read_fn, var_len=var_len)
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/scratch.py"", line 79, in run
     _print_first(read_fn(data_file, feature_spec))
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/scratch.py"", line 35, in _print_first
     print next(xs.__iter__())
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 139, in __iter__
     return iterator_ops.EagerIterator(self)
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 521, in __init__
     ds_variant = dataset._as_variant_tensor()  # pylint: disable=protected-access
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/contrib/data/python/ops/parsing_ops.py"", line 90, in _as_variant_tensor
     **dataset_ops.flat_structure(self))
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3963, in parse_example_dataset
     ctx=_ctx)
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 4015, in parse_example_dataset_eager_fallback
     attrs=_attrs, ctx=_ctx, name=name)
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""/Users/brianm/wrk/spotify-tensorflow-brianmartin/venv/lib/python2.7/site-packages/six.py"", line 737, in raise_from
     raise value
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected len(dense_defaults) == len(dense_keys) but got: 1 vs. 0 [Op:ParseExampleDataset]
 </denchmark-code>
 
 		",5.0,brianmartin,2018-10-31T00:18:08Z,"
 		<denchmark-link:https://github.com/brianmartin>@brianmartin</denchmark-link>
  Thank you very much for narrowing down the bug, we have fixed it and are going to submit it soon, will update you when that is done.
 		",,,,,1.0,"537,538",,testSkipEagerSerializedSparseAndSparseFeatureAndDenseWithNoDefault,self,537,538,MODIFY,2.0,tensorflow\python\framework\test_util.py,tensorflow\python\framework\test_util.py,1.0,"767,768",767,,,,,,,,tensorflow::data::ParseExampleDatasetOp::MakeDataset,"ctx,input,output",72,150,,,,,,,,,,,,,,,,,,,,,,1.0,"266,301,302","295,297",testSkipEagerSerializedContainingSparseFeature,self,266,302,1.0,"694,695,697,702,762,763","685,745",testSerializedContainingVarLenDense,self,685,846,1.0,"534,535,537,538","524,580",testSerializedContainingSparseAndSparseFeatureAndDenseWithNoDefault,self,524,580,1.0,"121,159,160",155,testSkipEagerEmptySerializedWithAllDefaults,self,121,160,1.0,"263,264,266","260,295",testSerializedContainingSparseFeature,self,260,295,,,,,run_all_in_graph_and_eager_modes,cls,763,770,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,836,835,decorator,f,832,872,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"472,473",462,testSerializedContainingDenseWithConcat,self,430,473,1.0,,82,_test,"self,input_tensor,feature_val,expected_values,expected_err",78,82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"694,695","678,680,685",_testSerializedContainingVarLenDenseLargerBatch,"self,batch_size",637,695,1.0,"425,426",416,testSerializedContainingDense,self,393,426,1.0,"117,118,119,120,121","117,155",testEmptySerializedWithAllDefaults,self,117,155,1.0,"344,345,347","339,382",testSerializedContaining3DSparseFeature,self,339,382,1.0,"347,390,391",382,testSkipEagerSerializedContaining3DSparseFeature,self,347,391,1.0,"84,85","82,83,84,85",_test,"self,input_tensor,feature_val,expected_values,expected_err,create_iterator_twice",80,85,1.0,"534,535","522,524",testSerializedContainingDenseWithDefaults,self,499,535,1.0,,680,testSerializedContainingVarLenDenseLargerBatch,self,680,683,1.0,222,"217,258",testSerializedContainingSparse,self,217,258,1.0,"222,263,264","258,260",testSkipEagerSerializedContainingSparse,self,222,264,1.0,"597,634,635",619,testSkipEagererializedContainingSparseAndSparseFeatureWithReuse,self,597,635,1.0,"304,344,345","337,339",testSkipEagerSerializedContainingSparseFeatureReuse,self,304,345,1.0,697,,testSkipEagerSerializedContainingVarLenDenseLargerBatch,self,697,700,1.0,"594,595,597","582,619",testSerializedContainingSparseAndSparseFeatureWithReuse,self,582,619,1.0,"496,497",485,testSerializedContainingDenseScalar,self,475,497,1.0,"702,762,763",745,testSkipEagerSerializedContainingVarLenDense,self,702,864,1.0,"301,302,304","297,337",testSerializedContainingSparseFeatureReuse,self,297,337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23443,xiaofei06,2018-11-01T23:45:42Z,2018-11-02T23:41:40Z,Python client link is broken,"
 Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
 System information
 
 TensorFlow version: unrelated, I'm using github ui viewing the code
 Doc Link: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/client/client_lib.py#L18
 
 
 This link <denchmark-link:https://tensorflow.org/api_guides/python/client>https://tensorflow.org/api_guides/python/client</denchmark-link>
  gives 404
 We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
 	",1.0,xiaofei06,2018-11-02T16:21:31Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
   -  PTAL
 		",2.0,xiaofei06,2018-11-02T18:06:14Z,"
 		Hi <denchmark-link:https://github.com/xiaofei06>@xiaofei06</denchmark-link>
 ,
 Thanks for the report.
 Normally I'd just ask you to send a PR, but this is a little bigger than it looks.
 It affects ~20 files. A fix is on it's way.
 		",,,,,09f82cbffdcaf5298bcbd4703c18b64ddd287aab,Mark Daoust,2018-11-02 16:39:17-07:00,MODIFY,0,tensorflow\contrib\crf\__init__.py,tensorflow\contrib\crf\__init__.py,0.0,,"17,18",MODIFY,0.0,tensorflow\contrib\framework\__init__.py,tensorflow\contrib\framework\__init__.py,,,,,,,,,,,,,0.0,,"18,19,20,21",,,,,MODIFY,0.0,tensorflow\contrib\layers\__init__.py,tensorflow\contrib\layers\__init__.py,0.0,,"17,18,19,20",MODIFY,0.0,tensorflow\contrib\learn\__init__.py,tensorflow\contrib\learn\__init__.py,0.0,,"22,23,24",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\contrib\rnn\__init__.py,tensorflow\contrib\rnn\__init__.py,0.0,,"17,18",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\contrib\util\__init__.py,tensorflow\contrib\util\__init__.py,0.0,,"18,19",,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\g3doc\convert\cmdline_examples.md,tensorflow\lite\g3doc\convert\cmdline_examples.md,0.0,"98,99,100,101","98,99,100,101,102",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\client\client_lib.py,tensorflow\python\client\client_lib.py,0.0,18,18,MODIFY,0.0,tensorflow\python\debug\__init__.py,tensorflow\python\debug\__init__.py,0.0,17,17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\lib\io\python_io.py,tensorflow\python\lib\io\python_io.py,MODIFY,0.0,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,MODIFY,0.0,tensorflow\python\ops\check_ops.py,tensorflow\python\ops\check_ops.py,MODIFY,0.0,tensorflow\python\ops\control_flow_ops.py,tensorflow\python\ops\control_flow_ops.py,MODIFY,0.0,tensorflow\python\ops\functional_ops.py,tensorflow\python\ops\functional_ops.py,MODIFY,0.0,tensorflow\python\ops\session_ops.py,tensorflow\python\ops\session_ops.py,0.0,16,"16,17,18,19,20",MODIFY,0.0,tensorflow\python\ops\sparse_ops.py,tensorflow\python\ops\sparse_ops.py,0.0,19,19,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\ops\state_ops.py,tensorflow\python\ops\state_ops.py,0.0,18,18,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\ops\string_ops.py,tensorflow\python\ops\string_ops.py,0.0,16,"16,17,18,19",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,16,"16,17,18,19",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,16,"16,17,18,19",0.0,16,"16,17,18,19,20",0.0,17,"17,18",0.0,16,"16,17,18,19,20",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23449,shivamkhare95,2018-11-02T07:25:15Z,2018-11-08T17:45:52Z,Bug in tensorflow lite java wrapper,"
 In file
 ""<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/java/src/main/java/org/tensorflow/lite/NativeInterpreterWrapper.java</denchmark-link>
 "" line 286:
 *
 In this function, output tensors.length should be used instead of inputTensors.length
 	",1.0,shivamkhare95,2018-11-08T00:48:21Z,"
 		Wow, good catch, that's a nasty bug. Will have a fix up shortly, thanks for the feedback.
 		",,,,,,,,,816426f66a9b3edbbdf4203684f7753b2974866d,Jared Duke,2018-11-08 09:43:27-08:00,MODIFY,0,tensorflow\lite\java\BUILD,tensorflow\lite\java\BUILD,0.0,143,,MODIFY,1.0,tensorflow\lite\java\src\main\java\org\tensorflow\lite\NativeInterpreterWrapper.java,tensorflow\lite\java\src\main\java\org\tensorflow\lite\NativeInterpreterWrapper.java,,,,,,,,,,,,,1.0,286,286,NativeInterpreterWrapper::getOutputTensorCount,,285,287,MODIFY,1.0,tensorflow\lite\java\src\test\java\org\tensorflow\lite\InterpreterFlexTest.java,tensorflow\lite\java\src\test\java\org\tensorflow\lite\InterpreterFlexTest.java,1.0,"43,44,45,46,48,49,50,51,52,53","41,43",MODIFY,1.0,tensorflow\lite\java\src\test\java\org\tensorflow\lite\InterpreterTest.java,tensorflow\lite\java\src\test\java\org\tensorflow\lite\InterpreterTest.java,1.0,"170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,186,187,189,190,191,192","169,170,171,172,173,175,177,178,179,180",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,InterpreterFlexTest::testFlexModel,,39,55,InterpreterTest::testRunForMultipleInputsOutputs,,169,193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23748,JamesGlooTeam,2018-11-14T16:19:40Z,2020-02-06T19:47:02Z,tensorflow.keras Dense layers complain if the input is a sparse Input layer.,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 OSX Mojave 10.14.1
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 na
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 1.10.0 (v1.10.0-rc1-19-g656e7a2b34)
 Python version:
 3.5
 Bazel version (if compiling from source):
 NA
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with
 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 Describe the current behavior
 See below, which I ran on a OSX Mojave Macbook Pro (Early 2015), ipython running python 3.5, tensorflow 1.10.0:
 <denchmark-code>In [1]: from tensorflow.keras.models import Model
 //anaconda/envs/dssm/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
   return f(*args, **kwds)
 
 In [2]: from tensorflow.keras.layers import Input, Dense
 
 In [3]: i = Input((4,), sparse=True)
 
 In [4]: d = Dense(4)(i)
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-4-0fb73bda26dc> in <module>
 ----> 1 d = Dense(4)(i)
 
 //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     718
     719         # Check input assumptions set before layer building, e.g. input rank.
 --> 720         self._assert_input_compatibility(inputs)
     721         if input_list and self._dtype is None:
     722           try:
 
 //anaconda/envs/dssm/lib/python3.5/site-packages/tensorflow/python/keras/engine/base_layer.py in _assert_input_compatibility(self, inputs)
    1408           spec.min_ndim is not None or
    1409           spec.max_ndim is not None):
 -> 1410         if x.shape.ndims is None:
    1411           raise ValueError('Input ' + str(input_index) + ' of layer ' +
    1412                            self.name + ' is incompatible with the layer: '
 
 AttributeError: 'SparseTensor' object has no attribute 'shape'
 </denchmark-code>
 
 Describe the expected behavior
 If I were using normal Keras, I'd expect no errors trying to do the above and for the model to compile subsequently without issue.
 Code to reproduce the issue
 See the code in my snippet above.
 Other info / logs
 	",1.0,JamesGlooTeam,2018-12-03T21:33:55Z,"
 		<denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  -- I recall you worked on something similar recently; can you comment on what is expected here?
 Note that in 1.12 the above gives a different error:
 <denchmark-code>---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 <ipython-input-4-0347ad7938f4> in <module>()
       2 from tensorflow.keras.layers import Input, Dense
       3 i = Input(shape=(4,), sparse=True)
 ----> 4 d = Dense(4)(i)
 
 google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     532       if not self.built:
     533         # Build layer if applicable (if the `build` method has been overridden).
 --> 534         self._maybe_build(inputs)
     535         # We must set self.built since user defined build functions are not
     536         # constrained to set self.built.
 
 google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
    1592     # Only call `build` if the user has manually overridden the build method.
    1593     if not hasattr(self.build, '_is_default'):
 -> 1594       self.build(input_shapes)
    1595 
    1596 
 
 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape)
     928     input_shape = tensor_shape.TensorShape(input_shape)
     929     if tensor_shape.dimension_value(input_shape[-1]) is None:
 --> 930       raise ValueError('The last dimension of the inputs to `Dense` '
     931                        'should be defined. Found `None`.')
     932     last_dim = tensor_shape.dimension_value(input_shape[-1])
 
 ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
 </denchmark-code>
 
 		",2.0,JamesGlooTeam,2018-12-03T21:59:17Z,"
 		<denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
  <denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  I looked into this issue last week. There are several things that need to be fixed.
 One is that sparse.placeholder could not recognize . It will always converts to  so that the error of:
 <denchmark-code>ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
 </denchmark-code>
 
 I created a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24048>#24048</denchmark-link>
  to fix the above shape inforamtion issue first.
 There might be some other places that need to be fixed to make <denchmark-link:https://github.com/JamesGlooTeam>@JamesGlooTeam</denchmark-link>
  example work though.
 (Note: as pointed out by <denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
 , the error is based in 1.12 and is different from the original error posted by <denchmark-link:https://github.com/JamesGlooTeam>@JamesGlooTeam</denchmark-link>
  )
 		",3.0,JamesGlooTeam,2018-12-03T22:12:55Z,"
 		This is because for SparseTensors, the shape is actually a Tensor. Tensors can't have None values.
 You can get around this on the Input layer by defining your batch_size:
 x = keras.Input(batch_size=10, shape=(4,), sparse=True)
 However, Dense layers (and most layers in general it seems) don't support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.
 		",74195b50fb5e1f22eb95ffd1646b4b0ceca2ea9b,Mark Daoust,2020-02-06 11:38:47-08:00,MODIFY,1,tensorflow\python\client\session_test.py,tensorflow\python\client\session_test.py,1.0,811,811,MODIFY,3.0,tensorflow\python\framework\sparse_tensor.py,tensorflow\python\framework\sparse_tensor.py,4.0,JamesGlooTeam,2018-12-03T22:14:37Z,"
 		Ideally, the shape of a SparseTensor would probably be a TensorShape, but I think that would be a pretty substantial rewrite of a lot of the sparse ops in order to allow None values to flow through
 		",5.0,JamesGlooTeam,2018-12-03T22:55:41Z,"
 		<denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  The PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24048>#24048</denchmark-link>
  propose to convert  to  for the shape tensor then the information could be preserved without making drastic changes.
 		",6.0,JamesGlooTeam,2019-03-04T17:52:54Z,"
 		Hi,
 I got the same issue, if the input is sparse then even when I apply the Dense layer I got an error.
 That's too bad because to train a neural network, specifying sparse=True in the input layer makes the learning phase 5 times faster ...
 Anyone has a solution for that ?
 Thanks a lot.
 		",1.0,190,,get_shape,self,184,190,MODIFY,1.0,tensorflow\python\keras\engine\training.py,tensorflow\python\keras\engine\training.py,1.0,"2237,2238,2239,2240,2241","2235,2236,2237,2238,2239,2240,2241,2242,2243,2244",MODIFY,3.0,tensorflow\python\keras\utils\composite_tensor_support_test.py,tensorflow\python\keras\utils\composite_tensor_support_test.py,1.0,"719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736",,testFeedSparsePlaceholderConstantShape,self,803,822,,,,,,,,,,,,,,,,,,,,,,1.0,"124,125,126,127,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,172","131,132,153",__init__,"self,indices,values,dense_shape",117,182,1.0,233,,shape,self,227,233,,,,,,,,,,,,,,,,,,,,,,7.0,JamesGlooTeam,2019-03-21T02:43:22Z,"
 		
 @omalleyt12 -- I recall you worked on something similar recently; can you comment on what is expected here?
 Note that in 1.12 the above gives a different error:
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 <ipython-input-4-0347ad7938f4> in <module>()
       2 from tensorflow.keras.layers import Input, Dense
       3 i = Input(shape=(4,), sparse=True)
 ----> 4 d = Dense(4)(i)
 
 google3/third_party/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     532       if not self.built:
     533         # Build layer if applicable (if the `build` method has been overridden).
 --> 534         self._maybe_build(inputs)
     535         # We must set self.built since user defined build functions are not
     536         # constrained to set self.built.
 
 google3/third_party/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
    1592     # Only call `build` if the user has manually overridden the build method.
    1593     if not hasattr(self.build, '_is_default'):
 -> 1594       self.build(input_shapes)
    1595 
    1596 
 
 google3/third_party/tensorflow/python/keras/layers/core.py in build(self, input_shape)
     928     input_shape = tensor_shape.TensorShape(input_shape)
     929     if tensor_shape.dimension_value(input_shape[-1]) is None:
 --> 930       raise ValueError('The last dimension of the inputs to `Dense` '
     931                        'should be defined. Found `None`.')
     932     last_dim = tensor_shape.dimension_value(input_shape[-1])
 
 ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
 
 
 i got the same error in tf1.13. Anyones has a quick local fix solution?
 Thanks a lot!
 		",_type_spec_from_value,value,2235,2244,test_ragged_tensor_model_predict,self,719,736,MODIFY,1.0,tensorflow\python\keras\utils\tf_utils.py,tensorflow\python\keras\utils\tf_utils.py,1.0,"387,388,389,390,391,392,393,394,395,396",,type_spec_from_value,value,387,396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\kernel_tests\sparse_ops_test.py,tensorflow\python\kernel_tests\sparse_ops_test.py,1.0,1040,1040,testPartialShapePlaceholder,self,1038,1041,,,,,,,,MODIFY,2.0,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,1.0,"3057,3061,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3096,3098,3099,3100,3101,3102,3103","3011,3012,3013,3014,3015,3016,3017,3018,3019,3068,3072,3074,3075,3084",sparse_placeholder,"dtype,shape,name",3011,3103,8.0,JamesGlooTeam,2019-03-21T17:27:40Z,"
 		SparseTensors aren't supported in Dense layer currently, although this is something we are considering. You can implement a custom layer that calls tf.sparse.sparse_dense_matmul on your SparseTensor
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717",,test_sparse_tensor_model_predict,self,697,717,1.0,"692,693,694,695",,_normalize_shape,"self,shape",692,695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"3010,3011,3012,3013,3014,3015,3016,3017",_normalize_sparse_shape,"shape,name",3010,3017,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,JamesGlooTeam,2019-04-25T14:09:47Z,"
 		<denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  Is there some progress on this issue?
 I'd like to build a large sparse logistic regression model with Keras and having a dense layer supporting sparse input in Keras would be quite cool.
 
 Should I wait for such a feature landing in Keras or should I implement my own layer?
 Is there already an existing snippet with such a layer somewhere?
 
 		",10.0,JamesGlooTeam,2019-07-22T13:28:24Z,"
 		Maybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : <denchmark-link:https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1>https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1</denchmark-link>
 
 		",11.0,JamesGlooTeam,2019-08-26T02:35:10Z,"
 		<denchmark-link:https://github.com/SharoneDayan>@SharoneDayan</denchmark-link>
 
 
 Maybe, the medium article I wrote on ingesting sparse inputs in Tensorflow Keras can help you out : https://medium.com/dailymotion/how-to-design-deep-learning-models-with-sparse-inputs-in-tensorflow-keras-fd5e754abec1
 
 On TensorFlow 2.0.0-rc0 I get ""ValueError: The two structures don't have the same nested structure."" trying your DenseLayerForSparse layer. Using sparse inputs as to regular Dense gives the ""ValueError: The last dimension of the inputs to Dense should be defined. Found None."" There doesn't seem to be any simple fix to this that doesn't involve in-depth understanding of TF2 internals..
 Fixing the batch_size to all inputs as <denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  suggests gives the same ""two structures don't have the same nested structure"" error, and it seems that SparseTensorSpec gets the training dataset size as its first axis, whereas the SparseTensor -structure gets the batch_size as its first axis. Same when trying the DenseLayerSparse workaround.
 Sparse inputs have worked out of the box for Keras so far, as in every other DL backend. These are a common use case with document and graph data, so it's strange that this feature is not working so close to TF2 full release, especially since SparseTensors are one of the advertised new features.
 		",12.0,JamesGlooTeam,2019-08-27T08:49:20Z,"
 		<denchmark-link:https://github.com/anttttti>@anttttti</denchmark-link>
 
 Possibly related:
 <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31999>#31999</denchmark-link>
 
 		",13.0,JamesGlooTeam,2019-09-24T22:11:35Z,"
 		Can confirm that updating to >=1.14 breaks most code dealing with sparse data, which is especially relevant for graphs and text as <denchmark-link:https://github.com/anttttti>@anttttti</denchmark-link>
  mentioned.
 This does not seem to be an issue that can be easily worked around, and the only stable solution is to use Keras 2.2.5 (not 2.3, because that's broken too since it was made to be similar to tf.keras).
 This seems like a huge step back and effectively makes it impossible to use sparse tensors in a Keras model.
 		",14.0,JamesGlooTeam,2019-10-12T04:09:17Z,"
 		For tensorflow 2.0 (not rc):
 x = Input(shape=(32,), sparse=True)
 y = Dense(1, activation='sigmoid')(x)
 model = Model(x, y)
 I am still getting:
 ValueError: The last dimension of the inputs to Dense should be defined. Found None.
 Anyone know how to resolve?
 		",15.0,JamesGlooTeam,2019-11-22T18:43:29Z,"
 		Hi <denchmark-link:https://github.com/kechan>@kechan</denchmark-link>
 , did you figure out the solution? I'm running into the same issue too.
 		",16.0,JamesGlooTeam,2019-11-22T20:16:25Z,"
 		<denchmark-link:https://github.com/quangkevin>@quangkevin</denchmark-link>
 
 No. I haven't looked into it again. I thought i just use Masking for my particular model and move on. But try install tf.nightly and see if the issue is still there. it appears this ""bug"" has been opened for a long while.
 		",17.0,JamesGlooTeam,2019-11-26T22:31:45Z,"
 		
 This is because for SparseTensors, the shape is actually a Tensor. Tensors can't have None values.
 You can get around this on the Input layer by defining your batch_size:
 x = keras.Input(batch_size=10, shape=(4,), sparse=True)
 However, Dense layers (and most layers in general it seems) don't support sparse inputs, so you would need to subclass Layer in order to call tf.sparse.sparse_dense_matmul on your inputs, or create a Lambda layer to convert your sparse inputs to dense.
 
 Hi omalleyt12,
 I have tried using the tf.sparse.sparse_dense_matmul() function to convert my sparse tensor into a dense one. But it still cannot fit into a dense layer:
 for p, dim in zip(adjacency_powers, dim_per_power):
 net_p = adj_times_x(sparse_adjacency, x, p)
 <denchmark-code>with tf.variable_scope('r%i_l%i_p%s' % (replica, layer_id, str(p))):
   layer = tf.layers.Dense(
       dim,
       kernel_regularizer=kernel_regularizer,
       activation=None, use_bias=False)
   net_p = layer.apply(net_p)
 </denchmark-code>
 
 def adj_times_x(adj, x, adj_pow=1):
 """"""Multiplies (adj^adj_pow)*x.""""""
 for i in range(adj_pow):
 x = tf.sparse_tensor_dense_matmul(adj, x)
 return x
 and the error is still ""ValueError: The last dimension of the inputs to Dense should be defined. Found None.""
 Any suggestions regarding this?
 Thanks
 		",18.0,JamesGlooTeam,2020-01-12T22:34:13Z,"
 		This is not fixed in 2.1.0 :(
 This is a major blocker for the <denchmark-link:https://danielegrattarola.github.io/spektral/>Spektral</denchmark-link>
  library. Can we get some attention on this issue?
 		",19.0,JamesGlooTeam,2020-02-06T18:54:49Z,"
 		<denchmark-link:https://github.com/cgarciae>@cgarciae</denchmark-link>
  -- can you file a new bug with a minimal repro in 2.x? This bug is very old, and it's hard to tell exactly what condition set doesn't work here.
 		",20.0,JamesGlooTeam,2020-02-06T18:54:51Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748>No</denchmark-link>
 
 		",21.0,JamesGlooTeam,2020-02-06T19:41:42Z,"
 		Hi <denchmark-link:https://github.com/karmel>@karmel</denchmark-link>
 ,
 I just submitted the fix for this. It should be synced out shortly.
 		",22.0,JamesGlooTeam,2020-02-06T19:47:04Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/23748>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23878,javiermas,2018-11-20T11:31:19Z,2018-12-18T16:50:57Z,Bug: instantiating dynamic_rnn with tf.int32 in input and state raises TypeError,"
 System information
 
 OS Platform: OS X 10.13.3
 Custom code
 tensorflow version: 1.12.0
 python version: 3.6.5
 
 Describe the current behavior
 Tensorflow raises a TypeError when creating a dynamic_rnn with tf.int32 type in its input and state. When changing the type to tf.float32 the error is not raised.
 Describe the expected behavior
 Ideally, a dynamic_rnn should support tf.in32 types. If there's any reason why instantiating a dynamic_rnn with tf.int32 type in its input and state should not be allowed, a custom error should be raised.
 Code to reproduce the issue
 The code below reproduces the error:
 <denchmark-code>import tensorflow as tf
 
 X = tf.placeholder(tf.int32, [None, 10, 1])
 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)
 
 </denchmark-code>
 
 The code below doesn't:
 <denchmark-code>
 import tensorflow as tf
 
 X = tf.placeholder(tf.float32, [None, 10, 1])
 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.float32)
 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.float32)
 
 </denchmark-code>
 
 Note the change in dtype.
 <denchmark-h:h2>Other info / logs
 TRACEBACK:</denchmark-h>
 
 TypeError                                 Traceback (most recent call last)
  in ()
 2 X = tf.placeholder(tf.int32, [None, 10, 1])
 3 cell = tf.nn.rnn_cell.LSTMCell(1, dtype=tf.int32)
 ----> 4 output, state = tf.nn.dynamic_rnn(cell=cell, inputs=X, dtype=tf.int32)#, initial_state=state)
 5
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)
 662         swap_memory=swap_memory,
 663         sequence_length=sequence_length,
 --> 664         dtype=dtype)
 665
 666     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)
 870       parallel_iterations=parallel_iterations,
 871       maximum_iterations=time_steps,
 --> 872       swap_memory=swap_memory)
 873
 874   # Unpack final output if not using output tuples.
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)
 3289       ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)
 3290     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants,
 -> 3291                                     return_same_structure)
 3292     if maximum_iterations is not None:
 3293       return result[1]
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants, return_same_structure)
 3002       with ops.get_default_graph()._mutation_lock():  # pylint: disable=protected-access
 3003         original_body_result, exit_vars = self._BuildLoop(
 -> 3004             pred, body, original_loop_vars, loop_vars, shape_invariants)
 3005     finally:
 3006       self.Exit()
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)
 2937         flat_sequence=vars_for_body_with_tensor_arrays)
 2938     pre_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
 -> 2939     body_result = body(*packed_vars_for_body)
 2940     post_summaries = ops.get_collection(ops.GraphKeys._SUMMARY_COLLECTION)  # pylint: disable=protected-access
 2941     if not nest.is_sequence(body_result):
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in (i, lv)
 3258         cond = lambda i, lv: (  # pylint: disable=g-long-lambda
 3259             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))
 -> 3260         body = lambda i, lv: (i + 1, orig_body(*lv))
 3261
 3262     if context.executing_eagerly():
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in _time_step(time, output_ta_t, state)
 838           skip_conditionals=True)
 839     else:
 --> 840       (output, new_state) = call_cell()
 841
 842     # Keras cells always wrap state as list, even if it's a single tensor.
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py in ()
 824     if is_keras_rnn_cell and not nest.is_sequence(state):
 825       state = [state]
 --> 826     call_cell = lambda: cell(input_t, state)
 827
 828     if sequence_length is not None:
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state, scope, *args, **kwargs)
 368     # method.  See the class docstring for more details.
 369     return base_layer.Layer.call(self, inputs, state, scope=scope,
 --> 370                                      *args, **kwargs)
 371
 372
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/layers/base.py in call(self, inputs, *args, **kwargs)
 372
 373       # Actually call layer
 --> 374       outputs = super(Layer, self).call(inputs, *args, **kwargs)
 375
 376     if not context.executing_eagerly():
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in call(self, inputs, *args, **kwargs)
 755       if not in_deferred_mode:
 756         self._in_call = True
 --> 757         outputs = self.call(inputs, *args, **kwargs)
 758         self._in_call = False
 759         if outputs is None:
 ~/jonassucks3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)
 1003            sigmoid(i + self._w_i_diag * c_prev) * self._activation(j))
 1004     else:
 -> 1005       c = (sigmoid(f + self._forget_bias) * c_prev + sigmoid(i) *
 1006            self._activation(j))
 1007
 TypeError: unsupported operand type(s) for +: 'Tensor' and 'float'
 	",1.0,javiermas,2018-11-26T20:33:53Z,"
 		Thanks for the clear repro example. Probably this is a duplicate of <denchmark-link:https://github.com/tensorflow/tensorflow/issues/14729>#14729</denchmark-link>
 
 		",2.0,javiermas,2018-11-27T15:30:06Z,"
 		Thanks for the report, I will take a look today or tomorrow.
 		",3.0,javiermas,2018-12-17T18:25:48Z,"
 		Sorry for the very late reply. After some digging, the root cause is because of the default forget gate bias being initialized as float. However, even casting it to be int32 will still causing the code to fail down the road since the activation function for LSTM (tanh and sigmoid) only support floating numbers. So the conclusion is that LSTM will only support floating numbers as the dtype for input and states.
 I will update the code to have a clear error message when the input dtype is not float number.
 		",36304bc4ceb6140e470420b65ce470092fc47ab2,Scott Zhu,2018-12-17 14:50:42-08:00,MODIFY,1,tensorflow\python\kernel_tests\rnn_test.py,tensorflow\python\kernel_tests\rnn_test.py,1.0,"176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193",,MODIFY,4.0,tensorflow\python\ops\rnn_cell_impl.py,tensorflow\python\ops\rnn_cell_impl.py,,,,,,,,,,,,,1.0,"1553,1554,1555,1556,1557,1558,1559",,_check_supported_dtypes,dtype,1553,1559,,,,,,,,,,,,,,,testInvalidDtype,self,176,193,,,,,,,,,,,,,,,,,,,,,,1.0,"1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550",,_check_rnn_cell_input_dtypes,inputs,1534,1550,1.0,436,,build,"self,inputs_shape",432,447,1.0,451,449,call,"self,inputs,state",449,456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23924,w4-sjcho,2018-11-22T14:45:32Z,2018-11-26T19:11:44Z,Memory leak when using tf.contrib.data.unbatch(),"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
 Python version: Python 3.6.7 :: Anaconda, Inc.
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 9.0
 GPU model and memory: 1080ti
 
 Describe the current behavior
 Memory usage continuously increase when using tf.contrib.data.unbatch().
 Describe the expected behavior
 Memory usage should not increase.
 Code to reproduce the issue
 <denchmark-code>from absl import app
 from absl import flags
 from absl import logging
 import tensorflow as tf
 
 
 FLAGS = flags.FLAGS
 flags.DEFINE_integer('epochs', 1000, '')
 flags.DEFINE_boolean('use_unbatch', False, '')
 
 
 def create_dataset(input_holder):
     dataset = tf.data.Dataset.from_tensor_slices((input_holder,))
 
     def generate_random_tensor(size):
         return tf.random_uniform([5, size, size], dtype=tf.float32)
 
     dataset = dataset.map(generate_random_tensor)
     if FLAGS.use_unbatch:
         dataset = dataset.apply(tf.contrib.data.unbatch())
     else:
         dataset = dataset.flat_map(
             lambda x: tf.data.Dataset.from_tensor_slices((x,)))
         # The output of the dataset becomes a single-element tuple w/o this.
         dataset = dataset.map(lambda x: x)
     return dataset
 
 
 def main(_):
     with tf.Session() as sess:
         size_holder = tf.placeholder(tf.int32, shape=[None])
         dataset = create_dataset(size_holder)
 
         iterator = dataset.make_initializable_iterator()
         get_next = iterator.get_next()
 
         for i in range(FLAGS.epochs):
             logging.info('Epoch #%d', i)
             sess.run(iterator.initializer, feed_dict={
                 size_holder: [1000 + (i % 100)],
             })
             try:
                 while True:
                     array = sess.run(get_next)
                     logging.info('  Generated: %s', array.shape)
             except tf.errors.OutOfRangeError:
                 pass
 
 
 if __name__ == '__main__':
     app.run(main)
 </denchmark-code>
 
 Memory usage will increase with --use_unbatch, while with --nouse_unbatch, memory usage does not increase.
 Other info / logs
 It seems like  call is missing in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/unbatch_dataset_op.cc#L42>UnbatchDatasetOp</denchmark-link>
 .
 	",1.0,w4-sjcho,2018-11-22T16:13:45Z,"
 		Possibly related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/23904>#23904</denchmark-link>
 
 		",2.0,w4-sjcho,2018-11-26T18:43:15Z,"
 		Thanks for tracking down the issue: indeed, the missing input_->Unref() seems to be the culprit.
 <denchmark-link:https://github.com/artsobolev>@artsobolev</denchmark-link>
  I think the root cause for this bug is different. I'll comment on the other thread.
 		",,,,,bb425754adacc784c2ad50ed94307eaa03626b41,Derek Murray,2018-11-26 11:08:59-08:00,MODIFY,1,tensorflow\core\kernels\data\unbatch_dataset_op.cc,tensorflow\core\kernels\data\unbatch_dataset_op.cc,1.0,57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::data::UnbatchDatasetOp::Dataset::~Dataset,,57,57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23987,lxl910915,2018-11-27T03:10:14Z,2018-11-28T02:38:29Z,LARSOptimizer does not initialize _learning_rate_tensor and _momentum_tensor,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution: Linux Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: tags/1.12
 Python version: 2.7
 cpu mode
 
 Describe the current behavior
 'LARSOptimizer' object has no attribute '_learning_rate_tensor'
 Code to reproduce the issue
 \tensorflow\tensorflow\contrib\opt\python\training\lars_optimizer_test.py
 	",1.0,lxl910915,2018-11-27T03:11:17Z,"
 		class LARSOptimizer should implement _prepare() method.
 		",2.0,lxl910915,2018-11-27T04:40:45Z,"
 		MR: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/6d3d0de39f3eaecdbf7fbdb52a7a02b2539efeeb>6d3d0de</denchmark-link>
 
 		",,,,,6d3d0de39f3eaecdbf7fbdb52a7a02b2539efeeb,qiezi,2018-11-27 11:18:09+08:00,MODIFY,1,tensorflow\contrib\opt\python\training\lars_optimizer.py,tensorflow\contrib\opt\python\training\lars_optimizer.py,1.0,"167,168,169,170,171,172,173,174,175,176",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_prepare,self,167,176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24286,clamagdeleine,2018-12-11T08:38:38Z,2018-12-20T02:08:39Z,wrong command for cloning tensorflow in TF for microcontroller doc,"
 Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
 System information
 
 TensorFlow version: master
 Doc Link: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/micro
 
 Describe the documentation issue
 In ""Getting Started"" chapter, the command for cloning tensorflos is:
 Download the TensorFlow source with git clone <denchmark-link:https://github.com/tensorflow>https://github.com/tensorflow</denchmark-link>
 
 It should be :
 git clone <denchmark-link:https://github.com/tensorflow/tensorflow.git>https://github.com/tensorflow/tensorflow.git</denchmark-link>
 
 We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
 Yes
 	",1.0,clamagdeleine,2018-12-19T20:18:11Z,"
 		Thanks for the report <denchmark-link:https://github.com/clamagdeleine>@clamagdeleine</denchmark-link>
 ,
 I've got a fix inflight.
 In the future, feel free to send a PR instead of an issue.
 		",,,,,,,,,caf2d701d27bf5b2e9378db5dfa63e08054ff497,Mark Daoust,2018-12-19 18:07:15-08:00,MODIFY,0,tensorflow\lite\experimental\micro\README.md,tensorflow\lite\experimental\micro\README.md,0.0,34,34,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24414,jayfurmanek,2018-12-18T06:06:14Z,2019-04-01T21:41:13Z,ppc64le: no_mkl_dnn_contraction_kernel define causes build failure,"
 Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 ppc64le
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): source
 TensorFlow version: master
 Python version: 3.6
 Installed using virtualenv? pip? conda?:  source
 Bazel version (if compiling from source): 0.19.2
 GCC/Compiler version (if compiling from source):  4.8
 CUDA/cuDNN version:  10.0/7.3
 GPU model and memory: V100
 
 
 A recent commit (<denchmark-link:https://github.com/tensorflow/tensorflow/commit/10ef7edc881ee715eaae48656fcb431fe128441f>10ef7ed</denchmark-link>
 ) changed the default contraction kernel to be MKL based. Code was added for non-intel platforms to avoid MKL, but it has an error.
 Provide the exact sequence of commands / steps that you executed before running into the problem
 Basic build: bazel build //tensorflow/tools/pip_package:build_pip_package
 Any other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 <denchmark-code>ERROR: /home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core/kernels:eigen_contraction_kernel:
 //tensorflow:linux_ppc64le
 //tensorflow/core/kernels:no_mkldnn_contraction_kernel
 Multiple matches are not allowed unless one is unambiguously more specialized.
 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 
 
 /home/jenkins/workspace/TensorFlow_PPC64LE_GPU_Build/tensorflow/core/kernels/BUILD:610:12: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/core/kernels:eigen_contraction_kernel:
 //tensorflow:linux_ppc64le
 //tensorflow/core/kernels:no_mkldnn_contraction_kernel
 Multiple matches are not allowed unless one is unambiguously more specialized.
 </denchmark-code>
 
 There ends up being multiple matches in the select call:
 <denchmark-code>defines = select({
         ""//tensorflow:android"": [],
         ""//tensorflow:arm"": [],
         ""//tensorflow:ios"": [],
         ""//tensorflow:linux_ppc64le"": [],
         "":no_mkldnn_contraction_kernel"": [],
         ""//conditions:default"": [
             ""TENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL"",
             ""TENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL"",
         ],
     }),
 </denchmark-code>
 
 Either the arch-specific entries in the select call should be removed, or the no_mkldnn_contraction_kernel entry should be removed for this to work.
 I'll follow up with a proposed patch set.
 	",1.0,jayfurmanek,2018-12-18T22:33:01Z,"
 		<denchmark-link:https://github.com/ezhulenev>@ezhulenev</denchmark-link>
  is already working on it. See <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24416>#24416</denchmark-link>
 .
 		",2.0,jayfurmanek,2019-01-04T19:18:35Z,"
 		addressed with <denchmark-link:https://github.com/tensorflow/tensorflow/commit/7c9323bedc48c98be3c07b72ec1d6f4dccdefb35>7c9323b</denchmark-link>
 
 		",3.0,jayfurmanek,2019-03-28T16:08:16Z,"
 		The fix for this issue was reverted with:
 [Temporarily disable MKL-DNN contraction kernels by default] <denchmark-link:https://github.com/tensorflow/tensorflow/commit/ca8791fc6db991c6c7406f3274ca742d867e0c6b>ca8791f</denchmark-link>
 
 Temporarily, but its been almost a month.
 I had suggested a fix that made sense to me: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24416>#24416</denchmark-link>
 
 But it wasn't accepted.
 <denchmark-link:https://github.com/penpornk>@penpornk</denchmark-link>
  What is the plan for this setting in the .bazelrc ?  If the idea is to switch it on and off regularly, please know that it breaks all non Intel builds.
 Please advise.
 		",1a26797203e62390623756fd67e69e7665b69389,Penporn Koanantakool,2019-03-31 09:52:41-07:00,MODIFY,0,tensorflow\core\kernels\BUILD,tensorflow\core\kernels\BUILD,0.0,"717,718,719,720,721,722,723,724,727,728,729,730,731,732,733,734,735,759,760,761,762,763,764,765","726,739",,,,,4.0,jayfurmanek,2019-03-28T16:19:32Z,"
 		The problem is  is only ever defined if on x86_64. So setting it at all (on or off) is a problem for other arches. My attempted fix in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24416>#24416</denchmark-link>
  was to let that be defined for other arches too, so that disabling it will have an effect.
 If I could get clarity on what was unacceptable in <denchmark-link:https://github.com/tensorflow/tensorflow/pull/24416>#24416</denchmark-link>
  perhaps I could change it so this setting would work for everyone. Any feedback?
 		",5.0,jayfurmanek,2019-03-28T19:01:32Z,"
 		I'm so sorry for the inconvenience! I forgot that disabling the feature would reintroduce this issue.
 
 @penpornk What is the plan for this setting in the .bazelrc ? If the idea is to switch it on and off regularly, please know that it breaks all non Intel builds.
 
 We planned to leave it on indefinitely. I switched it off to debug an issue.
 
 If I could get clarity on what was unacceptable in #24416 perhaps I could change it so this setting would work for everyone. Any feedback?
 
 I apologize that we didn't explain properly. Back then, we were about to turn the feature on (i.e., not matching with no_mkldnn_contraction_kernel). The following targets don't support the feature so we had to keep them to prevent matching with //conditions:default
 <denchmark-code>        ""//tensorflow:android"": [],
         ""//tensorflow:arm"": [],
         ""//tensorflow:ios"": [],
         ""//tensorflow:linux_ppc64le"": [],
 </denchmark-code>
 
 I'll delete these targets for now and add them back when I re-enable the feature.
 If you'd like to contribute to a permanent fix, the conditions are
 
 Never match with //conditions:default for //tensorflow:android, //tensorflow:arm, //tensorflow:ios, and //tensorflow:linux_ppc64le, regardless of the no_mkldnn_contraction_kernel target.
 If none of the above four target matches, match with no_mkldnn_contraction_kernel iff --define=tensorflow_mkldnn_contraction_kernel=0 is set.
 
 Thank you again for raising this issue!
 		",6.0,jayfurmanek,2019-04-01T10:23:09Z,"
 		<denchmark-link:https://github.com/jayfurmanek>@jayfurmanek</denchmark-link>
  Please let me know whether <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1a26797203e62390623756fd67e69e7665b69389>1a26797</denchmark-link>
  has solved the problem. Thank you!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,jayfurmanek,2019-04-01T18:32:53Z,"
 		Thanks <denchmark-link:https://github.com/penpornk>@penpornk</denchmark-link>
  ! What you did in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1a26797203e62390623756fd67e69e7665b69389>1a26797</denchmark-link>
  makes sense to me. I'll give it a shot on ppc64le to confirm.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,jayfurmanek,2019-04-01T21:41:13Z,"
 		Confirmed. No dual match on ppc64le when  is set.
 Thanks <denchmark-link:https://github.com/penpornk>@penpornk</denchmark-link>
 .
 Closing this issue.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,jayfurmanek,2019-04-01T21:41:15Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=24414>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=24414>No</denchmark-link>
 
 		",10.0,jayfurmanek,2019-04-02T02:06:04Z,"
 		<denchmark-link:https://github.com/jayfurmanek>@jayfurmanek</denchmark-link>
  Great. Thank you again for bringing this up and sorry it took us this long!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24598,ufukcbicici,2018-12-27T11:53:09Z,2018-12-28T00:12:24Z,Potential tf.boolean_mask bug when the mask array is empty,"
 System information
 
 OS Platform and Distribution : Windows 10
 TensorFlow installed from (source or binary): Binary
 TensorFlow version (use command below): 1.11.0
 Python version: 3.6.6
 CUDA/cuDNN version: V8.0.60
 GPU model and memory: Geforce GTX 1070 8GB
 
 
 I have actually experiencing almost the similar problem like in thread:  <denchmark-link:https://github.com/tensorflow/tensorflow/issues/24585>#24585</denchmark-link>
 
 Again, I want to partition a minibatch into different parts, process them in parallel using different computation units and then stitch them back together. However this time I used  instead of  for the partition operation, since the latter runs into problems when one of the partitions is empty. This code is below (it is copy&paste reproducible):
 <denchmark-code>import tensorflow as tf
 import numpy as np
 
 
 def build_conv_layer(input, filter_size, num_of_input_channels, num_of_output_channels, name_suffix=""""):
     # OK
     conv_weights = tf.Variable(
         tf.truncated_normal([filter_size, filter_size, num_of_input_channels, num_of_output_channels],
                             stddev=0.1, dtype=tf.float32))
     # OK
     conv_biases = tf.Variable(
         tf.constant(0.1, shape=[num_of_output_channels], dtype=tf.float32))
     conv = tf.nn.conv2d(input, conv_weights, strides=[1, 1, 1, 1], padding='SAME')
     relu = tf.nn.relu(tf.nn.bias_add(conv, conv_biases))
     pool = tf.nn.max_pool(relu, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
     return pool
 
 
 batch_size = 250
 child_count = 3
 channel_count = 32
 
 dataTensor = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name=""dataTensor"")
 indices_tensor = tf.placeholder(name=""indices_tensor"", dtype=tf.int32)
 batch_size_tensor = tf.placeholder(name=""batch_size_tensor"", dtype=tf.int32)
 
 condition_indices_list = []
 partition_list = []
 mask_list = []
 for child_index in range(child_count):
     mask_indices = tf.reshape(indices_tensor[:, child_index], [-1])
     condition_indices = tf.boolean_mask(tf.range(batch_size_tensor), mask_indices)
     partition = tf.boolean_mask(dataTensor, mask_indices)
     mask_list.append(mask_indices)
     condition_indices_list.append(condition_indices)
     partition_list.append(partition)
 
 transformed_list = [build_conv_layer(input=part, filter_size=5, num_of_input_channels=1, num_of_output_channels=32)
                     for part in partition_list]
 squared_list = [tf.square(part) for part in partition_list]
 stitched_conv_transform = tf.dynamic_stitch(indices=condition_indices_list, data=transformed_list)
 stitched_square_transform = tf.dynamic_stitch(indices=condition_indices_list, data=squared_list)
 sum = tf.reduce_sum(stitched_square_transform)
 grads = tf.gradients(sum, dataTensor)
 
 sess = tf.Session()
 samples = np.random.uniform(size=(batch_size, 28, 28, 1))
 indices_arr = np.zeros(shape=(batch_size, child_count), dtype=np.int32)
 indices_arr[:, 0] = 1
 indices_arr[-2] = np.array([0, 1, 0])
 indices_arr[-1] = np.array([0, 1, 0])
 
 feed_dict = {dataTensor: samples,
              batch_size_tensor: batch_size,
              # indices_tensor: np.argmax(np.random.uniform(size=(GlobalConstants.EVAL_BATCH_SIZE, child_count)), axis=1)}
              indices_tensor: indices_arr}
 outputs = []
 outputs.extend(mask_list)
 outputs.extend(transformed_list)
 outputs.extend(squared_list)
 outputs.append(stitched_conv_transform)
 outputs.append(stitched_square_transform)
 outputs.append(sum)
 outputs.append(grads)
 
 init = tf.global_variables_initializer()
 sess.run(init)
 for i in range(10000):
     results = sess.run(outputs, feed_dict=feed_dict)
     assert np.allclose(results[-1][0], 2.0*samples)
     print(""{0} runned."".format(i))
 </denchmark-code>
 
 To my disappointment, tf.boolean_mask runs into a similar problem, when indices_arr  contains no references to at least one partition and it produces an empty array for that partition as the result. The for loop in the end runs correctly a few times but then the program crashes with the following error:
 
 InternalError (see above for traceback): WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true / nonzero indices.  temp_storage_bytes: 1, status: invalid configuration argument
 [[{{node boolean_mask/Where}} = WhereT=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]]
 [[{{node DynamicStitch/_49}} = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_259_DynamicStitch"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]]
 
 I think this is the same error underlying the problem in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/24585>#24585</denchmark-link>
  where it crashes when  receives an empty index array since they could be using the same mechanism in the cub library (or whatever cub is). The  error also occurs after a few succesfull iterations like this one. What could be the reason here?
 	",1.0,ufukcbicici,2018-12-27T22:16:21Z,"
 		I was able to run your code snippet successfully on cpu however interestingly it failed computing on gpu.
 		",2.0,ufukcbicici,2018-12-27T23:39:30Z,"
 		Indeed this is a bug, fixed by pinning Where to the CPU. I'm submitting a patch soon.
 		",3.0,ufukcbicici,2018-12-28T06:12:11Z,"
 		Thank you <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  . How can I get the fix now?
 		",961e9f4505dff0c5a91d12b1f4e476b3cf2d295d,Alexandre Passos,2018-12-27 16:10:57-08:00,MODIFY,1,tensorflow\core\kernels\where_op.cc,tensorflow\core\kernels\where_op.cc,1.0,"140,141,142,143","140,141",MODIFY,0.0,tensorflow\core\kernels\where_op.h,tensorflow\core\kernels\where_op.h,,,,,,,,,,,,,0.0,,30,,,,,,,,,,,,,,,,,,,tensorflow::WhereCPUOp::Compute,context,128,189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24632,ageron,2018-12-30T16:10:30Z,2019-03-05T22:32:45Z,Interrupting tf.keras training while using the TensorBoard callback wreaks havoc,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Mac OS X 10.13.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 VERSION=""1.13.0-dev20181226"" (this is the TF 2.0-preview)
 GIT_VERSION=""b'v1.12.0-5133-gc343196842'""
 Python version:
 3.6.6
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 N/A
 GPU model and memory:
 N/A
 
 Describe the current behavior
 Using tf.keras in Jupyter (or a Python shell) with the TensorBoard callback, some problems occur if I interrupt training. These problems did not occur in TF 1.12:
 
 I get an exception if I call fit() again on the same model:
 
 <denchmark-code>tensorflow.python.framework.errors_impl.NotFoundError: Resource
 localhost/logdir:logs/run1/N10tensorflow22SummaryWriterInterfaceE does not exist.
 [Op:WriteScalarSummary] name: epoch_loss/
 </denchmark-code>
 
 I can workaround this problem by recompiling the model.
 
 I also get an exception if I interrupt training, then I delete the logs directory, then I try to use the TensorBoard callback on the same logs directory again:
 
 <denchmark-code>tensorflow.python.framework.errors_impl.UnknownError: The events file logs/run1/events.out.tfevents.1546185456.macmix.local.v2 has disappeared.
 	Failed to flush 1 events to logs/run1/events.out.tfevents.1546185456.macmix.local.v2
 	Could not flush events file. [Op:FlushSummaryWriter]
 </denchmark-code>
 
 This one is more severe: sometimes it recovers by itself after a while. Sometimes is doesn't and I cannot find any way to manually recover from this error, other than restarting the Jupyter kernel (or the Python shell).
 Describe the expected behavior
 I expect the TensorBoard callback to gracefully handle these issues, perhaps display a warning, but do not force a recompile or a kernel restart.
 Code to reproduce the issue
 import shutil
 import numpy as np
 import tensorflow as tf
 from tensorflow import keras
 
 X_train = np.random.rand(1000, 10)
 y_train = np.random.rand(1000)
 model = keras.models.Sequential([keras.layers.Dense(1)])
 model.compile(loss=""mse"", optimizer=""sgd"")
 tensorboard_cb = keras.callbacks.TensorBoard(""logs/run1"")
 model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])
 # NOTE: you must interrupt training (Ctrl-C) before it finishes
 
 # For issue #1, try this:
 model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])
 
 # For issue #2, try this (you may need to interrupt and retry a few times):
 shutil.rmtree(""logs"")
 model = keras.models.Sequential([keras.layers.Dense(1)])
 model.compile(loss=""mse"", optimizer=""sgd"")
 tensorboard_cb = keras.callbacks.TensorBoard(""logs/run1"")
 model.fit(X_train, y_train, epochs=1000, callbacks=[tensorboard_cb])
 
 Here is a gist with the full session output:
 <denchmark-link:https://gist.github.com/ageron/1d430d4a7716c7a2bae44ee82c321f01>https://gist.github.com/ageron/1d430d4a7716c7a2bae44ee82c321f01</denchmark-link>
 
 	",1.0,ageron,2018-12-30T16:15:23Z,"
 		Note: I checked, this issue does not occur with TF 1.12.
 		",2.0,ageron,2019-01-07T21:01:46Z,"
 		Hello <denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
 , In this issue <denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  has outlined that for r2.0 (a) tf.Keras model training when interrupted by TensorBoard callback and then resume training generates an exception (b) interrupting training and deleting logs and invoking TensorBoard callback generates an exception. Thanks.
 		",3.0,ageron,2019-01-25T05:21:41Z,"
 		<denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  are you working on TensorBoard integration? Can you take a look at this?
 		",826027dbd4277a2636fc2935ed245700fd01e7cd,Nick Felt,2019-02-22 13:44:32-08:00,MODIFY,0,tensorflow\python\BUILD,tensorflow\python\BUILD,0.0,3306,,MODIFY,0.0,tensorflow\python\kernel_tests\BUILD,tensorflow\python\kernel_tests\BUILD,4.0,ageron,2019-01-25T08:35:21Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  Yep, will do
 		",5.0,ageron,2019-01-28T19:37:29Z,"
 		Hi <denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
 , to clarify, this issue is occurring after you've 'd a cell in the Jupyter notebook?
 		",6.0,ageron,2019-01-28T19:50:19Z,"
 		Hi <denchmark-link:https://github.com/omalleyt12>@omalleyt12</denchmark-link>
  ,
 It's either in the Python shell (command line) after I press Ctrl-C during training, or in a Jupyter notebook after I interrupt the kernel during training.
 Hope this helps.
 		",0.0,1096,,,,,,MODIFY,38.0,tensorflow\python\kernel_tests\summary_ops_test.py,tensorflow\python\kernel_tests\summary_ops_test.py,1.0,"70,71,72",70,MODIFY,14.0,tensorflow\python\ops\summary_ops_v2.py,tensorflow\python\ops\summary_ops_v2.py,1.0,"135,136,137","135,136,137",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ageron,2019-01-28T19:55:21Z,"
 		I can confirm that I could reproduce the issue in a Python shell and in a Jupyter notebook, using the latest tf-nightly-2.0-preview (version '2.0.0-dev20190126', ""b'v1.12.0-6726-g5522d670af'"").
 		",testWrite_fromFunction.f,,70,72,set_as_default,self,135,137,MODIFY,0.0,tensorflow\tools\api\golden\v2\tensorflow.summary.-summary-writer.pbtxt,tensorflow\tools\api\golden\v2\tensorflow.summary.-summary-writer.pbtxt,0.0,,7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\tools\api\golden\v2\tensorflow.summary.pbtxt,tensorflow\tools\api\golden\v2\tensorflow.summary.pbtxt,0.0,"15,16,17,18",,,,,,,,,,,,,,,,,,,,,,,,8.0,ageron,2019-02-12T23:52:02Z,"
 		Thanks for the reports!
 Issue 1 has the root cause <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25707>#25707</denchmark-link>
  which is actually independent of Keras.
 Issue 2 is not strictly speaking the same problem, but the proposed solution to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25707>#25707</denchmark-link>
  would also fix issue 2, because it would mean each execution of the callback would create a new event file (the same as the 1.x behavior).
 		",,,,,,,,,,,,,,,1.0,"130,131,132",,testWrite_tensor_fromFunction.f,t,130,132,1.0,"336,337,338,339,340,341,342,343,344,345,346,347","336,337,338,339,340,341,342,345,346",testCreate_fromFunction,self,336,347,1.0,563,,testDereference_closesOpenFile,self,554,569,1.0,"161,162,163,164,166,167,168,169,172,173,174,175","161,162,165,166,167,169,172,173,174,175",__init__,"self,shared_name,init_op_fn,name,v2",161,175,1.0,"135,136,137,138,139,140,141","135,136,137,139",__init__,"self,resource,init_op_fn",135,141,1.0,"179,180","177,178,179,180",_close,self,177,180,1.0,"449,450,451,452,453,454",,create_noop_writer,,449,454,1.0,"141,142,143",,as_default,self,141,143,1.0,555,,write,"tag,tensor,step,metadata,name",544,588,1.0,"145,146,147",,init,self,145,147,1.0,"853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874",,_check_create_file_writer_args,"inside_function,kwargs",853,874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,ageron,2019-03-05T22:32:44Z,"
 		The root cause <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25707>#25707</denchmark-link>
  has been fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/826027dbd4277a2636fc2935ed245700fd01e7cd>826027d</denchmark-link>
 , and propagated into the Keras callback by <denchmark-link:https://github.com/wchargin>@wchargin</denchmark-link>
  in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/059ea3ba68db861e40d750eba688281011d2735f>059ea3b</denchmark-link>
 .
 Issue 1 should be completely fixed, and issue 2 should only occur if you delete the event files in the middle of training (without interrupting the keras callback, as described in the OP) while the writer is still open and then it tries to flush to the no-longer-existing file.  In that case I think it's reasonable to report the error and I believe that matches previous behavior.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"423,424,425,426,427",,testNoSharing_fromFunction.f1,,423,427,1.0,210,207,testWrite_recordIf_callable,self,203,221,1.0,87,86,testWrite_metadata,self,82,96,1.0,"339,340,341,342,343","339,340,341,342",testCreate_fromFunction.f,,339,343,1.0,"128,129,130,131,132","126,127,128,129",testWrite_tensor_fromFunction,self,125,138,1.0,"370,371,372,373,374,375,376,377,378,379,380,381,382","372,373,375,377,378,379",testCreate_fromFunction_unpersistedResource_raisesError,self,370,382,1.0,479,,testWriterFlush,self,475,490,1.0,"349,351,352,354,355,356,357","349,350,351,352,355,356,357",testCreate_graphTensorArgument_raisesError,self,349,357,1.0,"68,69,70,71,72","67,68,69,70",testWrite_fromFunction,self,65,80,1.0,"189,190,191,192,193,194,195,196","189,190,191,192",testWrite_recordIf_constant_fromFunction.f,,189,196,1.0,"522,523,524,525,526,527,528,529,530,531,532,533,534,535",,testClose_preventsLaterUse,self,522,535,1.0,"319,322,324,325,328,331,334","319,320,321,322,325,328,331,332,333,334",testCreate_withInitAndClose,self,319,334,1.0,"226,234","230,246",testWrite_recordIf_callable_fromFunction,self,223,246,1.0,"429,430,431,432,433",,testNoSharing_fromFunction.f2,,429,433,1.0,"254,255,256,257",,testWrite_recordIf_tensorInput_fromFunction.f,step,254,257,1.0,234,,testWrite_recordIf_callable_fromFunction.f,,233,239,1.0,"384,385,386,387,388,391,392,393,394,395,398,399,400,401,403,404,405,406,407,409,412,413,414,415,416","384,399,415",testNoSharing,self,384,418,1.0,519,,testEagerMemory,self,517,520,1.0,495,,testFlushFunction,self,492,514,1.0,464,468,testMaxQueue,self,461,473,1.0,"342,343,344,345,346,347,348,349,351,352,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379","342,345,346,349,350,351,352,355,356,357,358,359,360,361,363,365,366,372,373,375,377,378,379",testSharedName,self,342,379,1.0,"362,363",363,testCreate_fromFunction_graphTensorArgument_raisesError.f,,362,363,1.0,"359,360,361,362,363,364,365,366,367,368","359,360,361,363,365,366",testCreate_fromFunction_graphTensorArgument_raisesError,self,359,368,1.0,173,171,testWrite_recordIf_constant,self,170,182,1.0,"319,322,324,325,328,331,334,335,336,337,338,339,340","314,317,319,320,321,322,325,328,331,332,333,334,335,336,337,338,339,340",testWriterInitAndClose,self,314,340,1.0,"420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,452,453,454,455,456,457,458","439,451",testNoSharing_fromFunction,self,420,459,1.0,118,117,testWrite_tensor,self,114,123,1.0,"187,188,189,190,191,192,193,194,195,196","184,185,186,187,188,189,190,191,192",testWrite_recordIf_constant_fromFunction,self,184,201,1.0,55,55,testWrite,self,52,63,1.0,143,141,testWrite_stringTensor,self,140,147,1.0,108,107,testWrite_ndarray,self,105,112,1.0,"251,252,253,254,255,256,257","248,249,250,251",testWrite_recordIf_tensorInput_fromFunction,self,248,267,1.0,546,,testClose_closesOpenFile,self,537,552,1.0,"373,374,375","373,375",testCreate_fromFunction_unpersistedResource_raisesError.f,,373,375,1.0,"153,154,155","153,154,155",close,self,153,155,1.0,"149,150,151","149,150",flush,self,149,151,1.0,169,169,_flush,self,169,170,1.0,"328,329,330,331,332,333,334,335,336,337,338","328,329,330,331,332,333,334,335,336,337,338",_make_summary_writer,"name,factory,kwargs",328,338,1.0,"289,290,291,292,293",,create_file_writer_v2,"logdir,max_queue,flush_millis,filename_suffix,name",289,293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2523,goodfeli,2016-05-26T22:15:15Z,2016-05-27T20:06:38Z,Incorrect error message,"
 This error message is out of date:
 <denchmark-code>ValueError: Cannot execute operation using Run(): No default session is registered. Use 'with default_session(sess)' or pass an explicit session to Run(session=sess)
 </denchmark-code>
 
 As far as I can tell, there is no longer any default_session function. Instead, one must call sess.as_default().
 	",,,,,,,,,,,,,3e7aae461b657fd7960ff49b64bad125eabfe1e3,Derek Murray,2016-05-27 08:48:49-07:00,MODIFY,2,tensorflow\python\framework\ops.py,tensorflow\python\framework\ops.py,1.0,"3497,3498,3499,3500,3505","3497,3498,3499,3500,3505",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_using_default_session,"operation,feed_dict,graph,session",3479,3511,1.0,"3462,3465,3470","3462,3465,3470",_eval_using_default_session,"tensors,feed_dict,graph,session",3439,3476,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25262,skeydan,2019-01-28T19:40:43Z,2019-01-29T09:51:08Z,Usage of tf_stack.extract_stack in registry.py breaks TensorFlow R client,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 29
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): nightly
 Python version: 3.6
 Bazel version (if compiling from source): NA
 GCC/Compiler version (if compiling from source): NA
 CUDA/cuDNN version: NA
 GPU model and memory: NA
 
 Hi,
 this usage of tf_stack in registry.py
 <denchmark-code>stack = tf_stack.extract_stack()
 user_function = stack[2]
 </denchmark-code>
 
 breaks the TensorFlow for R client, because at that point, when called from R stack is of length 2, with both elements being of length 6.
 This is analogous to the recently fixed <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25067>#25067</denchmark-link>
 
 (thank you <denchmark-link:https://github.com/jtkeeling>@jtkeeling</denchmark-link>
 )
 It would be awesome if this could still be fixed for the 1.13 release, as I'm aware of no workaround and we have users that want to register a custom gradient.
 Many thanks!
 	",1.0,skeydan,2019-01-28T20:08:13Z,"
 		Yes, this is entirely analogous to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25067>#25067</denchmark-link>
 . I've authored a fix and sent for review.
 		",2.0,skeydan,2019-01-28T20:12:38Z,"
 		Great thank you for the quick fix!!!
 		",3.0,skeydan,2019-01-28T22:58:28Z,"
 		This is fixed right (cf other issue)?
 		",823b694639a3f49b6adbf9e73a08c529d583878e,James Keeling,2019-01-29 01:48:40-08:00,MODIFY,1,tensorflow\python\framework\registry.py,tensorflow\python\framework\registry.py,1.0,"67,68,69,70,71,72","67,68",,,,,4.0,skeydan,2019-01-28T23:14:50Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  This is another piece of code that uses tf_stack. I've sent you another change that fixes this one.
 		",5.0,skeydan,2019-01-29T02:59:06Z,"
 		Thanks! We'll wait for that to close.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,register,"self,candidate,name",44,73,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25426,annemariet,2019-02-01T17:00:05Z,2019-07-13T04:28:06Z,"Segmentation Fault with tf.io.decode_csv , numpy record_defaults and tensor input","
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Ubuntu 16.04.4 LTS on Windows Linux SubSystem
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 n/a
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 '2.0.0-preview' / ""b'v1.12.0-6503-g7cfe43a11d'""
 Python version:
 Python 3.6.7
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: No
 GPU model and memory: No
 
 You can collect some of this information using our environment capture <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with
 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)"" 
 Describe the current behavior
 Segmentation fault (core dumped)
 Describe the expected behavior
 prints result
 Code to reproduce the issue
 <denchmark-code>import numpy as np
 import tensorflow as tf
 record_defaults=np.zeros(5)    
 parsed_fields = tf.io.decode_csv(tf.constant('1,2,3,4,5'), record_defaults)
 </denchmark-code>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 It's fine with either tf.constants for record defaults or plain string to decode, eg:
 <denchmark-code>parsed_fields = tf.io.decode_csv('1,2,3,4,5', record_defaults)
 
 
 Numpy version: 
 numpy                     1.16.0          py36_blas_openblash1522bff_1000  [blas_openblas]  conda-forge
 </denchmark-code>
 
 	",1.0,annemariet,2019-02-01T20:09:21Z,"
 		I can confirm that I could reproduce this issue on MacOSX 10.13.6, python 3.6.8, TF version 2.0.0-dev20190126, git version b'v1.12.0-6726-g5522d670af', installed using pip3 install -U tf-nightly-2.0-preview.
 The decode_csv() function expects the record_defaults to be an array of tensors, so replacing np.zeros(5) with [tf.constant(0.)]*5 (or even with [0.]*5) solves the problem, but still segmentation faults should never happen.
 		",2.0,annemariet,2019-07-12T18:19:42Z,"
 		I can reproduce this on a recent nightly in eager mode.  In graph mode, a shape inference check raises a safe failure exceptio.
 		",3.0,annemariet,2019-07-13T04:28:07Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25426>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25426>No</denchmark-link>
 
 		",3ae3c3c9d43f870d6340cec529de03175e914595,Eugene Brevdo,2019-07-12 21:26:03-07:00,MODIFY,0,tensorflow\python\eager\pywrap_tfe_src.cc,tensorflow\python\eager\pywrap_tfe_src.cc,0.0,"2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,3039,3040,3041,3042,3043,3045,3052","3030,3037",MODIFY,2.0,tensorflow\python\kernel_tests\decode_csv_op_test.py,tensorflow\python\kernel_tests\decode_csv_op_test.py,,,,,,,,,,,,,1.0,"322,323,324,325,326,327,328,329,330",,testNumpyAttribute,self,322,330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,43,44,_test,"self,args,expected_out,expected_err_re",32,45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25463,rsepassi,2019-02-03T02:46:03Z,2019-02-07T04:39:39Z,TensorFlow GCS access does not work from colab,"
 System information
 Using colab.research.google.com
 Describe the current behavior
 Hangs.
 <denchmark-code>import tensorflow as tf
 tf.io.gfile.exists(“gs://tfds-data”)  # which is a public GCS bucket
 </denchmark-code>
 
 Describe the expected behavior
 Should not hang.
 	",1.0,rsepassi,2019-02-03T02:47:18Z,"
 		Linked <denchmark-link:https://github.com/tensorflow/datasets/issues/36>tensorflow/datasets#36</denchmark-link>
 
 		",2.0,rsepassi,2019-02-04T17:55:44Z,"
 		Yes, this one's a known bug: the problem is that it's trying an authenticated request, with some number of timeouts; IIRC, it'll eventually complete after ~10m. (!!!)
 Quick workaround: first, do
 <denchmark-code>from google.colab import auth
 auth.authenticate_user()
 </denchmark-code>
 
 and it'll work.
 This was intended to be fixed upstream, but it looks like that didn't work (at least, it still hangs for me). I'll take a look.
 		",3.0,rsepassi,2019-02-04T18:43:22Z,"
 		Thanks <denchmark-link:https://github.com/craigcitro>@craigcitro</denchmark-link>
 . Why is it trying to make an authenticated request? In the particular case I'm running into, it's a publicly accessible GCS bucket, so no auth necessary. Where is the ""authenticated request"" logic happening? I'm guessing it's a TF thing, not a colab thing, right? Should we update it to first check if credentials exist? (if so, make authenticated request, if not don't)
 In my particular case, TFDS is storing some files on a public GCS bucket and is trying to load them. The user shouldn't know anything about the GCS bucket. Calling auth.authenticate_user() forces the user to go to an oauth link, copy a verification code, and paste it back in. This isn't something we want to force users to do. In the short term TFDS will probably use the GCS HTTP API instead of tf.io.gfile.
 		",a252fe83f4dc29c0614983046825ec75a3529cb9,Craig Citro,2019-02-06 20:37:57-08:00,MODIFY,1,tensorflow\core\platform\cloud\google_auth_provider.cc,tensorflow\core\platform\cloud\google_auth_provider.cc,1.0,"153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,169,185,186,187,188,189,190,191","149,150,152,168,169",MODIFY,0.0,tensorflow\core\platform\cloud\google_auth_provider.h,tensorflow\core\platform\cloud\google_auth_provider.h,4.0,rsepassi,2019-02-07T06:20:54Z,"
 		I've added a $NO_GCE_CHECK environment variable that allows a user to completely sidestep the GCE metadata checks; anyone who's hitting the original issue (slow timeouts + retries attempting to fetch a public GCS resource) should be able to use this to get unblocked.
 For Colab in particular, I'm going to add this patch and enable it in our runtimes (where GCE metadata is never available).
 		",5.0,rsepassi,2019-02-07T07:53:42Z,"
 		Thanks!
 <denchmark-link:#>…</denchmark-link>
 
 
 On Wed, Feb 6, 2019 at 10:24 PM Craig Citro ***@***.***> wrote:
  I've added a $NO_GCE_CHECK environment variable that allows a user to
  completely sidestep the GCE metadata checks; anyone who's hitting the
  original issue (slow timeouts + retries attempting to fetch a public GCS
  resource) should be able to use this to get unblocked.
 
  For Colab in particular, I'm going to add this patch and enable it in our
  runtimes (where GCE metadata is never available).
 
  —
  You are receiving this because you authored the thread.
  Reply to this email directly, view it on GitHub
  <#25463 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/ABEGWxqzc1Y2eueoxJ6Cux6txS1tKe5Dks5vK8aIgaJpZM4af5x9>
  .
 
 
 
 		",,,,,0.0,54,54,,,,,MODIFY,2.0,tensorflow\core\platform\cloud\google_auth_provider_test.cc,tensorflow\core\platform\cloud\google_auth_provider_test.cc,1.0,"242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262",,,,,,,,,tensorflow::GoogleAuthProvider::GetToken,t,138,195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::TEST_F,"GoogleAuthProviderTest,NoGceCheckEnvironmentVariable",242,262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"73,75",72,tensorflow::GoogleAuthProviderTest::ClearEnvVars,,71,76,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2573,ruihou,2016-05-29T20:57:37Z,2016-06-08T00:07:20Z,avg/max_pool3d description has a bug.,"
 In file:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.nn.avg_pool3d.md>tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.nn.avg_pool3d.md
 </denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.max_pool3d.md>tensorflow/g3doc/api_docs/python/functions_and_classes/shard4/tf.nn.max_pool3d.md
 </denchmark-link>
 
 Original:
 : A list of  that has length .
 1-D tensor of length 5. The size of the window for each dimension of
 the input tensor. Must have .
 I think
 ksize[0] = ksize[1] = 1. should change to ksize[0] = ksize[4] = 1, according to the test file 
 
 
 tensorflow/tensorflow/python/kernel_tests/pooling_ops_3d_test.py
 
 
          Line 48
       in
       712e41c
 
 
 
 
 
 
  ksize=[1, window[0], window[1], window[2], 1], 
 
 
 
 
 
 	",1.0,ruihou,2016-06-01T19:42:41Z,"
 		This looks to be the case, and it seems like MaxPool3D, MaxPoolGrad3D, AvgPoolGrad3D all have the same issue. Thanks.
 		",,,,,,,,,1a5364efe43f76ab72a1f3651df394d6b121c915,A. Unique TensorFlower,2016-06-07 11:03:49-07:00,MODIFY,0,tensorflow\core\ops\nn_ops.cc,tensorflow\core\ops\nn_ops.cc,0.0,"499,519,541,562","499,519,541,562",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25844,margaretmz,2019-02-18T16:17:32Z,2019-05-31T01:18:29Z,[TF 2.0 API Docs] tf.lite.TFLiteConverter,"
 System information
 
 TensorFlow version: 2.0
 Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite/TFLiteConverter
 
 Describe the documentation issue
 
 Links (Fixed)
 
 Incorrect - <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/lite/python/lite.py>https://github.com/tensorflow/tensorflow/blob/master/lite/python/lite.py</denchmark-link>
 
 Correct - <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py</denchmark-link>
 
 
 Description (In PR #26067)
 
 This is used to convert from a TensorFlow ""GraphDef or SavedModel"" into either a TFLite FlatBuffer or graph visualization.
 Should be ""…GraphDef, Saved Model or tf.keras model…""
 
 Usage example (In PR #26067)
 
 This line of code should be repeated for each of the methods
 open(""converted_model.tflite"", ""wb"").write(tflite_model)
 
 Parameters Defined (Fixed)
 
 Missing 2 parameters
 
 optimizations
 representative_dataset
 
 We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
 Yes
 	",1.0,margaretmz,2019-03-02T12:25:58Z,"
 		Is this issue still unsolved? The PR seems to have solved it but it isn't still merged? Does that mean something is still left?
 		",2.0,margaretmz,2019-03-03T15:58:09Z,"
 		I'm waiting for PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/26067>#26067</denchmark-link>
  to merge.
 		",,,,,39d1b3c7ee8a619b28832ac95e689ba34bac9500,Margaret Maynard-Reid,2019-02-24 10:54:10-08:00,MODIFY,0,tensorflow\lite\python\lite.py,tensorflow\lite\python\lite.py,0.0,"294,373,378",294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25882,serycjon,2019-02-19T09:14:34Z,2019-04-25T20:16:04Z,tf.image.random_jpeg_quality only products images of single jpeg quality,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
 TensorFlow installed from (source or binary): pip install
 TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09
 Python version: 2.7.13
 CUDA/cuDNN version: 9.0 / 7.0.3
 GPU model and memory: GeForce GTX 1080 Ti, 10405 MB
 
 Describe the current behavior
 tf.image.random_jpeg_quality generates random jpeg quality on graph creation, which is then fixed.
 Describe the expected behavior
 tf.image.random_jpeg_quality generates random jpeg quality for each image/batch of images passed through it.
 Code to reproduce the issue
 import numpy as np
 import tensorflow as tf
 
 img = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)
 
 tf_img = tf.placeholder(tf.uint8)
 jpeg_augment = tf.image.random_jpeg_quality(tf_img,
                                             min_jpeg_quality=20,
                                             max_jpeg_quality=90)
 
 sess_config = tf.ConfigProto()
 sess_config.gpu_options.allow_growth = True
 
 with tf.Session(config=sess_config) as sess:
     sess.run(tf.global_variables_initializer())
     results = []
     for i in range(25):
         augmented = sess.run([jpeg_augment], feed_dict={tf_img: img})[0]
         results.append(augmented)
 
 results = np.array(results)
 same_as_first = results == results[0, ...]
 all_equal = np.all(same_as_first)
 print('all_equal: {}'.format(all_equal))
 assert not all_equal
 The code causing this is located at:
 
 
 
 tensorflow/tensorflow/python/ops/image_ops_impl.py
 
 
          Line 1634
       in
       a6d8ffa
 
 
 
 
 
 
  jpeg_quality = np.random.randint(min_jpeg_quality, max_jpeg_quality) 
 
 
 
 
 
 	",1.0,serycjon,2019-02-22T12:46:59Z,"
 		<denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  Can you answer the issue or reassign?
 		",2.0,serycjon,2019-02-22T12:57:45Z,"
 		I think this is more of a ""type:bug"" than ""type:feature""
 		",3.0,serycjon,2019-03-15T21:15:13Z,"
 		The root cause of this problem is that the  op's  parameter is a fixed , not a variable . Fixing  will necessitate a new version of  with a different signature. <denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  I'd be happy to put in a PR for this issue if that's ok.
 		",d87710ed89e01cf479d2b6ba619a879c32ad77d9,A. Unique TensorFlower,2019-03-21 14:46:55-07:00,ADD,0,None,tensorflow\core\api_def\base_api\api_def_EncodeJpegVariableQuality.pbtxt,,,,ADD,0.0,None,tensorflow\core\api_def\java_api\api_def_EncodeJpegVariableQuality.pbtxt,4.0,serycjon,2019-03-18T17:25:38Z,"
 		Sounds like a good idea. <denchmark-link:https://github.com/frreiss>@frreiss</denchmark-link>
  are you familiar with the op replacement process? You have to:
 
 Create a new op, call it v2.
 Check it in, with tests etc and wait for two weeks.
 Replace the call from v1 to v2.
 
 The v2 version should be a strict superset of v1. Ideally it should behave exactly like v1 with the default options.
 		",5.0,serycjon,2019-03-19T17:14:04Z,"
 		<denchmark-link:https://github.com/drpngx>@drpngx</denchmark-link>
  yes, I've done an op replacement in the past with the  op.
 I'll break this work down into a few smaller PRs to make the diff size more manageable:
 
 Add a new C++ op, EncodeJpegV2, along with C++ regression tests.
 Modify the random_jpeg_quality Python API to use EncodeJpegV2 and also fix issue #25882.
 Change the other Python code in image_ops_impl.py that uses gen_image_ops.encode_jpeg to use gen_image_ops.encode_jpeg_v2.
 
 Regarding the first step: I see that EncodeJpeg has a number of other arguments that are currently static attributes:
 REGISTER_OP(""EncodeJpeg"")
     .Input(""image: uint8"")
     .Attr(""format: {'', 'grayscale', 'rgb'} = ''"")
     .Attr(""quality: int = 95"")
     .Attr(""progressive: bool = false"")
     .Attr(""optimize_size: bool = false"")
     .Attr(""chroma_downsampling: bool = true"")
     .Attr(""density_unit: {'in', 'cm'} = 'in'"")
     .Attr(""x_density: int = 300"")
     .Attr(""y_density: int = 300"")
     .Attr(""xmp_metadata: string = ''"")
     .Output(""contents: string"")
 
 Do you have any preference about which arguments I should turn into 0-D tensors in the new op?
 		",6.0,serycjon,2019-03-20T00:17:51Z,"
 		I will let the API reviewer comment. My general intuition is that numbers can be changed, and everything else, in the worst case put in the graph. So quality, {x,y}_density would be input tensors.
 		",,,,,,,,ADD,0.0,None,tensorflow\core\api_def\python_api\api_def_EncodeJpegVariableQuality.pbtxt,,,,MODIFY,2.0,tensorflow\core\kernels\encode_jpeg_op.cc,tensorflow\core\kernels\encode_jpeg_op.cc,1.0,"140,141",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,serycjon,2019-03-20T20:28:23Z,"
 		(For API owners) This might be more appropriate for the <denchmark-link:https://github.com/tensorflow/addons>TensorFlow Addons</denchmark-link>
  repository, which has a number of special use ops like this. In either case, in the op registration code above, the Attrs should be Inputs. (Inputs are more flexible.)
 		",,,,,tensorflow::EncodeJpegVariableQualityOp::EncodeJpegVariableQualityOp,context,140,141,MODIFY,0.0,tensorflow\core\ops\image_ops.cc,tensorflow\core\ops\image_ops.cc,0.0,"452,453,454,455,456,457,458",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\go\op\wrappers.go,tensorflow\go\op\wrappers.go,1.0,"15600,15601,15602,15603,15604,15605,15606,15607,15608,15609,15610,15611,15612",,EncodeJpegVariableQuality,"Scope,Output,Output",15600,15612,,,,,,,,MODIFY,2.0,tensorflow\python\ops\image_ops_impl.py,tensorflow\python\ops\image_ops_impl.py,1.0,"1905,1918,1919,1920,1921,1922,1923,1924,1925","1898,1911",adjust_jpeg_quality,"image,jpeg_quality,name",1893,1929,8.0,serycjon,2019-04-02T01:35:21Z,"
 		I was about to start work on this, but it looks like an anonymous Googler has already committed a patch. Someone should close this issue.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195",,tensorflow::EncodeJpegVariableQualityOp::Compute,context,143,195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1880,1881,1882,1883,1884,1885,1886,1887,1888","1880,1881",random_jpeg_quality,"image,min_jpeg_quality,max_jpeg_quality,seed",1851,1889,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\python\ops\image_ops_test.py,tensorflow\python\ops\image_ops_test.py,1.0,"3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917",,MODIFY,0.0,tensorflow\tools\api\golden\v1\tensorflow.raw_ops.pbtxt,tensorflow\tools\api\golden\v1\tensorflow.raw_ops.pbtxt,0.0,"975,976,977,978",,9.0,serycjon,2019-04-25T20:16:05Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=25882>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=25882>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\tools\api\golden\v2\tensorflow.raw_ops.pbtxt,tensorflow\tools\api\golden\v2\tensorflow.raw_ops.pbtxt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testAdjustJpegQuality,self,3905,3917,1.0,"3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903",,testRandomJpegQuality,self,3882,3903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,"975,976,977,978",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25985,huan,2019-02-21T20:42:41Z,2019-03-11T14:24:36Z,reset_states() failure in a stateful network with initial_states set and training in batch - TypeError: 'NoneType' object is not subscriptable,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0-dev20190217
 Python version: 3.5.2
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: 10
 GPU model and memory: GTX 1080 8G
 
 Describe the current behavior
 As <denchmark-link:https://github.com/manojrege>@manojrege</denchmark-link>
  said from <denchmark-link:https://github.com/keras-team/keras/issues/11148>keras-team/keras#11148</denchmark-link>
 , when we use  with RNN in some case, we will get an exception:
 <denchmark-code>Traceback (most recent call last):
   File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
     ""__main__"", mod_spec)
   File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
     exec(code, run_globals)
   File ""/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py"", line 137, in <module>
     incremental_train(space)
   File ""/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py"", line 128, in incremental_train
     model.reset_states()
   File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"", line 1968, in reset_states
     layer.reset_states()
   File ""/usr/local/lib/python3.6/site-packages/keras/layers/recurrent.py"", line 681, in reset_states
     batch_size = self.input_spec[0].shape[0]        
 TypeError: 'NoneType' object is not subscriptable
 </denchmark-code>
 
 There's another issue talking about this problem at <denchmark-link:https://github.com/tensorflow/tensorflow/issues/25852>#25852</denchmark-link>
 
 Describe the expected behavior
 Should not throw exception.
 Code to reproduce the issue
 import tensorflow as tf
 
 # import pdb; pdb.set_trace()
 inputs = tf.keras.layers.Input(batch_shape=(1, 1, 1))
 
 state_h = tf.keras.layers.Input(batch_shape=(1, 1))
 state_c = tf.keras.layers.Input(batch_shape=(1, 1))
 
 states = [state_h, state_c]
 
 decoder_out = tf.keras.layers.LSTM(1, stateful=True)(
     inputs,
     initial_state=states
 )
 
 model = tf.keras.Model([inputs, state_h, state_c], decoder_out)
 model.reset_states()
 Other info / logs
 I can confirm that the Pull Request <denchmark-link:https://github.com/keras-team/keras/pull/11149/files>https://github.com/keras-team/keras/pull/11149/files</denchmark-link>
  can fix this problem.
 	",1.0,huan,2019-03-11T14:24:35Z,"
 		This should be now fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/83df61b4d4ad11f3b8cf05ee98d29e6fb5e25506>83df61b</denchmark-link>
 .
 		",2.0,huan,2019-03-11T15:54:33Z,"
 		Thanks, that's awesome.
 		",,,,,83df61b4d4ad11f3b8cf05ee98d29e6fb5e25506,Scott Zhu,2019-03-09 08:47:57-08:00,MODIFY,1,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,"699,700,701,702","697,700",MODIFY,1.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,,,,,,,,,,,,,1.0,"1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316",,test_full_input_spec,self,1305,1316,,,,,,,,,,,,,,,__call__,"self,inputs,initial_state,constants,kwargs",646,709,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26048,robertnishihara,2019-02-24T02:14:09Z,2019-03-05T17:13:30Z,Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.12.0-8779-g2ae06ca491 1.13.0-dev20190223 (as well as 1.12.0)
 Python version: Python 3.6.4 :: Anaconda, Inc.
 
 When tf.custom_gradient is used incorrectly (in this case, the returned grad function returns an empty list, the script segfaults.
 import tensorflow as tf
 
 tf.enable_eager_execution()
 
 @tf.custom_gradient
 def identity(x):
     def grad(dy):
         return []  # This return value is wrong!
     return x, grad
 
 x = tf.Variable(1.0)
 with tf.GradientTape() as t:
     y = identity(x)
 t.gradient(y, [x])
 The t.gradient call fails with
 <denchmark-code>2019-02-23 18:09:14.621207: F ./tensorflow/c/eager/tape.h:642] Check failed: state.op_tape.empty() 
 Abort trap: 6
 </denchmark-code>
 
 I think it'd be preferable to raise an exception instead of crashing.
 If I instead return too many values from grad, then the script runs, but this is most likely a bug and should probably raise an exception.
 import tensorflow as tf
 
 tf.enable_eager_execution()
 
 @tf.custom_gradient
 def identity(x):
     def grad(dy):
         return 1.0, 2.0  # Too many return values!
     return x, grad
 
 x = tf.Variable(1.0)
 with tf.GradientTape() as t:
     y = identity(x)
 t.gradient(y, [x])
 FYI <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
 
 	",,,,,,,,,,,,,710b322a8be78b8aff6b148575fcfe5301f42b64,Alexandre Passos,2019-03-05 09:11:05-08:00,MODIFY,1,tensorflow\c\eager\tape.h,tensorflow\c\eager\tape.h,1.0,"642,643,644",642,MODIFY,3.0,tensorflow\python\eager\backprop_test.py,tensorflow\python\eager\backprop_test.py,,,,,,,,,,,,,1.0,"135,136,137,138,139,140,141,142,143,144,145,146,147",,testCustomGradientEmptyError,self,135,147,MODIFY,2.0,tensorflow\python\ops\custom_gradient.py,tensorflow\python\ops\custom_gradient.py,1.0,"258,282,283,284,285,286",,,,,,,,,"tensorflow::eager::GradientTape<Gradient,BackwardFunction,TapeTensor>::ComputeGradient","vspace,target_tensor_ids,source_tensor_ids,sources_that_are_targets,output_gradients,result",475,671,,,,,,,,,,,,,,,,,,,,,,1.0,"139,140",,testCustomGradientEmptyError.testCustomGradientEmptyError.identity.grad,_,139,140,1.0,"138,139,140,141",,testCustomGradientEmptyError.identity,x,138,141,,,,,,,,,,,,,,,,,,,,,,,,,,_eager_mode_decorator,"f,args,kwargs",253,294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"282,283,284,285,286",,_eager_mode_decorator.actual_grad_fn,result_grads,272,287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26099,elmirador,2019-02-25T19:20:13Z,2019-04-23T04:42:21Z,tf.one_hot crashes when indices is tf.uint8,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / Windows 7
 TensorFlow installed from (source or binary): Official pip source (tensorflow-gpu)
 TensorFlow version (use command below): 1.12.0
 Python version: 3.6
 CUDA/cuDNN version: 9.0 / 7.5
 GPU model and memory: 1080Ti / 12GB
 
 Describe the current behavior
 tf.one_hot crashes when the indices tensor has dtype=tf.uint8
 The error message shows Check failed: new_num_elements == NumElements()
 
 <denchmark-link:https://gist.github.com/elmirador/4fc5148e5044478d668237209d265eac>https://gist.github.com/elmirador/4fc5148e5044478d668237209d265eac</denchmark-link>
 
 Other info / logs
 I've also tested under TF 1.4.1 and TF 1.10.0 (both on GPU) on different machines, both have the same problem.
 	",1.0,elmirador,2019-03-01T07:11:31Z,"
 		Hello, I am new to the tensorflow community and which going thorugh the code I couldn't find gen_array_ops. If possible I would like to contribute to the issue.
 		",2.0,elmirador,2019-03-01T07:53:24Z,"
 		
 Hello, I am new to the tensorflow community and which going thorugh the code I couldn't find gen_array_ops. If possible I would like to contribute to the issue.
 
 gen_array_ops.py is located under tensorflow/python/ops/.
 You can't find it on github because gen_array_ops is generated during building.
 Basically it's a wrapper around the underlying C++ code of tensorflow. If you're interested, the C++ code of one_hot ops is located at tensorflow/core/kernels/one_hot_op.cc.
 		",3.0,elmirador,2019-04-23T00:01:30Z,"
 		<denchmark-link:https://github.com/bono1567>@bono1567</denchmark-link>
  Don't know if you had a chance to look into it and thank you for your interest!
 Since you haven't posted an update for a while and I just looked into it, I will submit a fix now.
 		",a311216a9f028eec9e6b0d2ef175f5d46dff19b7,Anna R,2019-04-22 21:30:30-07:00,MODIFY,1,tensorflow\core\kernels\one_hot_op.cc,tensorflow\core\kernels\one_hot_op.cc,1.0,106,106,MODIFY,1.0,tensorflow\python\kernel_tests\one_hot_op_test.py,tensorflow\python\kernel_tests\one_hot_op_test.py,4.0,elmirador,2019-04-23T04:42:22Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26099>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26099>No</denchmark-link>
 
 		",,,,,,,,,1.0,"423,424,425,426,427,428",,testOneHotUint8WithLargeArray,self,423,428,,,,,,,,,,,,,,,tensorflow::OneHotOp::Compute,ctx,50,120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26143,lazysjb,2019-02-26T21:12:27Z,2019-06-04T13:17:06Z,[TF2.0] Error Logging for GradientTape,"
 Hello everyone,
 I was wondering if there is an option for error logging / or could we have tf output error message for gradient calculation. In the example below, the below will output None values in the current setting but will output correct gradients when the tf.Variables are a float type. My question is, could we please add an error message stating something like gradient calculation supports only float types?
 Best Regards,
 Seung-jae Bang
 <denchmark-code>def forward(a, b):
     """"""f = a * b""""""
     return a * b
 
 params = [tf.Variable(1), tf.Variable(2)]
 
 with tf.GradientTape() as tape:
     result = forward(*params)
 
 tape.gradient(result, params)
 </denchmark-code>
 
 System information
 
 Linux
 TensorFlow installed from pip install -U tf-nightly-2.0-preview - ""2.0.0-dev20190226""
 Python version: 3.6
 
 ccing: <denchmark-link:https://github.com/random-forests>@random-forests</denchmark-link>
 
 	",1.0,lazysjb,2019-03-04T17:12:49Z,"
 		I'd love to approve a PR adding this test. Feel like giving it a shot?
 		",2.0,lazysjb,2019-03-05T01:55:20Z,"
 		I would be happy to give it a shot - I haven't contributed to TF before, would you have any pointers?
 Also, would tensorflow/python/eager/backprop.py be the right place to make this change?
 cc: <denchmark-link:https://github.com/random-forests>@random-forests</denchmark-link>
 
 		",3.0,lazysjb,2019-03-05T16:39:13Z,"
 		Yes, the change would be to add some logging to that particular file.
 
 If you need any more pointers, happy to help.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Mon, Mar 4, 2019 at 5:58 PM lazysjb ***@***.***> wrote:
  I would be happy to give it a shot - I haven't contributed to TF before,
  would you have any pointers?
  Also, would tensorflow/python/eager/backprop.py be the right place to make
  this change?
 
  cc: @random-forests <https://github.com/random-forests>
 
  —
  You are receiving this because you were assigned.
  Reply to this email directly, view it on GitHub
  <#26143 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AAATxdlUmDjQehdKpLmQuotUBd4yku7iks5vTc9VgaJpZM4bTIcM>
  .
 
 
 -- 
  - Alex
 
 		",764fdc6fb1a9ac377440e1b537ff8a6b7e9f2063,A. Unique TensorFlower,2019-04-01 14:35:59-07:00,MODIFY,1,tensorflow\python\eager\backprop.py,tensorflow\python\eager\backprop.py,1.0,"815,816,817,818",,,,,,4.0,lazysjb,2019-03-08T14:20:08Z,"
 		i would also like to contribute can you please help me where i can add logging in backprop.py
 		",5.0,lazysjb,2019-03-08T16:51:54Z,"
 		You probably want to add the warning in tape.watch here 
 
 
 tensorflow/tensorflow/python/eager/backprop.py
 
 
          Line 804
       in
       d280d3d
 
 
 
 
 
 
  def watch(self, tensor): 
 
 
 
 
  and tape.gradient here 
 
 
 tensorflow/tensorflow/python/eager/backprop.py
 
 
          Line 890
       in
       d280d3d
 
 
 
 
 
 
  def gradient(self, 
 
 
 
 
 
 		",6.0,lazysjb,2019-03-23T11:04:05Z,"
 		Hi,
 I wish to start contributing to TensorFlow and hence would like to know if someone is already working on this issue.
 		",,,,,,,,,,,,,,,,,,,,,,watch,"self,tensor",808,825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,lazysjb,2019-03-25T10:00:19Z,"
 		<denchmark-link:https://github.com/achalagarwal>@achalagarwal</denchmark-link>
  I think <denchmark-link:https://github.com/shashvatshahi1998>@shashvatshahi1998</denchmark-link>
  is already working on this. <denchmark-link:https://github.com/shashvatshahi1998>@shashvatshahi1998</denchmark-link>
  can you confirm?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,lazysjb,2019-03-25T12:23:22Z,"
 		Ya I am working on that, but anyone else who is interested can start working.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,lazysjb,2019-06-04T07:21:25Z,"
 		Hey, This would be my first contribution to tf and i would like to know whether this issue is open and if anybody is contributing to this.
 		",10.0,lazysjb,2019-06-04T13:17:06Z,"
 		Closing this out since I understand it to be resolved by the PR. I have checked the code which output warning as expected. Thanks!
 		",11.0,lazysjb,2019-06-04T13:17:07Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26143>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26143>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26394,kpe,2019-03-06T12:08:17Z,2019-03-18T12:05:58Z,Allow building TF + nvidia GPU targeting &lt; sm35 if XLA is not enabled,"
 Please make sure that this is a build/installation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo
 TensorFlow installed from (source or binary): source
 TensorFlow version: 1.13.1
 Python version: 3.6.4
 Installed using virtualenv? pip? conda?: pip in venv
 Bazel version (if compiling from source): 0.21
 GCC/Compiler version (if compiling from source): 7.4.0
 CUDA/cuDNN version: 10.0
 GPU model and memory: GTX 650 Ti
 
 I was able to successfully build TF from source with XLA enabled and compute capability 3.0.
 However, when a session is created the python interpreter exits (complaining about insufficient compute capability):
 <denchmark-code>>>> import tensorflow as tf
 >>> tf.Session()
 2019-03-06 12:49:41.776396: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3500130000 Hz
 2019-03-06 12:49:41.776727: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x556553ef5ee0 executing computations on platform Host. Devices:
 2019-03-06 12:49:41.776741: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
 2019-03-06 12:49:41.809556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-03-06 12:49:41.810593: I tensorflow/compiler/xla/service/platform_util.cc:194] StreamExecutor cuda device (0) is of insufficient compute capability: 3.5 required, device is 3.0
 2019-03-06 12:49:41.810666: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
 
 </denchmark-code>
 
 if I reconfigure TF by disabling XLA, and rebuild (again with compute capability 3.0), than TF works fine.
 So I guess, a simple check if compute capability >= 3.5 when XLA is enabled, could at least prevent building non-functional TF.
 	",1.0,kpe,2019-03-06T21:28:06Z,"
 		Hey, a similar issue was filed a while ago, and a fix has been deployed <denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>here</denchmark-link>
 . Would be available in the master branch/the next release.
 		",2.0,kpe,2019-03-07T10:52:59Z,"
 		I actually found similar issues, but non of them was XLA related. And indeed as you pointed out (<denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>#25767</denchmark-link>
 ) the config script does no longer allow building TF with compute capability bellow 3.5. Even it looks like if XLA is disabled TF could still be built and function properly with capability 3.0.
 also just for referece - <denchmark-link:https://github.com/tensorflow/tensorflow/issues/24126>#24126</denchmark-link>
 
 		",3.0,kpe,2019-03-08T22:46:01Z,"
 		
 as you pointed out (#25767) the config script does no longer allow building TF with compute capability bellow 3.5.
 
 If this is the case, I'm not sure what is the bug here?
 		",8dc2d0eedac7760deb65254a8ef89878743299d7,A. Unique TensorFlower,2019-03-12 01:46:54-07:00,MODIFY,1,configure.py,configure.py,1.0,"1308,1309,1313,1314,1315","1308,1309",,,,,4.0,kpe,2019-03-09T00:24:21Z,"
 		The ""bug"" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.
 What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.
 		",5.0,kpe,2019-03-09T00:30:26Z,"
 		
 The ""bug"" is - you could build with 3.0 and disabled XLA but the config.py does not allow this.
 What leads to nonfunctional TF ist the combination of XLA and 3.0, and not 3.0 alone.
 
 Sorry, I'm still confused.
 In the second sentence, I'm understanding that TF built without XLA and run on sm30 does work?  But in the first sentence I'm understanding that you can't build TF with XLA disabled targeting sm30?  Those seem at odds.
 		",6.0,kpe,2019-03-09T00:45:42Z,"
 		Well, the first sentence is - you can build it if you patch configure.py to accept compute capability 3.0 (with disabled XLA). And in sentence two, there are two statements - a) XLA and 3.0 = broken-TF, and b) disabled XLA and 3.0 = working-TF.
 		",,,,,,,,,,,,,,,,,,,,,,set_tf_cuda_compute_capabilities,environ_cp,1271,1326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,kpe,2019-03-09T00:50:50Z,"
 		....I mean, I had to patch config.py in order to build an TF version for my old GPU (compute capability 3.0).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,kpe,2019-03-09T00:53:21Z,"
 		OK, so what you'd like is for the TF build team to change config.py so that it doesn't require sm35, if XLA is disabled?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,kpe,2019-03-09T00:59:38Z,"
 		Yes, this is what the issue was about. (edit: at least after I learned there was a  >= 3.5 check on the main branch already, preventing TF from being built with 3.0)
 		",10.0,kpe,2019-03-09T01:04:20Z,"
 		It may be the case that TF build team has decided that they do not support < sm35 at all, even though you observed that it happened to work in your particular case.  Over to them, this is not really an XLA question.
 		",11.0,kpe,2019-03-09T01:07:26Z,"
 		Yes, sorry for the confusion, it was really about, a better config sanity check.
 		",12.0,kpe,2019-03-10T21:21:03Z,"
 		<denchmark-link:https://github.com/chsigg>@chsigg</denchmark-link>
  may be able to confirm or deny, but here is what I can say:
 Below compute capability 3.0 TF will NOT work. XLA, or no xla, TF needs features in compute capability 3.0
 Below 3.5 is best effort, we do not test it, or even build it.  It may work, may not work, you are free to use at your own risk.
 Above 3.5 will work.
 6.0 is the most heavily tested one for us, and any issues we see for 6.0 or newer is promptly triaged.
 		",13.0,kpe,2019-03-11T09:51:37Z,"
 		We can probably change <denchmark-link:https://github.com/tensorflow/tensorflow/pull/25767>#25767</denchmark-link>
  to issue an error for compute capability < 3.0 (down from currently < 3.5).
 For compute capability < 3.5, we can issue a warning that XLA is not supported. XLA generates CUDA code for whatever GPU is present at runtime, so anything more than a warning during configuration seems prohibitive.
 		",14.0,kpe,2019-03-11T14:07:56Z,"
 		I can submit another PR if it is required, but I would appreciate it if you could specify the exact information that should be conveyed in the warning. The original PR did issue a warning and not an error. It was changed because it seemed like TensorFlow would surely fail to work below 3.5 (which is not really the case maybe?).
 		",15.0,kpe,2019-03-18T12:05:58Z,"
 		The compute capability check is now a warning for < 3.5, and and error for < 3.0. Closing.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26502,ctrysbita,2019-03-09T04:14:05Z,2019-04-20T07:00:39Z,IDE cannot resolve module tf.keras,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 No.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Linux Ubuntu 18.10
 TensorFlow installed from (source or binary):
 binary (pip)
 TensorFlow version (use command below):
 2.0_alpha
 Python version:
 3.6
 
 Describe the current behavior
 import tensorflow as tf
 then pyCharm cannot resolve module tf.keras and report an error:
 Cannot find reference 'keras' in '__init__.py'
 But when running program, everything works well.
 Describe the expected behavior
 tf.keras imported successfully with autocomplete of pyCharm.
 Code to reproduce the issue
 import tensorflow as tf
 Other info / logs
 It seems that there is no import command for keras module in __init_.py of tensorflow package.
 When I added from tensorflow.python import keras to __init__.pymanually, everything work well.
 Maybe there are some problem for package importing after keras was moved from _api to python.
 	",1.0,ctrysbita,2019-03-14T17:34:07Z,"
 		<denchmark-link:https://stackoverflow.com/a/47306203/6108843>https://stackoverflow.com/a/47306203/6108843</denchmark-link>
 
 		",2.0,ctrysbita,2019-03-14T17:55:09Z,"
 		Not very familiar with how these modules work. <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  would you know what's up?
 		",3.0,ctrysbita,2019-03-15T12:04:10Z,"
 		<denchmark-link:https://intellij-support.jetbrains.com/hc/en-us/community/posts/360002486739/comments/360000407199>https://intellij-support.jetbrains.com/hc/en-us/community/posts/360002486739/comments/360000407199</denchmark-link>
 
 		",88ca0db75e6daf66c6fb21609ee4100126f1b727,Anna R,2019-04-19 23:48:49-07:00,MODIFY,0,tensorflow\api_template.__init__.py,tensorflow\api_template.__init__.py,0.0,"35,40,41,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74","52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70",MODIFY,0.0,tensorflow\api_template_v1.__init__.py,tensorflow\api_template_v1.__init__.py,4.0,ctrysbita,2019-03-16T05:12:53Z,"
 		<denchmark-link:https://github.com/knaydenov>@knaydenov</denchmark-link>
  none of your solutions is pythonic. It should work like <denchmark-link:https://github.com/tensorflow>@tensorflow</denchmark-link>
  import keras in its examples.
 		",5.0,ctrysbita,2019-03-16T05:14:50Z,"
 		<denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  any solution to this issue? any commit for the next dev or stable release?
 		",6.0,ctrysbita,2019-03-16T06:04:41Z,"
 		Yes, this is not a bug from pyCharm but tensorflow itself.
 The bug is caused by missing tensorflow.python import keras in __init__.py of tensorflow package.
 		",0.0,"29,33,34,35,36,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57","32,33,34,35,36,37,39,40,41,42,43,44,45,46,69,70,71,72,73,74,75,76,77,78,79",,,,,MODIFY,0.0,tensorflow\compat_template.__init__.py,tensorflow\compat_template.__init__.py,0.0,"21,25,26,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52","28,29,30,31,32,33,34,35,36,37,38,39,40,41",MODIFY,0.0,tensorflow\compat_template_v1.__init__.py,tensorflow\compat_template_v1.__init__.py,0.0,"22,23,24,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44","27,28,29,30,31,32,33,34",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ctrysbita,2019-03-16T17:43:29Z,"
 		<denchmark-link:https://github.com/ctrysbita>@ctrysbita</denchmark-link>
  do you think whether they resolve the issue in future releases?
 		",,,,,,,,,MODIFY,0.0,tensorflow\python\BUILD,tensorflow\python\BUILD,0.0,199,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\tools\BUILD,tensorflow\python\tools\BUILD,0.0,"165,166","165,166",,,,,,,,,,,,DELETE,0.0,tensorflow\python\tools\component_api_helper.py,None,,,,,,,,8.0,ctrysbita,2019-03-18T20:47:01Z,"
 		keras is imported dynamically in the __init__.py file.
 I am not familiar with the way pyCharm looks for autocomplete symbols, but I will see if there is any workaround (may be adding 'keras' to __all__).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\python\tools\module_util.py,,,,,,,,,,,9.0,ctrysbita,2019-03-26T06:31:56Z,"
 		
 keras is imported dynamically in the __init__.py file.
 I am not familiar with the way pyCharm looks for autocomplete symbols, but I will see if there is any workaround (may be adding 'keras' to __all__).
 
 As we discussed here, It's a serious problem which has been created by tensorflow implementation. Could you please kindly consider it to resolve as soon as possible for next versions?
 		",10.0,ctrysbita,2019-03-29T03:13:39Z,"
 		seems this issue hasn't been resolved yet, any idea to fix it temporarily? thx!
 		",11.0,ctrysbita,2019-04-01T19:26:27Z,"
 		
 seems this issue hasn't been resolved yet, any idea to fix it temporarily? thx!
 
 At least for code completion to find it, a quick and dirty fix is to throw from tensorflow.python import keras at the top of <python_site_packages_dir>/tensorflow/__init__.py
 In pycharm you can get there quickly by just going to the declaration of tensorflow (ctrl+b default)
 		",12.0,ctrysbita,2019-04-10T04:22:56Z,"
 		<denchmark-link:https://github.com/jaingaurav>@jaingaurav</denchmark-link>
  <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
   Hello, could you please let us when this bug will be fixed?
 		",13.0,ctrysbita,2019-04-17T13:02:53Z,"
 		
 At least for code completion to find it, a quick and dirty fix is to throw from tensorflow.python import keras at the top of <python_site_packages_dir>/tensorflow/__init__.py
 In pycharm you can get there quickly by just going to the declaration of tensorflow (ctrl+b default)
 
 Doing this solved the code completion issue but led to other errors, namely API incompatibilities. I tried creating my own Model by subclassing and I got strange errors from the tensorflow_backend.py functions.
 		",14.0,ctrysbita,2019-04-20T07:00:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26502>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26502>No</denchmark-link>
 
 		",15.0,ctrysbita,2019-04-22T20:08:32Z,"
 		For folks hitting this issue, the fix has merged. Please try the latest 2.0 nightly and let us know if it is not yet resolved.
 		",16.0,ctrysbita,2019-04-30T22:41:26Z,"
 		Not resolved for me in pycharm
 		",17.0,ctrysbita,2019-04-30T22:50:27Z,"
 		<denchmark-link:https://github.com/DecentGradient>@DecentGradient</denchmark-link>
  which version of TensorFlow are you using?
 		",18.0,ctrysbita,2019-05-01T03:16:49Z,"
 		<denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  tf-nightly-2.0-preview-2.0.0.dev20190430
 		",19.0,ctrysbita,2019-05-01T03:38:56Z,"
 		I just installed latest tf-nightly-2.0-preview and can't repro the issue. For example, I see autocompletion suggestions in PyCharms when typing tf.keras.applications.
 Can you describe the issue you are seeing in more detail? Which name doesn't autocomplete?
 		",20.0,ctrysbita,2019-08-22T08:26:38Z,"
 		I am still having this issue with pycharm 2019.1.3
 I cannot import keras from tensorflow. ""from tensorflow import keras"" doesn't work either.
 I am using the nightly build of TF2.0: tf-nightly-gpu-2.0-preview 2.0.0.dev20190821
 		",21.0,ctrysbita,2019-08-24T19:43:47Z,"
 		Same issue with the new tensorflow==2.0.0-rc0 release. If in beta release I could just use the tensorflow.python.keras workaround, now keras.layers module doesn't see the layers classes (Dense, Embedding, etc) but packages in which every layer is grouped. If I import them separately from each module the call of a layer over an input doesn't seem to work. This behaviour might be only a problem in PyCharm (in cmd works just fine) but as the majority of developers are using PyCharm as IDE it would be nice to not struggle to use tensorflow library. In addition the tensorflow namespace doesn't contain anything at import, all the packages are grouped in tensorflow_core. I understand that the backend grouping of modules must be done that way, the problem is that the importing step doesn't match the guides and tutorials on the official site and makes everything hard to use.
 If it is any workaround on this issue until a fix is released (from tensorflow team or PyCharm) I would be grateful to use it.
 		",22.0,ctrysbita,2019-08-24T23:43:18Z,"
 		Someone should reopen this since neither tensorflow-gpu-2.0.0rc0 nor tf-nightly-gpu-2.0-preview enables IDE to resolve tf.keras. I don't really understand why tf2 still uses such a complex importing method. tf1 has reinvented logging and argparse, now tf2 is still reinventing import.
 		",23.0,ctrysbita,2019-08-25T11:04:00Z,"
 		🤕 I just upgraded to 2.0rc ..... and my code is full of yellows
 Earlier for 2.0b I used to use the hack for keras and other stuff
 <denchmark-code>import tensorflow.python as tfpy
 tfpy.keras
 </denchmark-code>
 
 Now that doesn't work .... but nonetheless code runs ....
 		",24.0,ctrysbita,2019-08-26T16:07:02Z,"
 		As tensorflow_core was mentioned above, please note that that is an undocumented experimental layout which might change in the future, so don't depend on it.
 It exists now in order to allow us to break some component packages, to modularize TensorFlow and reduce bloat. But at one point the future it will get replaced (with proper release notes)
 		",25.0,ctrysbita,2019-08-26T16:27:29Z,"
 		I added an update on this issue on the freshly opened <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31973>#31973</denchmark-link>
  : <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31973#issuecomment-524928419>#31973 (comment)</denchmark-link>
 
 TL;DR: we are working on this but the fix is quite complex
 		",26.0,ctrysbita,2019-09-01T10:35:05Z,"
 		
 I added an update on this issue on the freshly opened #31973 : #31973 (comment)
 TL;DR: we are working on this but the fix is quite complex
 
 Thanks for your great comment. We seriously need this feature; otherwise, pycharm will be completely useless!!!
 		",27.0,ctrysbita,2019-09-06T14:14:09Z,"
 		Out of curiosity, what IDEs do the core developers use? I think most heavy-duty IDEs for software development would be affected by this, but I also would have assumed the core developers would use such IDEs. Seeing that this was left in, I'm guessing the developers are using an IDE that wasn't affected.
 		",28.0,ctrysbita,2019-10-01T05:57:33Z,"
 		what is going on here? i suffering this error from moths to now
 		",29.0,ctrysbita,2019-10-01T17:01:26Z,"
 		Does not work with the stable version of tensorflow.keras(stable release 1st October 2019)!!!!!
 		",30.0,ctrysbita,2019-10-01T17:22:27Z,"
 		Hey everyone, if you still struggle, please check my stackoverflow post in which I solve this issue with
 TensorFlow 2.0.0 stable release version.
 <denchmark-link:https://stackoverflow.com/questions/58188704/tensorflow-2-0-unable-to-import-keras-in-pycharm>https://stackoverflow.com/questions/58188704/tensorflow-2-0-unable-to-import-keras-in-pycharm</denchmark-link>
 
 		",31.0,ctrysbita,2019-10-01T20:53:36Z,"
 		At least with PyCharm IDE (EAP release), it will work. I have tried it and it works. <denchmark-link:https://stackoverflow.com/a/58192020/5681083>Check here</denchmark-link>
 .
 Avoid importing tensorflow_core if you want to keep code compatible.
 		",32.0,ctrysbita,2019-10-15T22:13:04Z,"
 		
 I had the same problem and it bugged me for a good couple of hours :( . I used from tensorflow.python.keras import *** for importing the libraries and it worked fine for me.
 
 That is kind of dangerous since you may end up importing different modules.
 e.g.
 from tensorflow.python.keras.callbacks import TensorBoard; print(TensorBoard) gets
 <class 'tensorflow.python.keras.callbacks.TensorBoard'>
 but, from tensorflow.keras.callbacks import TensorBoard; print(TensorBoard) gets <class 'tensorflow.python.keras.callbacks_v1.TensorBoard'>
 		",33.0,ctrysbita,2019-10-15T22:23:28Z,"
 		This problem is annoying, i end up coding on Google Colab, there the module is recognized and the autocompletion works good (On experimental Colab Gui). I will stay this way while this is solved, but its a pain.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.0,ctrysbita,2019-11-01T09:14:35Z,"
 		With PyCharm 2019.3 (Early Access Preview or later) the issue disappears.
 		",35.0,ctrysbita,2019-11-26T03:52:43Z,"
 		why is this issue closed while it's still not fixed?
 		",36.0,ctrysbita,2019-11-26T06:50:28Z,"
 		Apparently they closed the issue because with ""PyCharm 2019.3 (Early Access Preview"" the issue disappears. However, at the time speaking, PyCharm 2019.3 still has not been released (it is R.C. now)
 		",37.0,ctrysbita,2020-03-19T02:49:43Z,"
 		Same issue with stable Intellij IDEA 2019.2.4 and tf2 in Match 2020...
 		",38.0,ctrysbita,2020-03-19T03:19:47Z,"
 		<denchmark-link:https://github.com/fzyzcjy>@fzyzcjy</denchmark-link>
  which import doesn't work for you? Can you try tensorflow 2.2.0rc0 ?
 		",39.0,ctrysbita,2020-03-19T03:21:30Z,"
 		<denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  All does not work... I will first try to update to IDEA 2019.3.4, and then report the situation to you. Thanks!
 		",40.0,ctrysbita,2020-03-19T04:34:53Z,"
 		Sounds good. Either updating Intellij IDEA to newer version or updating TensorFlow to 2.2.0rc0 should help. Let me know if you still see issues.
 		",41.0,ctrysbita,2020-03-19T05:25:04Z,"
 		<denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  After updating IDEA to 2019.3.4, it works! Thanks! :)
 		",42.0,ctrysbita,2020-03-20T02:27:03Z,"
 		tensorflow-version:'2.0.0'
 CUDA:10.0 Cudnn:7.6.4
 (I have tried update tensorflow to 2.2.0rc0 but I get import error:Failed to load the native TensorFlow runtime )
 pycharm version: 2019.2.4 -> 2019.3.4
 Problem solved!
 thanks <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  and <denchmark-link:https://github.com/fzyzcjy>@fzyzcjy</denchmark-link>
 
 		",43.0,ctrysbita,2020-03-22T19:58:36Z,"
 		tensorflow 1.13.1
 PyCharm 2019.3.4
 <denchmark-link:https://user-images.githubusercontent.com/17675462/77259152-40660000-6c88-11ea-80ba-4260dffafced.png></denchmark-link>
 
 <denchmark-link:https://user-images.githubusercontent.com/17675462/77259157-44921d80-6c88-11ea-98fa-0b29b8c1a969.png></denchmark-link>
 
 same problem...
 		",44.0,ctrysbita,2020-03-24T04:22:07Z,"
 		<denchmark-link:https://github.com/elisim>@elisim</denchmark-link>
  I'm using TF2.0 + PyCharm 19.3. Your code works for me.
 May be update to latest version can resolve the issue?
 		",45.0,ctrysbita,2020-03-24T09:30:14Z,"
 		<denchmark-link:https://github.com/ctrysbita>@ctrysbita</denchmark-link>
  It's works with TF2.0, but I need TF1.3 for my needs.
 		",46.0,ctrysbita,2020-04-02T02:12:08Z,"
 		1.3 is too old, we no longer support it or bring fixes to it. Please update to 1.15 or 2.1 (long term releases)
 		",47.0,ctrysbita,2020-09-29T12:03:59Z,"
 		still not solved for TF 2.1 (Pylance 2020.9.6)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26533,mandroid6,2019-03-10T12:42:15Z,2020-01-29T00:34:41Z,[TF 2.0 API Docs] tf.argsort,"
 Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
 System information
 
 TensorFlow version:  2.0
 Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/argsort
 
 Describe the documentation issue
 
 
 Usage example
 No usage example is provided.
 
 
 Visuals, if Applicable
 No visuals are included.
 
 
 We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
 	",1.0,mandroid6,2020-01-29T00:34:41Z,"
 		It has a usage example now. I don't think it needs a visual. Closing because it seems fixed: <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/argsort>https://www.tensorflow.org/api_docs/python/tf/argsort</denchmark-link>
 
 		",,,,,,,,,bd37836156e5114f15544f984f34f08d38555d4d,Ayush Agrawal,2019-03-18 14:03:28+05:18,MODIFY,1,tensorflow\python\ops\sort_ops.py,tensorflow\python\ops\sort_ops.py,1.0,"67,68,69,70,71,72,73,74,75,76,77",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,argsort,"values,axis,direction,stable,name",60,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26590,yugoren,2019-03-11T22:22:35Z,2019-03-12T20:48:21Z,[tf.keras.layers.LSTM] Initializer fails with input_length parameter,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 
 <denchmark-code>4.15.0-46-generic #49~16.04.1-Ubuntu SMP Tue Feb 12 17:45:24 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
 </denchmark-code>
 
 
 TensorFlow installed from (source or binary): conda as binary.
 TensorFlow version (use command below): 1.10, 1.12, and 1.13 confirmed
 Python version: Python 3.6.6 |Anaconda, Inc.| (default, Oct  9 2018, 12:34:16)
 
 (The following are irrelevant since I'm not even running with a session or in eager mode)
 
 CUDA/cuDNN version: CUDA 9.0
 GPU model and memory: GeForce GTX TITAN X
 
 MWE
 <denchmark-code>import tensorflow as tf
 lstm = tf.keras.layers.LSTM(512, input_length=32)
 </denchmark-code>
 
 Current behavior
 Here's python error message:
 <denchmark-code>Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2230, in __init__
     **kwargs)
   File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 462, in __init__
     super(RNN, self).__init__(**kwargs)
   File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
     method(self, *args, **kwargs)
   File ""~/.local/anaconda/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 138, in __init__
     raise TypeError('Keyword argument not understood:', kwarg)
 TypeError: ('Keyword argument not understood:', 'input_length')
 </denchmark-code>
 
 
  class inherits from , which has  as a parameter as described <denchmark-link:https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/keras/layers/RNN>here</denchmark-link>
 . Therefore the constructor of  should use this parameter but it does not as you can see <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L391-L399>here</denchmark-link>
  in the code.
 	",1.0,yugoren,2019-03-12T05:48:05Z,"
 		<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  Qianli, are you a good person to look at this?
 		",2.0,yugoren,2019-03-12T14:19:55Z,"
 		Yes, I will take a look within this week.
 		",3.0,yugoren,2019-03-12T14:24:12Z,"
 		Ah, seems that it is an error in the documentation, the input_length is no longer needed and is inferred from the input tensor shape. I will fix the documentation soon.
 		",8fcf86ec70a2a91e33f222d2be85675f0b773581,Scott Zhu,2019-03-12 11:55:11-07:00,MODIFY,0,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,0.0,"401,402,403,404,405,406,407","196,253,254,255,256,257,258,259,260,261,262,263",MODIFY,1.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,4.0,yugoren,2019-03-12T20:48:13Z,"
 		This should now be fixed.
 		",,,,,,,,,1.0,"1329,1330,1331,1332,1333,1334,1335,1336,1337",,test_input_dim_length,self,1329,1337,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26602,vbardiovskyg,2019-03-12T10:29:27Z,2019-04-18T08:40:52Z,Partial function specified through keyword on first position in tf.function,"
 Wrapping in tf.function a partial with first argument specified:
 <denchmark-code>def f(x, y):
   return x + y
 
 partial_func = functools.partial(f, x=5)
 tf_func = tf.function(partial_func)
 
 print(tf_func(5))
 </denchmark-code>
 
 This does not work in Python2.x, because <denchmark-link:https://github.com/tensorflow/tensorflow/blob/c46d383150564c8b72b05acc65182c16f7221694/tensorflow/python/util/tf_inspect.py#L175>tf_inspect.getfullargspec cannot represent such construct</denchmark-link>
  using Argspec.
 Unfortunately this also does not work in Python3, where Argspecs are already capable of representing this:
 <denchmark-code>TypeError: tf__f() got multiple values for argument 'x'
 </denchmark-code>
 
 	",1.0,vbardiovskyg,2019-03-12T15:41:35Z,"
 		Can we isolate this issue to misbehavior of getfullargspec instead of tf.function?
 I ask because as far as tf.function is concerned nothing breaks if we just wrap the partial call into a lambda *args, **kwds: partial_call(*args, **kwds) so we should probably just do that at intake time if the user passes a functools.partial function.
 		",2.0,vbardiovskyg,2019-03-14T14:24:48Z,"
 		I am not actually sure that this is a misbehavior of getfullargspec.
 Currently we are using ArgSpec to be able to bind arguments to (partial or normal) function later. We want to represent partial function fully with ArgSpec, but this is not possible with Python2.x: it is not possible to have arguments with defaults before arguments without defaults.
 I would consider this more an integration issue. For example, do we really need to represent partial function using getfullargspec on the inner function (the one with more arguments)? Could we somehow forget about the inner partial and work with the outer only? I haven't tried it myself.
 		",3.0,vbardiovskyg,2019-03-14T15:30:59Z,"
 		I think we should do the latter and forget about the inner partial.
 
 Or we should not rely on argspec for this at all and instead look at the
 arguments as they're presented at calling time (which I much prefer).
 <denchmark-link:#>…</denchmark-link>
 
 
 On Thu, Mar 14, 2019 at 7:30 AM Vojtech Bardiovsky ***@***.***> wrote:
  I am not actually sure that this is a misbehavior of getfullargspec.
 
  Currently we are using ArgSpec to be able to bind arguments to (partial or
  normal) function later. We want to represent partial function fully with
  ArgSpec, but this is not possible with Python2.x: it is not possible to
  have arguments with defaults before arguments without defaults.
 
  I would consider this more an integration issue. For example, do we really
  need to represent partial function using getfullargspec on the inner
  function (the one with more arguments)? Could we somehow forget about the
  inner partial and work with the outer only? I haven't tried it myself.
 
  —
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  <#26602 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AAATxWfus7xJCyx0oit5DEAcSXFRFacdks5vWl0OgaJpZM4bqjiP>
  .
 
 
 -- 
  - Alex
 
 		",4e4943edc3d2409bffb5776f99b941987d6eda82,Vojtech Bardiovsky,2019-04-10 01:50:12-07:00,MODIFY,0,tensorflow\core\protobuf\saved_object_graph.proto,tensorflow\core\protobuf\saved_object_graph.proto,0.0,"146,147","144,145,146,147,148,149",MODIFY,1.0,tensorflow\python\eager\def_function.py,tensorflow\python\eager\def_function.py,4.0,vbardiovskyg,2019-03-25T23:06:50Z,"
 		<denchmark-link:https://github.com/vbardiovskyg>@vbardiovskyg</denchmark-link>
  I think you're working on this, right?
 		",5.0,vbardiovskyg,2019-04-18T08:40:52Z,"
 		Since we are now depending on partial to do the argument binding, this becomes infeasible (i.e. we don't want to provide more functionality than partial already does).
 		",6.0,vbardiovskyg,2019-04-18T08:40:53Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26602>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26602>No</denchmark-link>
 
 		",1.0,319,,_defun_with_scope,"self,scope",293,319,MODIFY,4.0,tensorflow\python\eager\def_function_test.py,tensorflow\python\eager\def_function_test.py,1.0,"248,249,252,253,254","248,249,252,253,254",MODIFY,2.0,tensorflow\python\eager\function.py,tensorflow\python\eager\function.py,1.0,,"1100,1101",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,vbardiovskyg,2020-04-03T23:01:17Z,"
 		<denchmark-link:https://github.com/vbardiovskyg>@vbardiovskyg</denchmark-link>
   Please see comments in the following code (TF 2.2.0rc2).  Why does the first case succeed and second fail?  Is this a bug?  Thanks.
 import tensorflow as tf
 import functools
 
 def f(x, y):
     return x + y
 
 bind_x = functools.partial(f, x=1)
 bind_y = functools.partial(f, y=1)
 
 dataset = tf.data.Dataset.range(1)
 
 print(next(iter(dataset.map(bind_y))))  # This works
 print(next(iter(dataset.map(bind_x))))  # TypeError: tf__f() got multiple values for argument 'x'
 		",test_functools_partial_new_default,self,248,254,canonicalize_function_inputs,"self,args,kwargs",1061,1140,MODIFY,4.0,tensorflow\python\eager\function_test.py,tensorflow\python\eager\function_test.py,1.0,"1762,1763,1765,1767,1768,1769,1770,1771,1772,1773,1774","1762,1763,1764,1765,1768,1769,1770,1771,1772,1773,1774,1775,1776",testInputSignatureWithKeywordArgsFails,self,1762,1776,1.0,1765,"1764,1765",testInputSignatureWithKeywordArgsFails.foo,"a,kwargs",1764,1766,1.0,"1763,1765","1763,1764,1765",testInputSignatureWithKeywordArgs.foo,"a,b,kwargs",1763,1765,1.0,"1762,1763,1765,1767,1768,1769,1770,1771,1772,1773,1774","1762,1763,1764,1765,1768,1769,1770,1771,1772,1773,1774",testInputSignatureWithKeywordArgs,self,1762,1774,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\python\saved_model\function_deserialization.py,tensorflow\python\saved_model\function_deserialization.py,1.0,"186,187,188,189",,_defun_with_scope,"self,scope",186,189,1.0,"144,145","141,142,146",_deserialize_function_spec_as_nonmethod,"function_spec_proto,coder",119,146,MODIFY,1.0,tensorflow\python\saved_model\function_serialization.py,tensorflow\python\saved_model\function_serialization.py,1.0,,"35,36,37,38",_serialize_function_spec,"function_spec,coder",26,41,8.0,vbardiovskyg,2020-04-06T05:12:32Z,"
 		Hi <denchmark-link:https://github.com/mmilosav>@mmilosav</denchmark-link>
 ,
 this is due to how partial works. See the following snippet using partial outside of TensorFlow context:
 <denchmark-code>def f(x,y):
   print(x+y)
 
 bind_x = functools.partial(f, x=1)
 
 bind_x(3)  # TypeError: f() got multiple values for argument 'x'
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,1.0,249,249,test_functools_partial_new_default.f,"x,y",249,250,1.0,"248,249,252,253,254","248,249,252,253,254,255,256,257,258,259",test_functools_partial_single_keyword,self,248,259,1.0,249,249,test_functools_partial_single_keyword.f,"x,y",249,250,1.0,"976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011","975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,998,999",from_function_and_signature,"python_function,input_signature",974,1011,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,10.0,tensorflow\python\saved_model\load_test.py,tensorflow\python\saved_model\load_test.py,1.0,1231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_partial_with_passed_fn_as_default,"self,cycles",1231,1245,1.0,"1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229",,test_partial_keyword_hiding_default,"self,cycles",1213,1229,1.0,1174,"1160,1161,1162",test_partial,"self,cycles",1159,1174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1174,,test_partial_with_non_tensor_defaults.f,"x,y",1174,1175,1.0,"1199,1200,1201,1202,1203","1190,1191,1192",test_partial_with_positional,"self,cycles",1189,1203,1.0,"1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211","1205,1206,1207",test_partial_with_positional_captured_tensors,"self,cycles",1199,1211,1.0,1174,1177,test_partial_with_non_tensor_defaults,"self,cycles",1173,1184,1.0,,1177,test_partial_with_non_tensor_defaults.f,"x,y",1177,1178,1.0,"1200,1201",,test_partial_with_positional_captured_tensors.f,"x,y",1200,1201,1.0,"1214,1215,1216,1217,1218",,test_partial_keyword_hiding_default.f,"x,training,y",1214,1218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26639,dipanjanS,2019-03-13T04:01:20Z,2020-07-20T21:58:59Z,Nasnet models don't support custom image sizes even if include_top is set to False,"
 The general idea for fine-tuning pre-trained models with a custom image size is to set the include_top parameter to False when loading the models. However it doesn't seem to be working with the Nasnet models in tf.keras so far. All other models including Inception are working fine.
 Note: I was using tensorflow 2.0 alpha so I'm not sure if that is the problem.
 I believe maybe some issue somewhere in checking dimension size along with the include_top flag but I might be wrong.
 Following is the stack trace.
 <denchmark-code>
 Code executed:
 nasnet = tf.keras.applications.nasnet.NASNetLarge(include_top=False, weights='imagenet', 
                                                                                   input_shape=(100, 100, 3))
 
 Error Message:
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 <ipython-input-100-64f6d45dc54d> in <module>
       1 nasnet = tf.keras.applications.nasnet.NASNetLarge(include_top=False, weights='imagenet', 
 ----> 2                                                                                 input_shape=(100, 100, 3))
       3 nasnet.summary()
 
 /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/__init__.py in wrapper(*args, **kwargs)
      68       kwargs['models'] = models
      69       kwargs['utils'] = utils
 ---> 70     return base_fun(*args, **kwargs)
      71   return wrapper
      72 
 
 /opt/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/applications/nasnet.py in NASNetLarge(*args, **kwargs)
      37 @keras_modules_injection
      38 def NASNetLarge(*args, **kwargs):
 ---> 39   return nasnet.NASNetLarge(*args, **kwargs)
      40 
      41 
 
 /opt/anaconda3/lib/python3.6/site-packages/keras_applications/nasnet.py in NASNetLarge(input_shape, include_top, weights, input_tensor, pooling, classes, **kwargs)
     364                   classes=classes,
     365                   default_size=331,
 --> 366                   **kwargs)
     367 
     368 
 
 /opt/anaconda3/lib/python3.6/site-packages/keras_applications/nasnet.py in NASNet(input_shape, penultimate_filters, num_blocks, stem_block_filters, skip_reduction, filter_multiplier, include_top, weights, input_tensor, pooling, classes, default_size, **kwargs)
     166                                       data_format=backend.image_data_format(),
     167                                       require_flatten=True,
 --> 168                                       weights=weights)
     169 
     170     if backend.image_data_format() != 'channels_last':
 
 /opt/anaconda3/lib/python3.6/site-packages/keras_applications/imagenet_utils.py in _obtain_input_shape(input_shape, default_size, min_size, data_format, require_flatten, weights)
     290                                  'and loading `imagenet` weights, '
     291                                  '`input_shape` should be ' +
 --> 292                                  str(default_shape) + '.')
     293         return default_shape
     294     if input_shape:
 
 ValueError: When setting `include_top=True` and loading `imagenet` weights, `input_shape` should be (331, 331, 3).
 </denchmark-code>
 
 	",1.0,dipanjanS,2019-03-13T18:09:16Z,"
 		<denchmark-link:https://github.com/dipanjanS>@dipanjanS</denchmark-link>
  Could you provide a code to reproduce the bug? Thanks!
 		",2.0,dipanjanS,2019-03-13T20:01:48Z,"
 		Hi, it's mentioned in the previous comment, reposting here again separately.
 <denchmark-code>nasnet = tf.keras.applications.nasnet.NASNetLarge(include_top=False, weights='imagenet', 
                                                   input_shape=(100, 100, 3))
 </denchmark-code>
 
 Tensorflow version being used: '2.0.0-alpha0'
 		",3.0,dipanjanS,2019-03-13T22:10:16Z,"
 		Adding Francois who is the owner Keras overall.
 		",f7ee1bff1d90aa0ac0a5e16a71c3c60f7ad96fdb,A. Unique TensorFlower,2020-07-18 23:38:32-07:00,MODIFY,0,tensorflow\python\keras\applications\nasnet.py,tensorflow\python\keras\applications\nasnet.py,0.0,"357,433",,,,,,4.0,dipanjanS,2019-05-09T03:04:09Z,"
 		Same question
 		",5.0,dipanjanS,2019-05-12T03:45:24Z,"
 		<denchmark-link:https://github.com/fchollet>@fchollet</denchmark-link>
  any idea on this aspect?
 		",6.0,dipanjanS,2019-05-20T05:57:38Z,"
 		I need to wrote weights=None, then the training runs successfully.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,dipanjanS,2019-05-20T11:17:12Z,"
 		That is just random initialization of weights, the whole point of using this is to do transfer learning with pre-trained weights obtained from imagenet. Using weights=None defeats the purpose of doing that. That's the same like building your own CNN and copying the layers from Nasnet.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,dipanjanS,2019-05-20T16:20:01Z,"
 		I understand.but then you see in practice, nasnet was requiring fixed image size after loading imagenet.whats the solution to that besides dumping imagenet?-------- Original Message --------Subject: Re: [tensorflow/tensorflow] Nasnet models don't support custom image sizes even if include_top is set to False (<denchmark-link:https://github.com/tensorflow/tensorflow/issues/26639>#26639</denchmark-link>
 )From: Dipanjan Sarkar To: tensorflow/tensorflow CC: Kirosealin ,Manual That is just random initialization of weights, the whole point of using this is to do transfer learning with pre-trained weights obtained from imagenet. Using weights=None defeats the purpose of doing that. That's the same like building your own CNN and copying the layers from Nasnet.
 
 —You are receiving this because you are subscribed to this thread.Reply to this email directly, view it on GitHub, or mute the thread.
 [
 {
 ""<denchmark-link:https://github.com/context>@context</denchmark-link>
 "": ""<denchmark-link:http://schema.org>http://schema.org</denchmark-link>
 "",
 ""<denchmark-link:https://github.com/type>@type</denchmark-link>
 "": ""EmailMessage"",
 ""potentialAction"": {
 ""<denchmark-link:https://github.com/type>@type</denchmark-link>
 "": ""ViewAction"",
 ""target"": ""<denchmark-link:https://github.com/tensorflow/tensorflow/issues/26639>#26639</denchmark-link>
 ?email_source=notifications\u0026email_token=AFSLRD4TMTZWQ2OWYGGW2V3PWKC4RA5CNFSM4G5RM342YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYP2CY#issuecomment-493944075"",
 ""url"": ""<denchmark-link:https://github.com/tensorflow/tensorflow/issues/26639>#26639</denchmark-link>
 ?email_source=notifications\u0026email_token=AFSLRD4TMTZWQ2OWYGGW2V3PWKC4RA5CNFSM4G5RM342YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVYP2CY#issuecomment-493944075"",
 ""name"": ""View Issue""
 },
 ""description"": ""View this Issue on GitHub"",
 ""publisher"": {
 ""<denchmark-link:https://github.com/type>@type</denchmark-link>
 "": ""Organization"",
 ""name"": ""GitHub"",
 ""url"": ""<denchmark-link:https://github.com>https://github.com</denchmark-link>
 ""
 }
 }
 ]
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,dipanjanS,2019-05-20T16:29:19Z,"
 		fixing the API so it is consistent with the other pre-trained models I guess?
 		",10.0,dipanjanS,2019-12-28T15:27:32Z,"
 		The problem is not with the TensorFlow itself, but with the  module. The reason for such behavior of NASNet is described <denchmark-link:https://github.com/keras-team/keras-applications/pull/62>here</denchmark-link>
 . So it's more likely a bug than a feature, however, I think that documentation should be updated.
 		",11.0,dipanjanS,2020-07-01T09:38:27Z,"
 		I just ran into this, agree the documentation should be updated - both the tensorflow and keras documentation say that you should be able to set input_shape to something other than (331,331,3).
 		",12.0,dipanjanS,2020-07-20T21:58:58Z,"
 		The documentation for loading weights with correct input_shape has been updated. Thanks!
 		",13.0,dipanjanS,2020-07-20T21:59:00Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26639>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26639>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26645,Jasonnor,2019-03-13T05:28:54Z,2020-02-20T01:01:36Z,Testing guide page not exist (404),"
 Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template
 System information
 
 TensorFlow version: 1.13
 Doc Link: https://www.tensorflow.org/api_guides/python/test
 
 
 <denchmark-link:https://www.tensorflow.org/api_guides/python/test>Testing guide</denchmark-link>
  not exist which linked from <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/test>tf.test page</denchmark-link>
 .
 We welcome contributions by users. Will you be able to update submit a PR (use the doc style guide) to fix the doc Issue?
 I don't know if the page originally exists. 🤔
 	",1.0,Jasonnor,2019-03-13T21:16:47Z,"
 		Thanks for filing the issue! The link you referenced is pointing to a <denchmark-link:https://web.archive.org/web/20180908070834/https://tensorflow.org/api_guides/python/test>Testing Guide</denchmark-link>
  we had historically; but I do not think has been upgraded to TF 2.0.
 <denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
 , do you know if we have plans to update the guide?
 		",2.0,Jasonnor,2019-06-11T19:20:50Z,"
 		The testing guide is still missing.
 I went to <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/test>this page</denchmark-link>
 , which points to the testing guide, but I only see a 404 error. See <denchmark-link:https://www.tensorflow.org/api_guides/python/test>here</denchmark-link>
 .
 		",3.0,Jasonnor,2020-02-19T06:48:58Z,"
 		<denchmark-link:https://github.com/Jasonnor>@Jasonnor</denchmark-link>
 
 <denchmark-link:https://github.com/krishnap25>@krishnap25</denchmark-link>
 
 could you please confirm if the issue still persist
 		",13cca52d62148fb5e103c1265c95184b75f577f5,Mark Daoust,2020-02-19 16:52:48-08:00,MODIFY,0,tensorflow\python\platform\test.py,tensorflow\python\platform\test.py,0.0,16,"16,17,18,19,20,21,22",,,,,4.0,Jasonnor,2020-02-19T09:13:45Z,"
 		Not see a 404 now, but the link of test guild on <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/test>this page</denchmark-link>
  is same as it(api page). 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26665,murdockhou,2019-03-13T12:30:53Z,2019-03-14T23:40:32Z,"tensorflow 2.0, variable_scope(), TypeError: __call__() got an unexpected keyword argument 'partition_info'","
 I have convert a CNN model from tf1.x to tf2.0 by using tf_upgrade_v2, but when i used this converted model, i got an error:
 File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 2492, in default_variable_creator import_scope=import_scope, distribute_strategy=distribute_strategy) File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variables.py"", line 216, in __call__ return super(VariableMetaclass, cls).__call__(*args, **kwargs) File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in __init__ constraint=constraint) File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 545, in _init_from_args initial_value() if init_from_fn else initial_value, File ""/home/hsw/virtual_env/tf2.0/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py"", line 886, in <lambda> shape.as_list(), dtype=dtype, partition_info=partition_info) TypeError: __call__() got an unexpected keyword argument 'partition_info'
 it seems like something wrong in variables.py, and the converted model such as like this :
 <denchmark-code>    with tf.compat.v1.variable_scope('backbone', reuse=tf.compat.v1.AUTO_REUSE):
         net = tf.compat.v1.layers.separable_conv2d(inputs, 16, 3, 1, 'same',
                                      activation=tf.nn.elu,
                                      depthwise_initializer=tf.keras.initializers.glorot_normal(),
                                      pointwise_initializer=tf.keras.initializers.glorot_normal(),
                                      name='conv1')
         net = tf.compat.v1.layers.max_pooling2d(net, 2, 2, padding='same')
         net = tf.compat.v1.layers.separable_conv2d(net, 32, 3, 1, 'same',
                                      activation=tf.nn.elu,
                                      depthwise_initializer=tf.keras.initializers.glorot_normal(),
                                      pointwise_initializer=tf.keras.initializers.glorot_normal(),
                                      name='conv2')
 </denchmark-code>
 
 how should do to solve this problem?
 	",1.0,murdockhou,2019-03-13T17:19:07Z,"
 		Can I see the full stack trace?
 		",2.0,murdockhou,2019-03-14T01:29:48Z,"
 		<denchmark-link:https://user-images.githubusercontent.com/18358653/54325147-ac586600-463b-11e9-9f22-7060a5acb656.png></denchmark-link>
 
 This is the full stack trace.
 		",3.0,murdockhou,2019-03-14T16:30:50Z,"
 		Thanks! This seems to be an issue with the line
 <denchmark-code>          init_val = lambda: initializer(  # pylint: disable=g-long-lambda
               shape.as_list(), dtype=dtype, partition_info=partition_info)
 </denchmark-code>
 
 which calls the initializer. For some reason this is calling the v2 tf.keras initializers instead of the compat.v1 initializers. I think this is a bug in the rename script.
 <denchmark-link:https://github.com/pavithrasv>@pavithrasv</denchmark-link>
  can you revert the change in the rename script to rename tf v1 initializers to tf v2 initializers since they're not compatible?
 A workaround here is to replace the calls to tf.keras.initializers in the translated code with tf.compat.v1 initializers (take what was there originally and replace tf with tf.compat.v1).
 		",a236ae23782c04a057d17a8ad845500c7f15c432,Pavithra Vijay,2019-03-14 16:24:18-07:00,MODIFY,1,tensorflow\tools\compatibility\tf_upgrade_v2.py,tensorflow\tools\compatibility\tf_upgrade_v2.py,1.0,"771,772,775,776,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868","719,720,721,722,723,724,725,726,727,728,729,730",MODIFY,3.0,tensorflow\tools\compatibility\tf_upgrade_v2_test.py,tensorflow\tools\compatibility\tf_upgrade_v2_test.py,4.0,murdockhou,2019-03-14T18:26:14Z,"
 		Thank you Alex. We do not rename v1 initializers to v2. The issue is because we do not rename v1 initializers to compat.V1, because of this users starts seeing the V2 version.
 
 
 
 tensorflow/tensorflow/tools/compatibility/tf_upgrade_v2.py
 
 
          Line 781
       in
       75eca85
 
 
 
 
 
 
  ""tf.keras.initializers.zeros"": 
 
 
 
 
 
 I will add this renaming for all the initializers.
 		",5.0,murdockhou,2019-03-15T01:53:27Z,"
 		Thanks all, fixed this by using tf.compat.v1.keras replace tf.keras.
 		",6.0,murdockhou,2019-04-09T07:27:50Z,"
 		<denchmark-link:https://github.com/murdockhou>@murdockhou</denchmark-link>
  , Have you resolved this problem, how to do it?  I ｈａve the same problem.
 		",1.0,"420,421,422,423,424,425,426",,verify_compat_v1_rename_correctness,"self,values,ns_prefix",420,426,,,,,,,,,,,,,,,__init__,self,38,1682,,,,,,,,,,,,,,,,,,,,,,1.0,"1474,1475,1480,1481",,test_uniform_unit_scaling_initializer,self,1472,1483,1.0,"428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491",,testIntializers,self,428,491,,,,,,,,,,,,,,,,,,,,,,7.0,murdockhou,2019-06-27T02:14:49Z,"
 		<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  @
 I have similar issue. I attached my problem below.
 
 <denchmark-h:hr></denchmark-h>
 
 In tf2_util.py
 def conv2d( . . .  ~):
 . . . . . . . ~ skip
 with tf.compat.v1.variable_scope(scope) as sc:
 . . . . . . .  ~
 kernel = _variable_with_weight_decay('weights',
 shape=kernel_shape,
 use_xavier=use_xavier,
 stddev=stddev,
 wd=weight_decay)
 def _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):
 . . . . . . .  .~ skip
 if use_xavier:
 initializer = tf.keras.initializers.VarianceScaling(scale=1.0, mode=""fan_avg"", distribution=""uniform"")
 else:
 initializer = tf.compat.v1.truncated_normal_initializer(stddev=stddev)
 def _variable_on_cpu(name, shape, initializer, use_fp16=False):
 . . . . . . .  .~  skip
 with tf.device(""/cpu:0""):
 dtype = tf.float16 if use_fp16 else tf.float32
 var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)
 return var
 <denchmark-h:hr></denchmark-h>
 
 I guess have error above code 'tf.compat.v1.get_variable' on 'def _variable_on_cpu'
 Detail error explaination is below
 ##############################################################
 Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch  *
 train_one_step(train_data, train_label, model, optimizer)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416 call
 self._initialize(args, kwds, add_initializers_to=initializer_map)
 Downloads/3D/pointnet2/tf2_train.py:67 train_one_step  *
 pred = model.get_model(data, True, None)
 Downloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model  *
 transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)
 Downloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19 input_transform_net  *
 net = tf_util.conv2d(input_image, 64, [1,3],
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d  *
 kernel = _variable_with_weight_decay('weights',
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45 _variable_with_weight_decay  *
 var = _variable_on_cpu(name, shape, initializer)
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu  *
 var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521 _true_getter
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936 _get_single_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260 call
 return cls._variable_v1_call(*args, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call
 shape=shape)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60 getter
 return captured_getter(captured_previous, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope
 lifted_initializer_graph=lifted_initializer_graph, **kwds)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264 call
 return super(VariableMetaclass, cls).call(*args, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139 init
 initial_value() if init_from_fn else initial_value,
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908 
 partition_info=partition_info)
 <denchmark-code>TypeError: __call__() got an unexpected keyword argument 'partition_info'
 </denchmark-code>
 
 ##############################################################
 HOW CAN I SOLVE IT?
 THANKS, IN ADVANCE.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,murdockhou,2019-06-27T17:56:02Z,"
 		Can you file a separate issue?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,murdockhou,2019-06-28T00:16:15Z,"
 		Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch *
 train_one_step(train_data, train_label, model, optimizer)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416 call
 self._initialize(args, kwds, add_initializers_to=initializer_map)
 Downloads/3D/pointnet2/tf2_train.py:67 train_one_step *
 pred = model.get_model(data, True, None)
 Downloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model *
 transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)
 Downloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19 input_transform_net *
 net = tf_util.conv2d(input_image, 64, [1,3],
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d *
 kernel = _variable_with_weight_decay('weights',
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45 _variable_with_weight_decay *
 var = _variable_on_cpu(name, shape, initializer)
 Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu *
 var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569 get_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521 _true_getter
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936 _get_single_variable
 aggregation=aggregation)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260 call
 return cls._variable_v1_call(*args, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call
 shape=shape)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60 getter
 return captured_getter(captured_previous, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347 variable_capturing_scope
 lifted_initializer_graph=lifted_initializer_graph, **kwds)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264 call
 return super(VariableMetaclass, cls).call(*args, **kwargs)
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139 init
 initial_value() if init_from_fn else initial_value,
 anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908
 partition_info=partition_info)
 TypeError: call() got an unexpected keyword argument 'partition_info'
 Above Error is the main issue.
 I Guess it came from below this lines.
 def _variable_on_cpu(name, shape, initializer, use_fp16=False):
 . . . . . . . .~ skip
 with tf.device(""/cpu:0""):
 dtype = tf.float16 if use_fp16 else tf.float32
 var = tf.compat.v1.get_variable(name, shape, initializer=initializer, dtype=dtype)
 return var
 Thanks you <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
 
 		",10.0,murdockhou,2019-06-28T15:12:47Z,"
 		I meant a separate issue with full instructions to reproduce. This looks
 like it's coming from mixing tf v1 layers with tf v2 initializers.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Thu, Jun 27, 2019 at 5:23 PM tolry418 ***@***.***> wrote:
  Downloads/3D/pointnet2/tf2_train.py:94 train_one_epoch *
  train_one_step(train_data, train_label, model, optimizer)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:416
  call
  self._initialize(args, kwds, add_initializers_to=initializer_map)
  Downloads/3D/pointnet2/tf2_train.py:67 train_one_step *
  pred = model.get_model(data, True, None)
  Downloads/3D/pointnet2/models_from_pointnet/pointnet_cls.py:27 get_model *
  transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)
  Downloads/3D/pointnet2/models_from_pointnet/transform_nets.py:19
  input_transform_net *
  net = tf_util.conv2d(input_image, 64, [1,3],
  Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:164 conv2d *
  kernel = _variable_with_weight_decay('weights',
  Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:45
  _variable_with_weight_decay *
  var = _variable_on_cpu(name, shape, initializer)
  Downloads/3D/pointnet2/utils_from_pointnet/tf2_util.py:21 _variable_on_cpu
  *
  var = tf.compat.v1.get_variable(name, shape, initializer=initializer,
  dtype=dtype)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1503
  get_variable
  aggregation=aggregation)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:1246
  get_variable
  aggregation=aggregation)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:569
  get_variable
  aggregation=aggregation)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:521
  _true_getter
  aggregation=aggregation)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:936
  _get_single_variable
  aggregation=aggregation)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:260
  call
  return cls._variable_v1_call(*args, **kwargs)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:221
  _variable_v1_call
  shape=shape)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:60
  getter
  return captured_getter(captured_previous, **kwargs)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:347
  variable_capturing_scope
  lifted_initializer_graph=lifted_initializer_graph, **kwds)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py:264
  call
  return super(VariableMetaclass, cls).call(*args, **kwargs)
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py:139
  init
  initial_value() if init_from_fn else initial_value,
 
  anaconda3/envs/tf_2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py:908
  partition_info=partition_info)
 
  TypeError: *call*() got an unexpected keyword argument 'partition_info'
 
  Above Error is the main issue.
  I Guess it came from below this lines.
 
  def _variable_on_cpu(name, shape, initializer, use_fp16=False):
  . . . . . . . .~ skip
  with tf.device(""/cpu:0""):
  dtype = tf.float16 if use_fp16 else tf.float32
  var = *tf.compat.v1.get_variable*(name, shape, initializer=initializer,
  dtype=dtype)
  return var
 
  Thanks you @alextp <https://github.com/alextp>
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#26665?email_source=notifications&email_token=AAABHRJXDK7Y6SOKZWLZCTDP4VKWRA5CNFSM4G5UOJCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODYYWV5I#issuecomment-506555125>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AAABHRPL2UGAC5CO6RKBJLLP4VKWRANCNFSM4G5UOJCA>
  .
 
 
 -- 
  - Alex
 
 		",11.0,murdockhou,2019-07-24T02:01:13Z,"
 		Now tf 2.0.0beta0 has solved this problem... Thank <denchmark-link:https://github.com/pavithrasv>@pavithrasv</denchmark-link>
  !
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26684,girving,2019-03-14T05:08:31Z,2019-03-31T03:41:34Z,Repeatedly allocating a graph and summary writer leaks memory,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I have
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.3
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): v1.12.0-10061-gf3954bf900 1.13.1
 Python version: 3.7.2
 Bazel version (if compiling from source): 0.23.1
 GCC/Compiler version (if compiling from source): Apple LLVM version 10.0.0 (clang-1000.11.45.5)
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 Repeatedly allocating a graph and making a summary writer leaks memory.
 Describe the expected behavior
 Memory should be freed when the graph leaves scope.
 Code to reproduce the issue
 <denchmark-code>#!/usr/bin/env python3
 
 import resource
 import tensorflow as tf
 
 prev = 0
 while True:
     peak = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
     print(f'peak memory = {peak:,} (+{peak-prev:,})')
     prev = peak
 
     with tf.Graph().as_default(), tf.init_scope():
         tf.contrib.summary.create_file_writer('/tmp/tb')
 </denchmark-code>
 
 Other info / logs
 Here's what the output looks like:
 <denchmark-code>peak memory = 174,493,696 (+174,493,696)
   
 WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
 For more information, please see:
   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
   * https://github.com/tensorflow/addons
 If you depend on functionality not listed there, please file an issue.
 
 peak memory = 202,215,424 (+27,721,728)
 peak memory = 202,264,576 (+49,152)
 peak memory = 202,309,632 (+45,056)
 peak memory = 202,358,784 (+49,152)
 peak memory = 202,432,512 (+73,728)
 peak memory = 202,473,472 (+40,960)
 peak memory = 202,522,624 (+49,152)
 peak memory = 202,567,680 (+45,056)
 peak memory = 202,604,544 (+36,864)
 peak memory = 202,641,408 (+36,864)
 peak memory = 202,694,656 (+53,248)
 peak memory = 202,739,712 (+45,056)
 peak memory = 202,784,768 (+45,056)
 peak memory = 202,829,824 (+45,056)
 peak memory = 202,878,976 (+49,152)
 peak memory = 202,919,936 (+40,960)
 peak memory = 202,981,376 (+61,440)
 peak memory = 203,026,432 (+45,056)
 peak memory = 203,067,392 (+40,960)
 ...
 peak memory = 999,665,664 (+49,152)
 peak memory = 999,718,912 (+53,248)
 peak memory = 999,768,064 (+49,152)
 peak memory = 999,817,216 (+49,152)
 peak memory = 999,866,368 (+49,152)
 peak memory = 999,915,520 (+49,152)
 peak memory = 999,964,672 (+49,152)
 peak memory = 1,000,009,728 (+45,056)
 peak memory = 1,000,058,880 (+49,152)
 peak memory = 1,000,108,032 (+49,152)
 peak memory = 1,000,161,280 (+53,248)
 peak memory = 1,000,202,240 (+40,960)
 peak memory = 1,000,255,488 (+53,248)
 ...
 </denchmark-code>
 
 	",1.0,girving,2019-03-14T15:57:28Z,"
 		Forgot that I was using a slightly messy TensorFlow tree.  I've reconfirmed that the bug persists at <denchmark-link:https://github.com/tensorflow/tensorflow/pull/26705>#26705</denchmark-link>
 , which is <denchmark-link:https://github.com/tensorflow/tensorflow/commit/5b24fba0857394dab67359963726b3bcce071575>5b24fba</denchmark-link>
  plus a one line header include addition to make it build on my machine.
 		",2.0,girving,2019-03-15T16:20:00Z,"
 		No more TF bugs, <denchmark-link:https://github.com/skye>@skye</denchmark-link>
 ? :)
 		",3.0,girving,2019-03-15T16:42:58Z,"
 		<denchmark-link:https://github.com/nfelt>@nfelt</denchmark-link>
  are you familiar with summary writers?
 		",097fc1cdef5c56d4bb239a5a44bf950f0b1c4d37,Nick Felt,2019-03-30 20:38:06-07:00,MODIFY,3,tensorflow\python\kernel_tests\summary_ops_test.py,tensorflow\python\kernel_tests\summary_ops_test.py,1.0,733,,MODIFY,2.0,tensorflow\python\ops\summary_ops_v2.py,tensorflow\python\ops\summary_ops_v2.py,4.0,girving,2019-03-15T16:45:09Z,"
 		<denchmark-link:https://github.com/skye>@skye</denchmark-link>
  To clarify what I wrote wasn't intended to ask you to do anything, was just expressing sympathy as a fellow ex-tensorflow person who occasionally gets added to bugs.
 		",5.0,girving,2019-03-15T17:15:57Z,"
 		Haha no worries <denchmark-link:https://github.com/girving>@girving</denchmark-link>
 , I just switched teams this week, so still figuring out what to do with all my github issues :)
 		",6.0,girving,2019-03-15T20:12:37Z,"
 		Thanks for the report.  I can reproduce this against last night's tf-nightly on both macOS and Linux (for anyone else reproducing, maxrss is in bytes on macOS but kb on linux, so the raw numbers are about 1000x smaller).
 I also could still reproduce this even with tf.init_scope() removed.
 cc <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  if you have any intuition about what might be causing this.
 		",1.0,220,"219,220",__init__,"self,shared_name,init_op_fn,name,v2",208,220,,,,,,,,,,,,,,,testNoMemoryLeak_eagerMode,self,733,736,1.0,"727,728,729,730",727,testEagerMemory,self,727,730,1.0,"727,728,729,730",727,testNoMemoryLeak_graphMode,self,727,730,,,,,,,,1.0,539,,summary_writer_initializer_op,,526,539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,girving,2019-03-15T20:21:13Z,"
 		<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  I think the issue is that SUMMARY_WRITER_INIT_OP keeps a strong reference to the graph instead of a weak reference, so the graph can never be GC'd.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,girving,2019-03-20T19:54:16Z,"
 		It looks like there are two problems, one large and one small:
  Every  has a strong reference to the graph it lives inside (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1921>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/ops.py#L1921</denchmark-link>
 ).  This should probably be a weak reference.  This is presumably the cause of the leak, since we store a long lived reference to the op in  at <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L221>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/summary_ops_v2.py#L221</denchmark-link>
 .
   should be something like a <denchmark-link:https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary>https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary</denchmark-link>
  with keys being the graphs, not keys being strings that live forever.  Even if we fix the large issue the small issue would remain, and thus pretty much any use of  is a bug.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,girving,2019-03-21T15:42:45Z,"
 		Update: I made a brief attempt at removing that one reference, but it didn't work, so I think there are others.  Probably a more concerted push is required.
 		",10.0,girving,2019-03-25T17:51:41Z,"
 		<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  <denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  Did you get a chance to look at this?  I want to make sure it isn't dropped, since it's blocking my upgrade of TensorFlow.
 		",11.0,girving,2019-03-25T17:58:57Z,"
 		<denchmark-link:https://github.com/nfelt>@nfelt</denchmark-link>
  can you look?
 		",12.0,girving,2019-03-29T17:37:52Z,"
 		What's the likely ETA here?  I'm still blocked from upgrading due to this, so if no one is planning to fix I'd like to know for my own planning purposes.  In particular, if it's going to be weeks more I will have to fix it myself, but then you have to be happy with my weakref design decisions.
 		",13.0,girving,2019-03-29T17:49:28Z,"
 		I will try to take a closer look today, but if _SUMMARY_WRITER_INIT_OP isn't the main issue then I don't have a good guess at what the leak might be so it may take some time to figure this out.
 Just so I understand, which upgrade path exactly are you blocked on?  Is this upgrading from tf.summary to tf.contrib.summary?  Or did you notice that this leak occurs across a TF version update?
 		",14.0,girving,2019-03-29T17:52:22Z,"
 		<denchmark-link:https://github.com/nfelt>@nfelt</denchmark-link>
  Thanks!  The leak appears going from TensorFlow 1.12 to 1.13, so it's blocking the 1.13 upgrade (and therefore also my Python 3.7 upgrade).
 		",15.0,girving,2019-03-29T18:00:57Z,"
 		Note that it's quite possibly I simply failed to fix the _SUMMARY_WRITER_INIT_OP reference, but as I mentioned it does seem like every use of _graph_key is a bug, and therefore it feels likely that there are other strong references.
 		",16.0,girving,2019-03-29T20:41:14Z,"
 		Can you check whether using gc frees the graph?
 I.e., is this a simple reference cycle problem, or is a reference to the graph kept hidden someplace?
 		",17.0,girving,2019-03-29T20:41:49Z,"
 		Ah... Of course as long as the summary init op reference exists, that won't help.
 		",18.0,girving,2019-03-29T20:51:13Z,"
 		Nope, gc.collect has no effect.
 		",19.0,girving,2019-03-29T21:00:48Z,"
 		Per discussion with <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  I think what's easiest here is to revert the part of <denchmark-link:https://github.com/tensorflow/tensorflow/commit/aa8f428a9310b3fd8371bddf612e480b27618b2e>aa8f428</denchmark-link>
  that changed this from a graph collection to a python dict.  That seems very likely to be the root cause, and it was changed due to deprecation of global collections, but this is a 1.x-only usage anyway, and that seems like the easiest way to fix the regression.
 		",20.0,girving,2019-03-29T21:04:38Z,"
 		That sounds good.  Arguably global variables with references to graphs should be even more deprecated. :)
 		",21.0,girving,2019-03-31T03:41:35Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26684>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26684>No</denchmark-link>
 
 		",22.0,girving,2019-04-01T21:14:46Z,"
 		I've confirmed that this fixes both my minimized test case and my unminimized original code.  Thank you <denchmark-link:https://github.com/nfelt>@nfelt</denchmark-link>
 !
 		",23.0,girving,2019-04-01T22:05:24Z,"
 		Glad to hear that :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26808,alper111,2019-03-17T17:41:40Z,2019-03-27T21:01:15Z,[TF 2.0] unconnected_gradients = 'zero' does not work,"
 System information
 
 OS Platform and Distribution: MacOS 10.14.3
 TensorFlow installed from binary
 TensorFlow version: 2.0.0a0
 Python version: 3.7.2
 
 I try to get gradients w.r.t. model parameters. Though I was getting None values. Here is an example:
 <denchmark-code>> import tensorflow as tf
 > import tensorflow.keras.layers as layers
 
 > model = tf.keras.Sequential()
 > model.add(layers.Dense(10, input_shape=(2,)))
 > with tf.GradientTape() as tape:
 >   loss = tf.random.normal((10, 10))
 > grads = tape.gradient(loss, model.trainable_variables, unconnected_gradients='zero')
 > print(grads)
 [None, None]
 </denchmark-code>
 
 I expect these values to be zero. Though they are not.
 	",1.0,alper111,2019-03-26T23:37:52Z,"
 		We need to add special treatment for DT_RESOURCE tensors when building the zeros tensors <denchmark-link:https://github.com/tensorflow/tensorflow/blob/919b38007ea755a5b5ec87af324c91f55dce6717/tensorflow/python/eager/pywrap_tfe_src.cc#L1764>here</denchmark-link>
 . Maybe we can return a float32 with zeros here? <denchmark-link:https://github.com/akshaym>@akshaym</denchmark-link>
  could you look into this?
 		",2.0,alper111,2019-03-27T21:01:16Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=26808>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=26808>No</denchmark-link>
 
 		",,,,,a79ed9c304bf9c1971fe3df4f61a0d0ab515eff9,Akshay Modi,2019-03-27 13:58:46-07:00,MODIFY,0,tensorflow\python\eager\backprop.py,tensorflow\python\eager\backprop.py,0.0,"949,961",,MODIFY,1.0,tensorflow\python\eager\backprop_test.py,tensorflow\python\eager\backprop_test.py,,,,,,,,,,,,,1.0,"861,862,863,864,865,866,867,868,869,870,871",,testUnconnectedGradientsVariablesZeros,self,861,871,MODIFY,2.0,tensorflow\python\eager\imperative_grad.py,tensorflow\python\eager\imperative_grad.py,1.0,"32,33,34,35,36,37","32,33,34,35,36,37",MODIFY,0.0,tensorflow\python\eager\pywrap_tfe.h,tensorflow\python\eager\pywrap_tfe.h,0.0,184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,imperative_grad,"tape,target,sources,output_gradients,unconnected_gradients",32,37,,,,,MODIFY,0.0,tensorflow\python\eager\pywrap_tfe_src.cc,tensorflow\python\eager\pywrap_tfe_src.cc,0.0,"1677,1755,1756,1757",1754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"32,33,34,35,36,37","32,33,34,35,36,37",imperative_grad,"tape,target,sources,output_gradients,sources_raw,unconnected_gradients",32,37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26902,ppwwyyxx,2019-03-19T21:52:17Z,2019-12-26T17:00:29Z,tf_upgrade_v2 does not preserve file attributes and symbolic links,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):n/a
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
 TensorFlow installed from (source or binary):binary
 TensorFlow version (use command below): tf2 preview nightly yesterday
 Python version:3.7
 Bazel version (if compiling from source):n/a
 GCC/Compiler version (if compiling from source):n/a
 CUDA/cuDNN version:n/a
 GPU model and memory:n/a
 
 
 
 tf_upgrade_v2 changes executable files to non-executable files. I expect executable files are still executable after the upgrade.
 
 
 tf_upgrade_v2 always changes symbolic links to regular files. However I expect:
 (1)For in-place upgrade, modify the file the link points to, but the symbolic link should be the same.
 (2) For non-in-place upgrade, if --intree and --outtree is used, symbolic links which point to files within the tree should become symbolic links pointing to the new file in the outtree. Symbolic links which point to external files should become a regular file.
 (3) For non-in-place single-file upgrade, the output should be a regular file.
 
 
 	",1.0,ppwwyyxx,2019-07-17T23:17:32Z,"
 		Hi <denchmark-link:https://github.com/ppwwyyxx>@ppwwyyxx</denchmark-link>
 ! Do you have a simple example where tf_upgrade_v2 changes a file to be non-executable?
 I just tried the following:
 <denchmark-code>touch test1/bar.sh
 chmod +x test1/bar.sh
 pip install tf-nightly-2.0-preview
 tf_upgrade_v2 --intree=test1/ --outtree=test2/ --copyotherfiles=True
 </denchmark-code>
 
 After these commands are run, test2/bar.sh does have executable permissions. Could it be instead that file ownership changed?
 Sorry for delay looking at this issue.
 		",2.0,ppwwyyxx,2019-07-25T22:34:34Z,"
 		I submitted a change to fix symbolic link behavior. However, the behavior is slightly different than you proposed. Specifically, I keep symlink unchanged if it points to external file (as opposed to converting it to regular file).
 I was thinking that someone who has code in a different directory might want to update it separately, while keeping the symlink.
 I will keep this issue open for the executable permission. Per my previous comment, I need some example to reproduce it.
 		",3.0,ppwwyyxx,2019-11-11T19:28:04Z,"
 		<denchmark-link:https://github.com/ppwwyyxx>@ppwwyyxx</denchmark-link>
  Can you check whether the issue resolved in the  or ? If this was resolved, please close the issue. Thanks!
 		",0fa0d44944abd86578fa076802f5a8a7490d5656,Anna R,2019-07-24 18:11:15-07:00,MODIFY,1,tensorflow\tools\compatibility\ast_edits.py,tensorflow\tools\compatibility\ast_edits.py,1.0,"1077,1078,1079",,MODIFY,4.0,tensorflow\tools\compatibility\ast_edits_test.py,tensorflow\tools\compatibility\ast_edits_test.py,4.0,ppwwyyxx,2019-12-26T12:32:06Z,"
 		It has been 44 days with no activity and the awaiting response label was assigned. Is this still an issue?
 		",5.0,ppwwyyxx,2019-12-26T17:00:29Z,"
 		Closing due to no other updates
 		",6.0,ppwwyyxx,2019-12-26T17:00:31Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26902>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/26902>No</denchmark-link>
 
 		",1.0,"670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690",,testUpgradeCopyWithSymlinkInDifferentDir,self,670,690,,,,,,,,,,,,,,,process_tree_inplace,"self,root_directory",1061,1085,,,,,,,,,,,,,,,,,,,,,,1.0,"627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647",,testUpgradeInPlaceWithSymlinkInDifferentDir,self,627,647,1.0,"609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625",,testUpgradeInplaceWithSymlink,self,609,625,1.0,"649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668",,testUpgradeCopyWithSymlink,self,649,668,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27202,Xiadalei,2019-03-27T15:42:29Z,2019-04-06T00:44:28Z,Tflite JNI wraps seems failing to release int array.,"
 Hi, it seems that current impl of tflite jni overlooked ref release for array, And the current tflite really could make JNI reference table overflow some phones with Android 4.4.4 (API 19).
 How:
 Just invoke resizeInput every time you run interpreter, even put the same int array to it. You can see the reference table is booming.
 version:
 I've tried 1.13.1 and 0.0.0-nightly, it's the same.
 the relevant code is <denchmark-link:https://github.com/tensorflow/tensorflow/blob/c18034bc927f733e5e5a43d0c421775f969e0d04/tensorflow/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L104>here</denchmark-link>
 
 The strange thing is the code above, althrough didn't deal with release in some situation, should work fine with same int array.
 	",1.0,Xiadalei,2019-04-05T21:27:22Z,"
 		You're absolutely right, will push a fix shortly.
 		",2.0,Xiadalei,2019-04-06T00:44:28Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27202>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27202>No</denchmark-link>
 
 		",,,,,009fde664530e6616e5aa1f882ff497e9e435924,Jared Duke,2019-04-05 17:40:01-07:00,MODIFY,3,tensorflow\lite\java\src\main\native\nativeinterpreterwrapper_jni.cc,tensorflow\lite\java\src\main\native\nativeinterpreterwrapper_jni.cc,1.0,"104,112,114,115,116,117,118,119,120","104,113,114,115,116,117,121",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,areDimsDifferent,"env,tensor,dims",104,122,1.0,"104,112,114,115,116,117,118,119,120,124","104,113,114,115,116,117,121",AreDimsDifferent,"env,tensor,dims",104,125,1.0,452,449,Java_org_tensorflow_lite_NativeInterpreterWrapper_resizeInput,"env,clazz,interpreter_handle,error_handle,input_idx,dims",433,464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27282,mattn,2019-03-29T10:40:12Z,2019-06-21T23:48:20Z,/tensorflow/lite/experimental/c/c_api_types.h is not readable on Windows filesystem.,"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 TensorFlow installed from (source or binary): source
 TensorFlow version: f089b31
 Python version: No
 Installed using virtualenv? pip? conda?: No
 Bazel version (if compiling from source): No
 GCC/Compiler version (if compiling from source): gcc 8.3
 CUDA/cuDNN version: No
 GPU model and memory: No
 
 Describe the problem
 /tensorflow/lite/experimental/c/c_api_types.h is symbolic link to /tensorflow/lite/c/c_api_internal.h.  On DOS compatible file system, it is replaced with following text file.
 <denchmark-code>../../c/c_api_internal.h
 </denchmark-code>
 
 Provide the exact sequence of commands / steps that you executed before running into the problem
 Clone repository, and make sure file /tensorflow/lite/experimental/c/c_api_types.h  is NOT a symbolic link on Windows.
 Any other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 No
 	",1.0,mattn,2019-03-29T10:43:24Z,"
 		I suggest that the file c_api_types.h is replaced to be:
 <denchmark-code>#include ""tensorflow/lite/c/c_api_internal.h""
 </denchmark-code>
 
 Or fix all files which include c_api_types.h to include c_api_internal.h.
 		",2.0,mattn,2019-04-02T15:18:56Z,"
 		We'll be removing the symlink in the next week or so, which should resolve the problem. Is this breaking your build?
 		",3.0,mattn,2019-04-03T00:07:26Z,"
 		As I mentioned above, symbolic link is not created with git clone. Windows Git makes plain text file that the path is written.
 		",04e311bf7628eac8b0334a7419442f1009487d7f,Jared Duke,2019-06-21 16:44:25-07:00,MODIFY,0,tensorflow\lite\c\BUILD,tensorflow\lite\c\BUILD,0.0,"19,20,21",,MODIFY,0.0,tensorflow\lite\experimental\c\BUILD,tensorflow\lite\experimental\c\BUILD,4.0,mattn,2019-06-21T09:08:26Z,"
 		<denchmark-link:https://github.com/jdduke>@jdduke</denchmark-link>
  Do you have any plans to introduce patch <denchmark-link:https://github.com/tensorflow/tensorflow/pull/27427>#27427</denchmark-link>
 ?
 This issue is important for us.
 		",5.0,mattn,2019-06-21T23:03:46Z,"
 		Apologies for the delay, hoping to land a fix by EOD. Thanks for your patience.
 		",6.0,mattn,2019-06-21T23:48:21Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27282>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27282>No</denchmark-link>
 
 		",0.0,"41,42,43,44,45,46,47,48,49",,,,,,MODIFY,0.0,tensorflow\lite\experimental\c\c_api.h,tensorflow\lite\experimental\c\c_api.h,0.0,24,24,MODIFY,0.0,tensorflow\lite\experimental\c\c_api_test.cc,tensorflow\lite\experimental\c\c_api_test.cc,0.0,,23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,mattn,2019-06-22T01:55:28Z,"
 		<denchmark-link:https://github.com/jdduke>@jdduke</denchmark-link>
  do you mean exerimental feature will be merged into standard feature soon?
 		",,,,,,,,,DELETE,0.0,tensorflow\lite\experimental\c\c_api_types.h,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\experimental\objc\TensorFlowLiteObjC.podspec,tensorflow\lite\experimental\objc\TensorFlowLiteObjC.podspec,0.0,,27,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\tools\make\Makefile,tensorflow\lite\tools\make\Makefile,0.0,,"102,103",,,,,8.0,mattn,2019-06-24T17:48:02Z,"
 		Yes we're hoping to migrate the C API bindings out of experimental.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27292,SpringsteinM,2019-03-29T14:25:33Z,2019-04-12T00:31:01Z,keras.layers.RNN with contants,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): No
 TensorFlow version (use command below): 1.13 and 1.14
 Python version: 3.7
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.1 and 10.0
 GPU model and memory: 1080 Ti
 
 Describe the current behavior
 TypeError: can only concatenate list (not ""tuple"") to list in RNN::build() if a call the RNN with a Tensor as constants.
 Describe the expected behavior
 Basically the build() function of the RNNCellWithConstants should be called, with the input_shape = [(3,3,5), (3,3)]
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 
 
 class RNNCellWithConstants(tf.keras.layers.Layer):
 
     def __init__(self, **kwargs):
         self.state_size = 5
         super(RNNCellWithConstants, self).__init__(**kwargs)
 
     def build(self, input_shape):
         print(input_shape)
         self.built = True
 
     def call(self, inputs, states, constants):
         print(inputs, states, constants)
         return inputs, [inputs]
 
 
 # Test basic case.
 x = tf.keras.Input((None, 5))
 c = tf.keras.Input((3,))
 cell = RNNCellWithConstants()
 layer = tf.keras.layers.RNN(cell)
 y = layer(x, constants=c) # Works as expected.
 
 # Test basic case.
 x = tf.zeros([3, 3, 5], dtype=tf.float32)
 c = tf.zeros([3, 3], dtype=tf.float32)
 cell = RNNCellWithConstants()
 layer = tf.keras.layers.RNN(cell)
 y = layer(x, constants=c) # Crash with the following error
 </denchmark-code>
 
 Other info / logs
 Exception from example:
 <denchmark-code>Traceback (most recent call last):
   File ""bug.py"", line 25, in <module>
     y = layer(x, constants=c)
   File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 690, in __call__
     return super(RNN, self).__call__(inputs, **kwargs)
   File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 585, in __call__
     self._maybe_build(inputs)
   File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1706, in _maybe_build
     self.build(input_shapes)
   File ""/home/matthias/.local/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 555, in build
     self.cell.build([step_input_shape] + constants_shape)
 TypeError: can only concatenate list (not ""tuple"") to list
 </denchmark-code>
 
 If I correct the error temporarily I come to another problem, that the input shapes at build call are not correct any more: [(3, 5), (5,)]
 So I think the mistake lies in that distinction:
 <denchmark-code>    if is_keras_tensor:
       # Compute the full input spec, including state and constants
       full_input = [inputs] + additional_inputs
       # The original input_spec is None since there could be a nested tensor
       # input. Update the input_spec to match the inputs.
       full_input_spec = [None for _ in range(len(nest.flatten(inputs)))
                         ] + additional_specs
       # Perform the call with temporarily replaced input_spec
       self.input_spec = full_input_spec
       output = super(RNN, self).__call__(full_input, **kwargs)
       # Remove the additional_specs from input spec and keep the rest. It is
       # important to keep since the input spec was populated by build(), and
       # will be reused in the stateful=True.
       self.input_spec = self.input_spec[:-len(additional_specs)]
       return output
     else:
       if initial_state is not None:
         kwargs['initial_state'] = initial_state
       if constants is not None:
         kwargs['constants'] = constants
       return super(RNN, self).__call__(inputs, **kwargs)
 </denchmark-code>
 
 If I set is_keras_tensor to True, everything will behave as expected.
 	",1.0,SpringsteinM,2019-04-08T04:34:40Z,"
 		I want to work on this issue please <denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>
  can you guide me please?
 		",2.0,SpringsteinM,2019-04-12T00:30:57Z,"
 		Should be fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/3e8a80bce0f7ef0ab2ee49f3528a2652f26110f0>3e8a80b</denchmark-link>
  now.
 		",3.0,SpringsteinM,2019-04-12T00:31:02Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27292>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27292>No</denchmark-link>
 
 		",3e8a80bce0f7ef0ab2ee49f3528a2652f26110f0,Scott Zhu,2019-04-11 17:13:58-07:00,MODIFY,1,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,146,"143,147,148,149,150",MODIFY,16.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,,,,,,,,,,,,,1.0,,"408,409,410,411,412,413,414,415",test_rnn_cell_with_constants_layer.call,"self,inputs,states,constants",408,415,MODIFY,6.0,tensorflow\python\keras\layers\wrappers_test.py,tensorflow\python\keras\layers\wrappers_test.py,1.0,591,592,,,,,,,,build,"self,input_shape",141,155,,,,,,,,,,,,,,,,,,,,,,1.0,"1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337",,build,"self,input_shape",1324,1337,1.0,"467,468,469,470,471,472,473,475,476,477,478,479,480,482,483,484,485,486,487,488,489,490,491,492,493",,test_rnn_cell_with_non_keras_constants,self,467,493,1.0,383,"383,384,385,386",test_rnn_cell_with_constants_layer.__init__,"self,units,kwargs",383,386,1.0,,"417,418,419,420",test_rnn_cell_with_constants_layer.get_config,self,417,420,1.0,"543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573","543,544,545,547,548,549,550,556",test_rnn_cell_with_non_keras_constants_and_initial_state,self,543,573,,,,,test_Bidirectional_with_constants,self,586,625,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"36,39",36,__init__,"self,units,constant_size,kwargs",36,40,1.0,"36,39",36,__init__,"self,units,kwargs",36,39,1.0,52,"42,43,53",build,"self,input_shape",42,55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1348,1349,1350,1351",,get_config,self,1348,1351,1.0,,"388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406",test_rnn_cell_with_constants_layer.build,"self,input_shape",388,406,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1339,1340,1341,1342,1343,1344,1345,1346",,call,"self,inputs,states,constants",1339,1346,1.0,"543,544,545","538,539,540,541,542,543,544,545",test_rnn_cell_with_constants_layer_passing_initial_state.call,"self,inputs,states,constants",538,545,1.0,"383,423,424","380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,425,465",test_rnn_cell_with_constants_layer,self,379,465,1.0,,"513,514,515,516",test_rnn_cell_with_constants_layer_passing_initial_state.__init__,"self,units,kwargs",513,516,1.0,,"518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536",test_rnn_cell_with_constants_layer_passing_initial_state.build,"self,input_shape",518,536,1.0,"1318,1319,1320,1321,1322",,__init__,"self,units,constant_size,kwargs",1318,1322,1.0,"547,548,549,550","547,548,549,550",test_rnn_cell_with_constants_layer_passing_initial_state.get_config,self,547,550,1.0,"495,500","509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,538,539,540,541",test_rnn_cell_with_constants_layer_passing_initial_state,self,495,541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,634,635,test_Bidirectional_with_constants_layer_passing_initial_state,self,627,678,1.0,67,68,get_config,self,66,69,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27305,satnam6502,2019-03-29T21:37:25Z,2019-08-09T18:13:15Z,Document stride parameter for XlaBuilder::Slice,"
 Doc Link: <denchmark-link:https://www.tensorflow.org/xla/operation_semantics>https://www.tensorflow.org/xla/operation_semantics</denchmark-link>
 
 The documentation for <denchmark-link:https://www.tensorflow.org/xla/operation_semantics#slice>XlaBuilder::Slice</denchmark-link>
  does not mention the stride parameter.
 	",1.0,satnam6502,2019-08-09T17:04:25Z,"
 		In <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/client/xla_builder.h#L1206>xla_builder.h</denchmark-link>
 , the signature looks like:
 <denchmark-code>XlaOp Slice(XlaOp operand, absl::Span<const int64> start_indices,
             absl::Span<const int64> limit_indices,
             absl::Span<const int64> strides);
 </denchmark-code>
 
 In the docs, it looks like: Slice(operand, start_indices, limit_indices)
 		",,,,,,,,,369a886aab96fc081ad6637d7b413a339382b758,Sanjoy Das,2019-08-09 11:10:57-07:00,MODIFY,0,tensorflow\compiler\xla\g3doc\operation_semantics.md,tensorflow\compiler\xla\g3doc\operation_semantics.md,0.0,"2535,2536,2537,2538,2539",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2740,ppwwyyxx,2016-06-08T21:58:55Z,2016-06-30T18:31:52Z,ExponentialMovingAverage.average duplicates the current scope name,"
 Using tensorflow nightly.
 import tensorflow as tf
 with tf.name_scope('scope'):
     x = tf.Variable(42, dtype=tf.float32)
     ema = tf.train.ExponentialMovingAverage(decay=0.9)
     apply_op = ema.apply([x])
     average = ema.average(x)
     print average.name   # 'scope/scope/Variable/ExponentialMovingAverage:0'
     print ema.average_name(x)  # 'scope/Variable/ExponentialMovingAverage'
 	",1.0,ppwwyyxx,2016-06-13T23:49:16Z,"
 		The problem seems to come from <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L83>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/slot_creator.py#L83</denchmark-link>
 , where the name of an op is used to create a variable. But the name of the op already contain the current scope, therefore the variable to create will contain the scope twice.
 		",2.0,ppwwyyxx,2016-06-20T18:41:59Z,"
 		It's been a while. I suppose this is a bug right?..
 		",3.0,ppwwyyxx,2016-06-27T18:26:10Z,"
 		Yes that looks like a bug. I'm asking the author of the code to do the quick fix.
 		",a2b9788ce440c350d4e3fef53fe0c51ba1c10c1a,Frank Li,2016-06-29 17:33:57-07:00,MODIFY,1,tensorflow\python\training\moving_averages.py,tensorflow\python\training\moving_averages.py,1.0,"342,343,344,345",342,MODIFY,1.0,tensorflow\python\training\moving_averages_test.py,tensorflow\python\training\moving_averages_test.py,4.0,ppwwyyxx,2016-06-28T13:33:00Z,"
 		Unfortunately, after checking internally, fixing this bug would result in losing backward compatibility, which we would like to avoid both for internal and external users. So, right now, I am closing the issue.
 <denchmark-link:https://github.com/girving>@girving</denchmark-link>
 , <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
 
 		",5.0,ppwwyyxx,2016-06-28T14:48:40Z,"
 		Isn't it true that ExponentialMovingAverage is used currently almost always outside any scopes? It shouldn't break (many) existing use cases. If I remember correctly, the class simply doesn't work well if the scope name is duplicated and users have to do manual hacking to fix the variable names.
 The workaround that I'm using currently is to wrap the class with
 with tf.name_scope(None):
    ema = tf.train.ExponentialMovingAverage(...)
    # [rest of the code]
 		",6.0,ppwwyyxx,2016-06-28T21:57:27Z,"
 		It's not just ExponentialMovingAverage, but also the accumulators used by optimizers that have this behavior. Anyone who's using either of these inside a scope of any kind would suffer backwards-incompatible checkpoints due to the naming difference. Especially given there is an easy workaround (i.e. do not create / use ExponentialMovingAverage or Optimizers inside a name scope), the pain that fixing this would cause outweighs the benefits.
 		",1.0,"211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240",,testAverageVariablesNamesRespectScope,self,211,240,MODIFY,1.0,tensorflow\python\training\slot_creator_test.py,tensorflow\python\training\slot_creator_test.py,1.0,"76,77,78,79,80,81,82",,,,,,,,,average_name,"self,var",320,345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ppwwyyxx,2016-06-28T22:03:38Z,"
 		We don't need to share the create_slot function between EMA and optimizers. It's essentially just a variable creation.
 		",testCreateSlotFromVariableRespectsScope,self,76,82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,ppwwyyxx,2016-06-28T22:28:19Z,"
 		Two notes:
 
 create_slots is more than just a variable creation -- there's some subtle logic in there to deal with properly supporting slots for partitioned variables. So I'd think reusing the logic makes sense.
 Even if we were to only fix this for ExponentialMovingAverage, it would still be a backwards-incompatible graph change for anyone who was using EMA inside a name scope.
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,ppwwyyxx,2016-06-28T22:36:09Z,"
 		Is it possible to keep the compatibility for now but print a warning about
 a future change that will break compatibility (similar to numpy) ?
 		",10.0,ppwwyyxx,2016-06-29T17:31:12Z,"
 		<denchmark-link:https://github.com/ppwwyyxx>@ppwwyyxx</denchmark-link>
  I'm not sure I follow.  How would we be able to safely break backwards compatibility in the future?  An advance warning that models will break is not a sufficient fix.
 		",11.0,ppwwyyxx,2016-06-29T20:50:58Z,"
 		I mean it will provide a time window for people to make necessary changes to prepare for a future break. But right it won't safely break compatibility.
 		",12.0,ppwwyyxx,2016-06-29T21:50:29Z,"
 		<denchmark-link:https://github.com/ppwwyyxx>@ppwwyyxx</denchmark-link>
  I don't think it is worth making this change.  The name fix would be nice, and I agree that the current code is a mistake, but there is a high bar for breaking existing models.
 		",13.0,ppwwyyxx,2016-11-27T06:16:25Z,"
 		On latest tensorflow, the with tf.name_scope(None) hack still introduces variables with duplicated scope name:
 def f():
     v = tf.get_variable('W', [1])
     v = v + 1
     with tf.name_scope(None):
         ema = tf.train.ExponentialMovingAverage(decay=0.9, name='EMA')
         emaop = ema.apply([v])
         average_v = ema.average(v)
 
 with tf.variable_scope('scope'):
     f()
 print([k.name for k in tf.global_variables()])
 will print:
 <denchmark-code>[u'scope/W:0', u'scope/add/EMA:0', u'scope/scope/add/EMA/biased:0', 
 u'scope/scope/add/EMA/local_step:0']
 </denchmark-code>
 
 		",14.0,ppwwyyxx,2016-11-30T07:00:23Z,"
 		The recently-introduced new variables in EMA also brings error when using with reuse=True.
 The example below seems like a common pattern in batch normalization. It works before, but now:
 def f(v):
     ema = tf.train.ExponentialMovingAverage(0.9)
     vema = ema.apply([v])
     return vema
 
 with tf.variable_scope('s'):
     v1 = tf.get_variable('W', shape=[])
     v1 = v1 + 1
     f(v1)
 with tf.variable_scope('s', reuse=True):
     v2 = tf.get_variable('W', shape=[])
     v2 = v2 + 1
     f(v2)
 <denchmark-code>ValueError: Variable s/s_1/s_1/add/ExponentialMovingAverage/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
 </denchmark-code>
 
 		",15.0,ppwwyyxx,2016-12-01T23:30:41Z,"
 		There are a number of TF paradigms that previously worked because ExponentialMovingAverage didn't respect variable scopes (see <denchmark-link:https://github.com/tensorflow/tensorflow/issues/5652>#5652</denchmark-link>
 , for example). In the case you have here, I'm not sure that I understand the semantics of what you are trying to do: regardless of debiasing, it is an error to call apply on the same variable multiple times: ""ValueError: If the moving average of one of the variables is already being computed."" (from  description of ). Are you arguing that the wrong error message is thrown in this case?
 		",16.0,ppwwyyxx,2016-12-01T23:37:56Z,"
 		<denchmark-link:https://github.com/joel-shor>@joel-shor</denchmark-link>
  Oh. You can change 'W' to 'W2' and move it outside of the scope, it still throws error. I'm only trying to point out that EMA cannot work inside reuse=True.
 By the way I don't think I'm calling EMA on the same tensors, because calling  twice creates two .
 		",17.0,ppwwyyxx,2016-12-05T16:37:15Z,"
 		
 The issue should be fixed currently; zero_debias is no longer the default in ExponentialMovingAverage.
 There is a strong possibility that is will be the default in the future.
 ExponentialMovingAverage create variables, so for future safety you might consider using it as if it respected variable scopes (ie don't put it in resuse=True variable scopes that it doesn't need to be in)
 
 		",18.0,ppwwyyxx,2016-12-05T17:19:06Z,"
 		<denchmark-link:https://github.com/joel-shor>@joel-shor</denchmark-link>
  Thanks. Since there are dedicated issues for the bug we can use them to track. This issue is more about the naming -- and I see there are still very long variable names with duplicated scopes for the newly-introduced biased/local_step variables. Should there be a fix for this?
 def f(v):
     ema = tf.train.ExponentialMovingAverage(0.9)
     vema = ema.apply([v])
     return vema
 with tf.variable_scope('s'):
     v1 = tf.get_variable('W', shape=[10,10,10,10])
     v1 = v1 + 1
     f(v1)
 print [k.name for k in tf.all_variables()]
 # [u's/W:0', u's/s/add/ExponentialMovingAverage:0', u's/s/s/add/ExponentialMovingAverage/biased:0', u's/s/s/add/ExponentialMovingAverage/local_step:0']
 		",19.0,ppwwyyxx,2017-08-21T23:53:45Z,"
 		We are not going to fix this behavior. The reason is that there are two relevant scopes: the scope that the variable was created in, and the scope that the EMA was created in. There will be potential ambiguity if one is removed. Since removing the duplication isn't clearly better, we're going to keep this behavior.
 		",20.0,ppwwyyxx,2018-10-16T16:21:07Z,"
 		Hi <denchmark-link:https://github.com/joel-shor>@joel-shor</denchmark-link>
 ,  I am facing the same problem while trying to reuse batch-normalization in a network which is used twice. I get error ""/convnet_branch/conv1_1_bn/moments/Squeeze/ExponentialMovingAverage/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?""
 Can you suggest how to deal with this problem, I read all the above comments but couldn't figure out how it could be solved.
 Thanks a lot!
 		",21.0,ppwwyyxx,2018-11-25T08:34:31Z,"
 		Hi <denchmark-link:https://github.com/nainadhingra2012>@nainadhingra2012</denchmark-link>
 . On tensorflow 1.12.0, I had the same problem and fixed it by adding the line:
 <denchmark-code>        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
 </denchmark-code>
 
 before ema.apply
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27431,guillaumekln,2019-04-02T14:40:22Z,2019-04-11T18:20:19Z,Using layer classes as attribute throw an exception,"
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution: Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0.dev20190402
 Python version: 2.7.12
 
 Describe the current behavior
 When a layer class is used as attribute, the code will throw a TypeError exception when calling self._gather_children_attribute. It appears that the layer class is tracked.
 Describe the expected behavior
 Only layer instances should be tracked, not classes.
 Code to reproduce the issue
 import tensorflow as tf
 
 class Layer(tf.keras.layers.Layer):
 
     def __init__(self):
         super(Layer, self).__init__()
         self.layer_fn = tf.keras.layers.Dense
 
 layer = Layer()
 print(layer.variables)
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File ""tf2/class.py"", line 10, in <module>
     print(layer.variables)
   File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1330, in variables
     return self.weights
   File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 708, in weights
     return self.trainable_weights + self.non_trainable_weights
   File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 687, in trainable_weights
     nested = self._gather_children_attribute('trainable_weights')
   File ""/tmp/tf2/local/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1850, in _gather_children_attribute
     getattr(layer, attribute) for layer in nested_layers))
 TypeError: 'property' object is not iterable
 </denchmark-code>
 
 	",1.0,guillaumekln,2019-04-04T22:03:50Z,"
 		<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  this is a better project for someone working on TF Keras. I am realistically never going to get to it. (It's near code I've modified, but I don't think those modifications made the situation any worse.)
 		",2.0,guillaumekln,2019-04-04T22:07:33Z,"
 		Ack, I will take it from here.
 		",3.0,guillaumekln,2019-04-11T18:20:19Z,"
 		Should be fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/9d724a8e6034d321e97cdc9972d4d6e7adb3e3ca>9d724a8</denchmark-link>
  now.
 		",9d724a8e6034d321e97cdc9972d4d6e7adb3e3ca,Scott Zhu,2019-04-11 11:14:21-07:00,MODIFY,2,tensorflow\python\keras\engine\base_layer_test.py,tensorflow\python\keras\engine\base_layer_test.py,1.0,"549,550,551",,MODIFY,2.0,tensorflow\python\training\tracking\layer_utils.py,tensorflow\python\training\tracking\layer_utils.py,4.0,guillaumekln,2019-04-11T18:20:20Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27431>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27431>No</denchmark-link>
 
 		",,,,,,,,,1.0,"37,38",37,has_weights,obj,33,38,,,,,,,,,,,,,,,test_layer_class_not_tracked_as_sublayer.__init__,self,549,551,1.0,"544,545,546,547,548,549,550,551,552,553,554,555",,test_layer_class_not_tracked_as_sublayer,self,544,555,,,,,,,,,,,,,,,1.0,30,30,is_layer,obj,27,30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27455,LynnHo,2019-04-03T09:07:50Z,2019-04-09T21:52:57Z,TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.,"
 System information
 
 OS Platform and Distribution: Linux Ubuntu 18.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0-alpha0
 Python version: 3.6.8
 
 Describe the current behavior
 I built a keras model with only a tf.nn.relu, but the gradient seems to be None after being decorated by @tf.function
 Code to reproduce the issue
 
 tf.nn.relu + tf.keras.Model + @tf.function (this is the only case that produce None gradient)
 
 import tensorflow as tf
 
 z = tf.keras.Input(())
 h = tf.nn.relu(z)
 m = tf.keras.Model(z, h)
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 >>> None
 1.2 tf.nn.relu + tf.keras.Model without @tf.function
 def f(x):  # without @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 >>> tf.Tensor(20.0, shape=(), dtype=float32)
 
 tf.keras.layers.ReLU() + tf.keras.Model + @tf.function
 
 import tensorflow as tf
 
 z = tf.keras.Input(())
 h = tf.keras.layers.ReLU()(z)
 m = tf.keras.Model(z, h)
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 >>> tf.Tensor(20.0, shape=(), dtype=float32)
 2.2 tf.keras.layers.ReLU() + tf.keras.Model without @tf.function
 def f(x):  # without @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 >>> tf.Tensor(20.0, shape=(), dtype=float32)
 
 only tf.nn.relu
 
 import tensorflow as tf
 m = tf.nn.relu
 
 @tf.function
 def f(x):  # with @tf.function
     with tf.GradientTape() as t:
         t.watch(x)
         z = m(x ** 2)
     return t.gradient(z, x)
 
 print(f(tf.convert_to_tensor(10.0)))
 
 >>> tf.Tensor(20.0, shape=(), dtype=float32)
 So, I think its the problem between tf.nn.relu and tf.keras.Model? Besides, tf.nn.tanh has the same problem.
 	",1.0,LynnHo,2019-04-05T21:28:52Z,"
 		<denchmark-link:https://github.com/tomerk>@tomerk</denchmark-link>
  I think something is broken with the keras graph here since the tape isn't seeing it.
 Can you take a look, or help triage to the right person?
 		",2.0,LynnHo,2019-04-08T23:10:25Z,"
 		I have a fix for this that will be submitted soon.
 		",3.0,LynnHo,2019-04-09T21:52:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27455>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27455>No</denchmark-link>
 
 		",244cb0b925902a29c6a39c62fd1b80cb3797051b,A. Unique TensorFlower,2019-04-09 14:47:12-07:00,MODIFY,1,tensorflow\python\keras\engine\base_layer.py,tensorflow\python\keras\engine\base_layer.py,1.0,"2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181",,MODIFY,2.0,tensorflow\python\keras\layers\tensorflow_op_layer_test.py,tensorflow\python\keras\layers\tensorflow_op_layer_test.py,,,,,,,,,,,,,1.0,"205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226",,test_gradient_tape_in_function,self,205,226,,,,,,,,,,,,,,,_make_op,"self,inputs",2154,2184,,,,,,,,,,,,,,,,,,,,,,1.0,"213,214,215,216,217,218",,test_gradient_tape_in_function.f,x,213,218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27565,llan-ml,2019-04-06T09:31:57Z,2019-05-03T01:11:15Z,[TF==2.0.0a0] @tf.function raises ValueError when computing gradients,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 TensorFlow version (use command below): pip install tensorflow(-gpu)==2.0.0a0
 Python version: 3.6
 
 Describe the current behavior
 The code executes normally, but raise ValueError when computing gradients (tape.gradient) if I decorate the training function with @tf.function. The traceback is as follows:
 ---------------------------------------------------------------------------
 ValueError                                Traceback (most recent call last)
 ~/Workspaces/fgenl/run.py in ()
      80     for batch_id in range(num_batches_each_epoch):
      81         batch_data = data_generator.get_data() # v2
 ---> 82         loss, outputs = train_one_step(batch_data) # v2
      83         # _, loss, outputs, inputs = sess.run([opt_op, loss_, outputs_, batch_data])
      84         if loss_metrics is None:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
     424     # This is the first call of __call__, so we have to initialize.
     425     initializer_map = {}
 --> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
     427     if self._created_variables:
     428       try:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
     368     self._concrete_stateful_fn = (
     369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
 --> 370             *args, **kwds))
     371
     372     def invalid_creator_scope(*unused_args, **unused_kwds):
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
    1311     if self._input_signature:
    1312       args, kwargs = None, None
 -> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
    1314     return graph_function
    1315
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
    1578           or call_context_key not in self._function_cache.missed):
    1579         self._function_cache.missed.add(call_context_key)
 -> 1580         graph_function = self._create_graph_function(args, kwargs)
    1581         self._function_cache.primary[cache_key] = graph_function
    1582         return graph_function, args, kwargs
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
    1510             arg_names=arg_names,
    1511             override_flat_arg_shapes=override_flat_arg_shapes,
 -> 1512             capture_by_value=self._capture_by_value),
    1513         self._function_attributes)
    1514
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
     692                                           converted_func)
     693
 --> 694       func_outputs = python_func(*func_args, **func_kwargs)
     695
     696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
     315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
     316         # the function a weak reference to itself to avoid a reference cycle.
 --> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
     318     weak_wrapped_fn = weakref.ref(wrapped_fn)
     319
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
     684                   optional_features=autograph_options,
     685                   force_conversion=True,
 --> 686               ), args, kwargs)
     687
     688         # Wrapping around a decorator allows checks like tf_inspect.getargspec
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
     390     return _call_unconverted(f, args, kwargs)
     391
 --> 392   result = converted_f(*effective_args, **kwargs)
     393
     394   # The converted function's closure is simply inserted into the function's
 
 /tmp/tmpx0xgcbu3.py in tf__train_one_step(batch_data)
       6     outputs = ag__.converted_call(model, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (batch_data,), {})
       7     loss, info = ag__.converted_call('calculate_loss', loss_object, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_1, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (outputs, batch_data), {})
 ----> 8   gradients = ag__.converted_call('gradient', tape, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_2, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (loss, model.trainable_variables), {})
       9   update_list = [(grad, var) for grad, var in ag__.converted_call(zip, None, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_3, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (gradients, model.trainable_variables), {}) if grad is not None]
      10   ag__.converted_call('apply_gradients', optimizer, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun_4, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (update_list,), {})
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
     265
     266   if not options.force_conversion and conversion.is_whitelisted_for_graph(f):
 --> 267     return _call_unconverted(f, args, kwargs)
     268
     269   # internal_convert_user_code is for example turned off when issuing a dynamic
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs)
     186     return f.__self__.call(args, kwargs)
     187
 --> 188   return f(*args, **kwargs)
     189
     190
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
     954         flat_sources,
     955         output_gradients=output_gradients,
 --> 956         unconnected_gradients=unconnected_gradients)
     957
     958     if not self._persistent:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)
      70       sources,
      71       output_gradients,
 ---> 72       compat.as_str(unconnected_gradients.value))
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _aggregate_grads(gradients)
     565         indexed_slices = ops.IndexedSlices(
     566             grad,
 --> 567             math_ops.range(grad.shape[0]),
     568             constant_op.constant(grad.shape.as_list()))
     569         indexed_slices_list.append(indexed_slices)
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py in range(start, limit, delta, dtype, name)
    1258   with ops.name_scope(name, ""Range"", [start, limit, delta]) as name:
    1259     start = ops.convert_to_tensor(start, dtype=dtype, name=""start"")
 -> 1260     limit = ops.convert_to_tensor(limit, dtype=dtype, name=""limit"")
    1261     delta = ops.convert_to_tensor(delta, dtype=dtype, name=""delta"")
    1262
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
    1048   preferred_dtype = deprecation.deprecated_argument_lookup(
    1049       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
 -> 1050   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
    1051
    1052
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
    1106       name=name,
    1107       preferred_dtype=dtype_hint,
 -> 1108       as_ref=False)
    1109
    1110
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)
    1184
    1185     if ret is None:
 -> 1186       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    1187
    1188     if ret is NotImplemented:
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
     302                                          as_ref=False):
     303   _ = as_ref
 --> 304   return constant(v, dtype=dtype, name=name)
     305
     306
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
     243   """"""
     244   return _constant_impl(value, dtype, shape, name, verify_shape=False,
 --> 245                         allow_broadcast=True)
     246
     247
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
     281       tensor_util.make_tensor_proto(
     282           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
 --> 283           allow_broadcast=allow_broadcast))
     284   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
     285   const_tensor = g.create_op(
 
 ~/.pyenv/versions/anaconda3-5.2.0/envs/tf/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
     453   else:
     454     if values is None:
 --> 455       raise ValueError(""None values not supported."")
     456     # if dtype is provided, forces numpy array to be the type
     457     # provided if possible.
 
 ValueError: None values not supported.
 
 Describe the expected behavior
 The code should also execute normally when using @tf.function.
 Code to reproduce the issue
 Sorry, I do not have a simple snippet to reproduce this issue. But could you find something in the traceback? See below please.
 	",1.0,llan-ml,2019-04-06T12:18:35Z,"
 		I found a simple script to reproduce this issue, and it seems that the op tf.sparse.sparse_dense_matmul causes this issue.
 # -*- coding: utf-8 -*-
 # @Author  : Lin Lan (ryan.linlan@gmail.com)
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import numpy as np
 import scipy as sp
 import tensorflow as tf
 
 
 def sparse_to_tuple(sparse_mx):
     """"""Convert sparse matrix to tuple representation.""""""
     def to_tuple(mx):
         if not sp.sparse.isspmatrix_coo(mx):
             mx = mx.tocoo()
         coords = np.vstack((mx.row, mx.col)).transpose()
         values = mx.data
         shape = mx.shape
         return coords, values, shape
 
     if isinstance(sparse_mx, list):
         for i in range(len(sparse_mx)):
             sparse_mx[i] = to_tuple(sparse_mx[i])
     else:
         sparse_mx = to_tuple(sparse_mx)
 
     return sparse_mx
 
 
 def construct_tf_sparse_tensor(sp_sparse_matrix):
     if not sp.sparse.issparse(sp_sparse_matrix):
         raise TypeError
 
     tuple_format = sparse_to_tuple(sp_sparse_matrix)
     tf_sparse_tensor = tf.sparse.SparseTensor(
         indices=tuple_format[0],
         values=tuple_format[1],
         dense_shape=tuple_format[2])
     tf_sparse_tensor = tf.sparse.reorder(tf_sparse_tensor)
     return tf_sparse_tensor
 
 
 weights = tf.Variable(
     tf.random.uniform([512, 128]),
     dtype=tf.float32,
     trainable=True)
 optimizer = tf.optimizers.Adam()
 
 
 @tf.function
 def train(x):
     with tf.GradientTape() as tape:
         embeddings = tf.sparse.sparse_dense_matmul(
             x,
             weights)
         batch_embeddings = tf.nn.embedding_lookup(
             embeddings, [1, 2, 3, 4, 5, 7, 8, 9, 10])
         # embeddings = tf.nn.embedding_lookup(
         #     embeddings, list(range(512)))
         logits = tf.matmul(batch_embeddings, embeddings, transpose_b=True)
         loss = tf.reduce_mean(logits)
     gradients = tape.gradient(loss, [weights])
     optimizer.apply_gradients(zip(gradients, [weights]))
 
 
 random_array = np.random.rand(512, 512)
 sparse_array = sp.sparse.csr_matrix(
     np.asarray(random_array > 0.5, dtype=np.float32))
 sparse_tensor = construct_tf_sparse_tensor(sparse_array)
 train(sparse_tensor)
 
 To let the above code compute gradients normally, one way is to uncomment embeddings = tf.nn.embedding_lookup(embeddings, list(range(512))).
 		",2.0,llan-ml,2019-04-08T14:55:17Z,"
 		The Tape is unable to see the variables.
 So, use tape.watch(embeddings) after sparse_dense_matmul(). (Sparse Tensors cannot be watched, so watching x is not an option).
 That solves the problem.
 		",3.0,llan-ml,2019-04-08T17:36:40Z,"
 		<denchmark-link:https://github.com/captain-pool>@captain-pool</denchmark-link>
   The above code works well in eager mode. It only fails when we use  decoration. So, the tape only cannot see the variable with AutoGraph?
 		",110f0610ed0cf52d256e414906cf91d4e9d657e7,Alexandre Passos,2019-05-02 17:58:38-07:00,MODIFY,1,tensorflow\python\eager\backprop.py,tensorflow\python\eager\backprop.py,1.0,"571,572","571,572",,,,,4.0,llan-ml,2019-04-08T18:01:29Z,"
 		Well, that is exactly what's happening. For Autograph it works only with watch(). I'm still looking through the codebase to find the reason.
 		",5.0,llan-ml,2019-05-02T01:07:45Z,"
 		There seems to be a slightly more helpful error in tf-nightly, but it looks like it's unrelated to tape.watch or autograph. The shape of embeddings seems to be partially unknown after the sparse_dense_matmul, and this line fixed in my tests:
 <denchmark-code>        embeddings = tf.sparse.sparse_dense_matmul(
             x,
             weights)
         embeddings.set_shape((512, 128))  # This removes the error.
         batch_embeddings = tf.nn.embedding_lookup(
             embeddings, tf.constant([1, 2, 3, 4, 5, 7, 8, 9, 10]))
 </denchmark-code>
 
 Reassigning to triage the tape error.
 		",6.0,llan-ml,2019-05-02T18:15:51Z,"
 		There is a bug in the backprop code, where it does math_ops.range(grad.shape[0]) which uses the static shape of the grad tensor, which might be known (a number) or None. To use the dynamic shape we need something like math_ops.range(array_ops.shape(grad)[0]).
 		",,,,,,,,,,,,,,,,,,,,,,_aggregate_grads,gradients,546,583,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,llan-ml,2019-05-03T01:11:16Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27565>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27565>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27632,csukuangfj,2019-04-08T12:17:33Z,2019-05-01T20:30:40Z,[doc/keras] incorrect comment in the example for `tf.keras.layers.Add`,"
 
 Doc Link: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add
 
 Describe the documentation issue
 See the code example (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add#class_add</denchmark-link>
 )
     added = keras.layers.Add()([x1, x2])  # equivalent to added =
     keras.layers.add([x1, x2])
 It should be
 <denchmark-code>    added = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])
 </denchmark-code>
 
 	",1.0,csukuangfj,2019-04-08T13:32:26Z,"
 		<denchmark-link:https://github.com/csukuangfj>@csukuangfj</denchmark-link>
   I am added a pr regarding that issue.
 		",2.0,csukuangfj,2019-04-08T14:16:42Z,"
 		you're adding too many commits.
 		",3.0,csukuangfj,2019-04-08T14:18:31Z,"
 		git squash
 		",e90399a37b7b3984e2f49a89d886d4dfd78d42db,ymodak,2019-04-30 13:33:15-07:00,MODIFY,0,tensorflow\python\keras\layers\merge.py,tensorflow\python\keras\layers\merge.py,0.0,238,"238,239,240",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27705,perara,2019-04-10T07:26:10Z,2019-08-13T21:55:39Z,Keras subclassing and explicit dtype of Input,"
 System information
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
 Tensorflow Version: 2.0.0-alpha0
 Description
 When using Keras subclassing there is no apparent way of defining the dtype of the Input node of the network. In some cases, it would be neccecary to use tf.float16 instead of 32 but as of now i cannot find any way to adjust this. Also trying to set the dtype using self.dtype = tf.float16 is not permitted.
 	",1.0,perara,2019-04-11T10:36:20Z,"
 		In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
 		",2.0,perara,2019-04-11T20:29:21Z,"
 		Method 1:
 <denchmark-code>import tensorflow as tf
 
 
 class SomeClass(tf.keras.Model):
 
     def __init__(self, dtype):
         super(SomeClass, self).__init__(dtype=dtype)  # Set Input of dtype either like this
 
         self.h_1 = tf.keras.layers.Dense(10, dtype=dtype)
 
     def call(self, inputs):
         return self.h_1(inputs)
 
 
 if __name__ == ""__main__"":
 
     model = SomeClass(dtype=tf.float16)
     model([1, 2, 0])
 </denchmark-code>
 
 Exception:
 <denchmark-code>TypeError: _init_subclassed_network() got an unexpected keyword argument 'dtype'
 </denchmark-code>
 
 Method 2
 <denchmark-code>import tensorflow as tf
 
 
 class SomeClass(tf.keras.Model):
 
     def __init__(self, dtype):
         self.dtype = dtype  # Or this?
 
         self.h_1 = tf.keras.layers.Dense(10, dtype=dtype)
 
     def call(self, inputs):
         return self.h_1(inputs)
 
 
 if __name__ == ""__main__"":
 
     model = SomeClass(dtype=tf.float16)
     model([1, 2, 0])
 </denchmark-code>
 
 Exception:
 <denchmark-code>AttributeError: can't set attribute
 </denchmark-code>
 
 Use case
 The use case is when you want specifically to use custom dtypes for the whole net, typically when running on TensorCores etc.
 Freetext
 I dont know if this would be the correct way of defining this, but it should be possible (if its not already) to set the dtype of the input excplictly ( i assume there is a tf.cast in there somewhere anyways...)
 		",3.0,perara,2019-04-25T19:51:12Z,"
 		Hi! A workaround you can do now is to directly set the private property _dtype. I'll make a change that adds dtype as one of the allowed keyword arguments super().init.
 		",1b96e5212002b7ac5027a7538a8f6e5780b669f5,Katherine Wu,2019-04-30 19:30:39-07:00,MODIFY,0,tensorflow\python\keras\engine\base_layer.py,tensorflow\python\keras\engine\base_layer.py,0.0,154,"154,155,156",MODIFY,6.0,tensorflow\python\keras\engine\network.py,tensorflow\python\keras\engine\network.py,4.0,perara,2019-07-24T00:09:03Z,"
 		Automatically closing this out since I understand it to be resolved, but please let me know if I'm mistaken.Thanks!
 		",5.0,perara,2019-07-24T00:15:46Z,"
 		Reopening as I find some issues are there still with  . Here is the <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/4ee37c87967ef4abfa7c7cb3bed63c0c/tf_27705.ipynb>gist</denchmark-link>
 . Thanks!
 		",6.0,perara,2019-08-09T18:43:36Z,"
 		For method 2:
 dtype is a property therefore we see error
 The error message is fixed with latest tf-nightly 2.0 build version '2.0.0-dev20190809'
 AttributeError: Can't set the attribute ""dtype"", likely because it conflicts with an existing read-only @property of the object. Please choose a different name..
 		",1.0,"251,252,253,254,255,271","233,249,260",_init_graph_network,"self,inputs,outputs,name",233,328,MODIFY,1.0,tensorflow\python\keras\engine\network_test.py,tensorflow\python\keras\engine\network_test.py,1.0,"945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969",,MODIFY,1.0,tensorflow\python\keras\utils\generic_utils.py,tensorflow\python\keras\utils\generic_utils.py,1.0,"595,596",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"181,189,190,196,197,198,199,202,231","182,213,233",_base_init,"self,name,kwargs",181,248,1.0,"181,189,190,196,197,198,199,202","169,182,213",_base_init,"self,name",169,230,1.0,,"350,351,353",_init_subclassed_network,"self,name,dynamic",350,362,1.0,"371,372",,_init_subclassed_network,"self,name,kwargs",371,382,1.0,"251,252,253,254,255,271",260,_init_graph_network,"self,inputs,outputs,name,kwargs",251,349,7.0,perara,2019-08-13T21:55:35Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  Use method 1.
 <denchmark-code>ValueError: Layer dense expects 1 inputs, but it received 3 input tensors. Inputs received: [<tf.Tensor: id=1, shape=(), dtype=int32, numpy=1>, <tf.Tensor: id=2, shape=(), dtype=int32, numpy=2>, <tf.Tensor: id=3, shape=(), dtype=int32, numpy=0>]
 </denchmark-code>
 
 The above error is appearing because [1, 2, 0] is being converted into separate tensors. Using tf.constant to convert the entire array into a tensor (try model(tf.constant([[1, 2, 0]]))).
 		",test_model_initialization,self,945,969,validate_kwargs,"kwargs,allowed_kwargs,error_message",595,596,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,perara,2019-08-13T21:55:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27705>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27705>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27769,zzh8829,2019-04-12T05:30:00Z,2019-08-09T17:45:51Z,[TF 2.0 keras] Unable save and load weights for double nested models,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0
 Python version: 3.7
 
 Describe the current behavior
 load_weights throw exception on a doubly nested model
 Describe the expected behavior
 load_weights should work
 This problem only happens on two+ layers of nested model with non-trainable weights.
 The reason is save_weights and load_weights handles nested model differently
 save_weights -> call layer.weights for each layer
 load_weights -> recursively call model.weights if layer is a nested Model
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 from tensorflow.keras import Model
 from tensorflow.keras.layers import Input, Conv2D, BatchNormalization
 
 shape = (None, None, 3)
 
 def BNModel():
     x = inputs = Input(shape)
     x = Conv2D(3, 1)(x)
     x = BatchNormalization()(x)
     return Model(inputs, x)
 
 x = inner_inputs = Input(shape)
 x = BNModel()(x)
 x = BNModel()(x)
 inner_model = Model(inner_inputs, x)
 
 inputs = Input(shape)
 model = Model(inputs, inner_model(inputs))
 
 inner_model.save_weights('test.h5')
 inner_model.load_weights('test.h5')  # works fine
 
 model.save_weights('test.h5')
 model.load_weights('test.h5')   # Exception: axes don't match array !!!
 </denchmark-code>
 
 
 This bug is also reported on upstream keras <denchmark-link:https://github.com/keras-team/keras/pull/11847>keras-team/keras#11847</denchmark-link>
 
 Here is a detailed analysis on why this is happening <denchmark-link:https://github.com/keras-team/keras/pull/11847#issuecomment-482438283>keras-team/keras#11847 (comment)</denchmark-link>
 
 Full Exception
 <denchmark-code>  File ""test.py"", line 27, in <module>
     model.load_weights('test.h5')   # Exception: axes don't match array !!!
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1497, in load_weights
     hdf5_format.load_weights_from_hdf5_group(f, self.layers)
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 751, in load_weights_from_hdf5_group
     layer, weight_values, original_keras_version, original_backend)
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 377, in preprocess_weights_for_loading
     weights = convert_nested_model(weights)
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 365, in convert_nested_model
     original_backend=original_backend))
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 377, in preprocess_weights_for_loading
     weights = convert_nested_model(weights)
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 353, in convert_nested_model
     original_backend=original_backend))
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 459, in preprocess_weights_for_loading
     weights[0] = np.transpose(weights[0], (3, 2, 0, 1))
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 598, in transpose
     return _wrapfunc(a, 'transpose', axes)
   File ""/usr/local/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 51, in _wrapfunc
     return getattr(obj, method)(*args, **kwds)
 ValueError: axes don't match array
 </denchmark-code>
 
 	",1.0,zzh8829,2019-04-12T14:09:17Z,"
 		This only affected .h5 format, tensorflow checkpoints format works fine.
 I guess alternatively we can tell users to not use h5 format instead of fixing it
 		",2.0,zzh8829,2019-05-03T06:42:37Z,"
 		<denchmark-link:https://github.com/zzh8829>@zzh8829</denchmark-link>
   What is the alternative way to save a model/weights? I am having this proble min .hdf5 fromat too.
 		",3.0,zzh8829,2019-05-04T21:05:25Z,"
 		<denchmark-link:https://github.com/abhigyank>@abhigyank</denchmark-link>
  the alternative is save to  which will create tensorflow checkpoint files instead of hdf5.
 		",f42549a91a3759a9264ef4d44e9224be4ee3bdc3,Katherine Wu,2019-06-20 17:39:20-07:00,MODIFY,1,tensorflow\python\keras\saving\hdf5_format.py,tensorflow\python\keras\saving\hdf5_format.py,1.0,"275,276,277,278,279,282,283,284,285,287,288,290,291,292,293,294,295,296,297,298","275,276,277,278,279,280,281,282,283,284,285,287,289,290,291,292,294,296,297,298",MODIFY,3.0,tensorflow\python\keras\saving\hdf5_format_test.py,tensorflow\python\keras\saving\hdf5_format_test.py,4.0,zzh8829,2019-06-18T16:14:03Z,"
 		Any news on this issue?
 I tried the *.tf and it works.
 		",5.0,zzh8829,2019-06-20T09:50:07Z,"
 		It might seem like .tf saving works but in my experience the only difference is that it doesn't throw an error.
 Steps to reproduce:
 Make a model with nested models and set some layers to trainable=False
 Train for some epochs
 Save weights
 Evaluate and save metrics
 Clear everything
 Make model
 Load weights
 Evaluate
 		",6.0,zzh8829,2019-06-20T22:06:07Z,"
 		I am currently submitting a fix for H5.
 <denchmark-link:https://github.com/veqtor>@veqtor</denchmark-link>
  What problem are you seeing with using the TF format?
 		",1.0,"269,272,274,275,276,277,278,280,281,282,283,284,285,286,287,288,289,294","268,269,271,274,275,276,278,279,280,281,283,288,289,290,291,292,293,294",test_nested_model_weight_loading,self,260,298,,,,,,,,,,,,,,,convert_nested_model,weights,263,298,,,,,,,,,,,,,,,,,,,,,,1.0,"274,275,276,277,278","274,275,276,278",test_nested_model_weight_loading.test_nested_model_weight_loading.gen_model.seq_model,,274,278,1.0,"272,274,275,276,277,278,280,281,282,283,284,285,286","274,275,276,278,279,280,281,283",test_nested_model_weight_loading.gen_model,,272,286,,,,,,,,,,,,,,,,,,,,,,7.0,zzh8829,2019-07-01T00:23:59Z,"
 		<denchmark-link:https://github.com/k-w-w>@k-w-w</denchmark-link>
  I have tested your fix and it works for me  Thank you a lot!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,zzh8829,2019-08-05T11:24:18Z,"
 		<denchmark-link:https://github.com/k-w-w>@k-w-w</denchmark-link>
  How can I use your fix? I have the same problem.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,zzh8829,2019-08-05T19:49:39Z,"
 		<denchmark-link:https://github.com/19giorgosts>@19giorgosts</denchmark-link>
  The fix should be in tensorflow-nightly, which you can install using 
 		",10.0,zzh8829,2019-08-07T02:57:20Z,"
 		
 It might seem like .tf saving works but in my experience the only difference is that it doesn't throw an error.
 Steps to reproduce:
 Make a model with nested models and set some layers to trainable=False
 Train for some epochs
 Save weights
 Evaluate and save metrics
 Clear everything
 Make model
 Load weights
 Evaluate
 
 I am new coder to keras。 Can you show me a demo about your description?
 Thx
 		",11.0,zzh8829,2019-08-09T17:45:50Z,"
 		<denchmark-link:https://github.com/Lannist>@Lannist</denchmark-link>
  <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/0153524fb3f6e0b114ace9da25ac3f77/tf_27769_saveweights_tfformat.ipynb>Here</denchmark-link>
  is the colab gist to save/load the weights in *.tf format. Here is the <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/48daec236b2c8a5c8cf7a482f258ee8a/tf_27769_saveweights_h5format.ipynb>gist</denchmark-link>
  to save/load the weights in *.h5 format. The only difference between those two gist is in changing the extension. Thanks!
 I am closing the issue as it was resolved in tf-nightly. Please feel free to open if the issue persists again. Thanks!
 		",12.0,zzh8829,2019-08-09T17:45:52Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27769>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27769>No</denchmark-link>
 
 		",13.0,zzh8829,2020-05-06T09:08:50Z,"
 		is this change gonna be in tf 1 ?
 		",14.0,zzh8829,2020-05-13T01:39:19Z,"
 		
 is this change gonna be in tf 1 ?
 
 have you found the solution?
 I using the tensorflow 1.1.4 and meet the same error but can not find way to fix it
 		",15.0,zzh8829,2020-05-13T01:39:57Z,"
 		
 @19giorgosts The fix should be in tensorflow-nightly, which you can install using pip install tf-nightly
 
 how about tensorflow 1.1.4 or 1.1.5
 can not install tensorflow-nightly by pip
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27829,ageron,2019-04-14T07:26:47Z,2019-06-04T21:12:40Z,Cannot create a stateful RNN with recurrent dropout,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 MacOSX 10.13.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 tf.version.VERSION=2.0.0-dev20190413
 tf.version.GIT_VERSION=v1.12.0-12481-gc7ce6f4cd9
 Python version:
 3.6.8
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 N/A
 GPU model and memory:
 N/A
 
 Describe the current behavior
 I get an exception when trying to use recurrent_dropout in a stateful RNN:
 <denchmark-code>.../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
    1449
    1450   def __imul__(self, unused_other):
 -> 1451     raise RuntimeError(""Variable *= value not supported. Use ""
    1452                        ""`var.assign(var * value)` to modify the variable or ""
    1453                        ""`var = var * value` to get a new Tensor object."")
 
 RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
 </denchmark-code>
 
 The full stacktrace is below.
 Describe the expected behavior
 No exception.
 Code to reproduce the issue
 from tensorflow import keras
 
 model = keras.models.Sequential([
     keras.layers.GRU(128, return_sequences=True, stateful=True,
                      batch_input_shape=[32, None, 5],
                      recurrent_dropout=0.2)
 ])
 Other info / logs
 Complete stacktrace:
 <denchmark-code>---------------------------------------------------------------------------
 RuntimeError                              Traceback (most recent call last)
 <ipython-input-1-3e98e7412ec2> in <module>
       4     keras.layers.GRU(128, return_sequences=True, stateful=True,
       5                      batch_input_shape=[32, None, 5],
 ----> 6                      recurrent_dropout=0.2)
       7 ])
 
 .../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
     456     self._self_setattr_tracking = False  # pylint: disable=protected-access
     457     try:
 --> 458       result = method(self, *args, **kwargs)
     459     finally:
     460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access
 
 .../tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
     106     if layers:
     107       for layer in layers:
 --> 108         self.add(layer)
     109
     110   @property
 
 .../tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
     456     self._self_setattr_tracking = False  # pylint: disable=protected-access
     457     try:
 --> 458       result = method(self, *args, **kwargs)
     459     finally:
     460       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access
 
 .../tensorflow/python/keras/engine/sequential.py in add(self, layer)
     167           # and create the node connecting the current layer
     168           # to the input layer we just created.
 --> 169           layer(x)
     170           set_inputs = True
     171
 
 .../tensorflow/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
     620
     621     if initial_state is None and constants is None:
 --> 622       return super(RNN, self).__call__(inputs, **kwargs)
     623
     624     # If any of `initial_state` or `constants` are specified and are Keras
 
 .../tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     631                       base_layer_utils.AutoAddUpdates(self,
     632                                                       inputs)) as auto_updater:
 --> 633                 outputs = call_fn(inputs, *args, **kwargs)
     634                 auto_updater.set_outputs(outputs)
     635
 
 .../tensorflow/python/keras/layers/recurrent_v2.py in call(self, inputs, mask, training, initial_state)
     328           input_length=timesteps,
     329           time_major=self.time_major,
 --> 330           zero_output_for_mask=self.zero_output_for_mask)
     331       # This is a dummy tensor for testing purpose.
     332       runtime = _runtime('unknown')
 
 .../tensorflow/python/keras/backend.py in rnn(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)
    3558     # the value is discarded.
    3559     output_time_zero, _ = step_function(input_time_zero,
 -> 3560                                         initial_states + constants)
    3561     output_ta = tuple(
    3562         tensor_array_ops.TensorArray(
 
 .../tensorflow/python/keras/layers/recurrent_v2.py in step(cell_inputs, cell_states)
     316
     317       def step(cell_inputs, cell_states):
 --> 318         return self.cell.call(cell_inputs, cell_states, **kwargs)
     319
     320       last_output, outputs, states = K.rnn(
 
 .../tensorflow/python/keras/layers/recurrent.py in call(self, inputs, states, training)
    1706
    1707       if 0. < self.recurrent_dropout < 1.:
 -> 1708         h_tm1 *= rec_dp_mask[0]
    1709
    1710       if self.reset_after:
 
 .../tensorflow/python/ops/resource_variable_ops.py in __imul__(self, unused_other)
    1449
    1450   def __imul__(self, unused_other):
 -> 1451     raise RuntimeError(""Variable *= value not supported. Use ""
    1452                        ""`var.assign(var * value)` to modify the variable or ""
    1453                        ""`var = var * value` to get a new Tensor object."")
 
 RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable or `var = var * value` to get a new Tensor object.
 </denchmark-code>
 
 	",1.0,ageron,2019-04-16T10:41:41Z,"
 		<denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
 		",2.0,ageron,2019-04-16T12:52:33Z,"
 		Hi <denchmark-link:https://github.com/muddham>@muddham</denchmark-link>
  ,
 I did! It's in the section ""Code to reproduce the issue"". :)
 		",3.0,ageron,2019-04-18T21:29:56Z,"
 		<denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  I tried to reproduce the bug in TF2.0.0-alpha0 but I don't get the runtime error. I see a warning and a deprecation message as follows. Just for your info, I ran your code in Google colab
 WARNING: Logging before flag parsing goes to stderr.
 W0418 17:16:01.808634 139806816728960 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4081: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
 Instructions for updating:
 Please use rate instead of keep_prob. Rate should be set to rate = 1 - keep_prob.
 Please let me know what you think. Thanks!
 		",6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481,Scott Zhu,2019-06-04 13:56:56-07:00,MODIFY,1,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,1213,1213,MODIFY,2.0,tensorflow\python\keras\layers\recurrent_v2.py,tensorflow\python\keras\layers\recurrent_v2.py,4.0,ageron,2019-04-19T05:06:08Z,"
 		Apparently the problem is now fixed, I don't get the error anymore.  Thanks <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  .
 		",5.0,ageron,2019-04-19T05:06:09Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27829>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27829>No</denchmark-link>
 
 		",6.0,ageron,2019-05-24T20:58:00Z,"
 		<denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  does it still work for you? I tried using recurrent_dropout with a GRU (as you are) and it seems to break for me. The problem seems to be with recurrent_dropout, cos if you switch it out everything seems to work. This problem also exists with LSTMs, and not just GRUs.
 		",1.0,847,847,call,"self,inputs,mask,training,initial_state",807,927,MODIFY,1.0,tensorflow\python\keras\layers\recurrent_v2_test.py,tensorflow\python\keras\layers\recurrent_v2_test.py,1.0,"93,94,95,96,97,98,99,100",,,,,,,,,call,"self,inputs,states,training",1199,1218,,,,,,,,,,,,,,,,,,,,,,1.0,370,370,_defun_gru_call,"self,inputs,initial_state,training,mask",362,425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ageron,2019-05-25T01:29:56Z,"
 		Apparently the bug is back. Using VERSION='2.0.0-dev20190524' and GIT_VERSION='v1.12.1-2720-geafe861c2b'.
 		",test_recurrent_dropout_with_stateful_RNN,"self,layer",93,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,ageron,2019-05-28T19:02:20Z,"
 		I am also having a similar issue. Was wondering if there was an update or an older nightly where this is stable?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,ageron,2019-05-29T04:19:01Z,"
 		<denchmark-link:https://github.com/jlanday>@jlanday</denchmark-link>
  , it worked when I posted my comment on April 19th, so perhaps try a nightly from April 18th or 19th?
 		",10.0,ageron,2019-05-29T16:10:55Z,"
 		<denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  I tried to use the nightly version from both April 18th and 19th and it looks like it still doesn't work. Does version  work for you?
 		",11.0,ageron,2019-05-29T17:40:35Z,"
 		<denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  I don't see any error with . Gist is <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/4b32a507c8209a95d38335e8efc3e231/untitled203.ipynb>here</denchmark-link>
 .
 But I notice error is back with    pip install tf-nightly-gpu-2.0-preview==2.0.0-dev20190518 Thanks!
 		",12.0,ageron,2019-06-04T17:12:17Z,"
 		Thanks for reporting the issue, will send a fix very soon.
 		",13.0,ageron,2019-06-04T21:12:37Z,"
 		Should now be fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481>6a6e8c2</denchmark-link>
 .
 		",14.0,ageron,2019-06-04T21:12:41Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27829>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27829>No</denchmark-link>
 
 		",15.0,ageron,2019-06-21T01:13:54Z,"
 		def build_cell(self):
 <denchmark-code>    char_input = self.tf.keras.layers.Input(shape=(1), batch_size=1, name=""char_input"", dtype=self.tf.int32)
     char_input_one_hot = self.tf.keras.backend.one_hot(char_input, self.vocab_size)
     previous_hidden_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name=""previous_hidden_state_input"")
     previous_cell_state_input = self.tf.keras.layers.Input(shape=(self.num_units), batch_size=1, name=""previous_cell_state_input"")`
     
     
     hidden_input_stacked = self.tf.keras.layers.concatenate([self.tf.keras.backend.squeeze(char_input_one_hot, axis=1), previous_hidden_state_input], axis=1)
     #Forget gate
     forget_gate_f = self.tf.keras.layers.Dense(units=self.num_units, activation=""sigmoid"")(hidden_input_stacked) #Helps us to take decisions about what must be removed from previous hidden state
     #Input gate
     input_gate_i = self.tf.keras.layers.Dense(units=self.num_units, activation=""sigmoid"")(hidden_input_stacked) #Decides which values to update
     input_gate_g = self.tf.keras.layers.Dense(units=self.num_units, activation=""tanh"")(hidden_input_stacked)  #Creates a vector for new candidates to added to present cell state.
     #Output_gate
     output_state_o = self.tf.keras.layers.Dense(units=self.num_units, activation=""sigmoid"")(hidden_input_stacked)
     #Current cell state
     current_cell_state = self.tf.keras.layers.add([self.tf.keras.layers.multiply([input_gate_g,input_gate_i]),self.tf.keras.layers.multiply([previous_cell_state_input,forget_gate_f])], name=""current_cell_state"")
     #output_hidden_state
     output_hidden_state = self.tf.keras.layers.multiply([self.tf.keras.layers.Activation(""tanh"")(current_cell_state), output_state_o], name=""output_hidden_state"")
     #output_char_probs
     output_char_probs = self.tf.keras.layers.Dense(units=self.vocab_size, activation=""softmax"", name=""output_char_probs"")(output_hidden_state)
     
     cell = self.tf.keras.Model(inputs=[char_input, previous_hidden_state_input, previous_cell_state_input], outputs=[output_char_probs, output_hidden_state, current_cell_state])
     return cell`
 </denchmark-code>
 
 This code will break tensorflow 2.0, I found the problem to be in tensorflow/tensorflow/python/keras/layers/merge.py  / with the add and multiply functions. Please fix
 The error I received trying to feed input in:
 
 raise RuntimeError(""Variable *= value not supported. Use ""
 RuntimeError: Variable *= value not supported. Use `var.assign(var * value)` to modify the variable >     or `var = var * value` to get a new Tensor object.
 
 
 		",16.0,ageron,2019-12-31T01:34:46Z,"
 		Can the line:
 
 
 
 tensorflow/tensorflow/python/keras/layers/merge.py
 
 
          Line 245
       in
       3a094e6
 
 
 
 
 
 
  output += inputs[i] 
 
 
 
 
 
 be changed to
 output = output + inputs[i]
 to fix this?
 (and similar within Subtract etc.)
 It doesn't like the += notation when applied to a tf.Variable
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27845,proteneer,2019-04-15T02:59:25Z,2019-04-22T18:24:17Z,Wrong derivatives for complex second order derivatives.,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tensorflow==1.12.0
 Python version: 3.6.8
 
 You can collect some of this information using our environment capture <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with
 python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 Describe the current behavior
 Derivatives of non-holomorphic functions are incorrect when compared both against AD and finite differences.
 Describe the expected behavior
 Derivatives of non-holomorphic functions should becorrect.
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 import numpy as onp
 import autograd as ag
 import autograd.numpy as anp
 import numpy as onp
 import tensorflow as tf
 
 inp = anp.array(2.0)
 
 print(""input"", inp)
 
 def ag_fn(x):
     real = anp.cos(x+2)
     imag = anp.sin(x-1)
     return anp.abs(real+1j*imag)
 
 ag_hess = ag.hessian(ag_fn)
 
 print(""ag val:"", ag_fn(inp))
 print(""ag hess:"", ag_hess(inp))
 
 def tf_fn(x):
     real = tf.cos(x+2)
     imag = tf.sin(x-1)
     return tf.abs(tf.complex(real, imag))
 
 # tf_inp = tf.convert_to_tensor(inp)
 tf_inp = tf.placeholder(shape=tuple(), dtype=onp.float64)
 
 out_op = tf_fn(tf_inp)
 
 tf_grad = tf.gradients(out_op, tf_inp)[0]
 tf_hess = tf.hessians(out_op, tf_inp)[0]
 
 sess = tf.Session()
 delta = 1e-7
 
 _, d0, tf_ad = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp})
 _, d1, _ = sess.run([out_op, tf_grad, tf_hess], feed_dict={tf_inp: inp+delta})
 
 print(""tf_numerical derivative:"", (d1-d0)/delta)
 print(""tf_autodiff derivative:"", tf_ad)
 <denchmark-code>input 2.0
 ag val: 1.0655155566059393
 ag hess: -0.25533014019223726
 2019-04-14 22:55:43.481283: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 tf_numerical derivative: -0.25533013481293665
 tf_autodiff derivative: -1.0655155566059389
 </denchmark-code>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 Additional information: <denchmark-link:https://github.com/google/jax/issues/603>google/jax#603</denchmark-link>
 
 	",1.0,proteneer,2019-04-15T14:54:51Z,"
 		Ran this on tensorflow==2.0.0-dev20190327 and I get the same incorrect output.
 		",2.0,proteneer,2019-04-20T00:09:11Z,"
 		Thanks for the minimal code snippet to reproduce the issue. I was able to reproduce the behavior in TF 1.13 and latest nightly build.
 		",3.0,proteneer,2019-04-22T16:13:23Z,"
 		Thanks for filing the issue!
 If I replace tf.abs on your example with a manual implementation (tf.sqrt(real(x)*real(x) + imag(x)*imag(x))) the values are identical, so I think this is a problem with the gradient for the ComplexAbs op.
 		",2518fc3ef8b962b8487b930d9798d4696f0e53ee,Alexandre Passos,2019-04-22 11:14:05-07:00,MODIFY,3,tensorflow\python\kernel_tests\cwise_ops_unary_test.py,tensorflow\python\kernel_tests\cwise_ops_unary_test.py,1.0,"555,556,557,558,559",,MODIFY,1.0,tensorflow\python\ops\math_grad.py,tensorflow\python\ops\math_grad.py,4.0,proteneer,2019-04-22T18:24:18Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27845>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27845>No</denchmark-link>
 
 		",5.0,proteneer,2019-04-24T15:25:52Z,"
 		Thanks for fixing this guys!
 		",,,,,1.0,"1566,1567,1568,1569,1570","1566,1567,1568,1569",_ComplexAbsGrad,"op,grad",1564,1570,,,,,,,,,,,,,,,testComplexAbsGradGrad.g,x,555,559,1.0,"548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563",,testComplexAbsGradGrad,self,548,563,1.0,"550,551,552,553",,testComplexAbsGradGrad.f,x,550,553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
27847,ppwwyyxx,2019-04-15T03:56:42Z,2019-04-29T17:33:39Z,BUG: tfdbg session cannot be used with SessionRunHooks,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ArchLinux
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary):binary
 TensorFlow version (use command below):b'v1.13.0-rc2-5-g6612da8' 1.13.1
 Python version:3.7
 Bazel version (if compiling from source):n/a
 GCC/Compiler version (if compiling from source):n/a
 CUDA/cuDNN version:n/a
 GPU model and memory:n/a
 
 The following code:
 #-*- coding: utf-8 -*-
 #File:
 
 
 import numpy as np
 import tensorflow as tf
 from tensorflow.python import debug as tf_debug
 
 a = tf.placeholder(tf.float32, [10])
 b = a + 1
 c = b * 2
 
 class Hook(tf.train.SessionRunHook):
     def before_run(self, _):
         return tf.train.SessionRunArgs(fetches=c)
 
 class Hook2(tf.train.SessionRunHook):
     def before_run(self, _):
         return tf.train.SessionRunArgs(fetches=b)
 
 sess = tf.Session()
 sess = tf_debug.LocalCLIDebugWrapperSession(sess)
 
 class SessionCreator():
     def create_session(self):
         return sess
 final_sess = tf.train.MonitoredSession(session_creator=SessionCreator(), hooks=[Hook(), Hook2()])
 
 final_sess.run(b, feed_dict={a:np.arange(10)})
 Throws:
 <denchmark-code>Traceback (most recent call last):
   File ""tfdbg.py"", line 30, in <module>
     final_sess.run(b, feed_dict={a:np.arange(10)})
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 676, in run                                                                            
     run_metadata=run_metadata)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1171, in run                                                                           
     run_metadata=run_metadata)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1270, in run                                                                           
     raise six.reraise(*original_exc_info)
   File ""/usr/lib/python3.7/site-packages/six.py"", line 693, in reraise
     raise value
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run                                                                           
     return self._sess.run(*args, **kwargs)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1327, in run                                                                           
     run_metadata=run_metadata)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1091, in run                                                                           
     return self._sess.run(*args, **kwargs)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/debug/wrappers/framework.py"", line 463, in run                                                                              
     empty_fetches = not nest.flatten(fetches)
   File ""/home/wyx/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 2156, in Flatten                                                                       
     return _pywrap_tensorflow_internal.Flatten(nested)
 TypeError: '<' not supported between instances of 'Hook' and 'str'
 </denchmark-code>
 
 I believe this issue was introduced in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1f26c65254268730b7409f517d1ed1b554d01e50>1f26c65</denchmark-link>
  a year ago.  cannot handle fetches created by hooks.
 The fix will be to obtain  in a smarter way.
 	",1.0,ppwwyyxx,2019-04-29T17:33:39Z,"
 		Fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/e2d269edb9217411fc4119338df949e1a741432b>e2d269e</denchmark-link>
 
 		",2.0,ppwwyyxx,2019-04-29T17:33:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=27847>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=27847>No</denchmark-link>
 
 		",,,,,e2d269edb9217411fc4119338df949e1a741432b,Shanqing Cai,2019-04-29 10:26:45-07:00,MODIFY,1,tensorflow\python\debug\wrappers\framework.py,tensorflow\python\debug\wrappers\framework.py,1.0,"444,445,446,447,448,449,450,451,452,453",,MODIFY,3.0,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,,,,,,,,,,,,,1.0,"833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864",,testSessionRunHook,self,833,864,,,,,,,,,,,,,,,is_empty,x,444,453,,,,,,,,,,,,,,,,,,,,,,1.0,"853,854",,testSessionRunHook.create_session,self,853,854,1.0,"840,841",,testSessionRunHook.before_run,"self,_",840,841,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28070,jiarenyf,2019-04-23T09:48:26Z,2019-04-27T00:29:23Z,tf2.0a0 tf.nn.ctc_loss with AttributeError: Tensor.op is meaningless when eager execution is enabled.,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04/16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): pip install tensorflow-gpu==2.0.0-alpha
 TensorFlow version (use command below): 2.0.0-alpha0 / v1.12.0-9492-g2c319fb
 Python version: 3.7.0
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.0/fogotten
 GPU model and memory:
 
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 When running: python loss.py, error raised, see the error.txt for details. Maybe there is some internal implementation error on the function namely tf.nn.ctc_loss...
 1. helper.py
 <denchmark-code># Author: Jiarenyf ...
 # pylint: disable=invalid-name
 # pylint: disable=too-many-locals
 # pylint: disable=missing-docstring
 # pylint: disable=redefined-outer-name
 
 import tensorflow as tf
 
 #########################################
 
 
 def dense_to_sparse(tensor, eos_token):
     eos_token = tf.constant(eos_token, tensor.dtype)
     indices = tf.where(tf.not_equal(tensor, eos_token))
 
     values = tf.gather_nd(tensor, indices)
     shape = tf.shape(tensor, out_type=tf.int64)
     return tf.SparseTensor(indices, values, shape)
 
 </denchmark-code>
 
 2. loss.py
 <denchmark-code># Author: Jiarenyf ...
 # pylint: disable=invalid-name
 # pylint: disable=too-many-locals
 # pylint: disable=missing-docstring
 # pylint: disable=redefined-outer-name
 
 import tensorflow as tf
 from helper import dense_to_sparse
 
 #########################################
 
 
 def ctc_loss(label, logit, label_len, logit_len, classes):
     prediction_sparse = tf.cast(tf.nn.ctc_greedy_decoder(
         logit, logit_len, merge_repeated=True)[0][0], tf.int32)
     prediction = tf.sparse.to_dense(prediction_sparse, classes)
 
     label_sparse = dense_to_sparse(label, classes)
     accuracy = 1.0 - tf.edit_distance(
         prediction_sparse, label_sparse, normalize=True)
     loss = tf.nn.ctc_loss(
         label, logit, label_len, logit_len, blank_index=classes)
 
     return loss, accuracy, prediction
 
 
 #########################################
 
 
 LOSS_DICT = {
     'ctc': ctc_loss,
 }
 
 
 #########################################
 
 if __name__ == '__main__':
     frames = 8
     classes = 20
     batch_size = 16
     label_len = tf.ones(batch_size, tf.int32)
     label = tf.ones((batch_size, 5), tf.int32)
     logit_len = tf.zeros(batch_size, tf.int32)
     logit = tf.zeros((frames, batch_size, classes+1))
     print(ctc_loss(label, logit, label_len, logit_len, classes+1))
 
 </denchmark-code>
 
 3. error.txt
 <denchmark-code>Traceback (most recent call last):
   File ""loss.py"", line 45, in <module>
     print(ctc_loss(label, logit, label_len, logit_len, classes+1))
   File ""loss.py"", line 22, in ctc_loss
     label, logit, label_len, logit_len, blank_index=classes)
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 672, in ctc_loss_v2
     name=name)
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/ops/ctc_ops.py"", line 784, in ctc_loss_dense
     return compute_ctc_loss(*args)[0]
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py"", line 520, in __call__
     ret, op = _call(self._signature, *args, **kwargs)
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/function.py"", line 1022, in _call
     compute_shapes=False)
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
     return func(*args, **kwargs)
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3466, in create_op
     input_ops = set([t.op for t in inputs])
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 3466, in <listcomp>
     input_ops = set([t.op for t in inputs])
   File ""/home/jiarenyf/miniconda3/envs/tensorflow_2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 934, in op
     ""Tensor.op is meaningless when eager execution is enabled."")
 AttributeError: Tensor.op is meaningless when eager execution is enabled.
 
 </denchmark-code>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,jiarenyf,2019-04-23T09:52:45Z,"
 		<denchmark-link:https://github.com/shashvatshahi1998>@shashvatshahi1998</denchmark-link>
  <denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
  <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 
 Could you please help me, thank you.
 		",2.0,jiarenyf,2019-04-23T10:32:38Z,"
 		<denchmark-link:https://github.com/jiarenyf>@jiarenyf</denchmark-link>
  refer to this issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/27739>#27739</denchmark-link>
  for further updates as your code is showing same Attribute error.
 		",3.0,jiarenyf,2019-04-24T04:03:42Z,"
 		I found that changing the tf.nn.ctc_loss to tf.compat.v1.nn.ctc_loss is ok, but when would the error in 2.0.0-alpha being fixed...
 <denchmark-code># Author: Jiarenyf ...
 # pylint: disable=invalid-name
 # pylint: disable=too-many-locals
 # pylint: disable=missing-docstring
 # pylint: disable=redefined-outer-name
 
 import tensorflow as tf
 from helper import dense_to_sparse
 
 #########################################
 
 
 def ctc_loss(label, logit, label_len, logit_len, classes):
     prediction_sparse = tf.cast(tf.nn.ctc_greedy_decoder(
         logit, logit_len, merge_repeated=True)[0][0], tf.int32)
     prediction = tf.sparse.to_dense(prediction_sparse, classes)
 
     label_sparse = dense_to_sparse(label, classes)
     accuracy = 1.0 - tf.edit_distance(
         prediction_sparse, label_sparse, normalize=True)
     loss = tf.compat.v1.nn.ctc_loss(label_sparse, logit, logit_len)
     # loss = tf.nn.ctc_loss(
     # label, logit, label_len, logit_len, blank_index=classes)
 
     return loss, accuracy, prediction
 
 
 #########################################
 
 
 LOSS_DICT = {
     'ctc': ctc_loss,
 }
 
 
 #########################################
 
 if __name__ == '__main__':
     num = 5
     frames = 9
     classes = 20
     batch_size = 16
     label = tf.zeros((batch_size, num), tf.int32)
     label_len = tf.ones(batch_size, tf.int32) * num
     logit = tf.concat([
         tf.ones((frames//num, batch_size, 1)),
         tf.zeros((frames//num, batch_size, classes)),
     ], axis=-1)
     for i in range(num-1):
         tmp1 = tf.concat([
             tf.zeros((frames//num, batch_size, classes)),
             tf.ones((frames//num, batch_size, 1)),
         ], axis=-1)
         tmp2 = tf.concat([
             tf.ones((frames//num, batch_size, 1)),
             tf.zeros((frames//num, batch_size, classes)),
         ], axis=-1)
         logit = tf.concat([logit, tmp1, tmp2], axis=0)
 
     logit_len = tf.ones(batch_size, tf.int32) * frames
     loss, accuracy, prediction = map(
         lambda r: r.numpy(),
         ctc_loss(label, logit, label_len, logit_len, classes+1))
     for l, a, p in zip(loss, accuracy, prediction):
         print(f""Loss: {'%.3f'%l}; Accuracy: {a*100}%; Prediction: {p}"")
 
 </denchmark-code>
 
 		",778ca5c0bcf87c9e2df73fe9b8074bae5b8c3e58,Alexandre Passos,2019-04-26 17:19:34-07:00,MODIFY,2,tensorflow\python\kernel_tests\ctc_loss_op_test.py,tensorflow\python\kernel_tests\ctc_loss_op_test.py,1.0,"343,344,345",,MODIFY,2.0,tensorflow\python\ops\ctc_ops.py,tensorflow\python\ops\ctc_ops.py,4.0,jiarenyf,2019-04-27T00:29:24Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28070>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28070>No</denchmark-link>
 
 		",,,,,,,,,1.0,"805,806,807,808","805,807",grad,grad_loss,805,808,,,,,,,,,,,,,,,testCtcLossV2.assert_same_loss_and_grads,loss,342,349,1.0,"331,332,333,334,335,336,337,338,343,344,345","329,330,331,332,333,334,339",testCtcLossV2,self,311,357,,,,,,,,,,,,,,,1.0,"1163,1164,1165,1166,1167,1168,1169,1170,1171","1111,1118,1160,1161",_scan,"fn,elems,initial,reverse,inclusive,final_only",1063,1173,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28158,ipod825,2019-04-25T19:18:29Z,2019-04-29T19:40:10Z,Keras ValueError stops autograph building,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 TensorFlow installed from (source or binary):
 pip
 TensorFlow version (use command below):
 2.0.0-dev20190424
 Python version:
 3.7.1
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 cudatoolkit-10.0.130-0
 cudnn-7.3.1-cuda10.0_0
 GPU model and memory:
 GeForce RTX 2080 Ti
 
 Describe the current behavior
 Calling keras layer without calling build() automatically infers the shapes of the trainable variables. This works both in eager mode and graph mode in the current 2.0-alpha version. However, running the provided code in 2.0.0-dev20190424 version, it gives the following error message:
 <denchmark-code>W0425 12:08:40.775576 139922429134656 tf_logging.py:161] Entity <function update at 0x7f41dd7038c8> could not be transform
 ed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Erro
 r details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the
 AutoGraph team. Cause: ValueError during conversion: Weights for model sequential have not yet been created. Weights are c
 reated when the Model is first called on inputs or `build()` is called with an `input_shape`.
 </denchmark-code>
 
 Describe the expected behavior
 Code to reproduce the issue
 import os
 
 import tensorflow as tf
 from tensorflow.keras import layers, models, optimizers
 
 os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
 
 model = models.Sequential([layers.Dense(1, activation='relu')])
 optimizer = optimizers.SGD()
 
 # Is this line needed in graph mode?
 # model.build((None, 1))
 
 
 @tf.function
 def update(batch):
     with tf.GradientTape() as tape:
         output = model(batch)
     grads = tape.gradient(output, model.trainable_variables)
     optimizer.apply_gradients(zip(grads, model.trainable_variables))
 
 
 if __name__ == ""__main__"":
 
     batch = tf.zeros((1, 1), dtype=tf.float32)
     update(batch)
 	",1.0,ipod825,2019-04-26T22:47:29Z,"
 		I could reproduce the issue with tf-nightly. However, there is no error with TF2.0.0-alpha0. Thanks!
 		",2.0,ipod825,2019-04-29T19:40:11Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28158>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28158>No</denchmark-link>
 
 		",3.0,ipod825,2019-04-29T19:49:15Z,"
 		Just submitted a fix that should allow your code to work without calling model.build.
 		",008300cc7667da8f8a7d36806470c01a524153d0,Dan Moldovan,2019-04-29 12:29:20-07:00,MODIFY,1,tensorflow\python\autograph\converters\directives.py,tensorflow\python\autograph\converters\directives.py,1.0,"116,117,118","116,117",MODIFY,3.0,tensorflow\python\autograph\converters\directives_test.py,tensorflow\python\autograph\converters\directives_test.py,,,,,,,,,,,,,1.0,"112,113",,test_value_verification_does_not_trigger_properties.test_fn,,112,113,,,,,,,,,,,,,,,visit_Attribute,"self,node",113,119,,,,,,,,,,,,,,,,,,,,,,1.0,"102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117",,test_value_verification_does_not_trigger_properties,self,102,117,1.0,"107,108",,test_value_verification_does_not_trigger_properties.b,self,107,108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28346,olesalscheider,2019-05-02T22:11:23Z,2019-07-29T17:19:53Z,TrtGraphConverterV2 does not preserve output names in the signature_def,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below):  master from April 22nd
 Python version: 3.6.7
 Bazel version (if compiling from source): 0.24
 GCC/Compiler version (if compiling from source): 7.4
 CUDA/cuDNN version: 10.0 / 7.5.0
 GPU model and memory: GTX 1080 Ti
 
 Describe the current behavior
 If you use TrtGraphConverterV2 to convert a function in a saved_model to use TRT it does not preserve the output names in the signature_def of the saved model.
 If the saved function (decorated with tf.function) returned a dict {'output_a': a, 'output_b': b} the names 'output_a' and 'output_b' are in the saved_model. After conversion with TrtGraphConverterV2 they are changed to the default names 'output_0' and 'output_1'.
 Describe the expected behavior
 The names of the outputs should not change. This breaks all code that loads the model and relies on the correct names.
 Code to reproduce the issue
 Take any saved_model that contains a function returning a dict.
 Then run this:
 <denchmark-code>conversion_params = trt_convert.DEFAULT_TRT_CONVERSION_PARAMS._replace(precision_mode=trt_convert.TrtPrecisionMode.FP16, max_batch_size=1, max_workspace_size_bytes=8000000000)
 
 trt_converter = trt_convert.TrtGraphConverterV2(input_saved_model_dir='your_saved_model', input_saved_model_signature_key='your_key', conversion_params=conversion_params)
 trt_converter.convert()
 trt_converter.save('your_saved_model')
 </denchmark-code>
 
 Use saved_model_cli to inspect the saved_model.
 	",1.0,olesalscheider,2019-05-03T12:57:25Z,"
 		<denchmark-link:https://github.com/olesalscheider>@olesalscheider</denchmark-link>
  In order to expedite the trouble-shooting process, please provide a code snippet to reproduce the issue reported here. Thanks!
 		",2.0,olesalscheider,2019-05-03T16:56:04Z,"
 		You can use this code to reproduce the issue:
 <denchmark-link:https://gist.githubusercontent.com/olesalscheider/366f33115016ac9d5f2976ec17124496/raw/f5b68bf571f325742c1bc24658f0de04b3d3b33c/wrong_outputs.py>https://gist.githubusercontent.com/olesalscheider/366f33115016ac9d5f2976ec17124496/raw/f5b68bf571f325742c1bc24658f0de04b3d3b33c/wrong_outputs.py</denchmark-link>
 
 The output names should be the same before and after conversion but they are not.
 		",3.0,olesalscheider,2019-05-06T14:36:57Z,"
 		<denchmark-link:https://github.com/olesalscheider>@olesalscheider</denchmark-link>
  Able to reproduce the issue.
 Our saved model has the following structured outputs:
 {'output_a': TensorSpec(shape=(), dtype=tf.float32, name='output_a'), 'output_b': TensorSpec(shape=(), dtype=tf.float32, name='output_b')}
 Running TF-TRT conversion...
 Our converted model has the following structured outputs:
 {'output_0': TensorSpec(shape=(), dtype=tf.float32, name='output_0'), 'output_1': TensorSpec(shape=(), dtype=tf.float32, name='output_1')}
 		",e03ab548c4696efcdbe1cca599da1289c25093b4,Guangda Lai,2019-07-29 10:17:33-07:00,MODIFY,1,tensorflow\python\compiler\tensorrt\trt_convert.py,tensorflow\python\compiler\tensorrt\trt_convert.py,1.0,"917,918,919,920",,MODIFY,21.0,tensorflow\python\compiler\tensorrt\trt_convert_test.py,tensorflow\python\compiler\tensorrt\trt_convert_test.py,4.0,olesalscheider,2019-07-25T22:10:04Z,"
 		Thanks for reporting this. I can reproduce the problem, will make a fix soon.
 		",5.0,olesalscheider,2019-07-29T17:19:54Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28346>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28346>No</denchmark-link>
 
 		",6.0,olesalscheider,2020-11-09T08:31:02Z,"
 		Is it actually fixed??????
 		",1.0,"600,601",,testTrtGraphConverter_DynamicOp,self,596,632,,,,,,,,,,,,,,,convert,self,890,930,,,,,,,,,,,,,,,,,,,,,,1.0,"523,524,525,526,527,528,529,530,531,532,533,534",,testRetainSignatureInfo_TwoInputs,self,523,534,1.0,"510,511,512,513,514,515,516,517,518,519,520",520,testRetainSignatureInfo_OneInput,self,510,520,1.0,"483,484,485",483,_CompareSavedModel._GetStructuredOutputs,export_dir,483,485,1.0,"542,543",,testRetainSignatureInfo_OneOutputSignatureKey.run,self,542,543,1.0,"72,73",,mkdtemp,self,72,73,7.0,olesalscheider,2020-11-09T16:37:22Z,"
 		<denchmark-link:https://github.com/bixia1>@bixia1</denchmark-link>
  do you know if this is still a problem?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"299,309","294,295,303,304,307",testTrtGraphConverter_BasicConversion,self,294,310,1.0,"517,518",,testRetainSignatureInfo_OneInput.run,"self,inp",517,518,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496","481,482,483",_CompareSavedModel,"self,model_class",447,496,1.0,"469,470,471,472,473,474,475,476,477,478,479,480,481",481,_CompareSavedModel._CompareSignatureDef,"original_def,converted_def,is_input",469,481,1.0,"504,505",,testRetainSignatureInfo_NoInputs.run,self,504,505,1.0,"531,532",,testRetainSignatureInfo_TwoInputs.run,"self,inp1,inp2",531,532,1.0,"638,639",,_TestStaticOp,"self,use_function_backup",634,683,1.0,"555,556,557,558,559,560",,testRetainSignatureInfo_TwoOutputSignatureKeys.run,"self,inp",555,560,1.0,"537,538,539,540,541,542,543,544,545",,testRetainSignatureInfo_OneOutputSignatureKey,self,537,545,1.0,"406,415","404,413",testTrtGraphConverter_DestroyEngineCache,self,398,445,1.0,"450,451,452,453,454,455,456,457,458,459,460",,_CompareSavedModel._GetModelPaths,model_class,450,460,1.0,"548,549,550,551,552,553,554,555,556,557,558,559,560,561,562",,testRetainSignatureInfo_TwoOutputSignatureKeys,self,548,562,1.0,"331,359,369,372,394","329,357,367,370,392",testTrtGraphConverter_BasicConversion_v2,self,323,395,1.0,"499,500,501,502,503,504,505,506,507",,testRetainSignatureInfo_NoInputs,self,499,507,1.0,"462,463,464,465,466,467",,_CompareSavedModel._GetSignatureDef,export_dir,462,467,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28406,syoyo,2019-05-05T07:11:03Z,2019-06-07T21:08:15Z,[tflite doc] CONV_2D_TRANSPOSE -&gt; TRANSPOSE_CONV,"
 <denchmark-h:h2>Existing URLs containing the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/lite/guide/ops_compatibility>https://www.tensorflow.org/lite/guide/ops_compatibility</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 TensorFlow r1.13.
 CONV_2D_TRANSPOSE op is not present in TensorFlow Lite schema.
 After glimpsed toco source code,  tf.nn.conv2d_transpose(Conv2DBackpropInput) is converted to TRANSPOSE_CONV.
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/toco/import_tensorflow.cc#L1778>https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/toco/import_tensorflow.cc#L1778</denchmark-link>
 
 So updating tflite documentation(replace CONV_2D_TRANSPOSE with TRANSPOSE_CONV  ) would be nice.
 	",1.0,syoyo,2019-05-07T13:13:13Z,"
 		<denchmark-link:https://github.com/syoyo>@syoyo</denchmark-link>
  Can you please elaborate the above information, cause CONV_2D_TRANSPOSE method is visible . Please see attached.
 <denchmark-link:https://user-images.githubusercontent.com/48215502/57302011-dee08580-70f7-11e9-8df4-66881b712aa7.png></denchmark-link>
 
 		",2.0,syoyo,2019-05-07T13:34:40Z,"
 		@muddaham Yes that's what I found. Documentation should be updated by syncing with the implementation(or tflite schema).
 		",3.0,syoyo,2019-06-07T20:28:39Z,"
 		I'm preparing a fix for this.
 		",8651de2f625d6fcc63437b5964b5fffca98c411e,Andrew Selle,2019-06-07 14:05:57-07:00,MODIFY,0,tensorflow\lite\g3doc\guide\ops_compatibility.md,tensorflow\lite\g3doc\guide\ops_compatibility.md,0.0,271,271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28495,goswamig,2019-05-08T00:27:46Z,2019-05-15T21:26:34Z,Move the Dockerfiles to ubuntu-18.04,"
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/gpu.Dockerfile#L22>Current Dockerfile</denchmark-link>
  we have is based out of 16.04, its better if we can move to 18.04.
 The corresponding version of TF Serving is already using 18.04 based ubuntu in their Dockerfile.
 	",1.0,goswamig,2019-05-14T18:20:13Z,"
 		<denchmark-link:https://github.com/tfboyd>@tfboyd</denchmark-link>
  <denchmark-link:https://github.com/av8ramit>@av8ramit</denchmark-link>
   Do you know if someone is already working on this?
 		",2.0,goswamig,2019-05-14T18:30:52Z,"
 		I have already moved all of my testing to 18.04 based docker files.  I have not issues moving the defaults to 18.04.  We can virtually copy my performance docker files.  <denchmark-link:https://github.com/tensorflow/benchmarks/tree/master/perfzero/docker>https://github.com/tensorflow/benchmarks/tree/master/perfzero/docker</denchmark-link>
 
 I would approve that without hesitation.
 		",,,,,6206385a0b8dcb0a71e716c3b019cca820062a06,Austin Anderson,2019-05-14 15:50:22-07:00,MODIFY,0,tensorflow\tools\dockerfiles\partials\ubuntu\devel-nvidia.partial.Dockerfile,tensorflow\tools\dockerfiles\partials\ubuntu\devel-nvidia.partial.Dockerfile,0.0,42,42,MODIFY,0.0,tensorflow\tools\dockerfiles\partials\ubuntu\nvidia.partial.Dockerfile,tensorflow\tools\dockerfiles\partials\ubuntu\nvidia.partial.Dockerfile,,,,,,,,,,,,,0.0,31,31,,,,,MODIFY,0.0,tensorflow\tools\dockerfiles\partials\ubuntu\version.partial.Dockerfile,tensorflow\tools\dockerfiles\partials\ubuntu\version.partial.Dockerfile,0.0,1,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28585,wcq19941215,2019-05-10T07:31:28Z,2019-05-16T16:54:35Z,The package org.tensorflow.lite.nnapi  does not exist,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution：Ubuntu 16.04:
 Mobile device：Pixel 2
 TensorFlow installed from：source
 TensorFlow version:1.13.1
 Python version:3.6
 Bazel version :0.24.1
 
 
 when i run the demo which in <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo</denchmark-link>
 
 so ,I found the bug,don't have  org.tensorflow.lite.nnapi
 	",1.0,wcq19941215,2019-05-10T07:33:26Z,"
 		Excuse me, how long will this nnapi package be completed in org.tensorflow:tensorflow-lite:0.0.0-nightly?
 		",2.0,wcq19941215,2019-05-13T09:30:40Z,"
 		<denchmark-link:https://github.com/wcq19941215>@wcq19941215</denchmark-link>
  Could you provide more details about the bug and context? Also, it would be great if you can provide any commands you followed. Please provide as many details as possible to resolve the issue faster. Thanks!
 		",3.0,wcq19941215,2019-05-13T09:50:14Z,"
 		<denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
  .The detail about bug in <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java</denchmark-link>
 
 and the code is:
 import org.tensorflow.lite.nnapi.NnApiDelegate;
 but I can't find the pack in org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly or org.tensorflow:tensorflow-lite:0.1.2-nightly
 so ,i used bazel ,want to get a new tensorflowlite aar , of course ,i can get a org.tensorflow.lite.nnapi.NnApiDelegate but it don't work
 the bazel command i used is:
 bazel build --cxxopt='--std=c++11' -c opt             
 --config=android_arm --config=monolithic          
 //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops
 Thanks for your help.^v^
 		",f0836d2a3bdc83b9487d703f30669723bd2662fb,Jared Duke,2019-05-16 09:51:47-07:00,MODIFY,0,tensorflow\lite\delegates\nnapi\java\src\main\java\org\tensorflow\lite\nnapi\NnApiDelegate.java,tensorflow\lite\delegates\nnapi\java\src\main\java\org\tensorflow\lite\nnapi\NnApiDelegate.java,0.0,,19,MODIFY,0.0,tensorflow\lite\java\BUILD,tensorflow\lite\java\BUILD,4.0,wcq19941215,2019-05-15T08:37:49Z,"
 		Facing same issue in the tflite demo app (tflite-gpu-nightly)...
 		",5.0,wcq19941215,2019-05-16T08:14:55Z,"
 		<denchmark-link:https://github.com/anilsathyan7>@anilsathyan7</denchmark-link>
  Have you solved this problem?
 		",6.0,wcq19941215,2019-05-16T15:45:13Z,"
 		Thanks for flagging the issue, this is an issue with how sources are aggregated in the aar that gets uploaded. We'll have it fixed in the night TFLite nightly (org.tensorflow:tensorflow-lite:0.0.0-nightly).
 		",0.0,16,"16,17,18,19,20,21",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,wcq19941215,2019-05-16T16:54:36Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28585>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28585>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,wcq19941215,2019-05-17T00:23:24Z,"
 		<denchmark-link:https://github.com/jdduke>@jdduke</denchmark-link>
  That ‘means you will add org.tensorflow.lite.nnapi to  org.tensorflow:tensorflow-lite:0.0.0-nightly no long?Thanks for your help
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,wcq19941215,2019-06-11T07:43:55Z,"
 		Facing same issue in the tflite demo app (tflite-gpu-nightly)...
 error: org.tensorflow.lite.nnapi does not exist
 错误: 程序包org.tensorflow.lite.nnapi不存在
 		",10.0,wcq19941215,2019-06-11T07:55:25Z,"
 		<denchmark-link:https://github.com/chenjiaoAngel>@chenjiaoAngel</denchmark-link>
  You can try to compile a local AAR yourself. I tried it and successfully compiled lite.nnapi aar , but it can't work. It should be caused by conflict with other packages.
 		",11.0,wcq19941215,2019-06-11T08:07:54Z,"
 		Hi, thank you!
 SO the final solution is ? And how to build the local AAR ?
 Best wishes,
 Jiao
 --------------------------
 Tel: +8618101358192
 Email: chenjiao04@baidu.com<mailto:chenjiao04@baidu.com>
 BaiduHi: AngelCJ
 --------------------------
 Baidu
 
 SYS
 
 
 
 发件人: wcq19941215 <notifications@github.com>
 答复: tensorflow/tensorflow <reply@reply.github.com>
 日期: 2019年6月11日 星期二 下午4:03
 收件人: tensorflow/tensorflow <tensorflow@noreply.github.com>
 抄送: ""Chen,Jiao(SYS)"" <chenjiao04@baidu.com>, Mention <mention@noreply.github.com>
 主题: Re: [tensorflow/tensorflow] The package org.tensorflow.lite.nnapi does not exist (<denchmark-link:https://github.com/tensorflow/tensorflow/issues/28585>#28585</denchmark-link>
 )
 
 
 <denchmark-link:https://github.com/chenjiaoAngel>@chenjiaoAngel</denchmark-link>
 <<denchmark-link:https://github.com/chenjiaoAngel>https://github.com/chenjiaoAngel</denchmark-link>
 > You can try to compile a local AAR yourself. I tried it and successfully compiled lite.nnapi aar , but it can't work. It should be caused by conflict with other packages.
 
 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub<<denchmark-link:https://github.com/tensorflow/tensorflow/issues/28585>#28585</denchmark-link>
 ?email_source=notifications&email_token=AJG4D2HRB4EY6JORJ72JBVTPZ5L5DA5CNFSM4HMASMUKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXMIUGY#issuecomment-500730395>, or mute the thread<<denchmark-link:https://github.com/notifications/unsubscribe-auth/AJG4D2H3CL37SPBDGIIL22LPZ5L5DANCNFSM4HMASMUA>https://github.com/notifications/unsubscribe-auth/AJG4D2H3CL37SPBDGIIL22LPZ5L5DANCNFSM4HMASMUA</denchmark-link>
 >.
 		",12.0,wcq19941215,2019-06-11T08:23:53Z,"
 		<denchmark-link:https://github.com/chenjiaoAngel>@chenjiaoAngel</denchmark-link>
  if you want to compile a local AAR ,you will use bazel.Finally, I did not solve this problem because there are other things that have been delayed.But I think that the latest aar package should have solved this problem, just need to update it in the AS.
 		",13.0,wcq19941215,2019-06-11T15:15:14Z,"
 		The NNAPI delegate is in the latest TFLite nightly @ org.tensorflow:tensorflow-lite:0.0.0-nightly. The GPU delegate is in a separate .aar, and should be used  the core TFLite nightly. Add org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly to your dependencies. Note that you might need to <denchmark-link:https://stackoverflow.com/questions/23025433/how-to-clear-gradle-cache>clear your gradle cache</denchmark-link>
  for the new class to be visible.
 		",14.0,wcq19941215,2019-06-12T00:45:47Z,"
 		<denchmark-link:https://github.com/jdduke>@jdduke</denchmark-link>
  Thanks,It can work
 		",15.0,wcq19941215,2019-06-12T00:51:14Z,"
 		Since gradle automatically caches a copy of it while downloading it, gradle actually reloads the local cache when clean project
 .SO the final solution is delete the "".gradle/caches/"" ,and rebuild project.
 On Windows: %USER_HOME%.gradle/caches/
 On Mac/Unix: $HOME/.gradle/caches/
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28614,davisyoshida,2019-05-10T22:16:44Z,2019-05-14T16:25:25Z,Keras RNN example from docs does not support statefulness when multilayer,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 
 
 TensorFlow installed from (source or binary): Binary
 
 
 TensorFlow version (use command below): 1.13.1
 
 
 Python version: 3.6.7 (Anaconda)
 
 
 CUDA/cuDNN version: 9.2/7.3.1
 
 
 GPU model and memory: GTX 1070 Ti
 
 
 
 Modifying the example code given <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN>here</denchmark-link>
  to have  leads to the following error:
 <denchmark-code>Traceback (most recent call last):
   File ""tmp.py"", line 6, in <module>
     y = layer(x)
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 701, in __call__                                                                                                                                
     return super(RNN, self).__call__(inputs, **kwargs)
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 538, in __call__                                                                                                                               
     self._maybe_build(inputs)
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1603, in _maybe_build                                                                                                                          
     self.build(input_shapes)
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 636, in build                                                                                                                                   
     self.reset_states()
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 906, in reset_states                                                                                                                            
     tensor_shape.as_shape(dim).as_list()))
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 2833, in set_value
     value = np.asarray(value, dtype=dtype(x))
   File ""/home/davis/software/anaconda3/envs/p36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 1015, in dtype
     return x.dtype.base_dtype.name
 AttributeError: 'list' object has no attribute 'dtype'
 </denchmark-code>
 
 Describe the expected behavior
 Code should run with no error
 Code to reproduce the issue
 <denchmark-code>cells = [tf.keras.layers.LSTMCell(32), tf.keras.layers.LSTMCell(64)]
 x = tf.keras.Input(batch_shape=(42, None, 5)) 
 layer = tf.keras.layers.RNN(cells, stateful=True)
 y = layer(x)
 </denchmark-code>
 
 	",1.0,davisyoshida,2019-05-14T14:48:21Z,"
 		Thank you for reporting the issue, will fix it soon.
 		",2.0,davisyoshida,2019-05-14T16:25:23Z,"
 		This should be now fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/12250556493fe7757bd97f397e3483e7c0e022b1>1225055</denchmark-link>
 .
 		",,,,,12250556493fe7757bd97f397e3483e7c0e022b1,Scott Zhu,2019-05-14 08:07:42-07:00,MODIFY,0,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,0.0,"3731,3732","3731,3732",MODIFY,2.0,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,,,,,,,,,,,,,1.0,"815,816,817,818,819,820,821,823,824,825,826,828,829,830,832,833,835,836,837,838,840,842,843,844","815,816,817,818,819,820,821,822,823,824,825,827,828,829,830,831,832,833,834,835,837,838,839,841,842,844",reset_states,"self,states",792,844,MODIFY,1.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,1.0,"1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"816,817","816,817",reset_states.create_state_variable,state,816,817,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_stateful_rnn_with_stacking,"self,cell",1307,1332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28685,EFanZh,2019-05-14T01:17:34Z,2019-05-29T18:10:46Z,The cycle detection algorithm in the variable creation has bad performance,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Don’t know
 TensorFlow installed from (source or binary): Binary
 TensorFlow version (use command below): 1.13.1
 Python version: 3.7.3
 Bazel version (if compiling from source): Not used
 GCC/Compiler version (if compiling from source): Not used
 CUDA/cuDNN version: Not related
 GPU model and memory: Not related
 
 Describe the current behavior
 Maybe related: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/17439>#17439</denchmark-link>
 .
 I noticed some slow variable creations. It happens when the initial value is a tensor with complex dependencies.  After some digging, I found that it may be caused by the algorithm used in the cycle detection code:
 
 
 
 tensorflow/tensorflow/python/ops/variables.py
 
 
         Lines 2505 to 2517
       in
       d102214
 
 
 
 
 
 
  def _has_cycle(op, path): 
 
 
 
  """"""Detect cycles in the dependencies of `initial_value`."""""" 
 
 
 
  if op.name in path: 
 
 
 
  return True 
 
 
 
  path.add(op.name) 
 
 
 
  for op_input in op.inputs: 
 
 
 
  if _has_cycle(op_input.op, path): 
 
 
 
  return True 
 
 
 
  for op_control_input in op.control_inputs: 
 
 
 
  if _has_cycle(op_control_input, path): 
 
 
 
  return True 
 
 
 
  path.remove(op.name) 
 
 
 
  return False 
 
 
 
 
 
 Describe the expected behavior
 Creating a variable should be completed within acceptable time.
 Code to reproduce the issue
 You can reproduce the problem using the following code:
 import time
 
 import tensorflow as tf
 
 
 def _build_tensor(depth):
     fibonacci = [tf.zeros(shape=()), tf.ones(shape=())]
 
     for _ in range(depth):
         fibonacci.append(fibonacci[-2] + fibonacci[-1])
 
     return fibonacci[-1]
 
 
 def main():
     for depth in range(15, 25):
         with tf.Graph().as_default():
             tensor = _build_tensor(depth)
 
             start_time = time.perf_counter()
             tf.Variable(initial_value=tensor)
             end_time = time.perf_counter()
 
             print('Depth = {}, Time = {}'.format(depth, end_time - start_time))
 
 
 if __name__ == '__main__':
     main()
 Other info / logs
 Here is my output from running the code:
 <denchmark-code>Depth = 15, Time = 0.008952808999310946
 Depth = 16, Time = 0.014324793000014324
 Depth = 17, Time = 0.019670226999551232
 Depth = 18, Time = 0.03077544999996462
 Depth = 19, Time = 0.04785999100022309
 Depth = 20, Time = 0.07621835800000554
 Depth = 21, Time = 0.12341592400025547
 Depth = 22, Time = 0.19620911800029717
 Depth = 23, Time = 0.31814610099991114
 Depth = 24, Time = 0.5071976489998633
 </denchmark-code>
 
 Notice the time used for creating a variable grows exponentially.
 <denchmark-h:hr></denchmark-h>
 
 The cycle detection algorithm could be optimized to have a linear time complexity. Also, the algorithm should avoid stack overflow if the initial value has a long dependency chain.
 	",1.0,EFanZh,2019-05-15T09:24:40Z,"
 		<denchmark-link:https://github.com/EFanZh>@EFanZh</denchmark-link>
   It looks like you haven't used a template to create this issue. Please resubmit your issue using a template from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose>here</denchmark-link>
 . We ask users to use the template because it reduces overall time to resolve a new issue by avoiding extra communication to get to the root of the issue. We will close this issue in lieu of the new one you will create from the template. Thank you for your cooperation.
 		",2.0,EFanZh,2019-05-16T02:38:21Z,"
 		<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>
  I have updated my issue, please check.
 		",3.0,EFanZh,2019-05-16T05:27:36Z,"
 		<denchmark-link:https://github.com/EFanZh>@EFanZh</denchmark-link>
  Able to reproduce the issue with the provided code.
 Depth = 15, Time = 0.014409996000040337
 Depth = 16, Time = 0.018489084000066214
 Depth = 17, Time = 0.02723818900005881
 Depth = 18, Time = 0.048183349999931124
 Depth = 19, Time = 0.07164499200007413
 Depth = 20, Time = 0.11124446999997417
 Depth = 21, Time = 0.17927562900001703
 Depth = 22, Time = 0.2863872639999272
 Depth = 23, Time = 0.4592743739999605
 Depth = 24, Time = 0.7376542980000522
 		",fc20e9fe8223336b491cedd4bc428867bf0e7daa,Sergei Lebedev,2019-05-29 11:08:23-07:00,MODIFY,3,tensorflow\python\kernel_tests\variables_test.py,tensorflow\python\kernel_tests\variables_test.py,1.0,"114,115,116,117,118,119,120,121,122",,MODIFY,3.0,tensorflow\python\ops\variables.py,tensorflow\python\ops\variables.py,4.0,EFanZh,2019-05-29T11:10:48Z,"
 		Thanks a lot for reporting this <denchmark-link:https://github.com/EFanZh>@EFanZh</denchmark-link>
 ! The fix is on the way.
 I'm curious, what was the use-case which triggered this regression for you? In my experience complex initializers are not that common.
 		",5.0,EFanZh,2019-05-29T12:11:32Z,"
 		<denchmark-link:https://github.com/superbobry>@superbobry</denchmark-link>
  I was creating a variable to store the the result of a complex computation. The problem is that I need to store the computation result multiples, so I think I could utilize the initializer of the variable to to it, by which, I mean setting the initial value of that variable to the computation result and use the initializer as the storing operation, then my program gets stuck.
 Currently, as a workaround, I am using an zero value as the initial value of the variable, and created another assignment operation to store the computation result. But this method has two extra operations created: a tf.zeros and a tf.assign. It is not as clean as using the computation result as the initial value.
 		",6.0,EFanZh,2019-05-29T18:10:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28685>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28685>No</denchmark-link>
 
 		",1.0,"2517,2518,2519,2520,2522,2523,2525,2526,2527,2528","2516,2518,2520,2521,2522,2523,2524,2525,2527",_has_cycle,"op,path",2516,2528,,,,,,,,,,,,,,,testCyclicInitializer,self,114,122,1.0,"128,129,130,131,132",,testCycleDetectionIsLinear._build_tensor,depth,128,132,1.0,"125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145",,testCycleDetectionIsLinear,self,125,145,,,,,,,,1.0,2512,2511,_try_guard_against_uninitialized_dependencies,"name,initial_value",2482,2514,1.0,"2520,2522,2523,2525,2526,2527,2528,2529,2530,2532","2520,2521,2522,2523,2524,2525,2527",_has_cycle,"op,state",2520,2533,,,,,,,,,,,,,,,,,,,,,,7.0,EFanZh,2019-05-30T00:57:10Z,"
 		<denchmark-link:https://github.com/superbobry>@superbobry</denchmark-link>
  I see the updated implementation is using recursion, which I think may cause stack overflow if the dependency chain becomes too long. Is it possible to implement cycle detection using a stack and loop to avoid stack overflow?
 Also, the usage of self.cached_session() in testCycleDetectionIsLinear seems unnecessary. Because new graphs are created and used inside the loop, with self.cached_session() has no effect.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,EFanZh,2019-05-30T21:17:19Z,"
 		Recursion is indeed bounded in Python, but the default limit (1000 on Linux) seems reasonable for most (if not all) initializer graphs. Recursion in Python is also slower than iteration, but again, I'm not convinced it is worth optimizing in that particular case.
 Thanks for spotting redundant self.cached_session()!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28725,samsamoa,2019-05-15T01:46:51Z,2019-06-13T15:46:54Z,Autograph fails for keyword-only arguments,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
 TensorFlow installed from (source or binary): pip install tf-nightly-gpu-2.0-preview
 TensorFlow version (use command below): v1.12.1-1847-gc095504 2.0.0-dev20190514
 Python version: 3.7.3
 
 Describe the current behavior
 Autograph complains when compiling functions with keyword-only arguments.
 Example output:
 W0515 01:46:22.158518 139635868194560 ag_logging.py:145] Entity <function f at 0x7eff8120c1e0> could not be transformed and will be executed as-is. Some features (e.g. tensor-dependent conditionals and loops) may not work as expected. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: Unexpected error transforming <function f at 0x7eff8120c1e0>. If you believe this is due to a bug, please set the verbosity to 10 (on Linux, export AUTOGRAPH_VERBOSITY=10) and attach the full output when filing the bug report. Caused by: inconsistent nodes: None (NoneType) and None (NoneType)
 Describe the expected behavior
 Autograph works for keyword-only arguments
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 @tf.function
 def f(*, a):
      return a*2
 
 f(a=0)
 </denchmark-code>
 
 	",1.0,samsamoa,2019-05-16T09:29:08Z,"
 		I have reproduce the mentioned output with TensorFlow version 2.0.0-dev20190515 on Colab.
 		",2.0,samsamoa,2019-05-31T17:20:55Z,"
 		Thanks for filing the bug! At a glance, it looks related to the lone * arg without a name; probably its name field is None in the AST. We should have fix soon.
 		",3.0,samsamoa,2019-06-13T15:46:55Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28725>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28725>No</denchmark-link>
 
 		",22ba2ebfc9779ca61e574ddf6411ee5565381cec,Dan Moldovan,2019-06-13 08:44:08-07:00,MODIFY,0,tensorflow\python\autograph\impl\BUILD,tensorflow\python\autograph\impl\BUILD,0.0,"51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68",,ADD,0.0,None,tensorflow\python\autograph\impl\api_py3_test.py,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\autograph\pyct\ast_util.py,tensorflow\python\autograph\pyct\ast_util.py,1.0,"286,287,297,298,299","286,287",MODIFY,1.0,tensorflow\python\autograph\pyct\pretty_printer.py,tensorflow\python\autograph\pyct\pretty_printer.py,1.0,"94,95,96,97,107,108,109,110","94,104",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,parallel_walk,"node,other",258,323,generic_visit,"self,node,name",60,126,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28849,jackshi0912,2019-05-20T04:36:19Z,2019-07-15T19:24:17Z,Python3 type annotation does not work with @tf.function + for loop -&gt; tf.while_loop conversion,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0 alpha
 Python version: 3.6.7
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 
 Describe the current behavior
 If you use python3 type annotation such as x:tf.Tensor = tf.constant(0) (I aliased tf.Tensor for various shape to keep my sanity for reinforcement learning problems) in a @tf.function and the function contains a for loop to be translated to tf.while_loop (that doesn't even have to use the tensor that's annotated), the code will fail as if you did not turn on eager execution.
 Describe the expected behavior
 Python3 type hinting should not fail the code.
 Code to reproduce the issue
 <denchmark-code>@tf.function
 def tf_for_tf_break():
     x: tf.Tensor = tf.constant(0)
     for i in tf.range(5):
         x += i
     return x
 
 print(tf_for_tf_break())
 </denchmark-code>
 
 Other info / logs
 WARNING: Logging before flag parsing goes to stderr.
 W0519 21:35:32.992043 140297958307648 tf_logging.py:161] Entity <function tf_for_tf_break at 0x7f99a9edde18> could not be transformed and will be staged without change. Error details can be found in the logs when running with the env variable AUTOGRAPH_VERBOSITY >= 1. Please report this to the AutoGraph team. Cause: AttributeError during conversion: 'NoneType' object has no attribute '_fields'
 Traceback (most recent call last):
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 393, in function_to_graph
 node = node_to_graph(node, context)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 436, in node_to_graph
 node = converter.standard_analysis(node, context, is_initial=True)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/core/converter.py"", line 493, in standard_analysis
 graphs = cfg.build(node)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 813, in build
 visitor.visit(node)
 File ""/usr/lib/python3.6/ast.py"", line 253, in visit
 return visitor(node)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/cfg.py"", line 672, in visit_FunctionDef
 self.visit(stmt)
 File ""/usr/lib/python3.6/ast.py"", line 253, in visit
 return visitor(node)
 File ""/usr/lib/python3.6/ast.py"", line 257, in generic_visit
 for field, value in iter_fields(node):
 File ""/usr/lib/python3.6/ast.py"", line 171, in iter_fields
 for field in node._fields:
 AttributeError: 'NoneType' object has no attribute '_fields'
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 369, in converted_call
 experimental_partial_types=partial_types)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 513, in to_graph
 arg_values, arg_types)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 190, in entity_to_graph
 node, name, ns = function_to_graph(o, program_ctx, arg_values, arg_types)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 396, in function_to_graph
 raise errors.InternalError('conversion', e)
 tensorflow.python.autograph.pyct.errors.InternalError: AttributeError during conversion: 'NoneType' object has no attribute '_fields'
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File ""/home/jackshi/MagneticAccelerator/descrete_optimization/tf_scratch.py"", line 12, in 
 print(tf_for_tf_break())
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 426, in call
 self._initialize(args, kwds, add_initializers_to=initializer_map)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 370, in _initialize
 *args, **kwds))
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1313, in _get_concrete_function_internal_garbage_collected
 graph_function, _, _ = self._maybe_define_function(args, kwargs)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1580, in _maybe_define_function
 graph_function = self._create_graph_function(args, kwargs)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1512, in _create_graph_function
 capture_by_value=self._capture_by_value),
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 694, in func_graph_from_py_func
 func_outputs = python_func(*func_args, **func_kwargs)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 317, in wrapped_fn
 return weak_wrapped_fn().wrapped(*args, **kwds)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 686, in wrapper
 ), args, kwargs)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 390, in converted_call
 return _call_unconverted(f, args, kwargs)
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 188, in _call_unconverted
 return f(*args, **kwargs)
 File ""/home/jackshi/MagneticAccelerator/descrete_optimization/tf_scratch.py"", line 7, in tf_for_tf_break
 for i in tf.range(5):
 File ""/home/jackshi/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 449, in iter
 ""Tensor objects are only iterable when eager execution is ""
 TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
 	",1.0,jackshi0912,2019-05-21T05:47:39Z,"
 		I am able to reproduce the issue on colab with TF 2.0alpha . This is the error I got TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
 		",2.0,jackshi0912,2019-07-15T19:24:18Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=28849>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=28849>No</denchmark-link>
 
 		",,,,,d3dd0726fc2a6be6235bc1e9c8825056278d3470,Dan Moldovan,2019-07-15 12:21:54-07:00,MODIFY,1,tensorflow\python\autograph\pyct\static_analysis\activity.py,tensorflow\python\autograph\pyct\static_analysis\activity.py,1.0,"274,275",,MODIFY,2.0,tensorflow\python\autograph\pyct\static_analysis\activity_py3_test.py,tensorflow\python\autograph\pyct\static_analysis\activity_py3_test.py,,,,,,,,,,,,,1.0,"49,50,51,52,53,54,55,56,57,58,59,60,61,62",,test_annotated_assign,self,49,62,,,,,,,,,,,,,,,visit_AnnAssign,"self,node",274,275,,,,,,,,,,,,,,,,,,,,,,1.0,"53,54,55",,test_annotated_assign.test_fn,c,53,55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29060,apls777,2019-05-27T14:33:15Z,2019-06-11T22:28:20Z,"""Cache iterator is in an invalid state"" error","
 System information
 
 OS Platform and Distribution: macOS High Sierra 10.13.6
 TensorFlow for CPU installed from PyPI
 TensorFlow version: v1.13.0-rc2-5-g6612da8951, 1.13.1
 Python version: 3.6.6
 
 Describe the current behavior
 Minimal not working example:
 import tensorflow as tf
 from tensorflow.python.framework.errors_impl import OutOfRangeError
 
 
 dataset = tf.data.Dataset.range(10)
 dataset = dataset.cache('cache1')
 dataset = dataset.map(lambda a: a)
 dataset = dataset.batch(4)
 
 batch = dataset.make_one_shot_iterator().get_next()
 
 with tf.Session() as sess:
     while True:
         try:
             res = sess.run(batch)
             print(res)
         except OutOfRangeError:
             print('out-of-range')
             break
 The code above properly iterates through the dataset only the first run when a cache doesn't exist. But when it loads the dataset from the cache file it crashes with an error:
 <denchmark-code>tensorflow.python.framework.errors_impl.InternalError: Cache iterator is in an invalid state. (Perhaps GetNext called after end_of_sequence?)
 	 [[{{node IteratorGetNext}}]
 </denchmark-code>
 
 A workaround
 It happens because the map operation follows right after the cache. It starts working as expected if some other dataset operation is added between cache and map steps. For example:
 ...
 
 dataset = tf.data.Dataset.range(10)
 dataset = dataset.cache('cache1')
 dataset = dataset.filter(lambda x: True)  # a fake filter is added
 dataset = dataset.map(lambda a: a)
 dataset = dataset.batch(4)
 
 ...
 	",1.0,apls777,2019-05-28T09:27:10Z,"
 		<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>
  I have executed the first part of the code in TF 1.12.0, did not receive any error. Please try and let us know how it progresses. Thanks!
 		",2.0,apls777,2019-05-28T10:04:23Z,"
 		<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>
  It works in TF 1.12.0. Just changed the filename to  instead of , otherwise, it throws an error for the first run:
 <denchmark-code>tensorflow.python.framework.errors_impl.NotFoundError: ; No such file or directory
 	 [[{{node IteratorGetNext}} = IteratorGetNext[output_shapes=[[?]], output_types=[DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
 </denchmark-code>
 
 		",3.0,apls777,2019-05-28T10:38:00Z,"
 		<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>
  As it is working now, are you happy for this issue to be closed?
 		",003e400902b85626d32727589142d12269306703,Jiri Simsa,2019-06-11 15:25:55-07:00,MODIFY,3,tensorflow\core\kernels\data\cache_dataset_ops.cc,tensorflow\core\kernels\data\cache_dataset_ops.cc,1.0,"366,367,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388","367,368,369,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389",MODIFY,1.0,tensorflow\python\data\kernel_tests\cache_test.py,tensorflow\python\data\kernel_tests\cache_test.py,4.0,apls777,2019-05-28T10:51:15Z,"
 		<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>
  No, clearly it’s a bug that should be fixed in the latest stable version.
 		",5.0,apls777,2019-05-30T11:16:29Z,"
 		<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>
  I ran the code in TF 1.13 GPU the output I got was below.
 [0 1 2 3]
 [4 5 6 7]
 [8 9]
 out-of-range
 In TF 1.13 CPU I got the below error.
 AttributeError: 'BatchDataset' object has no attribute 'make_one_shot_iterator'
 		",6.0,apls777,2019-05-30T12:20:58Z,"
 		<denchmark-link:https://github.com/muddham>@muddham</denchmark-link>
  Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 ? What version of Python do you use?
 		",1.0,"167,168,169,170,171",,testReadingPastEndOfSequence,self,167,171,,,,,,,,,,,,,,,tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::FileWriterIterator::EXCLUSIVE_LOCKS_REQUIRED,mu_,332,389,1.0,"167,168,169","167,168,169,170",tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::RestoreInternal,"ctx,reader",150,175,1.0,"450,451,461","454,455,456,457,467,468",tensorflow::data::CacheDatasetOp::FileDataset::FileIterator::FileReaderIterator::GetNextInternal,"ctx,out_tensors,end_of_sequence",443,477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,apls777,2019-06-06T11:59:44Z,"
 		
 @muddham Did you run the code twice to load the dataset from a cache? Are you sure you're running it on TF 1.13 .1? What version of Python do you use?
 
 I am able to reproduce the issue with TF 1.13.1 and Python 3.6.7.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,apls777,2019-06-11T04:51:18Z,"
 		<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>
  thank you for reporting the issue and providing a minimal reproducible example.
 I can confirm this is an issue. The problem is that the existing cache transformation produces an error if the dataset transformation that consumes its input asks for an input after the cache transformations has reached the end of its input. This happens in your repro because  will get fused into  which will ask for  worth of elements at once. Because the input cardinality (10) is not divisible evenly by the batch size (4), the invalid cache op kernel behavior is triggered. Inserting a transformation between  and  will prevent the fusion from happening (and so does disabling the fusion through tf.data <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data/experimental/OptimizationOptions#map_and_batch_fusion>options</denchmark-link>
 .
 I have a fix for this and expect it to merged to master by the end of this week.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,apls777,2019-06-11T22:28:21Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29060>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29060>No</denchmark-link>
 
 		",10.0,apls777,2019-06-25T16:32:51Z,"
 		<denchmark-link:https://github.com/apls777>@apls777</denchmark-link>
 
 I'm running into this issue when caching before interleave
 tf 2.0.0-beta1 and python 3.6.8
     filenames_dataset = filenames_dataset.cache(""./some_path"")
 
     return filenames_dataset.interleave(
         lambda f: map_file_to_xy_dataset(f, predict_task_fn, params),
         cycle_length=params[CYCLE_LENGTH_KEY],
         block_length=params[BLOCK_LENGTH_KEY],
         num_parallel_calls=tf.data.experimental.AUTOTUNE)
 <denchmark-code>Iterator ""Iterator::Model::Prefetch::Batch::Shuffle::ParallelInterleaveV2"" returned OutOfRange without setting `*end_of_sequence`. This indicates that an error may have occurred. Original message: Attempting to call get_next after iteration should have finished. [Op:IteratorGetNextSync]
 </denchmark-code>
 
 		",11.0,apls777,2019-06-25T17:18:14Z,"
 		<denchmark-link:https://github.com/devstein>@devstein</denchmark-link>
  thank you for reporting the problem. This is a different issue, so please create a new issue for it and reference it here. Thank you.
 		",12.0,apls777,2019-06-25T17:19:49Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  Will do!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29187,sbagroy986,2019-05-30T20:57:03Z,2019-06-04T15:41:33Z,TF 2.0: Cannot use recurrent_dropout with LSTMs/GRUs,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No (one line modification to stock example)
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tensorflow-gpu==2.0.0-alpha0 (also fails with every other tf 2.0 build I have explored)
 Python version: 3.6
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: Tried multiple
 GPU model and memory: Tried multiple
 
 Describe the current behavior
 The program crashes with a TypeError as below:
 TypeError: An op outside of the function building code is being passed a ""Graph"" tensor. It is possible to have Graph tensors leak out of the function building context by including a tf.init_scope in your function building code. For example, the following function will fail: @tf.function def has_init_scope(): my_constant = tf.constant(1.) with tf.init_scope(): added = my_constant * 2 The graph tensor has name: encoder/unified_gru/ones_like:0
 This occurs when trying to backprop the gradients through the LSTM/GRU with recurrent_dropout enabled.
 Describe the expected behavior
 No error
 
 Since this problem shows up at the time of training, one needs to have the entire training pipeline (dataset, model etc.) setup to demonstrate this bug. As a result, I used the <denchmark-link:https://www.tensorflow.org/alpha/tutorials/text/nmt_with_attention>Neural Machine Translation tutorial</denchmark-link>
  from TensorFlow and modified their model to include . The entire code can be found in <denchmark-link:https://colab.research.google.com/drive/1dLE58i2tttY6J_Yr8dX8f57Ai0_54kTE>this Colab notebook</denchmark-link>
 ; run the code blocks all the way till the block where we're training the model to see the bug.
 Other info.logs
 x---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
  in ()
       8 
       9   for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):
 ---> 10     batch_loss = train_step(inp, targ, enc_hidden)
      11     total_loss += batch_loss
      12 
 
 6 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
     436         # Lifting succeeded, so variables are initialized and we can run the
     437         # stateless function.
 --> 438         return self._stateless_fn(*args, **kwds)
     439     else:
     440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    1286     """"""Calls a graph function specialized to the inputs.""""""
    1287     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
 -> 1288     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
    1289 
    1290   @property
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
     572     """"""
     573     return self._call_flat(
 --> 574         (t for t in nest.flatten((args, kwargs))
     575          if isinstance(t, (ops.Tensor,
     576                            resource_variable_ops.ResourceVariable))))
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
     625     # Only need to override the gradient in graph mode and when we have outputs.
     626     if context.executing_eagerly() or not self.outputs:
 --> 627       outputs = self._inference_function.call(ctx, args)
     628     else:
     629       self._register_gradient()
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
     413             attrs=(""executor_type"", executor_type,
     414                    ""config_proto"", config),
 --> 415             ctx=ctx)
     416       # Replace empty list with None
     417       outputs = outputs or None
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
      68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):
      69       raise core._SymbolicException
 ---> 70     raise e
      71   # pylint: enable=protected-access
      72   return tensors
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
      58     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
      59                                                op_name, inputs, attrs,
 ---> 60                                                num_outputs)
      61   except core._NotOkStatusException as e:
      62     if name is not None:
 
 TypeError: An op outside of the function building code is being passed
 a ""Graph"" tensor. It is possible to have Graph tensors
 leak out of the function building context by including a
 tf.init_scope in your function building code.
 For example, the following function will fail:
   @tf.function
   def has_init_scope():
     my_constant = tf.constant(1.)
     with tf.init_scope():
       added = my_constant * 2
 The graph tensor has name: encoder/unified_gru/ones_like:0
 
 	",1.0,sbagroy986,2019-05-31T09:46:08Z,"
 		Have tried with TensorFlow version 2.0.0-alpha and was able to reproduce the issue.
 		",2.0,sbagroy986,2019-06-03T18:19:34Z,"
 		Thanks for reporting the issue, let me take a look.
 		",3.0,sbagroy986,2019-06-04T15:41:33Z,"
 		Thanks for reporting the issue, it should now be fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/180f28a26660ca2e1ba27477f4f9592db5f9c4e8>180f28a</denchmark-link>
 
 		",180f28a26660ca2e1ba27477f4f9592db5f9c4e8,Scott Zhu,2019-06-04 08:31:21-07:00,MODIFY,1,tensorflow\python\keras\layers\recurrent_v2.py,tensorflow\python\keras\layers\recurrent_v2.py,1.0,"323,324",322,MODIFY,1.0,tensorflow\python\keras\layers\recurrent_v2_test.py,tensorflow\python\keras\layers\recurrent_v2_test.py,4.0,sbagroy986,2019-06-04T15:41:35Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29187>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29187>No</denchmark-link>
 
 		",5.0,sbagroy986,2019-06-04T15:55:52Z,"
 		Btw, the current colab might not apply the dropout correctly if you only enable the dropout/recurrent_dropout on the GRU layer. Under the hood, the keras layer will check whether the current context is in training or inference, and only apply the dropout during training. If the GRU layer was using by a keras model together with model.fit/eval/predict, then the training context will be applied correctly. However, if the user is writing their own custom training loop, then the training context need to be set manually, eg by
 tf.keras.backend.set_learning_phase(1)  # training
 run_train_step()
 
 tf.keras.backend.set_learning_phase(0)
 run_eval_step()
 The other alternative is that make sure the encoder/decoder's call() method is training state aware. eg, the method could take a new kwarg training=None, and set to different value during training and inference. The training value need to be popagated to GRU's call() method as well.
 		",6.0,sbagroy986,2019-06-06T21:14:32Z,"
 		<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
 : Thanks a ton for your help on this!
 Quick follow-up: has this been fixed in the GPU version as well? I tried the (nightly) version from yesterday and it didn't seem to work.
 		",1.0,"66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90",,test_reset_dropout_mask_between_batch,"self,layer",66,90,,,,,,,,,,,,,,,call,"self,inputs,mask,training,initial_state",311,360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,sbagroy986,2019-07-31T17:18:57Z,"
 		The issue still persists in the beta release
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,sbagroy986,2019-08-12T05:52:03Z,"
 		I still have this issue in beta 2.0.0b1
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,sbagroy986,2019-08-12T14:47:54Z,"
 		For any of you that still facing the issue, could u provide a snippet to reproduce the issue?
 		",10.0,sbagroy986,2019-08-31T11:13:19Z,"
 		
 could u provide a snippet to reproduce the issue?
 
 Similar error even without GRU.
 <denchmark-code>cp_callback = ModelCheckpoint(filepath=""checkpoints/"", save_weights_only=False, verbose=0, save_best_only=True)
 
 feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
 
 model = tf.keras.Sequential([
   feature_layer,
   layers.Dense(128, activation='relu'),
   layers.Dense(128, activation='relu'),
   layers.Dense(1, activation='sigmoid')
 ])
 
 model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'], run_eagerly=True)
 model.fit(train_ds, validation_data=val_ds, epochs=5, callbacks=[cp_callback])
 </denchmark-code>
 
 		",11.0,sbagroy986,2019-09-09T17:32:00Z,"
 		<denchmark-link:https://github.com/knobel-dk>@knobel-dk</denchmark-link>
 , I am bit confused about your message, this issue was about the recurrent_dropout for the LSTM/GRU layer, but your code doesn't have any LSTM/GRU layer within it.
 Could you be more specific about the error you are facing?
 		",12.0,sbagroy986,2019-09-10T09:22:24Z,"
 		
 @knobel-dk, I am bit confused about your message, this issue was about the recurrent_dropout for the LSTM/GRU layer, but your code doesn't have any LSTM/GRU layer within it.
 Could you be more specific about the error you are facing?
 
 Thanks. Yes I have confused myself too. Those stateful Jupyter notebooks.. I fixed my problem by updating the TF2 version. Thanks.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29191,ethereon,2019-05-31T00:08:02Z,2019-08-01T17:32:32Z,tf.function spuriously fails for branched super() calls,"
 tf.function fails spuriously under Python 3.7.3 for the following example:
 import tensorflow as tf
 
 class Base(tf.Module):
     def __call__(self, x):
         return x + 1.
 
 class Sub(Base):
     def __call__(self, x):
         return super().__call__(x) if True else 1.
 
 @tf.function
 def test():
     return Sub()(tf.constant(42.))
 
 print(test())
 (Colab: <denchmark-link:https://colab.research.google.com/drive/1_pS1K0biwse1oTIuuEv0lZYavbE61HAP>https://colab.research.google.com/drive/1_pS1K0biwse1oTIuuEv0lZYavbE61HAP</denchmark-link>
 )
 This produces the following error:
 RuntimeError: in converted code:
 
     bug.py:16 test  *
         return Sub()(tf.constant(42.))
     bug.py:12 __call__  *
         return super().__call__(x) if True else 1.
 
     RuntimeError: super(): no arguments
 Observations
 
 Everything works correctly without the tf.function decoration
 The branch in __call__ seems necessary to trigger the bug. Skipping the branch doesn't trigger it. Replacing True with False doesn't trigger it.
 The bug can be triggered even if the condition evaluates to False (for example, replacing True with x < 0 for x=42)
 Replacing tf.constant(42.) with 42. doesn't trigger the bug
 
 Tested on TensorFlow 2.0 nightly 2.0.0-dev20190529 on Ubuntu 16.04 with Python 3.7.3
 	",1.0,ethereon,2019-05-31T05:11:11Z,"
 		change
 <denchmark-code>return super().__call__(x) if True else 1.
 </denchmark-code>
 
 to
 <denchmark-code>return super(Sub, self).__call__(x) if True else 1.
 </denchmark-code>
 
 there will be no errors
 		",2.0,ethereon,2019-05-31T05:38:12Z,"
 		<denchmark-link:https://github.com/zakizhou>@zakizhou</denchmark-link>
  Sure, but the point is that  without any arguments is <denchmark-link:https://www.python.org/dev/peps/pep-3135/>perfectly valid in Python 3</denchmark-link>
  and as such, this should not result in a failure. Furthermore, the sensitivity to the conditional hints at further issues in 
 		",3.0,ethereon,2019-06-03T11:39:02Z,"
 		I am able to reproduce the issue with Tensorflow  nightly 2.0.0-dev20190529. Thanks!
 		",4d4f7ed0a4605975e45efe9ca1750653190aeedf,Dan Moldovan,2019-08-01 10:30:13-07:00,MODIFY,1,tensorflow\python\autograph\converters\call_trees.py,tensorflow\python\autograph\converters\call_trees.py,1.0,"177,184,185","177,184",MODIFY,2.0,tensorflow\python\autograph\converters\function_scopes.py,tensorflow\python\autograph\converters\function_scopes.py,4.0,ethereon,2019-06-07T23:56:15Z,"
 		I believe this is related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/26029>#26029</denchmark-link>
  - the Python3-style super() is indeed not handled correctly yet; we're working to fix that.
 It's indeed strange that the issue doesn't replicate reliably - I'll have a closer look.
 		",5.0,ethereon,2019-07-27T02:56:01Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
 : this continues to be an issue even with the recent attempt at supporting argument-less super.
 In the new <denchmark-link:https://github.com/tensorflow/tensorflow/blob/612ceb6c488e228fa5246d2452799cf2691ef5f1/tensorflow/python/autograph/operators/py_builtins.py#L92-L130>super_in_original_context</denchmark-link>
  implementation, the technique used for getting  seems a bit fragile. In particular, existence of conditionals (and, presumably, other scoping constructs) around the  call causes it to break in various ways.
 I've included a couple of samples in <denchmark-link:https://colab.research.google.com/drive/1aoqSo9StvVHDPHhH7Ao4WYm_t6UycnWd>this colab notebook</denchmark-link>
 
 		",6.0,ethereon,2019-07-27T11:09:50Z,"
 		<denchmark-link:https://github.com/ethereon>@ethereon</denchmark-link>
  Thank you for follow-up and the samples! Indeed, our current resolution for handling  is not yet complete, and will only work if the call is in a method (or outside control flow, as you pointed out).
 <denchmark-link:https://github.com/kkimdev>@kkimdev</denchmark-link>
  has been working on addressing this. We intentionally oversimplified the solution to assume the original caller of  is exactly one frame up the call stack in the converted code, and we're working on a follow-up improvement that removes that incorrect assumption and instead walks the call stack to find the caller, which can be any number of levels above. We hope to have this ready shortly, and will ping this thread when ready.
 In the mean time, the following should be a reliable wokraround:
 <denchmark-code>base_self = super()
 return base_self.__call__(x) if True else 1
 </denchmark-code>
 
 <denchmark-code>base_self = super()
 if True:
   return base_self.__call__(x)
 return 1
 </denchmark-code>
 
 <denchmark-code>base_self = super()
 def local_nested_function():
   return base_self.__call__(x)
 return local_nested_function()
 </denchmark-code>
 
 Again, all these workarounds should hopefully not be needed soon.
 		",1.0,"59,60,65,66,71,72","59,60,65,70",visit_Lambda,"self,node",46,76,MODIFY,2.0,tensorflow\python\autograph\core\converter.py,tensorflow\python\autograph\core\converter.py,1.0,"269,273",,MODIFY,4.0,tensorflow\python\autograph\core\converter_testing.py,tensorflow\python\autograph\core\converter_testing.py,1.0,"138,139",138,visit_Call,"self,node",101,187,,,,,,,,,,,,,,,,,,,,,,1.0,"98,99,105,107","96,103",visit_FunctionDef,"self,node",78,116,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ethereon,2019-08-01T17:26:00Z,"
 		This should not be more robustly handled with the commit that will land soon. If you have a chance to give it a try, I'd love to know if we missed any corner case.
 		",__init__,"self,namer,entity_info,program_ctx,target_name",269,273,prepare,"self,test_fn,namespace,recursive",124,142,MODIFY,4.0,tensorflow\python\autograph\core\function_wrappers.py,tensorflow\python\autograph\core\function_wrappers.py,1.0,"43,44,48,49","43,47,48",__init__,"self,function_name,options",43,60,1.0,"43,44,48,49","43,47,48",__init__,"self,function_name,scope_name,options",43,61,1.0,"105,107",106,with_function_scope,"thunk,scope_name,options",105,108,1.0,"105,107","104,106",with_function_scope,"thunk,options",104,107,,,,,,,,,,,,,,,MODIFY,3.0,tensorflow\python\autograph\core\function_wrappers_test.py,tensorflow\python\autograph\core\function_wrappers_test.py,1.0,"54,55",54,test_all_disabled,self,53,57,1.0,36,36,test_name_scope,self,31,40,MODIFY,2.0,tensorflow\python\autograph\impl\api.py,tensorflow\python\autograph\impl\api.py,1.0,"365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,390,392","365,366,375,377",converted_call,"f,options,args,kwargs,caller_fn_scope",365,546,8.0,ethereon,2019-08-01T17:32:33Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29191>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29191>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,1.0,269,268,__init__,"self,namer,entity_info,program_ctx",268,271,,,,,,,,,,,,,,,1.0,60,60,compiled,"self,node,namespace,symbols",55,92,1.0,60,60,compiled.converted_call,"f,unused_opts,args,kwargs,unused_function_ctx",60,65,1.0,60,60,compiled.converted_call,"f,unused_opts,args,kwargs",60,65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,45,45,test_auto_cotrol_deps,self,42,51,1.0,"365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,390,392","365,366,375,377",converted_call,"f,options,args,kwargs",365,531,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\autograph\impl\conversion.py,tensorflow\python\autograph\impl\conversion.py,1.0,"656,657,658,659,660,661,662,668","661,665,668,670,673",MODIFY,0.0,tensorflow\python\autograph\operators\BUILD,tensorflow\python\autograph\operators\BUILD,0.0,103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,5.0,tensorflow\python\autograph\operators\py_builtins.py,tensorflow\python\autograph\operators\py_builtins.py,MODIFY,30.0,tensorflow\python\autograph\operators\py_builtins_py3_test.py,tensorflow\python\autograph\operators\py_builtins_py3_test.py,MODIFY,20.0,tensorflow\python\autograph\operators\py_builtins_test.py,tensorflow\python\autograph\operators\py_builtins_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,convert_func_to_ast,"f,program_ctx,do_rename",626,679,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"52,53,55,56,57,58,59,60,61,62","52,53,55",eval_in_original_context,"f,args,caller_level_delta",52,62,1.0,"73,74,75,76,77,78,79","73,74,75,76,79,84",eval_in_original_context,"f,args,caller_fn_scope",73,85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"41,44,46,49,50,51,52,55","41,42,44,45,47,50,51,52,53,54,55",1.0,"187,188,189,190,191","189,190,191",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,88,89,95,96,97,98,101,102,107,109,110,111,113,114,115,131,132","65,66,72,73,74,75,76,79,84,86,87,89,90,91,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,123,124,126,127,128,129,130,131,132,133",super_in_original_context,"f,args,caller_level_delta",65,133,1.0,"52,53,55,56,57,58,59,60,61,62,63,65,66,67,68,69,70","52,53,55,65,66",_find_originating_frame,"caller_fn_scope,innermost",52,70,1.0,"88,89,95,96,97,98,101,102,107,109,110,111,113,114,115,131,132,134,135,136,137,138,140,141,142,143,144,145,146,147","89,90,91,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,123,124,126,127,128,129,130,131,132,133",super_in_original_context,"f,args,caller_fn_scope",88,147,test_super_in_original_context_niladic_call,self,41,55,1.0,107,"105,106,107",test_super_in_original_context_with_varargs.with_varargs,"self,args",105,107,1.0,33,"32,33",test_super_with_no_arg_in_original_context.plus_twenty,"self,x",32,33,1.0,"49,50,51,52","50,51,52",test_super_in_original_context_niladic_call.test_method,self,49,52,1.0,"92,93,94,97,99,102,104,107,108,109,110","92,95,97,98,99,100,101,102,105,106,107,110",test_super_in_original_context_with_varargs,self,92,110,1.0,"35,36,37,38,39","35,37,38",_basic_function_scope,self,35,39,1.0,"49,50,51,52,55,57,60,62,65,66,67,68,69","47,50,51,52,53,54,55,57,60,61,62,63,64,65,66,69",test_super_in_original_context_with_locals,self,47,69,1.0,"107,108,109,110,111,112,113,114,115,116","107,110,112,115,116",test_super_in_original_context_inner_lambda.test_method,"self,x",107,116,1.0,46,47,test_super_in_original_context_niladic_call.overridden_method,"self,x",46,47,1.0,"65,66,67,68,69,70","65,66,69",test_super_in_original_context_caller_with_locals.test_method,"self,x",65,70,1.0,"75,78,80,83,84,85,86,87,88,89,90,91,92,93,94,97","75,76,77,79,80,81,84,85,86,89,90,92,95,97",test_super_in_original_context_inner_function,self,75,97,1.0,"84,85,86","84,85,86",test_super_in_original_context_with_args.with_args,"self,x,y,z",84,86,1.0,97,"97,98",test_super_in_original_context_with_varargs.plus_twenty,"self,x",97,98,1.0,41,"40,41,42",test_super_with_no_arg_in_original_context.no_arg,self,40,42,1.0,,"125,126,127",test_super_in_original_context_with_kwargs.with_kwargs,"self,kwargs",125,127,1.0,"29,30","29,30",overridden_method,"self,x",29,30,1.0,,"76,77",test_super_in_original_context_with_args.plus_twenty,"self,x",76,77,1.0,62,"62,63",test_super_in_original_context_caller_with_locals.overridden_method,"self,x",62,63,1.0,,"117,118",test_super_in_original_context_with_kwargs.plus_twenty,"self,x",117,118,1.0,80,"80,81",test_super_in_original_context_inner_function.overridden_method,"self,x",80,81,1.0,104,105,test_super_in_original_context_inner_lambda.overridden_method,"self,x",104,105,1.0,"83,84,85,86,87,88,89,90,91,92,93,94","84,85,86,89,90,92",test_super_in_original_context_inner_function.test_method,"self,x",83,94,1.0,"99,102,104,107,108,109,110,111,112,113,114,115,116,119","99,100,101,102,105,106,107,110,112,115,116,117,118,119",test_super_in_original_context_inner_lambda,self,99,119,1.0,"73,75,78,80,83,84,85,86,87,88,89,90","71,74,75,76,77,79,80,81,84,85,86,89,90",test_super_in_original_context_with_args,self,71,90,1.0,"112,113,114,115,116,119","112,115,116,117,118,119,120,122,125,126,127,130",test_super_in_original_context_with_kwargs,self,112,130,1.0,52,"52,53",test_super_in_original_context_with_locals.plus_twenty,"self,x",52,53,1.0,"90,91,92","90,92",test_super_in_original_context_inner_function.test_super_in_original_context_inner_function.test_method.inner_fn,,90,92,1.0,"60,62,65,66","60,61,62,63,64,65,66",test_super_in_original_context_with_locals.with_locals,self,60,66,1.0,"57,60,62,65,66,67,68,69,70,73","57,60,61,62,63,64,65,66,69,71",test_super_in_original_context_caller_with_locals,self,57,73,1.0,"27,29,30,33,35,36,37,38,39,41,44","27,28,29,30,31,32,33,35,37,38,40,41,42,44,45",test_super_with_no_arg_in_original_context,self,27,45,test_eval_in_original_context_inner_function.test_eval_in_original_context_inner_function.test_fn.inner_fn,,187,191,1.0,"183,184,185,187,188,189,190,191,193","189,190,191,192,193",test_eval_in_original_context_inner_function.test_fn,,183,193,1.0,"176,177,179,181,183,184,185,187,188,189,190,191,193,195,196","176,177,179,181,182,189,190,191,192,193,196",test_super_with_one_arg_in_original_context,self,176,196,1.0,"189,190,191,193","189,190,191,192,193",test_super_with_one_arg_in_original_context.one_arg,self,189,193,1.0,"174,176,177,179","172,173,174,176,177,179",test_eval_in_original_context,self,172,179,1.0,166,"164,165,166",test_eval_in_original_context.caller_2,lvl_delta,164,166,1.0,213,"211,212,213,214",test_super_with_two_args_in_original_context.two_args,self,211,214,1.0,"205,206,207,208,209,210",205,test_super_in_original_context_unary_call.test_method,self,205,210,1.0,"198,205,206,207,208,209,210,213,215","198,201,202,203,204,205,211,212,213,214,217",test_super_with_two_args_in_original_context,self,198,217,1.0,"181,183,184,185,187,188,189,190,191,193,195","181,182,189,190,191,192,193",test_eval_in_original_context_inner_function,self,181,195,1.0,"215,223,224,225,226,227,230",217,test_super_in_original_context_binary_call,self,215,230,1.0,"223,224,225,226,227",,test_super_in_original_context_binary_call.test_method,self,223,227,1.0,,"160,162",test_eval_in_original_context.caller_1,lvl_delta,160,162,1.0,181,"181,182",test_super_with_one_arg_in_original_context.plus_twenty,"self,x",181,182,1.0,"166,167,168,169,170","166,168,169,170",_basic_function_scope,self,166,170,1.0,,"203,204",test_super_with_two_args_in_original_context.plus_twenty,"self,x",203,204,1.0,"168,169,170","168,169,170",test_eval_in_original_context.caller_3,lvl_delta,168,170,1.0,"41,42",,plus_twenty,"self,x",41,42,1.0,"174,176,177","174,176,177",test_eval_in_original_context.test_fn,,174,177,1.0,"197,198,205,206,207,208,209,210,213","198,201,202,203,204,205,211,212,213",test_super_in_original_context_unary_call,self,197,213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29250,ghchinoy,2019-06-01T16:02:49Z,2019-06-20T11:30:03Z,[TF 2.0 API Docs] tf.autograph.set_verbosity,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/set_verbosity>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/set_verbosity</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 Description could be clearer:
 
 Reference to Abseil's logging format could be referenced rather than only to Abseil, itself (user would have to hunt through docs to see the logging output format referenced)
 There's a slight misspelling in the args for alsologtostdout
 “ it is recommended to set this value to a larges number, like 10” should be “ it is recommended to set this value to a large number, like 10”
 
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Yes.
 	",1.0,ghchinoy,2019-06-20T11:30:03Z,"
 		Closing this issue since the associated PR has been merged. Feel free to reopen if the problem still persists. Thanks!
 		",,,,,,,,,61133370602a5fc7b3313ea7f8f745dae66b3c38,G. Hussain Chinoy,2019-06-06 21:13:16-06:00,MODIFY,1,tensorflow\python\autograph\utils\ag_logging.py,tensorflow\python\autograph\utils\ag_logging.py,1.0,"49,73,74,82","49,73,81",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,set_verbosity,"level,alsologtostdout",41,88,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29270,hendercine,2019-06-01T20:38:39Z,2019-06-12T22:28:46Z,tf.autograph.experimental.Feature,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/experimental/Feature>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/autograph/experimental/Feature</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 There is no description provided.
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Parameters are undefined.
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Return values are not defined.
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Errors are not defined?
 <denchmark-h:h3>Usage example</denchmark-h>
 
 There is not a usage example.
 	",1.0,hendercine,2019-06-12T21:22:57Z,"
 		experimental.Feature is not a function, so most of those fields don't apply. I'll send a CL to clarify in that it's an enumeration type and show an example.
 		",,,,,,,,,629e5a8a56653b16cb784a52009ff1ceaf3db73b,Dan Moldovan,2019-06-12 15:26:17-07:00,MODIFY,0,tensorflow\python\autograph\core\converter.py,tensorflow\python\autograph\core\converter.py,0.0,"91,92,93,94,95,96,97,98,99,100,101,102,103,104",91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29277,MagsMagnoli,2019-06-01T20:52:40Z,2021-01-20T18:03:33Z,[TF 2.0 API Docs] tf.nn.dropout,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/dropout>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/dropout</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Usages are linked but none are detailed inline on the page
 	",1.0,MagsMagnoli,2021-01-06T16:29:00Z,"
 		<denchmark-link:https://github.com/MagsMagnoli>@MagsMagnoli</denchmark-link>
 ,
 The documentation has been updated for TensorFlow v2.4. Could you please take a look at it and let us know if this is still an issue? Thanks!
 		",2.0,MagsMagnoli,2021-01-13T17:03:29Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
 		",3.0,MagsMagnoli,2021-01-20T18:03:32Z,"
 		Closing as stale. Please reopen if you'd like to work on this further.
 		",951be80fc6873434b8ab2bef65d437b094037c86,Chenkai Kuang,2020-09-11 12:10:17-07:00,MODIFY,0,tensorflow\python\distribute\BUILD,tensorflow\python\distribute\BUILD,0.0,"1789,1792,1793,1797","1792,1795",MODIFY,7.0,tensorflow\python\distribute\parameter_server_strategy_v2.py,tensorflow\python\distribute\parameter_server_strategy_v2.py,,,,,,,,,,,,,1.0,"250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271","250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276",_make_variable_creator._make_variable_creator.variable_creator.init_shard_fn,shard_index,250,276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271","182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289",_make_variable_creator,self,182,289,1.0,"149,150,151,152,153,154,155,156,157,158,159,160,161,162,164,165,166,167,168,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269","150,151,152,153,154,155,158,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269",_create_variable,"self,next_creator,kwargs",148,269,1.0,"232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258","232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258",_create_variable.init_shard_fn,shard_index,232,258,1.0,"151,152,153,154","151,152,153,154",_scope,"self,strategy",151,154,1.0,271,"271,272,273,274,275,276,277,278,279,280,281,282",_create_variable_round_robin,"self,next_creator,kwargs",271,282,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271","194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287",_make_variable_creator.variable_creator,"next_creator,kwargs",194,287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29342,ageron,2019-06-03T01:51:19Z,2019-07-03T23:37:49Z,tf.config.set_soft_device_placement() seems to have no effect,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 MacOSX 10.13.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 VERSION='2.0.0-dev20190527'
 GIT_VERSION='v1.12.1-2821-gc5b8e15064'
 Python version:
 3.5
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 CUDA 10.0 (it's just a Colab GPU instance)
 GPU model and memory:
 Tesla P4 15079MiB
 
 Describe the current behavior
 The tf.config.set_soft_device_placement() function seems to have no effect when I create an integer variable and try to place it on a GPU, I still get an exception.
 Describe the expected behavior
 I expect soft placement to fallback to using the CPU. No error.
 Code to reproduce the issue
 import tensorflow as tf
 tf.config.set_soft_device_placement(True)
 with tf.device(""/gpu:0""):
     f = tf.Variable(42)
 Other info / logs
 The code above causes the following exception:
 <denchmark-code>---------------------------------------------------------------------------
 NotFoundError                             Traceback (most recent call last)
 <ipython-input-3-1babaf613bc3> in <module>()
       2 tf.config.set_soft_device_placement(True)
       3 with tf.device(""/gpu:0""):
 ----> 4     f = tf.Variable(42)
 
 10 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
     260       return cls._variable_v1_call(*args, **kwargs)
     261     elif cls is Variable:
 --> 262       return cls._variable_v2_call(*args, **kwargs)
     263     else:
     264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
     254         synchronization=synchronization,
     255         aggregation=aggregation,
 --> 256         shape=shape)
     257 
     258   def __call__(cls, *args, **kwargs):
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
     235                         shape=None):
     236     """"""Call on Variable class. Useful to force the signature.""""""
 --> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
     238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
     239       previous_getter = _make_getter(getter, previous_getter)
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
    2549       synchronization=synchronization,
    2550       aggregation=aggregation,
 -> 2551       shape=shape)
    2552 
    2553 
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
     262       return cls._variable_v2_call(*args, **kwargs)
     263     else:
 --> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
     265 
     266 
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
     462           synchronization=synchronization,
     463           aggregation=aggregation,
 --> 464           shape=shape)
     465 
     466   def __repr__(self):
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, shape)
     616               shared_name=shared_name,
     617               name=name,
 --> 618               graph_mode=self._in_graph_mode)
     619         # pylint: disable=protected-access
     620         if (self._in_graph_mode and initial_value is not None and
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode)
     223   dtype = initial_value.dtype.base_dtype
     224   return variable_handle_from_shape_and_dtype(
 --> 225       shape, dtype, shared_name, name, graph_mode, initial_value)
     226 
     227 
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py in variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, extra_handle_data)
     139                                                    shared_name=shared_name,
     140                                                    name=name,
 --> 141                                                    container=container)
     142   if extra_handle_data is None:
     143     extra_handle_data = handle
 
 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py in var_handle_op(dtype, shape, container, shared_name, name)
    1416       else:
    1417         message = e.message
 -> 1418       _six.raise_from(_core._status_to_exception(e.code, message), None)
    1419   # Add nodes to the TensorFlow graph.
    1420   dtype = _execute.make_type(dtype, ""dtype"")
 
 /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)
 
 NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node {{node VarHandleOp}}
 	 (OpKernel was found, but attributes didn't match) Requested Attributes: container="""", dtype=DT_INT32, shape=[], shared_name=""cd2c89b7-88b7-44c8-ad83-06c2a9158347""
 	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
   device='GPU'; dtype in [DT_INT64]
   device='GPU'; dtype in [DT_COMPLEX128]
   device='GPU'; dtype in [DT_COMPLEX64]
   device='GPU'; dtype in [DT_BOOL]
   device='GPU'; dtype in [DT_DOUBLE]
   device='GPU'; dtype in [DT_FLOAT]
   device='GPU'; dtype in [DT_HALF]
   device='CPU'
   device='XLA_CPU'
   device='XLA_GPU'
  [Op:VarHandleOp] name: Variable/
 </denchmark-code>
 
 	",1.0,ageron,2019-06-03T02:18:24Z,"
 		Also, if I activate soft placement and I try to place an operation on a GPU device that does not exist, I still get an exception. I expected TF to fallback to using the CPU:
 import tensorflow as tf
 tf.config.set_soft_device_placement(True)
 with tf.device(""/gpu:1""):
     f = tf.Variable(42.0)
 Raises this exception:
 <denchmark-code>---------------------------------------------------------------------------
 RuntimeError                              Traceback (most recent call last)
 <ipython-input-1-3a03536f5e27> in <module>
       2 tf.config.set_soft_device_placement(True)
       3 with tf.device(""/gpu:1""):
 ----> 4         f = tf.Variable(42.5)
       5
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
     259       return cls._variable_v1_call(*args, **kwargs)
     260     elif cls is Variable:
 --> 261       return cls._variable_v2_call(*args, **kwargs)
     262     else:
     263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
     253         synchronization=synchronization,
     254         aggregation=aggregation,
 --> 255         shape=shape)
     256
     257   def __call__(cls, *args, **kwargs):
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
     234                         shape=None):
     235     """"""Call on Variable class. Useful to force the signature.""""""
 --> 236     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
     237     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
     238       previous_getter = _make_getter(getter, previous_getter)
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
    2542       synchronization=synchronization,
    2543       aggregation=aggregation,
 -> 2544       shape=shape)
    2545
    2546
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
     261       return cls._variable_v2_call(*args, **kwargs)
     262     else:
 --> 263       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
     264
     265
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
     462           synchronization=synchronization,
     463           aggregation=aggregation,
 --> 464           shape=shape)
     465
     466   def __repr__(self):
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, shape)
     607             initial_value = ops.convert_to_tensor(
     608                 initial_value() if init_from_fn else initial_value,
 --> 609                 name=""initial_value"", dtype=dtype)
     610           # Don't use `shape or initial_value.shape` since TensorShape has
     611           # overridden `__bool__`.
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype, dtype_hint)
    1095   preferred_dtype = deprecation.deprecated_argument_lookup(
    1096       ""dtype_hint"", dtype_hint, ""preferred_dtype"", preferred_dtype)
 -> 1097   return convert_to_tensor_v2(value, dtype, preferred_dtype, name)
    1098
    1099
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
    1153       name=name,
    1154       preferred_dtype=dtype_hint,
 -> 1155       as_ref=False)
    1156
    1157
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors, accept_composite_tensors)
    1232
    1233     if ret is None:
 -> 1234       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    1235
    1236     if ret is NotImplemented:
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
     303                                          as_ref=False):
     304   _ = as_ref
 --> 305   return constant(v, dtype=dtype, name=name)
     306
     307
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
     244   """"""
     245   return _constant_impl(value, dtype, shape, name, verify_shape=False,
 --> 246                         allow_broadcast=True)
     247
     248
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
     252   ctx = context.context()
     253   if ctx.executing_eagerly():
 --> 254     t = convert_to_eager_tensor(value, ctx, dtype)
     255     if shape is None:
     256       return t
 
 ~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     109       return ops.EagerTensor(
     110           value, handle, device, dtype, tensor)
 --> 111     t = ops.EagerTensor(value, handle, device, dtype)
     112     scalar_cache[cache_key] = t
     113     return t
 
 RuntimeError: Error copying tensor to device: /job:localhost/replica:0/task:0/device:GPU:1. /job:localhost/replica:0/task:0/device:GPU:1 unknown device.
 </denchmark-code>
 
 		",2.0,ageron,2019-06-03T07:12:43Z,"
 		Hey, have you had a look at this example from the GPU guide at tensorflow.org
 tf.debugging.set_log_device_placement(True)
 # Create some tensors
 a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
 b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
 c = tf.matmul(a, b)
 print(c)
 To me it looks like TF is choosing which device is then suitable to execute this op:
 
 If enabled, an op will be placed on CPU if any of the following are true
 1. there's no GPU implementation for the OP
 2. no GPU devices are known or registered
 3. need to co-locate with reftype input(s) which are from CPU
 
 from <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/config.py>TF config</denchmark-link>
 
 		",3.0,ageron,2019-06-03T10:32:31Z,"
 		Hi <denchmark-link:https://github.com/lufol>@lufol</denchmark-link>
 ,
 Thanks for your answer. Yes I saw this doc, that's actually why I filed this bug. I don't see what soft placement would change in this example.
 I expect soft placement to change something when the user requests a specific device but there is no op for that device, or the device does not exist. This would be useful if you want to write a program and deploy it on machines that may or may not have GPUs, for example. So on a machine without any GPU, I expect the following code to work without any error, and just fallback to placing the variable on the CPU:
 tf.config.set_soft_device_placement(True)
 with tf.device(""/gpu:0""):
     x = tf.Variable(1.0)
 Perhaps I'm misunderstanding what set_soft_device_placement() is designed for?
 		",7360531c13113a19120d798278cd20bec2e5e0c3,Xiao Yu,2019-07-03 16:35:41-07:00,MODIFY,1,tensorflow\c\eager\c_api.cc,tensorflow\c\eager\c_api.cc,1.0,633,633,MODIFY,1.0,tensorflow\c\eager\c_api_experimental_test.cc,tensorflow\c\eager\c_api_experimental_test.cc,4.0,ageron,2019-06-03T10:34:38Z,"
 		Okay, I got it: the semantics of soft placement have changed since TF 1.
 In TF 1, the following code works fine on a machine without any GPU (I just tried it):
 import tensorflow as tf
 
 with tf.device(""/gpu:42""): # there is no such device
     i = tf.Variable(123) # plus integers are not allowed on GPUs
 
 # but let's make TF super soft and tolerant:
 config = tf.ConfigProto()
 config.allow_soft_placement = True
 with tf.Session(config=config) as sess:
     sess.run(i.initializer)
     print(sess.run(i))
 Prints 123, no problem. :)
 That's why I was surprised that it didn't work in TF 2.0. I'm actually not sure when you would ever need to call set_soft_device_placement(False) in TF 2.0, what's the use case?
 		",5.0,ageron,2019-06-03T12:13:56Z,"
 		From what I understand, you either use
 
 
 tf.config.set_soft_device_placement(True) to let tf automatically decide, which device to use.
 
 
 or you use tf.config.set_soft_device_placement(False) and decide for each tensor where to place it like this with tf.device('/CPU:0'): ...
 
 
 So I guess in your case the first option would be best.
 		",6.0,ageron,2019-06-04T04:24:30Z,"
 		Hi <denchmark-link:https://github.com/lufol>@lufol</denchmark-link>
 ,
 I just ran some tests, and it really seems like tf.config.set_soft_device_placement(False) makes no difference at all. Whether it's True or False, the default behavior is applied.
 I'm quite puzzled.
 		",1.0,,"63,64",tensorflow::ExecuteWithProfiling,async,41,112,MODIFY,0.0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,"3204,3231","3260,3278",MODIFY,2.0,tensorflow\core\common_runtime\colocation_graph.cc,tensorflow\core\common_runtime\colocation_graph.cc,1.0,,"58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111",TFE_OpSetDevice,"op,device_name,status",632,634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,ageron,2019-06-04T12:03:48Z,"
 		Have tried to reproduce on Colab with TF 2.0.0-dev20190527  with set_soft_device_placement as True as well as False and was able to get same result in both the scenarios as mentioned in the issue.
 		",,,,,tensorflow::FilterSupportedDevices,"devices,supported_device_types,default_device",58,111,MODIFY,0.0,tensorflow\core\common_runtime\colocation_graph.h,tensorflow\core\common_runtime\colocation_graph.h,0.0,"251,252,253,254,255,256,257,258",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\common_runtime\eager\BUILD,tensorflow\core\common_runtime\eager\BUILD,0.0,89,,,,,,,,,,,,,MODIFY,6.0,tensorflow\core\common_runtime\eager\context.cc,tensorflow\core\common_runtime\eager\context.cc,1.0,81,,tensorflow::EagerContext::EagerContext,"opts,default_device_placement_policy,default_mirroring_policy,async,device_mgr,device_mgr_owned,rendezvous,custom_kernel_creator,cluster_flr",63,112,8.0,ageron,2019-06-04T19:15:06Z,"
 		Yes <denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  , you are right.
 But I guess at least the doc does not explicitly says to execute the tensors on CPU if  according to the doc:
 
 If enabled, an op will be placed on CPU if any of the following are true
 1. there's no GPU implementation for the OP
 2. no GPU devices are known or registered
 3. need to co-locate with reftype input(s) which are from CPU
 
 Maybe one could add something to the doc explaining the behaviour if tf.config.set_soft_device_placement() is set to False?
 Behaviour is reproducable <denchmark-link:https://colab.research.google.com/drive/1rtGjX9O_3rEI7yarMHMHBvu0l222dFXd>here</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344",,tensorflow::ColocationGraph::FilterSupportedDevices,"devices,supported_device_types,default_device",1291,1344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,752,tensorflow::EagerContext::InitializeRemoteWorker,"remote_eager_workers,remote_device_mgr,remote_contexts,context_id,rendezvous_creator",732,760,1.0,"574,575,581,582,583,584,585,592,593","574,583,584",tensorflow::EagerContext::GetClient,"device_name,client",574,603,1.0,"571,572",,tensorflow::EagerContext::GetClient,"device,client",570,572,MODIFY,1.0,tensorflow\core\common_runtime\eager\context.h,tensorflow\core\common_runtime\eager\context.h,1.0,191,,MODIFY,3.0,tensorflow\core\common_runtime\eager\eager_operation.cc,tensorflow\core\common_runtime\eager\eager_operation.cc,1.0,"19,21,22,23,24,26","19,20,22,24,25",9.0,ageron,2019-06-04T20:32:25Z,"
 		<denchmark-link:https://github.com/jaingaurav>@jaingaurav</denchmark-link>
  any comments?
 		",10.0,ageron,2019-06-13T20:18:59Z,"
 		Sorry for the delay. There have been multiple discussions about this internally. It seems that soft placement is respected by tf.function but not by the tf.device placement. We'd like to resolve this, however given it is a behavior change we're unlikely to be able to change the default in 1.0.
 		",11.0,ageron,2019-06-13T23:47:35Z,"
 		Thanks <denchmark-link:https://github.com/jaingaurav>@jaingaurav</denchmark-link>
 . I'm just thinking about TF 2.x, I understand that it must keep the same behavior in TF 1.x.
 		",12.0,ageron,2019-07-03T23:37:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29342>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29342>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\core\common_runtime\eager\eager_operation.h,tensorflow\core\common_runtime\eager\eager_operation.h,MODIFY,9.0,tensorflow\core\common_runtime\eager\execute.cc,tensorflow\core\common_runtime\eager\execute.cc,MODIFY,2.0,tensorflow\core\common_runtime\process_function_library_runtime.h,tensorflow\core\common_runtime\process_function_library_runtime.h,MODIFY,1.0,tensorflow\core\distributed_runtime\eager\eager_service_impl.cc,tensorflow\core\distributed_runtime\eager\eager_service_impl.cc,MODIFY,1.0,tensorflow\core\util\device_name_utils.cc,tensorflow\core\util\device_name_utils.cc,MODIFY,0.0,tensorflow\core\util\device_name_utils.h,tensorflow\core\util\device_name_utils.h,0.0,"177,178,179,180",,MODIFY,0.0,tensorflow\python\eager\BUILD,tensorflow\python\eager\BUILD,0.0,"726,727,728,729,730,731,732,733,734,735,736,737,738",,,,,,,,,,,,,ADD,0.0,None,tensorflow\python\eager\device_placement_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::EagerContext::AllowSoftPlacement,,191,191,,,,,,,,,,,,,,,1.0,,663,tensorflow::EagerContext::InitializeRemoteMaster,"server,worker_env,worker_session,remote_eager_workers,remote_device_manager,remote_contexts,context_id,r,local_device_mgr,keep_alive_secs,cluster_flr",620,730,1.0,"557,558,559,560,561,562,563,564,565,566,567","561,562,563,564,565,567",tensorflow::EagerContext::IsLocalDeviceName,device_name,557,567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::EagerOperation::SetDeviceName,device,19,27,1.0,"65,66,67,68",65,tensorflow::EagerOperation::SetDevice,device,65,68,1.0,"69,70,71",,tensorflow::EagerOperation::GetDeviceName,,69,71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1034,1041,1042,1043",,1.0,"144,145,146",,1.0,205,205,1.0,"509,510,511,512,513,514,515,516,517,518,519,520,521",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::EagerExecute,"op,retvals,num_retvals",1026,1055,1.0,"71,72,73,74,75,76,77,78",,tensorflow::DevicesToString,devices,71,78,1.0,"806,807,808,809,810,811,812,813,814,815,817",840,tensorflow::EagerRemoteExecute,"op,retvals,num_retvals",802,913,1.0,,"219,220,222,223,224,229,230,231,232,235,236",tensorflow::SelectDevice,"ndef,ctx,device",219,237,1.0,"404,405,406,407,423,437,438,439,442","411,412",tensorflow::ShouldCompileWithXLA,"op,ctx,compile_with_xla",404,449,1.0,"475,476,559,572,573","495,496,509,510",tensorflow::EagerLocalExecute,"op,retvals,num_retvals",465,687,1.0,"110,111,112,113,114",,tensorflow::DeviceNameOrUnspecified,name,110,114,1.0,,"342,343,344,360,374,375,378",tensorflow::ShouldCompileWithXLA,"op,device,ctx,compile_with_xla",342,385,1.0,"238,239,240,241,243,244,245,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,293,294,295,296,297,298",,tensorflow::SelectDevice,"op,ndef,ctx,device",238,299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::ProcessFunctionLibraryRuntime::GetFunctionLibraryDefinition,,144,146,1.0,142,,tensorflow::ProcessFunctionLibraryRuntime::device_set,,142,142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"51,52",,tensorflow::EagerOperation::DebugString,,46,64,1.0,"18,19,21,22,23,24,26","18,19,20,22,24,25",tensorflow::EagerOperation::SetDevice,device,18,26,tensorflow::eager::EagerServiceImpl::ExecuteOp,"operation,server_context,queue_response",185,238,tensorflow::DeviceNameUtils::GetTaskName,"pn,task",509,521,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29393,llan-ml,2019-06-04T14:02:21Z,2019-06-17T21:49:36Z,[2.0alpha0 AutoGraph] tf.function does not automatically transform nested class methods,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0alpha0
 Python version: 3.6.5
 
 Describe the current behavior
 When we define multiple methods for a class and only decorate one of them with @tf.function, the nested methods are not automatically transformed and some errors raise.
 Describe the expected behavior
 We only need decorate the outermost method.
 Code to reproduce the issue
 # -*- coding: utf-8 -*-
 # @Author  : Lin Lan (ryan.linlan@gmail.com)
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
 
 
 class Foo(tf.keras.Model):
     def __init__(self):
         super(Foo, self).__init__()
         self.dense = tf.keras.layers.Dense(20)
         self.embeddings = tf.Variable(tf.random.normal((100, 5)), dtype=tf.float32)
 
     @tf.function
     def call(self, inputs):
         embeddings = tf.nn.embedding_lookup(
             self.embeddings, inputs)
         return self._inner(embeddings)
 
     # @tf.function
     def _inner(self, embeddings):
         batch = tf.shape(embeddings)[0]
         ta = tf.TensorArray(tf.float32, size=batch)
         for i in tf.range(batch):
             this = self.dense(embeddings[i][tf.newaxis, :])
             ta = ta.write(i, this)
         return ta.stack()
 
 
 foo = Foo()
 res = foo([0, 2, 4, 6, 8])
 
 Other info / logs
 TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
 Also decorating the method _inner eliminate the error.
 	",1.0,llan-ml,2019-06-04T14:07:46Z,"
 		It took me a lot of time to find this bug (or intended behavior?) from my original code. It would be better to add a warning in the doc of tf.function.
 This <denchmark-link:https://www.tensorflow.org/alpha/guide/effective_tf2#refactor_your_code_into_smaller_functions>section</denchmark-link>
  is confusing given this issue. Also the third paragraph of this <denchmark-link:https://www.tensorflow.org/alpha/guide/autograph#the_tffunction_decorator>section</denchmark-link>
 .
 		",2.0,llan-ml,2019-06-05T09:21:26Z,"
 		Have tried with TF version 2.0.0-alpha on Colab and was able to reproduce the issue as mentioned in the description.
 		",3.0,llan-ml,2019-06-05T17:44:57Z,"
 		You shouldn't need to decorate self._inner. <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  why isn't it being caught here?
 		",665bd441195ce352b0d5ce74d5fd2dc19fa4a614,Dan Moldovan,2019-06-17 14:46:06-07:00,MODIFY,1,tensorflow\python\autograph\impl\conversion.py,tensorflow\python\autograph\impl\conversion.py,1.0,"368,369,401","368,400,401,402",MODIFY,4.0,tensorflow\python\autograph\impl\conversion_test.py,tensorflow\python\autograph\impl\conversion_test.py,4.0,llan-ml,2019-06-05T17:56:10Z,"
 		This is a bug. We'll have it fixed in the nightly soon.
 		",5.0,llan-ml,2019-06-15T12:33:03Z,"
 		Hi <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  Any updates regarding to this issue?
 		",6.0,llan-ml,2019-06-15T15:41:34Z,"
 		Yep! It will likely be fixed today, or sometime next week at the latest.
 		",1.0,"65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98",,test_is_whitelisted_for_graph_callable_whitelisted_call,self,65,98,MODIFY,2.0,tensorflow\python\autograph\pyct\inspect_utils.py,tensorflow\python\autograph\pyct\inspect_utils.py,1.0,"219,220",,,,,,,,,is_whitelisted_for_graph,"o,check_call_override",327,414,,,,,,,,,,,,,,,,,,,,,,1.0,"77,78",,test_is_whitelisted_for_graph_callable_whitelisted_call.whitelisted_method,self,77,78,1.0,"88,89",,test_is_whitelisted_for_graph_callable_whitelisted_call.converted_method,self,88,89,1.0,"74,75",,test_is_whitelisted_for_graph_callable_whitelisted_call.__call__,self,74,75,,,,,,,,,,,,,,,7.0,llan-ml,2019-06-17T21:49:37Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29393>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29393>No</denchmark-link>
 
 		",_get_unbound_function,m,216,223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,230,228,getdefiningclass,"m,owner_class",226,238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29406,Rwothoromo,2019-06-04T17:00:20Z,2019-06-14T06:48:52Z,[TF 2.0 API Docs] tf.data.experimental.make_saveable_from_iterator,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_saveable_from_iterator>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/experimental/make_saveable_from_iterator</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 The returns section is missing.
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Raises are neither listed nor defined.
 	",1.0,Rwothoromo,2019-06-14T00:23:07Z,"
 		Issue can be closed
 		",2.0,Rwothoromo,2019-06-14T06:48:51Z,"
 		Closing the issue since its been merged. Thanks!
 		",,,,,82ccb44214ae7d9019826f658dc27a87e37c89f3,Kabir Kwatra,2019-06-05 12:42:25-07:00,MODIFY,1,tensorflow\python\data\experimental\ops\iterator_ops.py,tensorflow\python\data\experimental\ops\iterator_ops.py,1.0,"36,37,38,39,40,41",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,make_saveable_from_iterator,iterator,31,72,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29439,danieltudosiu,2019-06-05T12:45:29Z,2019-11-12T00:19:09Z,Unittest and test_session interaction,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.12.1 / 1.13.1 / 1.14.0rc0
 Python version: 3.5 / 3.6 / 3.7
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: 10.0.130
 GPU model and memory: 7.5.0
 
 Environment capture available at: <denchmark-link:https://pastebin.com/N26BUeSy>https://pastebin.com/N26BUeSy</denchmark-link>
 
 Describe the current behavior
 Additional ""ghost"" tests are being run but skipped when using unittest with Tensorflow TestCase class. This behavior is present in 1.12.1. When upgrading to 1.13.1 or 1.14.0rc0, the tests are being skipped entirely as the ""ghost"" test is in regards to the test_session method that you have within the tensorflow.python.framework.testutils and unittest believes that the tests are not actually tests.
 Describe the expected behavior
 No ""ghost"" tests should be run at all in 1.12.1 and the tests work in 1.13.1 and 1.14.0rc0.
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 import numpy as np
 import unittest
 
 print(tf.__version__)
 
 def get_entry_np(t, indices_d1, indices_d2, batch_size):
     result = np.zeros(batch_size)
     for i in range(batch_size):
         result[i] = t[i, indices_d1[i], indices_d2[i]]
     return result
     
 
 def get_entry_tf(t, indices_d1, indices_d2, batch_size):
     indices = tf.stack([tf.range(batch_size), indices_d1, indices_d2], axis=1)
     return tf.gather_nd(t, indices)
 
 ## Start of region of interest
 # Please enable and disable this region with Tensorflow 1.12.1 and then with 1.13.1 or 1.14.0rc0 and the behaviour will be seen   
 try:
     delattr(tf.test.TestCase,'test_session')
 except AttributeError:
     pass
 
 class OwnTestCase(tf.test.TestCase):
     pass
 ## End of region of interest
 class TestCaseTest(tf.test.TestCase):
         
     def test_get_entry(self):
         success = True
         for _ in range(10):
             # sample input
             batch_size, d1, d2 = map(int, np.random.randint(low=2, high=100, size=3))
             test_input = np.random.random([batch_size, d1, d2])
             test_indices_d1 = np.random.randint(low=0, high=d1-1, size=[batch_size])
             test_indices_d2 = np.random.randint(low=0, high=d2-1, size=[batch_size])
             # evaluate the numpy version
             test_result = get_entry_np(test_input, test_indices_d1, test_indices_d2, batch_size)
             # evaluate the tensorflow version
             with self.cached_session() as sess:
                 tf_input = tf.constant(test_input, dtype=tf.float32)
                 tf_indices_d1 = tf.constant(test_indices_d1, dtype=tf.int32)
                 tf_indices_d2 = tf.constant(test_indices_d2, dtype=tf.int32)
                 tf_result = get_entry_tf(tf_input, tf_indices_d1, tf_indices_d2, batch_size)
                 tf_result = sess.run(tf_result)
                 # check that outputs are similar
                 success = success and np.allclose(test_result, tf_result)
     
         self.assertEqual(success, True)
 </denchmark-code>
 
 	",1.0,danieltudosiu,2019-06-05T13:36:47Z,"
 		To expand on this, unittest is search for method names and finds tf.test.TestCast.test_session. This is treated as a test which is erroneous and so gets logged as ""Not a test."". However it also causes actual tests to be ignored as well in this manner. Deleting the method entirely allows tests to run as the above code shows.
 		",2.0,danieltudosiu,2019-06-11T17:35:46Z,"
 		Why would it cause actual tests to be ignored? I'm confused.
 		",3.0,danieltudosiu,2019-06-11T17:53:08Z,"
 		What appears to be happening is the unittest framework is being used to search for methods beginning with ""test"". The test_session method is found to be one of these erroneously and causes tests to be skipped. We haven't been able to figure out why this wasn't a problem in the past. Our tests in TF 1.12 work correctly but having upgraded to 1.13 (and then tried 1.14 rc0) they acquire this skipping behaviour.
 		",1e30076f57bf30838b5cb2e59f05e13deb944d1b,Gunhan Gulsoy,2019-11-11 16:18:19-08:00,MODIFY,0,tensorflow\python\framework\test_util.py,tensorflow\python\framework\test_util.py,0.0,"2170,2171,2172,2173,2174",2170,,,,,4.0,danieltudosiu,2019-06-11T18:11:15Z,"
 		<denchmark-link:https://github.com/gunan>@gunan</denchmark-link>
  do you know who maintains TestCase?
 		",5.0,danieltudosiu,2019-06-11T19:52:20Z,"
 		I have been aware of the issue around ""test_session"", but afaik it has been that way for a very very long time. Here is the line that skips ""test_session""
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1747>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L1747</denchmark-link>
 
 A little investigation showed that we moved around the ""skip"" logic recently with this commit:
 <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f9f50b6cf831cdfef15d952152f43ba6542a14ad>f9f50b6</denchmark-link>
 
 And I think that may be why you are seeing your tests that have the name "".test_session"" skipped.
 Pinging <denchmark-link:https://github.com/eddie-zhou>@eddie-zhou</denchmark-link>
  <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  for the change.
 For the resolution, I think this is just the fallout from a small bad design decision we had within TensorFlowTestCase. We should document this behaviour to avoid the pain you went through. But my recommendation for the resolution would be to rename the tests, but have this behaviour documented. Maybe instead of ""Not a test"" the message can say ""due to test_session method in tensorflowTestCase, all tests with this name are skipped. Please rename your tests""?
 		",6.0,danieltudosiu,2019-06-11T20:14:30Z,"
 		I believe <denchmark-link:https://github.com/gunan>@gunan</denchmark-link>
  is correct, and his suggestion seems reasonable.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,danieltudosiu,2019-06-11T21:15:07Z,"
 		We can try something like that. We're changing tests around a bit anyway so we can do that. The issue we noticed was that tests were getting skipped that didn't even use test_session nor have that in their names at all. We found it hard to correlate behaviour with what methods we were calling. Our solution of deleting test_session from the class works but of course is horrid. Thanks for the help.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,danieltudosiu,2019-06-11T23:09:07Z,"
 		I agree with you that it is not a great solution.
 Thankfully, test_session is deprecated and is planned to be removed. At which point we can remove it completely from the TestCase class and remove the hack. Sorry for the inconvenience!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,danieltudosiu,2019-11-12T00:19:10Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29439>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29439>No</denchmark-link>
 
 		",10.0,danieltudosiu,2020-03-06T23:22:36Z,"
 		For someone who might find this in the future by Googling:
 If you are using pytest, you could write a conftest.py (in the project root, etc.) file including the following hook, to have test_sessions not collected rather than skipped:
 def pytest_collection_modifyitems(session, config, items):
   """"""Do not collect TensorFlowTestCase.test_sesion as a test case.""""""
   items[:] = [item for item in items if not (
     item.location[0].endswith('test_util.py') and item.name == 'test_session')]
 Reference: <denchmark-link:https://docs.pytest.org/en/latest/reference.html>https://docs.pytest.org/en/latest/reference.html</denchmark-link>
 
 		",11.0,danieltudosiu,2020-03-11T06:48:45Z,"
 		Unfortunately, TF is not using pytest, our testing libraries are derived from <denchmark-link:https://docs.python.org/2/library/unittest.html>python unittest</denchmark-link>
 . So I do not think the above solution will work in this case.
 		",12.0,danieltudosiu,2020-06-19T00:40:34Z,"
 		
 What appears to be happening is the unittest framework is being used to search for methods beginning with ""test"". The test_session method is found to be one of these erroneously and causes tests to be skipped. We haven't been able to figure out why this wasn't a problem in the past. Our tests in TF 1.12 work correctly but having upgraded to 1.13 (and then tried 1.14 rc0) they acquire this skipping behaviour.
 
 Yes this appears to have happened
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29501,winston-zillow,2019-06-06T16:50:40Z,2019-06-18T19:20:34Z,tensorflow debugger `run -t` fails on keras,"
 See the description at <denchmark-link:https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model>https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model</denchmark-link>
 
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.13.1
 Python version: 3.7
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 Describe the current behavior
 Exception thrown
 Describe the expected behavior
 Run the number of iteration as specified in the run -t command
 Code to reproduce the issue
 See  <denchmark-link:https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model>https://stackoverflow.com/questions/56452641/tensorflow-debugger-run-t-failed-running-keras-model</denchmark-link>
 
 Other info / logs
 	",1.0,winston-zillow,2019-06-14T11:08:58Z,"
 		I am able to reproduce the reported issue with tensorflow 1.13.1 version. Thanks!
 		",2.0,winston-zillow,2019-06-17T19:56:51Z,"
 		<denchmark-link:https://github.com/winston-zillow>@winston-zillow</denchmark-link>
   Thanks for the bug report. We'll push a commit to fix this at HEAD soon.
 		",3.0,winston-zillow,2019-06-18T19:20:36Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29501>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29501>No</denchmark-link>
 
 		",b2bdbfb9260fe58d9c5bfe9df11fc51535e5fef3,Shanqing Cai,2019-06-18 12:17:09-07:00,MODIFY,0,tensorflow\python\debug\BUILD,tensorflow\python\debug\BUILD,0.0,1126,,MODIFY,0.0,tensorflow\python\debug\examples\examples_test.sh,tensorflow\python\debug\examples\examples_test.sh,,,,,,,,,,,,,0.0,"106,107,108,109,110",,,,,,MODIFY,2.0,tensorflow\python\debug\wrappers\framework.py,tensorflow\python\debug\wrappers\framework.py,1.0,"519,520,521,522,523,524,525,526,527","519,520,521,522,523,524,525,526,527",MODIFY,2.0,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,tensorflow\python\debug\wrappers\local_cli_wrapper_test.py,1.0,"846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_with_debugging,"self,run_start_resp,fetches,feed_dict,options,run_metadata,callable_runner,callable_runner_args,callable_options",519,527,testDebuggingKerasFitWithProfilingWorks,self,846,861,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"594,595,596,597,598,599,600,601,602",,_run_with_profiling,"self,run_start_resp,fetches,feed_dict,options,run_metadata,callable_runner,callable_runner_args,callable_options",594,602,,,,,,,,,,,,,,,1.0,"829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844",,testDebuggingKerasFitWithSkippedRunsWorks,self,829,844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29509,lwu025,2019-06-06T18:47:01Z,2020-01-30T22:50:07Z,How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):NO
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
 TensorFlow installed from (source or binary):source
 TensorFlow version (use command below):1.13.1
 Python version: 2.7
 Bazel version (if compiling from source): 0.22.0
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 
 I'm trying to train my own deeplab model using this <denchmark-link:https://github.com/tensorflow/models/tree/master/research/deeplab>code</denchmark-link>
  and convert it to tflite.
 My target is to get a model similar to <denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite>this</denchmark-link>
 
 However, the model is obtained contains operations like:
 <denchmark-link:https://user-images.githubusercontent.com/43549654/59057361-135a7500-884f-11e9-9546-e2bd20e69c95.png></denchmark-link>
 
 SpaceToBatchND and BatchToSpaceND operations are not supported by tflite + opengles backend, they reduced the model's performance on my device.
 In your hosted deeplab model, those three ops are replaced by DEPTHWISE_CONV_2D v2, which has options to set dilation factor. This would be the best solution for me but I'm not sure how to convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2).
 FYI, I have tried the graph_transforms tool under tensorflow/tools/graph_transforms to flatten the atrous conv. It upsampled the kernels instead of Space_To_Batch + Batch_To_Space. But this transform leads to much more computations that I cannot afford.
 Describe the expected behavior
 convert SpaceToBatchND-Conv2D-BatchToSpaceND into a singe DEPTHWISE_CONV_2D v2(dilation=2)
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 You can try any model under deeplab model zoo for example <denchmark-link:url>http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz</denchmark-link>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,lwu025,2019-09-09T13:21:37Z,"
 		Hey, I am currently facing the same problem. One thing that I noticed is when working with quantized model this conversion is being done. Problem is then I end up with a uint8 model.
 		",2.0,lwu025,2019-10-10T18:29:03Z,"
 		TOCO has a pass to do this kind of transformation:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/toco/graph_transformations/identify_dilated_conv.cc</denchmark-link>
 
 Isn't it working in your case?
 		",3.0,lwu025,2020-01-17T10:42:00Z,"
 		I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.
 		",f54bb6f5578b931d79884302768996ba1073f685,Haoliang Zhang,2020-01-28 15:32:46-08:00,MODIFY,0,tensorflow\compiler\mlir\lite\BUILD,tensorflow\compiler\mlir\lite\BUILD,0.0,"270,286",,ADD,0.0,None,tensorflow\compiler\mlir\lite\tests\dilated-conv.mlir,4.0,lwu025,2020-01-17T17:34:42Z,"
 		
 I'm seeing the same issue in TF 2.0.0 and TF2.1.0. This makes e.g. Deeplab V3 effectively not runnable on GPU as it relies extensively on atrous convolutions.
 
 Are you using the old converter or the new MLIR-based converter?
 		",5.0,lwu025,2020-01-18T02:59:01Z,"
 		Seems to happen regardless of which converter is chosen. I can ask my client to provide an untrained SavedModel as a repro, if that helps.
 		",6.0,lwu025,2020-01-18T03:11:34Z,"
 		I'm pretty sure it would repro with segmentation specific variant of MobileNet V3, i.e. this one: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py</denchmark-link>
 , using ""large_segmentation"" config: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_configs.py#L69</denchmark-link>
 .
 		",,,,,,,,ADD,0.0,None,tensorflow\compiler\mlir\lite\transforms\dilated_conv.cc,,,,ADD,0.0,None,tensorflow\compiler\mlir\lite\transforms\dilated_conv.h,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,lwu025,2020-01-20T02:33:58Z,"
 		This is where it bails in my case (in identify_dilated_conv.cc):
 <denchmark-code>120   Operator* bias_add_op = !has_bias_before_bts ? final_op : next_op;
 121   if (bias_add_op->type != OperatorType::kAdd) {
 122     // Bias op is required before or after BatchToSpace
 123     return false;
 124   }
 </denchmark-code>
 
 		",,,,,,,,,MODIFY,1.0,tensorflow\compiler\mlir\lite\transforms\prepare_tf.cc,tensorflow\compiler\mlir\lite\transforms\prepare_tf.cc,1.0,"507,508,509,510,511,512",,mlir::TFL::PrepareTFPass::runOnFunction,,497,532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,lwu025,2020-01-20T03:21:09Z,"
 		Upon export, a  MobileNet V3 block turns into this:
 <denchmark-link:https://user-images.githubusercontent.com/46361887/72696297-7eac3b00-3af0-11ea-8cf6-d45bfc7e40be.png></denchmark-link>
 
 In its original form, the block is defined as follows: <denchmark-link:https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155>https://github.com/1e100/mobilenet_v3/blob/master/mobilenet_v3_keras.py#L155</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,lwu025,2020-01-20T03:25:24Z,"
 		It expects that there will be bias add either before or after BatchToSpaceNd, but it looks like the bias is folded into DepthwiseConv2D. What that Mul is doing there, I don't know.
 		",10.0,lwu025,2020-01-20T04:06:18Z,"
 		Minimal repro:
 #!/usr/bin/env python3
 
 import pathlib
 
 import tensorflow as tf
 from tensorflow import keras
 
 input = keras.Input([128, 128, 3])
 x = keras.layers.Conv2D(8, 5, dilation_rate=2, padding=""same"", use_bias=False)(input)
 x = keras.layers.BatchNormalization()(x)
 output = keras.layers.ReLU()(x)
 
 m = keras.Model(inputs=input, outputs=output)
 out_dir = pathlib.Path(""/tmp/minimal_bug_repro"")
 m.save(str(out_dir), save_format=""tf"")
 
 converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))
 tflite_model = converter.convert()
 output_file = out_dir / ""model.tflite""
 output_file.write_bytes(tflite_model)
 
 print(f""Converted model was written to {output_file}"")
 You get the following on the other end:
 <denchmark-link:https://user-images.githubusercontent.com/46361887/72698007-1b71d700-3af7-11ea-9176-bf05f210c17d.png></denchmark-link>
 
 Which is not runnable on the GPU.
 		",11.0,lwu025,2020-01-21T19:36:35Z,"
 		Thanks <denchmark-link:https://github.com/depthwise>@depthwise</denchmark-link>
  for the repro example.
 I'm working on adding support of dilated conv into the MLIR-based converter. I will update this thread when I'm finished.
 		",12.0,lwu025,2020-01-28T23:42:22Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>No</denchmark-link>
 
 		",13.0,lwu025,2020-01-29T11:56:28Z,"
 		FYI, <denchmark-link:https://github.com/haozha111>@haozha111</denchmark-link>
 , this PR improved the issue, but did not fully fix it. Here's an updated repro which demonstrates the remaining issue:
 <denchmark-code>#!/usr/bin/env python3
 
 import pathlib
 
 import tensorflow as tf
 from tensorflow import keras
 
 input = keras.Input([128, 128, 3])
 x1 = keras.layers.Conv2D(8, 5, dilation_rate=6, padding=""same"", use_bias=False)(input)
 x1 = keras.layers.BatchNormalization()(x1)
 output1 = keras.layers.ReLU()(x1)
 
 x2 = keras.layers.Conv2D(8, 5, dilation_rate=12, padding=""same"", use_bias=False)(input)
 x2 = keras.layers.BatchNormalization()(x2)
 output1 = keras.layers.ReLU()(x2)
 
 output = tf.concat([x1, x2], axis=3)
 
 m = keras.Model(inputs=input, outputs=output)
 out_dir = pathlib.Path(""/tmp/minimal_bug_repro"")
 m.save(str(out_dir), save_format=""tf"")
 
 converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir))
 tflite_model = converter.convert()
 output_file = out_dir / ""model.tflite""
 output_file.write_bytes(tflite_model)
 
 print(f""Converted model was written to {output_file}"")
 </denchmark-code>
 
 This produces the following:
 <denchmark-link:https://user-images.githubusercontent.com/46361887/73354717-3bb63a00-424b-11ea-9767-5762e6863f05.png></denchmark-link>
 
 Each of the convs in isolation is fixed, but if I concatenate them we're back to the status quo ante.
 		",14.0,lwu025,2020-01-29T11:57:07Z,"
 		Such dilations are commonly used in the ASPP module of segmentation models.
 		",15.0,lwu025,2020-01-29T22:53:45Z,"
 		That's a bit interesting. I haven't tested for this case.
 So do you mean if you have only one conv2d in your graph, then it can be correctly folded?
 		",16.0,lwu025,2020-01-29T22:58:27Z,"
 		Looks like conv2d's by themselves work fine. The simpler repro I posted before looks ""correct"", although I have not tested this in a full blown, trained model yet.
 		",17.0,lwu025,2020-01-30T00:22:24Z,"
 		I tested with your new code, and convert it. Then I'm getting the tflite graph looks like the following:
 <denchmark-link:https://user-images.githubusercontent.com/6316921/73408979-bbc3ba80-42b2-11ea-91ab-46f8a05219f5.png></denchmark-link>
 
 I also have the corresponding tflite file, but not sure how to attach it here.
 This suggests that the graph is converted as expected. I'm wondering if my previous change has been pushed into the nightly already. Maybe you can download tomorrow's nightly and give a try.
 		",18.0,lwu025,2020-01-30T00:25:48Z,"
 		OK, I'll try again. My example was converted using a build directly from master as of last night. Maybe something else got submitted in the interim.
 		",19.0,lwu025,2020-01-30T00:30:43Z,"
 		did you set converter.experimental_new_converter = True between those two lines?
 converter = tf.lite.TFLiteConverter.from_saved_model(str(out_dir)) tflite_model = converter.convert()
 		",20.0,lwu025,2020-01-30T04:40:35Z,"
 		I'm actually converting as follows, using the master branch as of this afternoon:
 bazel run --copt=""-Wno-unused-result"" :tflite_convert -- \
     --saved_model_dir=/tmp/minimal_bug_repro \
     --experimental_new_converter=True --output_file=/tmp/minimal_bug_repro/model.tflite
 The original savedmodel is created with TF 2.1.0 release version, the conversion is done with master tflite_convert.
 Here's the original model and its conversion, in case it's useful in debugging: <denchmark-link:https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip>https://storage.googleapis.com/depthwise-temp/minimal_bug_repro.zip</denchmark-link>
 
 		",21.0,lwu025,2020-01-30T04:41:31Z,"
 		For me the graph looks like the image I posted previously, incorrect.
 		",22.0,lwu025,2020-01-30T19:46:44Z,"
 		I could reproduce the issue with your saved model, but the graph I posted before is from the keras testing code. I'm not sure why there is a difference here.
 Digging it further, I found that there is a difference in the MLIR representation of the two models. For saved model, the IR looks like:
 ...
 %10 = ""tf.Identity""(%arg0) {T = f32, device = """"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>
 %11 = ""tf.Identity""(%10) {T = f32, device = """"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>
 %12 = ""tf.SpaceToBatchND""(%11, %0, %1) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = """"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>
 %13 = ""tf.SpaceToBatchND""(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = """"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>
 %14 = ""tf.Conv2D""(%12, %8) {T = f32, data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>
 %15 = ""tf.BatchToSpaceND""(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = """"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = ""tf.FusedBatchNormV3""(%15, %6, %7, %7, %6) {T = f32, U = f32, data_format = ""NHWC"", device = """", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)
 %16 = ""tf.Conv2D""(%13, %9) {T = f32, data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>
 %17 = ""tf.BatchToSpaceND""(%16, %3, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = """"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 %18 = ""tf.Mul""(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %19 = ""tf.Add""(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %20 = ""tf.ConcatV2""(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = """"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>
 return %20 : tensor<1x128x128x16xf32>
 }<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 %18 = ""tf.Mul""(%17, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %19 = ""tf.Add""(%18, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %20 = ""tf.ConcatV2""(%y, %19, %5) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = """"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>
 return %20 : tensor<1x128x128x16xf32>
 }
 For the keras test model, the IR is:
 %7 = ""tf.Const""() {value = dense<12> : tensor<2xi32>} : () -> tensor<2xi32>
 %8 = ""tf.Const""() {value = dense<[[24, 28], [24, 28]]> : tensor<2x2xi32>} : () -> tensor<2x2xi32>
 %9 = ""tf.Const""() {value = dense<3> : tensor} : () -> tensor
 %10 = ""tf.Identity""(%arg0) {T = f32, device = """"} : (tensor<1x128x128x3xf32>) -> tensor<1x128x128x3xf32>
 %11 = ""tf.SpaceToBatchND""(%10, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = """"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<36x26x26x3xf32>
 %12 = ""tf.Conv2D""(%11, %2) {T = f32, data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>
 %13 = ""tf.BatchToSpaceND""(%12, %3, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = """"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 %y, %batch_mean, %batch_variance, %reserve_space_1, %reserve_space_2, %reserve_space_3 = ""tf.FusedBatchNormV3""(%13, %0, %1, %1, %0) {T = f32, U = f32, data_format = ""NHWC"", device = """", epsilon = 1.000000e-03 : f32, is_training = false} : (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>) -> (tensor<1x128x128x8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<8xf32>, tensor<*xf32>)
 %14 = ""tf.SpaceToBatchND""(%10, %7, %8) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = """"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>
 %15 = ""tf.Conv2D""(%14, %6) {T = f32, data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<144x15x15x3xf32>, tensor<5x5x3x8xf32>) -> tensor<144x11x11x8xf32>
 %16 = ""tf.BatchToSpaceND""(%15, %7, %5) {T = f32, Tblock_shape = i32, Tcrops = i32, device = """"} : (tensor<144x11x11x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 %17 = ""tf.Mul""(%16, %cst) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %18 = ""tf.Add""(%17, %cst_0) : (tensor<1x128x128x8xf32>, tensor<8xf32>) -> tensor<1x128x128x8xf32>
 %19 = ""tf.ConcatV2""(%y, %18, %9) {N = 2 : i64, T = f32, Tidx = i32, _cloned = true, device = """"} : (tensor<1x128x128x8xf32>, tensor<1x128x128x8xf32>, tensor) -> tensor<1x128x128x16xf32>
 return %19 : tensor<1x128x128x16xf32>
 I think the issue is that the IR for saved model is a bit strange, notice those lines:
 %13 = ""tf.SpaceToBatchND""(%11, %3, %4) {T = f32, Tblock_shape = i32, Tpaddings = i32, device = """"} : (tensor<1x128x128x3xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<144x15x15x3xf32>
 %14 = ""tf.Conv2D""(%12, %8) {T = f32, data_format = ""NHWC"", device = """", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = ""VALID"", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true} : (tensor<36x26x26x3xf32>, tensor<5x5x3x8xf32>) -> tensor<36x22x22x8xf32>
 %15 = ""tf.BatchToSpaceND""(%14, %0, %2) {T = f32, Tblock_shape = i32, Tcrops = i32, device = """"} : (tensor<36x22x22x8xf32>, tensor<2xi32>, tensor<2x2xi32>) -> tensor<1x128x128x8xf32>
 The block_shape parameter before/after the Conv2D op is different, which causes the pattern to not match successfully. Will investigate why the IR is different.
 		",23.0,lwu025,2020-01-30T20:01:20Z,"
 		Hi, Nupur, do you know what might be the cause for the difference between calling the python API and the command line tool?
 		",24.0,lwu025,2020-01-30T20:34:06Z,"
 		I also tried to convert the model via command line 'tflite_convert' directly (no bazel) in tf-nightly, and the model could be converted correctly. I think the issue is probably with bazel, it might be calling the TF 1 saved model loader which doesn't produce the expected result.
 Can you please try using tf-nightly and then run 'tflite_conveter' tool directly (please avoid using bazel)?
 Thanks.
 		",25.0,lwu025,2020-01-30T20:36:48Z,"
 		Just to be clear, once again, the SavedModel I posted was saved using TF 2.1.0, not the new ""master"" build. It's only the converter that was built from master. Let me try to replicate this with a wheel of TF build off master to see if the model is being saved incorrectly in the first place. Build times being what they are, this will take a bit of time.
 		",26.0,lwu025,2020-01-30T20:44:35Z,"
 		I think the saved model is generated correctly, it doesn't matter if you generate it from TF 2.1.0 or the master build.
 The issue is with the converter. When you build from master, and then run bazel, even if it's pulling the latest code, I think there is some mis-configuration in bazel that causes the model loading to incorrectly use the old TF 1.x codepath. So what I'm suggesting maybe the easiest approach is to download the tf-nightly, and then call 'tflite_convert' tool directly (in a virtualenv).
 		",27.0,lwu025,2020-01-30T22:29:04Z,"
 		Nightly as of last night LGTM, in both my full deeplab model and the minimal example. Thanks for the quick fix, much appreciated. This was blocking things in unpleasant ways.
 		",28.0,lwu025,2020-01-30T22:50:07Z,"
 		Great to hear that!
 		",29.0,lwu025,2020-01-30T22:50:09Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/29509>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29656,Sangwon91,2019-06-11T17:45:05Z,2019-06-13T21:06:11Z,Bug on `gather_nd` with gradient.,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): pip
 TensorFlow version (use command below): tf2-gpu-beta
 Python version: 3.6.8
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 A simple test code
 v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)
 
 with tf.GradientTape() as tape:
     l = tf.gather_nd(v, [[1, 1]])
     l = tf.reduce_sum(l)
     
 grads = tape.gradient(l, v)
 print(grads)
 gives following error message
 <denchmark-code>---------------------------------------------------------------------------
 LookupError                               Traceback (most recent call last)
 <ipython-input-12-28efd3aa3042> in <module>
       5     l = tf.reduce_sum(l)
       6 
 ----> 7 grads = tape.gradient(l, v)
       8 print(grads)
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
    1000         output_gradients=output_gradients,
    1001         sources_raw=flat_sources_raw,
 -> 1002         unconnected_gradients=unconnected_gradients)
    1003 
    1004     if not self._persistent:
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
      74       output_gradients,
      75       sources_raw,
 ---> 76       compat.as_str(unconnected_gradients.value))
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)
     131   """"""
     132   mock_op = _MockOp(attr_tuple, inputs, outputs, op_name, skip_input_indices)
 --> 133   grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
     134   if grad_fn is None:
     135     return [None] * num_inputs
 
 ~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/registry.py in lookup(self, name)
      95     else:
      96       raise LookupError(
 ---> 97           ""%s registry has no entry for: %s"" % (self._name, name))
 
 LookupError: gradient registry has no entry for: ResourceGatherNd
 </denchmark-code>
 
 Describe the expected behavior
 the grads should be [[0, 0], [0, 1]] but error occurs.
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,Sangwon91,2019-06-11T17:50:07Z,"
 		I just found very weird behavior.
 If I replace v to v+0 like below,
 v = tf.Variable(np.random.uniform(size=[2,2]), dtype=tf.float32)
 
 with tf.GradientTape() as tape:
     l = tf.gather_nd(v+0, [[1, 1]])
     l = tf.reduce_sum(l)
     
 grads = tape.gradient(l, v)
 print(grads)
 it gives expected result...
 <denchmark-code>tf.Tensor(
 [[0. 0.]
  [0. 1.]], shape=(2, 2), dtype=float32)
 </denchmark-code>
 
 It seems the direct access to variables with gather_nd ruines something unexpected...
 		",2.0,Sangwon91,2019-06-12T08:35:40Z,"
 		<denchmark-link:https://github.com/Sangwon91>@Sangwon91</denchmark-link>
  I could able to reproduce the reported issue with Tensorflow 2.0.0.beta0. Thanks!
 		",3.0,Sangwon91,2019-06-13T21:06:12Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29656>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29656>No</denchmark-link>
 
 		",a7ef0da19be94d5f189c8a3af960f1da77e58b41,Alexandre Passos,2019-06-13 14:03:29-07:00,MODIFY,0,tensorflow\python\kernel_tests\BUILD,tensorflow\python\kernel_tests\BUILD,0.0,837,837,MODIFY,2.0,tensorflow\python\kernel_tests\resource_variable_ops_test.py,tensorflow\python\kernel_tests\resource_variable_ops_test.py,,,,,,,,,,,,,1.0,"280,281,282,283,284,285,286,287,288,289,290",,testGradientGatherNdIndexedSlices,self,280,290,MODIFY,1.0,tensorflow\python\ops\array_grad.py,tensorflow\python\ops\array_grad.py,1.0,"567,568,569,570,571,572,573,574,575,576",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"267,268,269,270,271,272,273,274,275,276,277",,testGradientGatherNd,self,267,277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_ResourceGatherNdGrad,"op,grad",567,576,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29856,mindlapse,2019-06-17T01:49:41Z,2019-06-27T02:58:15Z,tf.keras.layers.UpSampling2D(interpolation='bilinear') has a smearing defect on the right & bottom edges,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): I've provided a link to a Colab notebook demonstrating the issue below, comparing keras upsampling to what it should look like with a correct implementation as seen in tf.image.resize.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.0.0-beta0-16-g1d91213fe7
 Python version: 3
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Upsampling using tf.keras.layers.UpSampling2D() results in unnatural smearing of the right and bottom edges of the image.  This problem is amplified when the upsampling is repeated.
 Describe the expected behavior
 Keras layers should use sensible default behaviour and not have this smearing issue.  This causes serious problems for autoencoders, GANs, and cost months of time.  Correct behaviour is seen with tf.image.resize(o, size=size, method=tf.image.ResizeMethod.BILINEAR).  Keras upsampling should use this as the default instead of the current defective behaviour.  Note: In TensorFlow 1.x, the tf.image.resize method had an 'align_corners' parameter that toggled between defective and proper behaviour and was set to False (defective behaviour) by default.  In TensorFlow 2, this parameter has been removed and the correct behaviour (align_corners=True behaviour) is now the default.  The keras layer should follow the same path.
 
 Here is a Colab notebook that demonstrates the issue:
 <denchmark-link:https://colab.research.google.com/drive/1rgCzJcMo4DN_9_hutr9l2vSrTRPfcd6K>https://colab.research.google.com/drive/1rgCzJcMo4DN_9_hutr9l2vSrTRPfcd6K</denchmark-link>
 
 Other info / logs
 	",1.0,mindlapse,2019-06-17T07:42:50Z,"
 		Actually, I think it's even worse than that, it seems bilinear doesn't work
 <denchmark-link:https://colab.research.google.com/drive/1BG1gRC86Hj9CqyLTD9quyW0vtNJWZK3j>https://colab.research.google.com/drive/1BG1gRC86Hj9CqyLTD9quyW0vtNJWZK3j</denchmark-link>
 
 		",2.0,mindlapse,2019-06-18T09:05:10Z,"
 		Have tried with code snippet provided and was able to reproduce the issue on Colab with TensorFlow version 2.0beta.
 		",3.0,mindlapse,2019-06-27T02:58:16Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29856>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29856>No</denchmark-link>
 
 		",15f6c30d7977c92ba452eb5c1873b8c9f0968a5f,Martin Wicke,2019-06-26 19:53:26-07:00,MODIFY,0,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,0.0,"2613,2614,2616,2617","2613,2615",MODIFY,0.0,tensorflow\python\ops\image_ops_impl.py,tensorflow\python\ops\image_ops_impl.py,,,,,,,,,,,,,0.0,1212,1212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29881,durandg12,2019-06-17T17:27:01Z,2019-06-23T01:30:39Z,The call method of DenseFeatures and SequenceFeatures use deprecated attribute _num_buckets,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
 TensorFlow installed from (source or binary): from pip install
 TensorFlow version (use command below): v1.12.1-3259-gf59745a381 2.0.0-beta0
 Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14
 
 Describe the current behavior
 By simply calling the call method of DenseFeatures and SequenceFeatures defined with categorical_column_with_identity and sequence_categorical_column_with_identity feature columns (along with embedding_column), we get warnings that the deprecated attributes _num_buckets are used (instead of the non-deprecated num_buckets I guess).
 Also note, on the code example below, that a third warning about a deprecated method, add_dispatch_support.<locals>.wrapper arises. I do not understand it but there are instructions for updating given in the warning, see the code below.
 Describe the expected behavior
 I think that we should not get warnings about deprecated objects when we are not calling any deprecated method, attribute, etc. I think that somewhere in the code of the call method of DenseFeatures and SequenceFeatures there is a use of _num_buckets that should be replaced by num_buckets. Following the updating instructions about the third warning may be enough to get rid of it.
 Code to reproduce the issue
 <denchmark-code>import numpy as np
 import tensorflow as tf
 from tensorflow.feature_column import categorical_column_with_identity, embedding_column, \
                                             sequence_categorical_column_with_identity
 from tensorflow.keras.layers import DenseFeatures
 from tensorflow.keras.experimental import SequenceFeatures
 
 #print(tf.version.GIT_VERSION, tf.version.VERSION)
 
 nb_features = 10
 emb_dim = 3
 
 fc = categorical_column_with_identity('feature1', nb_features)
 emb_fc = embedding_column(fc, emb_dim)
 layer = DenseFeatures(emb_fc)
 
 seq_fc = sequence_categorical_column_with_identity('feature2', nb_features)
 emb_seq_fc = embedding_column(seq_fc, emb_dim)
 seq_layer = SequenceFeatures(emb_seq_fc)
 
 data1 = np.array(range(nb_features))
 batch_size, sequence_length = 2, 5
 raw_data2 = np.array(range(nb_features))
 data2 = np.reshape(raw_data2, (batch_size, sequence_length))
 
 dict_data = {'feature1': data1, 'feature2': data2}
 print(layer(dict_data))
 print(seq_layer(dict_data))
 </denchmark-code>
 
 produces the following three warnings:
 <denchmark-code>WARNING: Logging before flag parsing goes to stderr.
 W0617 19:19:42.823884 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
 Instructions for updating:
 The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
 W0617 19:19:42.827694 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:2655: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.where in 2.0, which has the same broadcast rule as np.where
 W0617 19:19:42.833320 140735678825344 deprecation.py:323] From /Users/myusername/Documents/tf2beta/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column_v2.py:3040: SequenceCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
 Instructions for updating:
 The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
 </denchmark-code>
 
 	",1.0,durandg12,2019-06-18T10:08:52Z,"
 		I could see the warning message on colab with Tf 2.0.0.beta0. Thanks!
 		",2.0,durandg12,2019-06-19T08:17:10Z,"
 		Similar problem here from beta1:
 
 tensorflow\python\ops\math_grad.py:1250: add_dispatch_support..wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.where in 2.0, which has the same broadcast rule as np.where
 
 		",3.0,durandg12,2019-06-19T12:24:35Z,"
 		I've got exactly the same issue ^^ I hope someone fixes it
 		",d7e858192d1de827bc03705ac62e1bd38daf06d8,Martin Wicke,2019-06-22 18:25:57-07:00,MODIFY,3,tensorflow\python\feature_column\feature_column_v2.py,tensorflow\python\feature_column\feature_column_v2.py,1.0,3859,3857,MODIFY,1.0,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,4.0,durandg12,2019-06-22T03:04:26Z,"
 		Same here
 		",5.0,durandg12,2019-06-22T14:21:52Z,"
 		+1 - most annoying comment ever <3
 		",6.0,durandg12,2019-06-23T01:30:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29881>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29881>No</denchmark-link>
 
 		",1.0,"3632,3635,3639,3650,3652",3647,where_v2,"condition,x,y,name",3622,3664,,,,,,,,,,,,,,,_transform_input_tensor,"self,input_tensor",3836,3870,1.0,"3102,3103,3104",3102,create_state,"self,state_manager",3100,3112,1.0,2714,2714,_to_sparse_input_and_drop_ignore_values,"input_tensor,ignore_value",2679,2720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,durandg12,2019-08-27T10:43:26Z,"
 		The warnings are still here in tf2.0.0-rc0.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,durandg12,2019-09-25T12:19:21Z,"
 		Actually there is even one new warning when using tf2.0.0-rc0, I didn't see it the first time I tested. For the same code as above, now the output is:
 <denchmark-code>WARNING: Logging before flag parsing goes to stderr.
 W0925 14:12:28.993772 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3089: IdentityCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
 Instructions for updating:
 The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
 W0925 14:12:28.994673 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:353: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
 Instructions for updating:
 Please use `layer.add_weight` method instead.
 W0925 14:12:29.005573 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/ops/embedding_ops.py:802: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.where in 2.0, which has the same broadcast rule as np.where
 W0925 14:12:29.010264 140735485318016 deprecation.py:323] From /path/to/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:3089: SequenceCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.
 Instructions for updating:
 The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.
 </denchmark-code>
 
 See the new warning
 <denchmark-code>Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
 Instructions for updating:
 Please use `layer.add_weight` method instead.
 </denchmark-code>
 
 Is it possible to re-open this issue? Or should I open a new one?
 Edit: it says I unassigned <denchmark-link:https://github.com/bananabowl>@bananabowl</denchmark-link>
  but I don't know how to do that, I just posted this new comment and then edited it.
 Edit 2: mentioning <denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  as he is the author of the commit which caused this issue to be closed.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,durandg12,2019-10-01T13:58:38Z,"
 		All warnings have disappeared in tf2.0.0. This one can stay closed for good.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29989,whhu,2019-06-20T02:46:50Z,2019-07-17T03:38:32Z,Segmentation fault when saving checkpoints with saveable Dataset Iterator,"
 System information
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810
 
 
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 
 
 TensorFlow installed from (source or binary): Binary
 
 
 TensorFlow version (use command below): 1.13.1
 
 
 Python version: 3.6.8
 
 
 Bazel version (if compiling from source): None
 
 
 GCC/Compiler version (if compiling from source): None
 
 
 CUDA/cuDNN version: None
 
 
 GPU model and memory: None
 
 
 tf.version: 'v1.13.1-0-g6612da8951' 1.13.1
 
 
 Describe the current behavior
 Segmentation fault in saving an initializable dataset iterator when entering the tf.train.MonitoredSession context manager.
 Describe the expected behavior
 The initializable iterator is saved and restored properly, behaving the same with the one shot iterator.
 Code to reproduce the issue
 """"""Illustrate saveable dataset iterator
 """"""
 import tensorflow as tf
 
 DATASET_SIZE = 4
 SAVE_STEPS = 2
 TRAIN_STEP = 3
 CHECKPOINT_DIR = '/tmp/tf_dataset_saveable'
 
 def test_saveable():
     """"""test saveable""""""
     graph = tf.Graph()
     with graph.as_default():
         dataset = tf.data.Dataset.range(DATASET_SIZE).repeat()
 #        dataset_iterator = dataset.make_one_shot_iterator()
         dataset_iterator = dataset.make_initializable_iterator()
         dataset_init = dataset_iterator.initializer
         data = dataset_iterator.get_next()
 
         saveable = tf.contrib.data.make_saveable_from_iterator(dataset_iterator)
         tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)
 
         global_step = tf.train.get_or_create_global_step()
         inc_global_step = tf.assign_add(global_step, 1)  # critical
 
         saver = tf.train.Saver()
         checkpoint_dir = CHECKPOINT_DIR
         scaffold = tf.train.Scaffold(saver=saver)
         checkpoint_hook = tf.train.CheckpointSaverHook(
             checkpoint_dir=checkpoint_dir,
             save_steps=SAVE_STEPS, scaffold=scaffold)
 
         hooks = [checkpoint_hook]
         session_creator = tf.train.ChiefSessionCreator(
             scaffold=scaffold, checkpoint_dir=checkpoint_dir)
         with tf.train.MonitoredSession(
                 session_creator=session_creator, hooks=hooks) as mon_sess:
             gstep = mon_sess.run(global_step)
             if not gstep:
                 mon_sess.run(dataset_init)
             for _ in range(TRAIN_STEP):
                 print(mon_sess.run([global_step, data]))
                 mon_sess.run(inc_global_step)
 
 if __name__ == '__main__':
     test_saveable()
 Other info / logs
 (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ python tf_dataset_saveable.py 
 WARNING:tensorflow:From /home/huwh1/virtualenv/tf-1.13-py3/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Colocations handled automatically by placer.
 
 WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
 For more information, please see:
   * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
   * https://github.com/tensorflow/addons
 If you depend on functionality not listed there, please file an issue.
 
 WARNING:tensorflow:From tf_dataset_saveable.py:20: make_saveable_from_iterator (from tensorflow.contrib.data.python.ops.iterator_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use `tf.data.experimental.make_saveable_from_iterator(...)`.
 2019-06-20 10:43:20.947675: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-06-20 10:43:20.951984: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
 2019-06-20 10:43:20.952497: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x3f10ca0 executing computations on platform Host. Devices:
 2019-06-20 10:43:20.952539: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
 Segmentation fault (core dumped)
 (tf-1.13-py3) [huwh1@huwh1-centos worksync]$ 
 	",1.0,whhu,2019-06-21T09:45:11Z,"
 		I tried on colab with Tensorflow 1.13.1. I am able to reproduce the issue.
 		",2.0,whhu,2019-07-03T17:15:49Z,"
 		allenl@ -- not sure if these APIs are expected to work all together. Can you advise as to whether there's a better way?
 		",3.0,whhu,2019-07-03T17:18:33Z,"
 		Oops. Actually tagging <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
 
 		",51d1f486fcbe5bc8857586250fd5b10ca110d631,Jiri Simsa,2019-07-15 17:28:29-07:00,MODIFY,1,tensorflow\core\kernels\data\iterator_ops.cc,tensorflow\core\kernels\data\iterator_ops.cc,1.0,110,110,MODIFY,1.0,tensorflow\python\data\experimental\kernel_tests\serialization\serialization_integration_test.py,tensorflow\python\data\experimental\kernel_tests\serialization\serialization_integration_test.py,4.0,whhu,2019-07-03T17:36:23Z,"
 		Is the issue that the iterator isn't initialized when the first checkpoint is written? You may need to get MonitoredTrainingSession to do the initialization so it happens before saving. Otherwise the APIs should go fine together AFAIK.
 Either way, it probably shouldn't segfault. <denchmark-link:https://github.com/saxenasaurabh>@saxenasaurabh</denchmark-link>
  may be more familiar with the serialization op itself.
 		",5.0,whhu,2019-07-15T21:52:19Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  is correct, you need to run the  before you can save the iterator.
 I am going to create a change that will produce an informative error message for this case (as opposed to segfaulting).
 		",6.0,whhu,2019-07-16T00:30:26Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989>No</denchmark-link>
 
 		",1.0,"84,85,86,87,88,89,90,91",,testUninitializedIterator,self,84,91,,,,,,,,,,,,,,,tensorflow::data::IteratorResource::Save,"ctx,writer",104,118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,whhu,2019-07-16T07:25:51Z,"
 		Many thanks to the fruitful discussions. Adding dataset initializer to the collection TABLE_INITIALIZERS fixes the segfaulting. Nevertheless, the iterator starts from the very beginning of the dataset every time after recovering from the checkpoint.
 Is there any solution to the problem at present? Maybe something like init_fn in tf.train.Scaffold ...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,whhu,2019-07-16T17:44:35Z,"
 		That should not be the case. The whole point of saving the iterator is that you can checkpoint its state (and we have tests that verify that this functionality works).
 Please create a new issue with instructions on how to reproduce your issue so that someone can investigate why is your program resetting the iterator state.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,whhu,2019-07-16T18:10:07Z,"
 		I think this is a missing feature. We added <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/data/CheckpointInputPipelineHook>CheckpointInputPipelineHook</denchmark-link>
  to fix exactly this <denchmark-link:https://github.com/tensorflow/tensorflow/blob/5e2a91f65cfb23f996136b9201d9312c9c36b941/tensorflow/python/data/experimental/ops/iterator_ops.py#L194>problem</denchmark-link>
 . However, it requires an estimator as an arg. It doesn't necessarily have to. We just did it this way for simplicity.
 It should be fairly straightforward to extend  CheckpointInputPipelineHook to support non-estimator use-cases e.g. by explicitly passing the required args i.e. num_worker_replicas, task_type, task_id, model_dir , save_checkpoints_secs, save_checkpoints_steps. I would be happy to review if you want to send in a PR.
 As a really hacky workaround you could just build a mock Estimator object that implements the expected fields. That may be prone to breakages though.
 		",10.0,whhu,2019-07-17T03:38:32Z,"
 		Thank you very much. It helps greatly! :-)
 		",11.0,whhu,2019-07-17T03:38:33Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=29989>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=29989>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30028,JonasAmrich,2019-06-21T11:52:09Z,2019-07-09T20:24:41Z,Python package is missing ModuleSpec in tensorflow.__spec__ in tf 1.14.0,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-4.9.125-linuxkit-x86_64-with-Ubuntu-18.04-bionic
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
 TensorFlow installed from (source or binary): preinstalled in docker image
 TensorFlow version (use command below): v1.14.0-rc1-22-gaf24dc91b5 1.14.0
 Python version: 3.6.8
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 Describe the current behavior
 In TF 1.14.0 the module spec in tensorflow.__spec__ is None:
 <denchmark-code>>>> import tensorflow
 >>> print(tensorflow.__spec__)
 None
 </denchmark-code>
 
 Describe the expected behavior
 This is different from tf 1.13.1 where it works as expected:
 <denchmark-code>>>> import tensorflow
 >>> print(tensorflow.__spec__)
 ModuleSpec(name='tensorflow', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7f038cfc3cf8>, origin='/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py', submodule_search_locations=['/usr/local/lib/python3.5/dist-packages/tensorflow'])
 </denchmark-code>
 
 Missing spec causes some problems, e.g. pkgutil now fails when trying to find tensorflow. Note that the first call to find_loader is successful, it only fails after tensorflow is imported:
 <denchmark-code>Python 3.6.8 (default, Jan 14 2019, 11:02:34) 
 [GCC 8.0.1 20180414 (experimental) [trunk revision 259383]] on linux
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 >>> import pkgutil
 >>> pkgutil.find_loader('tensorflow')
 <_frozen_importlib_external.SourceFileLoader object at 0x7f62372c7160>
 >>> import tensorflow
 >>> pkgutil.find_loader('tensorflow')
 Traceback (most recent call last):
   File ""/usr/lib/python3.6/pkgutil.py"", line 490, in find_loader
     spec = importlib.util.find_spec(fullname)
   File ""/usr/lib/python3.6/importlib/util.py"", line 102, in find_spec
     raise ValueError('{}.__spec__ is None'.format(name))
 ValueError: tensorflow.__spec__ is None
 
 The above exception was the direct cause of the following exception:
 
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/usr/lib/python3.6/pkgutil.py"", line 496, in find_loader
     raise ImportError(msg.format(fullname, type(ex), ex)) from ex
 ImportError: Error while finding loader for 'tensorflow' (<class 'ValueError'>: tensorflow.__spec__ is None)
 </denchmark-code>
 
 Code to reproduce the issue
 See above
 Other info / logs
 I've tested this using official tf docker image (tensorflow/tensorflow:1.14.0-py3) and also using python docker image (python:3.6) with tensorflow installed with pip.
 	",1.0,JonasAmrich,2019-06-24T10:04:10Z,"
 		importlib throws the same error. Can't do custom imports in 1.14.
 <denchmark-code>Traceback (most recent call last):
   File ""path/test/test_model.py"", line 62, in testTFGraph
     save_tensorflow(sess, path, output=['output'])
   File ""path/model.py"", line 55, in save_tensorflow
     if not is_installed('tensorflow'):
   File ""path/_util.py"", line 18, in is_installed
     if importlib.util.find_spec(p) is None:
   File ""path/lib/python3.7/importlib/util.py"", line 114, in find_spec
     raise ValueError('{}.__spec__ is None'.format(name))
 ValueError: tensorflow.__spec__ is None
 </denchmark-code>
 
 		",2.0,JonasAmrich,2019-06-24T13:59:28Z,"
 		Another data point: this also happens in nightly
 		",3.0,JonasAmrich,2019-06-27T20:48:56Z,"
 		Sorry for the breakage! This is caused by adding a module wrapper that prints deprecation messages. I will send a change to fix it.
 		",b789a3b37b59e6795f799645a6e8b1a6c70fc346,Anna R,2019-06-28 12:23:10-07:00,MODIFY,4,tensorflow\python\util\deprecation_wrapper.py,tensorflow\python\util\deprecation_wrapper.py,1.0,"101,103,104,105,106","101,102,103,104,106,119",MODIFY,0.0,tensorflow\python\util\deprecation_wrapper_test.py,tensorflow\python\util\deprecation_wrapper_test.py,4.0,JonasAmrich,2019-07-09T20:24:41Z,"
 		Closing this issue since the associated PR has been merged. Thanks!
 		",5.0,JonasAmrich,2019-07-09T20:24:42Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30028>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30028>No</denchmark-link>
 
 		",6.0,JonasAmrich,2019-07-31T12:25:10Z,"
 		Is there a work around in the mean time?
 		",0.0,"30,31,34",32,,,,,MODIFY,0.0,tensorflow\tools\api\tests\BUILD,tensorflow\tools\api\tests\BUILD,0.0,"59,60,61,62,63,64,65,66,67,68,69",,MODIFY,0.0,tensorflow\tools\api\tests\deprecation_test.py,tensorflow\tools\api\tests\deprecation_test.py,0.0,"26,27,28",,__getattr__,"self,name",101,121,1.0,"87,88,93,94","91,92,93,94,95",__init__,"self,wrapped,module_name",86,95,1.0,"97,98,99,100,101,103,104,105,106","97,99,101,102,103,104,106,119",__getattribute__,"self,name",97,120,1.0,127,,__setattr__,"self,arg,val",122,127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,JonasAmrich,2019-08-05T14:07:58Z,"
 		We'll get the 1.14.1 patch release this week.
 		",,,,,,,,,ADD,0.0,None,tensorflow\tools\api\tests\module_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,JonasAmrich,2019-08-13T17:53:33Z,"
 		Update: Instead of a 1.14.1 patch release, we will get a 1.15 version in a few weeks. Sorry for the extra delay this causes.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30113,53RT,2019-06-25T09:44:55Z,2020-01-13T09:57:42Z,tf.image.encode_png doesn't support 16 bit and inconsistent behavior in eager mode,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): tested with 1.9.0, 1.12.0 and 1.14.0
 Python version: 3.7
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Creating a numpy array with uint16 datatype and passing it to tf.image.encode_png() yields different results in eager execution mode. The first time the array is passed it somehow gets transformed to a uint8 array and for the following encodings it works as expected.
 Using a tf.session the uint16 input is always transformed to uint8
 Describe the expected behavior
 Just return a bytestring of a 16bit PNG
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 import numpy as np
 import tensorflow as tf
 #tf.enable_eager_execution()
 
 np.random.seed(1)
 A = np.random.randint(low=0, high=65535, size=100, dtype=np.uint16).reshape(10,10,1)
 B = A.copy()
 np.allclose(A,B) # is true
 
 # Eager Execution
 a_encoded = tf.image.encode_png(A).numpy()
 b_encoded = tf.image.encode_png(B).numpy()
 
 print(len(a_encoded),len(b_encoded)) # prints 178 and 278, 278 expected both times
 assert(a_encoded == b_encoded) # Fails
 
 # Session Mode
 encode_a = tf.image.encode_png(A)
 encode_b = tf.image.encode_png(B)
 
 with tf.Session() as sess:
     a_encoded = sess.run(encode_a)
     b_encoded = sess.run(encode_b)
 
 print(len(a_encoded),len(b_encoded)) # prints 178 and 178 but 278 expected 
 assert(a_encoded == b_encoded) # True
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,53RT,2019-06-28T06:09:13Z,"
 		I am able to reproduce the issue with eager execution using TF 1.12 &TF 1.9 but in session i am getting the below error.
 RuntimeError: The Session graph is empty. Add operations to the graph before calling run().
 But in TF1.14 i am able to reproduce the issue with session mode but i am getting below error with eager execution.
 ValueError: tf.enable_eager_execution must be called at program startup.
 		",2.0,53RT,2019-07-12T01:00:44Z,"
 		Apologies for the delay in response. I get following results using TF 1.14. Can you please confirm?Thanks!
 Eager Mode
  278, 278
 Session Mode
 178, 178
 		",3.0,53RT,2019-07-12T11:41:26Z,"
 		Hi, thanks for having a look at it.
 The PNG header of the encoded image should be something around 60-70 bytes from looking at the outputs.
 So for the 10x10px uint8 image the expected byte length is ~ 100+70 and for the uint16 it should be around 2*100+70.
 With a fresh install of TF 1.14.0 and Python 3.6 I've got
 Eager Mode
 178,278 
 Session Mode
 178``,``178
 But it should be 278 always, in both modes. So in session mode the encoding doesn't work and in eager mode it works after the first encoding which is currently my workaround.
 Note: I executed it in a notebook and after a kernel restart I get the 178, 278 result for eager execution and if I execute the cell again, it will be 278, 278. So I guess there is some kind of initialization going on that sets the correct datatype after the first faulty execution of tf.image.encode.
 		",7807ec92bf8f44b5fd6de5b5342f041b168cf1f3,Akshay Modi,2019-11-22 18:15:05-08:00,MODIFY,0,tensorflow\python\eager\BUILD,tensorflow\python\eager\BUILD,0.0,624,,MODIFY,3.0,tensorflow\python\eager\pywrap_tfe_src.cc,tensorflow\python\eager\pywrap_tfe_src.cc,4.0,53RT,2019-07-12T12:13:07Z,"
 		It seems the problem exists also in TF 2.0 (if the usage hasn't changed).
 Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 >>> import numpy as np
 >>> import tensorflow as tf
 >>>
 >>> print(tf.__version__)
 2.0.0-beta1
 >>>
 >>> np.random.seed(1)
 >>> A = np.random.randint(low=0, high=65535, size=100, dtype=np.uint16).reshape(10,10,1)
 >>> B = A.copy()
 >>> np.allclose(A,B) # is true
 True
 >>>
 >>> a_encoded = tf.image.encode_png(A).numpy()
 >>> b_encoded = tf.image.encode_png(B).numpy()
 >>>
 >>> print(len(a_encoded),len(b_encoded)) # prints 178 and 278, 278 expected both times
 178 278
 		",5.0,53RT,2019-11-03T19:14:28Z,"
 		I could reproduce the issue with 2.0 and master, and could see the issue lies in the generated code of encode_png_eager_fallback (and to args_to_matching_eager).
 The issue is that in encode_png_eager_fallback, eager tensor for input image is created with ""preferred_dtype"" passed as uint8 (since EncodePng's defined T type is UINT8).
 However, input image itself is a numpy array with dtype of uint16. So, the second time it runs, input image is converted to a tensor as uint16. The reason is that the second time encode_png_eager_fallback is not called, so T type of UINT8 is not used implicitly.
 I think the behavior may need to be fixed one way or another, though I it might also break many other places if the fix is not careful.
 Not sure the best way to get around it, though <denchmark-link:https://github.com/53RT>@53RT</denchmark-link>
  if you convert input numpy array to tensor first then the example will run correctly.
 <denchmark-link:https://github.com/orgs/tensorflow/teams/api-owners>@tensorflow/api-owners</denchmark-link>
 
 		",6.0,53RT,2019-11-05T00:04:59Z,"
 		<denchmark-link:https://github.com/akshaym>@akshaym</denchmark-link>
  do you understand what is happening here?
 		",1.0,"157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179",,GetAttrToDefaultsMap,op_def,157,179,MODIFY,3.0,tensorflow\python\eager\pywrap_tfe_test.py,tensorflow\python\eager\pywrap_tfe_test.py,1.0,"297,298",,MODIFY,1.0,tensorflow\python\framework\test_ops.cc,tensorflow\python\framework\test_ops.cc,1.0,"707,708,709,710,711,712,713",,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1007,,get_uid,,1007,1007,1.0,"151,152,153,154,155",,GetAllAttrToDefaultsMaps,,151,155,,,,,,,,,,,,,,,,,,,,,,7.0,53RT,2019-11-22T21:59:39Z,"
 		Sorry for the delayed response.
 Eager internally has 2 paths (and you've found a place where they don't agree - a discrepancy I will remove soon).
 Unfortunately, this means removing the behavior of being 278 is unfortunately going to go away in eager, and all the above cases will return 178.
 As suggested by <denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
 , the best thing to do is to convert the numpy array to a tensor explicitly.
 		",testOpDefDefaultType.func,im,297,298,tensorflow::DTypeWithDefaultOp::Compute,ctx,707,713,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,53RT,2019-12-27T05:29:03Z,"
 		<denchmark-link:https://github.com/53RT>@53RT</denchmark-link>
 , Looks like its fixed in TF-nightly (2.1).
 Please find the <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/7bd1e30565e22d6f8a9087006016e52f/untitled327.ipynb>gist</denchmark-link>
  and let us know. Thanks!
 		",,,,,,,,,,,,,,,1.0,"305,306",,testOpDefDefaultType.func_captured,,305,306,1.0,"274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309",,testOpDefDefaultType,self,274,309,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,53RT,2020-01-10T12:37:31Z,"
 		It has been 14 days with no activity and the awaiting response label was assigned. Is this still an issue?
 		",10.0,53RT,2020-01-13T05:16:03Z,"
 		<denchmark-link:https://github.com/53RT>@53RT</denchmark-link>
 , Issue is fixed in Tf-nightly.
 Can you confirm. Thanks
 		",11.0,53RT,2020-01-13T09:57:42Z,"
 		<denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
  sorry for not responding.
 Yes I could reproduce the results and PNG encoding/decoding did worked for 8 and 16 bit with
 <denchmark-code>tf.__version__
 '2.2.0-dev20200112'
 </denchmark-code>
 
 Thanks for fixing it
 		",12.0,53RT,2020-01-13T09:57:43Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30113>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/30113>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30149,ageron,2019-06-25T20:34:21Z,2019-07-01T18:20:41Z,"Autograph ""Failed to parse source code"" error when using lambda in for loop","
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 MacOSX 10.13.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 VERSION=2.0.0-dev20190625
 GIT_VERSION=v1.12.1-4885-g71241a6afd
 Python version:
 3.6.8
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 N/A
 GPU model and memory:
 N/A
 
 Describe the current behavior
 I get an autograph error when running the following code (see the full stacktrace below):
 import tensorflow as tf
 ds = tf.data.Dataset.range(10).window(5, shift=1, drop_remainder=True)
 for window in ds.flat_map(lambda window: window.batch(5)):
     print(window.numpy())
 The error is ValueError: Failed to parse source code of <function <lambda> at 0x11194c488>
 Describe the expected behavior
 Everything works fine when I define the dataset on the previous line like this:
 import tensorflow as tf
 ds = tf.data.Dataset.range(10).window(5, shift=1, drop_remainder=True)
 ds = ds.flat_map(lambda window: window.batch(5))
 for window in ds:
     print(window.numpy())
 Code to reproduce the issue
 See above.
 Other info / logs
 Full stack trace with AUTOGRAPH_VERBOSITY=10:
 <denchmark-code>2019-06-25 22:24:13.172683: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-06-25 22:24:13.197405: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa5e8a657c0 executing computations on platform Host. Devices:
 2019-06-25 22:24:13.197445: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
 Converted call: <function <lambda> at 0x134b81488>
     args: (<_VariantDataset shapes: (), types: tf.int64>,)
     kwargs: {}
 
 Not whitelisted: <method-wrapper '__call__' of function object at 0x134b81488>: default rule
 Not whitelisted: <function <lambda> at 0x134b81488>: default rule
 Entity <function <lambda> at 0x134b81488> is not cached for key <code object <lambda> at 0x13b5f7ed0, file ""<ipython-input-1-8a83c4c9b193>"", line 4> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x13b641588>, frozenset())
 Converting <function <lambda> at 0x134b81488>
 WARNING: Logging before flag parsing goes to stderr.
 E0625 22:24:13.215670 140735810999168 ag_logging.py:133] Error converting <function <lambda> at 0x134b81488>
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 78, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 118, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 635, in to_graph
     return conversion.convert(entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
     free_nonglobal_var_names)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
     entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 441, in convert_entity_to_ast
     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 601, in convert_func_to_ast
     node, source = parser.parse_entity(f, future_features=future_features)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 123, in parse_entity
     ' source to:\n{}\nBut that did not work.'.format(source))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 66, in raise_parse_failure
     '{}'.format(entity, source, comment))
 ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 ERROR: Error converting <function <lambda> at 0x134b81488>
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 78, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 118, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 635, in to_graph
     return conversion.convert(entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
     free_nonglobal_var_names)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
     entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 441, in convert_entity_to_ast
     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 601, in convert_func_to_ast
     node, source = parser.parse_entity(f, future_features=future_features)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 123, in parse_entity
     ' source to:\n{}\nBut that did not work.'.format(source))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 66, in raise_parse_failure
     '{}'.format(entity, source, comment))
 ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 Error transforming entity <function <lambda> at 0x134b81488>
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 78, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 118, in parse_entity
     return parse_str(source, preamble_len=len(future_features)), source
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 140, in parse_str
     module_node = gast.parse(src)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/gast/gast.py"", line 240, in parse
     return ast_to_gast(_ast.parse(*args, **kwargs))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/ast.py"", line 35, in parse
     return compile(source, filename, mode, PyCF_ONLY_AST)
   File ""<unknown>"", line 5
     for window in ds.flat_map(lambda window: window.batch(5)):
                                                              ^
 SyntaxError: unexpected EOF while parsing
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 635, in to_graph
     return conversion.convert(entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 322, in convert
     free_nonglobal_var_names)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 240, in _convert_with_cache
     entity, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 441, in convert_entity_to_ast
     nodes, name, entity_info = convert_func_to_ast(o, program_ctx)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/conversion.py"", line 601, in convert_func_to_ast
     node, source = parser.parse_entity(f, future_features=future_features)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 123, in parse_entity
     ' source to:\n{}\nBut that did not work.'.format(source))
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/pyct/parser.py"", line 66, in raise_parse_failure
     '{}'.format(entity, source, comment))
 ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 528, in converted_call
     experimental_optional_features=options.optional_features)
   File ""/Users/ageron/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 639, in to_graph
     entity, e.__class__.__name__, str(e)))
 tensorflow.python.autograph.impl.api.ConversionError: converting <function <lambda> at 0x134b81488>: ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 W0625 22:24:13.223130 140735810999168 ag_logging.py:146] Entity <function <lambda> at 0x134b81488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x134b81488>: ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 WARNING: Entity <function <lambda> at 0x134b81488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function <lambda> at 0x134b81488>: ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. Tried to strip down the source to:
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 for window in ds.flat_map(lambda window: window.batch(5)):
 But that did not work.
 2019-06-25 22:24:13.243343: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1541] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
 [0 1 2 3 4]
 [1 2 3 4 5]
 [2 3 4 5 6]
 [3 4 5 6 7]
 [4 5 6 7 8]
 [5 6 7 8 9]
 </denchmark-code>
 
 	",1.0,ageron,2019-06-26T09:29:02Z,"
 		I have reproduced the issue in Colab using TF VERSION=2.0.0-dev20190625.Thanks!
 		",2.0,ageron,2019-06-26T19:41:50Z,"
 		This is related to a limitation in Python's inspect.getsource, which can't always get the source code of lambda functions. Specifically, getsource returns the entire source code line, which isn't always well-formed Python code, as you could see from this example.
 The workaround is to declare the lambda function on a single line, as the OP indicates.
 Normally, the error message should describe that (albeit in more detail), but it should definitely suggest the workaround of declaring the lambda on a separate line - <denchmark-link:https://github.com/ageron>@ageron</denchmark-link>
  can you confirm that the error message included that guidance?
 Related, we should remove the extraneous imports from the error message. The message should spell just:
 <denchmark-code>ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 for window in ds.flat_map(lambda window: window.batch(5)):
 </denchmark-code>
 
 		",3.0,ageron,2019-06-28T14:26:28Z,"
 		Yes, I can confirm that the message If this is a lambda function, the error may be avoided by creating the lambda in a standalone statement. was part of the (very long) error message. But it's neither at the beginning nor at the end, so it's easily overlooked. I would recommend shortening the error message (except when AUTOGRAPH_VERBOSITY=10), to something like this:
 ValueError: Failed to parse source code of <function <lambda> at 0x134b81488>, which Python reported as:
 for window in ds.flat_map(lambda window: window.batch(5)):
 The error may be avoided by creating the lambda in a standalone statement.
 Alternatively, isn't it possible to parse this line to extract the lambda? After all, it's right there. :)
 		",38df5d8ef43e884674f22670dbfd19ec26782f17,Dan Moldovan,2019-07-01 11:18:50-07:00,MODIFY,2,tensorflow\python\autograph\pyct\parser.py,tensorflow\python\autograph\pyct\parser.py,1.0,66,66,,,,,4.0,ageron,2019-06-28T15:16:52Z,"
 		I agree - will simplify the error message.
 Yes, we do attempt to parse the line, but in this case is it not well-formed Python code - in our example, it's a for loop without a body. One could imagine a partial parser which attempts to parse as much as possible of the code that is well-formed, but the Python parser doesn't know how to do that, and even then there may still be situations of ambiguity where the results would be incorrect.
 A much more robust fix would be to fix the parser so that it records the exact extents of the lambda, with column numbers. Currently it only records the line number, which is the root of the problem.
 		",5.0,ageron,2019-07-01T18:20:42Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30149>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30149>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,parse_entity.raise_parse_failure,comment,63,66,1.0,"53,66,71,122","53,66,71,122",parse_entity,"entity,future_features",39,122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30165,movinghoon,2019-06-26T08:01:16Z,2019-07-08T09:55:02Z,TF 2.0 - Put Tensor into some Numpy functions continuously increases memory usage,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 TensorFlow installed from (source or binary): pip package tensorflow==2.0.0-beta1
 TensorFlow version (use command below): v2.0.0-beta0-17-g8e423e3 2.0.0-beta1
 Python version: 3.7.3
 CUDA/cuDNN version: 10.0.0/7.3.1
 GPU model and memory: Titan Xp 11Gb
 
 Describe the current behavior
 Memory leak when we put Tensor into some Numpy functions (ex - np.array(), np.zeros_like()). Following attached code continuously increases memory usage.
 Describe the expected behavior
 No memory usage explosion.
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 import numpy as np
 import time
 
 x = tf.random.normal((1024, 1024))
 for i in range(int(1e7)):
     y = np.array(x)
     time.sleep(0.01)
 </denchmark-code>
 
 	",1.0,movinghoon,2019-06-27T12:22:18Z,"
 		I have tried on Colab with TF GPU version 2.0beta1 and was able to reproduce the issue. The code snippet caused crashed in Colab after using all available RAM.
 		",2.0,movinghoon,2019-06-27T22:50:44Z,"
 		<denchmark-link:https://github.com/superbobry>@superbobry</denchmark-link>
  is this related to the recent  changes?
 		",3.0,movinghoon,2019-06-28T13:50:56Z,"
 		I couldn't reproduce the issue using the nightly when running on CPU. Will be able to try on GPU on Monday.
 		",d5b287d6c93332ba73b99b375bd21f81266e3112,Sergei Lebedev,2019-07-08 02:52:53-07:00,MODIFY,1,tensorflow\c\eager\c_api.cc,tensorflow\c\eager\c_api.cc,1.0,592,,MODIFY,1.0,tensorflow\c\eager\c_api_internal.h,tensorflow\c\eager\c_api_internal.h,4.0,movinghoon,2019-07-01T09:10:05Z,"
 		I've reproduced the leak on GPU, fix is on the way.
 		",5.0,movinghoon,2019-07-08T09:55:03Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30165>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30165>No</denchmark-link>
 
 		",,,,,1.0,,"95,96,97,98,99,100,101,102,103",TFE_TensorHandle::CreateLocalHandle,"t,d,h",95,103,MODIFY,7.0,tensorflow\python\eager\pywrap_tensor.cc,tensorflow\python\eager\pywrap_tensor.cc,1.0,"654,655,656,657,658,659,660,661",,MODIFY,1.0,tensorflow\python\framework\ops.py,tensorflow\python\framework\ops.py,1.0,763,763,TFE_TensorHandleResolve,"h,status",558,598,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,EagerTensor_numpy,self,653,662,numpy,self,746,764,MODIFY,1.0,tensorflow\python\lib\core\ndarray_tensor.cc,tensorflow\python\lib\core\ndarray_tensor.cc,1.0,"402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417",,tensorflow::TF_TensorToMaybeAliasedPyArray,"tensor,out_ndarray",402,417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\python\lib\core\ndarray_tensor.h,tensorflow\python\lib\core\ndarray_tensor.h,0.0,"29,30,31",,,,,,,,,,,,,MODIFY,3.0,tensorflow\python\lib\core\py_func.cc,tensorflow\python\lib\core\py_func.cc,1.0,"173,176,177,178,179,180,181,182,183,184",175,tensorflow::DoCallPyFunc,"call,out_log_on_error",166,266,,,,,,,,,,,,,,,,,,,1.0,275,,tensorflow::ConvertToEagerTensor,"value,dtype",235,291,1.0,"80,81,82,87,88,89,90,94,95","79,80,81,82,83,84,88,89,90,91,92,93,94,95,96,97,98,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,121,122",TensorHandleToNumpy,handle,79,124,1.0,"80,81,82,87,88,89,90,94,95","80,81,82,83,84,88,89,90,91,92,93,94,95,96",TFE_TensorHandleToNumpy,"handle,status",80,96,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"75,85,87,88","75,85,87,88",tensorflow::MakeArgTuple,"call,tuple",75,108,1.0,"75,85,87,88","75,85,87,88",tensorflow::MakeArgTuple,"call,ctx,tuple",75,108,,,,,,,,MODIFY,2.0,tensorflow\python\ops\script_ops.py,tensorflow\python\ops\script_ops.py,1.0,,67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,func,Tout,is_grad_func",53,67,1.0,"142,143,144,145,146",,_ctx,self,142,146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"746,747,748,751,754",,EagerTensor_getbuffer,"self,view,flags",735,756,1.0,59,58,NumpyToTensorHandle,obj,58,74,1.0,59,,NumpyToTFE_TensorHandle,obj,59,75,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30248,junghau,2019-06-29T15:10:02Z,2019-07-02T10:06:42Z,tf.io.write_file not working in tf.function decorated function,"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 and Windows 10
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0-beta1
 Python version: 3.7
 
 Describe the current behavior
 tf.io.write_file creates file in eager execution but produces no output file when decorated with @tf.function.
 Describe the expected behavior
 tf.io.write_file should create an output file whether or not being decorated with @tf.function.
 Code to reproduce the issue
 import tensorflow as tf
 
 @tf.function
 def writeJPEG_graph(img_decoded, filename):
     out = tf.cast(img_decoded, tf.uint8)
     out = tf.image.encode_jpeg(out, quality=100)
     tf.io.write_file(filename, out)
     
 def writeJPEG_eager(img_decoded, filename):
     out = tf.cast(img_decoded, tf.uint8)
     out = tf.image.encode_jpeg(out, quality=100)
     tf.io.write_file(filename, out)
 
 img = tf.fill([256,256,3], 127) # example gray image
 writeJPEG_graph(img, ""./tfwrite_graph.jpg"") # ""tfwrite_graph.jpg"" not created
 writeJPEG_eager(img, ""./tfwrite_eager.jpg"") # ""tfwrite_eager.jpg"" created
 	",1.0,junghau,2019-07-01T08:38:17Z,"
 		I have tried on colab with TF version 2.0 beta1 and was able to reproduce the issue.Thanks!
 		",2.0,junghau,2019-07-02T10:06:43Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30248>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30248>No</denchmark-link>
 
 		",3.0,junghau,2019-08-21T15:31:35Z,"
 		This also happens in 1.14, and sadly inhibits visualizing images augmented in tf.data.Dataset pipelines (since they are run as a Graph)
 		",3baef3b569344f0af6071950a5fc9d828a4ee6a6,Dan Moldovan,2019-07-02 03:04:10-07:00,MODIFY,0,tensorflow\core\ops\io_ops.cc,tensorflow\core\ops\io_ops.cc,0.0,477,,,,,,4.0,junghau,2019-08-21T18:04:51Z,"
 		Unfortunately the fix didn't make it into the 1.14 release, and will only be available in 2.0.
 In 1.14, write_file should still be usable using the old-style tf.control_dependencies.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30378,nguerinjr,2019-07-03T22:05:20Z,2019-07-18T21:39:49Z,Problems with keras model saving when there's a loss added with add_loss,"
 System information
 System: windows 10, wsl with ubuntu 18 LTS
 Tensorflow Version: 2.0.0b1 in CPU mode (default, installed from pip)
 Python version: 3.6.8
 It also happens in real linux environments (actually, it's easy to simulate each these errors)
 Describe the current behavior
 I'm having many problems when saving/loading keras models with custom loss (added with add_loss). I'll describe each one of the scenarios below (I think they're all related)
 Code to reproduce the issue
 inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))
 tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)
 model = tf.keras.Model(inputs=inp, outputs=tensor)
 model.add_loss(tf.keras.losses.mean_absolute_error(tensor, tensor + 1))
 model.compile('adam')
 tf.keras.experimental.export_saved_model(model, 'model.tf')
 
 When not using a keras layer as loss, it produces a non-valid JSON:
 
 Traceback (most recent call last):
 File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 8, in 
 tf.keras.experimental.export_saved_model(model, 'model.tf')
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 169, in export_saved_model
 _export_model_json(model, saved_model_path)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 177, in _export_model_json
 model_json = model.to_json()
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1449, in to_json
 model_config, default=serialization.get_json_type, **kwargs)
 File ""/usr/lib/python3.7/json/init.py"", line 238, in dumps
 *kw).encode(obj)
 File ""/usr/lib/python3.7/json/encoder.py"", line 199, in encode
 chunks = self.iterencode(o, _one_shot=True)
 File ""/usr/lib/python3.7/json/encoder.py"", line 257, in iterencode
 return _iterencode(o, 0)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/util/serialization.py"", line 69, in get_json_type
 raise TypeError('Not JSON Serializable:', obj)
 TypeError: ('Not JSON Serializable:', b'\n\x03add\x12\x03Add\x1a\x0fconv2d/Identity\x1a\x05add/y\x07\n\x01T\x12\x020\x01')
 Now, a code that uses keras layers
 Code to reproduce the issue
 inp = tf.keras.Input(batch_size=32, shape=(32, 32, 3))
 tensor = tf.keras.layers.Conv2D(filters=16, kernel_size=3)(inp)
 model = tf.keras.Model(inputs=inp, outputs=tensor)
 lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))
 model.add_loss(lbd([tensor, tensor + 1]))
 model.compile('adam')
 tf.keras.experimental.export_saved_model(model, 'model.tf')
 Traceback (most recent call last):
 File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 16, in 
 tf.keras.experimental.export_saved_model(model, 'model.tf')
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 166, in export_saved_model
 input_signature)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 236, in _save_v1_format
 _export_mode(mode_keys.ModeKeys.TRAIN, has_saved_vars, **export_args)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 299, in _export_mode
 compile_clone=compile_clone)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 538, in clone_and_build_model
 clone = clone_model(model, input_tensors=input_tensors)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 326, in clone_model
 model, input_tensors=input_tensors, layer_fn=clone_function)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 202, in _clone_functional_model
 model._insert_layers(ancillary_layers, relevant_nodes=relevant_nodes)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1633, in _insert_layers
 for node in layer.inbound_nodes
 ValueError: min() arg is an empty sequence
 
 When using a keras layer, it loses their inbound_nodes (I've debugged it). It puts them apart from other layers, but loses information of the objects (it's not a question of passing or not custom_objects as param).
 
 Now, trying to use non-experimental saves/loads.
 Code to reproduce the issue
 inp = tf.keras.Input(batch_size=8, shape=(32, 32, 3))
 tensor = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(inp)
 model = tf.keras.Model(inputs=inp, outputs=tensor)
 lbd = tf.keras.layers.Lambda(lambda i: tf.keras.losses.mean_absolute_error(i[0], i[1]))
 model.add_loss(lbd([tensor, tensor + 1]))
 model.compile('adam')
 model.save('model.keras.tf', save_format='tf')
 tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})
 
 A first annoying thing is that it's based on .ext. Even though it only saves in tf2 (put any extension), in load it verifies these extensions. It not a clear way of working in my opinion. Maybe some additional information on the files saved could make it not use the extensions.
 
 Traceback (most recent call last):
 File ""/home/nguerinjr/Documents/deep_coding_project/teste.py"", line 26, in 
 tf.keras.models.load_model('model.keras.tf', custom_objects={'lambda': lbd})
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 141, in load_model
 return saved_model.load_from_saved_model_v2(filepath, compile)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model.py"", line 1225, in load_from_saved_model_v2
 model._training_config))  # pylint: disable=protected-access
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
 result = method(self, *args, **kwargs)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 337, in compile
 self._compile_weights_loss_and_weighted_metrics()
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 458, in _method_wrapper
 result = method(self, *args, **kwargs)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1494, in _compile_weights_loss_and_weighted_metrics
 self.total_loss = self._prepare_total_loss(masks)
 File ""/home/nguerinjr/Documents/deep_coding_project/venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1595, in _prepare_total_loss
 raise ValueError('The model cannot be compiled '
 ValueError: The model cannot be compiled because it has no loss to optimize.
 
 The second is related to the loading, is that it does not accept no loss to compile. Keras accepts it (you can see in the examples). It accepts because it accounts for the non-default losses added. This is another saving/loading bug.
 
 Both experimental and non-experimental functions seem to have some problems in saving / loading. I think if you simulate some custom scenarios with keras, you'll find a bunch of other errors. So, it's worth to take a whole look at them (specially considering that Keras is the default prototyping tool in tf2).
 In the current situation, i've not thought of a simple and easy way to save keras models with custom components (those which are not in the list of arguments of compile)
 	",1.0,nguerinjr,2019-07-04T00:01:18Z,"
 		Hi <denchmark-link:https://github.com/nguerinjr>@nguerinjr</denchmark-link>
 ,
 Thanks for creating this issue. I've also struggled with this, but it's possible: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1ad6aae48d97937d7caed2627a1ff1f0889b2cc7>1ad6aae</denchmark-link>
 .
 The trick is here: 
 
 
 tensorflow/tensorflow/python/keras/saving/hdf5_format_test.py
 
 
          Line 821
       in
       1ad6aae
 
 
 
 
 
 
  outputs = keras.layers.Lambda(lambda x: x[0])((y, custom_loss)) 
 
 
 
 
 
 to connect the loss calculation with the rest of the network. It's not ideal, I know. There really should be an auxiliary outputs type argument in Model, or add_loss should retrace graph, but the workaround isn't terrible in this case.
 Hope this helps.
 		",2.0,nguerinjr,2019-07-04T11:47:17Z,"
 		<denchmark-link:https://github.com/nguerinjr>@nguerinjr</denchmark-link>
  ,
 Can you please confirm if <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>
 's workaround is working for you.
 		",3.0,nguerinjr,2019-07-05T16:53:31Z,"
 		Hi <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>
 , <denchmark-link:https://github.com/rmothukuru>@rmothukuru</denchmark-link>
  ,
 Yes, that's working in the scenario of the codes I've put here.
 I'll implement this workaround in my real code, where I have a bunch of custom_objects.
 As you've mentioned, <denchmark-link:https://github.com/ppham27>@ppham27</denchmark-link>
 , it's not ideal. Since there are those problems in the code, it's a good deal to make things work this way.
 Thanks!
 		",a377701899b71b5f6bb0f157be763c283c7ff7e9,Philip Pham,2019-07-18 14:37:33-07:00,MODIFY,3,tensorflow\python\keras\distribute\distribute_strategy_test.py,tensorflow\python\keras\distribute\distribute_strategy_test.py,1.0,"1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850",,MODIFY,2.0,tensorflow\python\keras\engine\base_layer.py,tensorflow\python\keras\engine\base_layer.py,4.0,nguerinjr,2019-07-05T17:23:33Z,"
 		Great for what it's worth, I have a pending internal change that will trace the history when using add_loss and add_metric, so you don't need that line. Ideally, it will land at the end of next week, but I can't say for certain since I'm not actually on the TensorFlow team.
 		",5.0,nguerinjr,2019-07-18T21:39:50Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30378>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30378>No</denchmark-link>
 
 		",,,,,1.0,1089,,add_metric,"self,value,aggregation,name",1021,1089,MODIFY,3.0,tensorflow\python\keras\engine\network.py,tensorflow\python\keras\engine\network.py,1.0,"1661,1662,1663,1664,1665,1666,1667,1668",,MODIFY,5.0,tensorflow\python\keras\models.py,tensorflow\python\keras\models.py,1.0,"212,213,214,224,227,232","140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,193,198,199,200,201",test_fit_and_evaluate,"self,distribution,model_fn,l1,l2",1819,1850,1.0,"1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772",,_functional_with_add_loss_and_metric,"input_shape,num_classes,l1,l2",1749,1772,1.0,"1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802",,_sequential_with_add_loss_and_metric,"input_shape,num_classes,l1,l2",1775,1802,,,,,,,,1.0,1002,"1002,1003,1004,1005",add_loss,"self,losses,inputs",888,1005,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_graph_network_add_loss,"self,symbolic_loss",1661,1668,_clone_functional_model,"model,input_tensors,layer_fn",129,233,MODIFY,2.0,tensorflow\python\keras\saving\hdf5_format_test.py,tensorflow\python\keras\saving\hdf5_format_test.py,1.0,"818,820","818,819,820,821",test_functional_model_with_custom_loss_and_metric,self,811,848,1.0,"818,820","818,819,820,821",test_functional_model_with_custom_loss_and_metric._make_model,,815,823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1670,1671,1672,1673,1674,1675",,_graph_network_add_metric,"self,value,aggregation,name",1670,1675,1.0,"1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885",,_diff_layers,"inputs,outputs,layers",1871,1885,,,,,,,,1.0,"58,59,60,61,62,63,64,65,66,67,68,69,70,71",,_insert_ancillary_layers,"model,ancillary_layers,metrics_names,new_nodes",58,71,1.0,"236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262","245,246,247,248,249,250,251,253,254,255,256,257,258,259,260,261,262",_remove_ancillary_layers,"model,layer_map,layers",236,262,1.0,"74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126",,_make_new_nodes,"nodes_by_depth,layer_fn,layer_map,tensor_map",74,126,1.0,"301,302,306,307,308,309,310,311,312,313,314,315,316,317,319,320,321,322,323,325,332,333,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367","271,277,278,279",_clone_sequential_model,"model,input_tensors,layer_fn",265,367,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30474,Dobiasd,2019-07-08T05:57:13Z,2019-07-13T02:44:27Z,[TF2.0] Bug allowing misuse of the batch dimension of a convolution layer,"
 tensorflow-1.14.0 rightfully complains about the following minimal example with ValueError: could not broadcast input array from shape (20,6,6,32) into shape (10,6,6,32).
 tensorflow==2.0.0-beta1 however happily runs it and prints (20, 6, 6, 32).
 import numpy as np
 import tensorflow.keras.backend as k
 from tensorflow.keras.layers import Input, Conv2D, Lambda
 from tensorflow.keras.models import Model
 
 def custom_reshape(inputs):
     return k.reshape(inputs, (-1, 8, 8, 3))
 
 inputs = Input(shape=(8, 8, 6))
 x = Lambda(custom_reshape)(inputs)
 x = Conv2D(32, (3, 3))(x)
 model = Model(inputs=inputs, outputs=x)
 model.compile(loss='mean_squared_error', optimizer='nadam')
 print(model.summary())
 batch_size = 10
 result = model.predict(np.ones((batch_size, 8, 8, 6)), batch_size=batch_size)
 print(result.shape)
 As <denchmark-link:https://groups.google.com/a/tensorflow.org/forum/#!topic/testing/txsgcR3cubQ>per discussion</denchmark-link>
  this seems to be a bug in TF 2.0.
 	",1.0,Dobiasd,2019-07-09T13:01:30Z,"
 		I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.
 		",2.0,Dobiasd,2019-07-09T13:22:24Z,"
 		<denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
 
 
 I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.
 
 You mean the other way around, right? I.e., the ValueError we get with 1.14.0 should be the correct behavior, while not raising this exception in 2.0.0-beta1 seems to be the problem.
 		",3.0,Dobiasd,2019-07-10T08:10:35Z,"
 		
 @gadagashwini
 
 I am able to reproduce the issue on Colab with Tensorflow 1.14.0 and works as expected with 2.0.0.beta1.
 
 You mean the other way around, right? I.e., the ValueError we get with 1.14.0 should be the correct behavior, while not raising this exception in 2.0.0-beta1 seems to be the problem.
 
 Yes <denchmark-link:https://github.com/Dobiasd>@Dobiasd</denchmark-link>
  I could able to get the  with Tensorflow  and no exception with Tensorflow .Thanks!
 		",37fcf0a0e04b2014864936397c25e6c398135772,Pavithra Vijay,2019-07-12 19:42:23-07:00,MODIFY,2,tensorflow\python\keras\engine\training_test.py,tensorflow\python\keras\engine\training_test.py,1.0,"1253,1254",,MODIFY,1.0,tensorflow\python\keras\engine\training_utils.py,tensorflow\python\keras\engine\training_utils.py,4.0,Dobiasd,2019-07-13T02:44:28Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30474>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30474>No</denchmark-link>
 
 		",5.0,Dobiasd,2019-07-13T23:57:32Z,"
 		<denchmark-link:https://github.com/Dobiasd>@Dobiasd</denchmark-link>
  With tf-nightly-2.0-preview the code seems to work correctly:
 ...
 ...
 Traceback (most recent call last):
   File ""p.py"", line 16, in <module>
     result = model.predict(np.ones((batch_size, 8, 8, 6)), batch_size=batch_size)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training.py"", line 872, in predict
     use_multiprocessing=use_multiprocessing)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 717, in predict
     callbacks=callbacks)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py"", line 395, in model_iteration
     aggregator.aggregate(batch_outs, batch_start, batch_end)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 308, in aggregate
     result.aggregate(batch_element, batch_start, batch_end)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.py"", line 233, in aggregate
     batch_element.shape, self.results.shape))
 ValueError: Mismatch between expected batch size and model output batch size. Output shape = (20, 6, 6, 32), expected output shape = shape (10, 6, 6, 32)
 ubuntu@ubuntu:/v# python
 Python 2.7.15+ (default, Nov 27 2018, 23:36:35) 
 [GCC 7.3.0] on linux2
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 >>> import tensorflow as tf
 >>> tf.version.VERSION
 '2.0.0-dev20190713'
 >>> tf.version.GIT_VERSION
 'v1.12.1-6246-g5d4a6cee73'
 >>> 
 		",6.0,Dobiasd,2019-07-14T05:02:10Z,"
 		<denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
 
 
 With tf-nightly-2.0-preview the code seems to work correctly
 
 Thanks for the confirmation. I guess this was to be expected since <denchmark-link:https://github.com/tensorflow/tensorflow/commit/37fcf0a0e04b2014864936397c25e6c398135772>the fix</denchmark-link>
  includes an explicit test for this. 
 		",1.0,"229,230,231,232,233,234",,aggregate,"self,batch_element,batch_start,batch_end",222,248,,,,,,,,,,,,,,,test_invalid_batch_dimension.custom_reshape,inputs,1253,1254,1.0,"1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268",,test_invalid_batch_dimension,self,1251,1268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30574,jbgh2,2019-07-10T18:05:41Z,2019-09-19T16:07:57Z,Decode_wav sample rate output cannot be passed to tf.signal.linear_to_mel_weight_matrix,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Combined output of decode_wav with the sample in signal/mfccs_from_log_mel_spectrograms
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 TensorFlow installed from (source or binary): Conda binary
 TensorFlow version (use command below): 2.0.0-dev20190702 (git version unknown)
 Python version: 3.6.7
 Bazel version (if compiling from source): No
 GCC/Compiler version (if compiling from source): No
 CUDA/cuDNN version:
 GPU model and memory: Surface Book Nvidia GPU
 
 Describe the current behavior
 The output of decode_wav is tuple of (wav_data, sample_rate)
 Sample_rate is int32 but linear_to_mel_weight_matrix expects a float32 sample_rate.
 If the sample rate is cast using tf.cast(sample_rate, float32) and then a TypeError is thrown with the message:
 TypeError: Using a tf.Tensor as a Python bool is not allowed. Use if t is not None: instead of if t: to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
 Describe the expected behavior
 Sample rate output of decode_wav can be used as input to linear_to_mel_weight_matrix.
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 <denchmark-code>import tensorflow as tf
 
 def load_and_mel_file(path_tensor):
     #From: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/signal/mfccs_from_log_mel_spectrograms
     pcm, sample_rate = tf.audio.decode_wav(path_tensor)
     sr_f = tf.cast(sample_rate, tf.float32) #Mismatch in types between output of decode_wav and input to linear_to_mel_weight_matrix
     print(pcm, sample_rate, sr_f)
 
     # A 1024-point STFT with frames of 64 ms and 75% overlap.
     stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                            fft_length=1024)
     spectrograms = tf.abs(stfts)
 
     # Warp the linear scale spectrograms into the mel-scale.
     num_spectrogram_bins = stfts.shape[-1]
     lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
     linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
       num_mel_bins, num_spectrogram_bins, sr_f, lower_edge_hertz,
       upper_edge_hertz)
     mel_spectrograms = tf.tensordot(
       spectrograms, linear_to_mel_weight_matrix, 1)
     mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
       linear_to_mel_weight_matrix.shape[-1:]))
 
     # Compute a stabilized log to get log-magnitude mel-scale spectrograms.
     log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)
     print(log_mel_spectrograms)
     
     return log_mel_spectrograms
 
 path_ds = tf.data.Dataset.list_files(""*.wav"")
 mel_ds = path_ds.map(load_and_mel_file)
 </denchmark-code>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 
 
 TypeError                                 Traceback (most recent call last)
  in 
 1 #Build datasets
 ----> 2 train_ds = build_data_pairs_from_dir(train_dir)
 3 test_ds = build_data_pairs_from_dir(test_dir)
 
  in build_data_pairs_from_dir(source_dir)
 54
 55     #Convert to MEL
 ---> 56     clean_mel_ds = clean_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)
 57     noisy_mel_ds = noisy_path_ds.map(load_and_mel_file, num_parallel_calls=AUTOTUNE)
 58
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in map(self, map_func, num_parallel_calls)
 1887       return DatasetV1Adapter(
 1888           ParallelMapDataset(
 -> 1889               self, map_func, num_parallel_calls, preserve_cardinality=False))
 1890
 1891   @deprecation.deprecated(None, ""Use `tf.data.Dataset.map()"")
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in init(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)
 3333         self._transformation_name(),
 3334         dataset=input_dataset,
 -> 3335         use_legacy_function=use_legacy_function)
 3336     self._num_parallel_calls = ops.convert_to_tensor(
 3337         num_parallel_calls, dtype=dtypes.int32, name=""num_parallel_calls"")
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in init(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)
 2677       resource_tracker = tracking.ResourceTracker()
 2678       with tracking.resource_tracker_scope(resource_tracker):
 -> 2679         self._function = wrapper_fn._get_concrete_function_internal()
 2680         if add_to_graph:
 2681           self._function.add_to_graph(ops.get_default_graph())
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal(self, *args, **kwargs)
 1418     """"""Bypasses error checking when getting a graph function.""""""
 1419     graph_function = self._get_concrete_function_internal_garbage_collected(
 -> 1420         *args, **kwargs)
 1421     # We're returning this concrete function to someone, and they may keep a
 1422     # reference to the FuncGraph without keeping a reference to the
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
 1412     if self.input_signature:
 1413       args, kwargs = None, None
 -> 1414     graph_function, _, _ = self._maybe_define_function(args, kwargs)
 1415     return graph_function
 1416
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _maybe_define_function(self, args, kwargs)
 1716         graph_function = self._function_cache.primary.get(cache_key, None)
 1717         if graph_function is None:
 -> 1718           graph_function = self._create_graph_function(args, kwargs)
 1719           self._function_cache.primary[cache_key] = graph_function
 1720         return graph_function, args, kwargs
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
 1602             arg_names=arg_names,
 1603             override_flat_arg_shapes=override_flat_arg_shapes,
 -> 1604             capture_by_value=self._capture_by_value),
 1605         self._function_attributes)
 1606
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
 784                                           converted_func)
 785
 --> 786       func_outputs = python_func(*func_args, **func_kwargs)
 787
 788       # invariant: func_outputs contains only Tensors, CompositeTensors,
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in wrapper_fn(*args)
 2671           attributes=defun_kwargs)
 2672       def wrapper_fn(*args):  # pylint: disable=missing-docstring
 -> 2673         ret = _wrapper_helper(*args)
 2674         ret = structure.to_tensor_list(self._output_structure, ret)
 2675         return [ops.convert_to_tensor(t) for t in ret]
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py in _wrapper_helper(*args)
 2616         nested_args = (nested_args,)
 2617
 -> 2618       ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
 2619       # If func returns a list of tensors, nest.flatten() and
 2620       # ops.convert_to_tensor() would conspire to attempt to stack
 c:\users\benhe.conda\envs\homl2\lib\site-packages\tensorflow_core\python\autograph\impl\api.py in wrapper(*args, **kwargs)
 220         except Exception as e:  # pylint:disable=broad-except
 221           if hasattr(e, 'ag_error_metadata'):
 --> 222             raise e.ag_error_metadata.to_exception(type(e))
 223           else:
 224             raise
 TypeError: in converted code:
 <ipython-input-60-92023d8b5863>:27 load_and_mel_file  *
     linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
 c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\ops\signal\mel_ops.py:155 linear_to_mel_weight_matrix
     lower_edge_hertz, upper_edge_hertz, dtype)
 c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\ops\signal\mel_ops.py:74 _validate_arguments
     if sample_rate <= 0.0:
 c:\users\benhe\.conda\envs\homl2\lib\site-packages\tensorflow_core\python\framework\ops.py:692 __bool__
     raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
 
 TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
 
 
 	",1.0,jbgh2,2019-07-11T06:40:47Z,"
 		<denchmark-link:https://github.com/jbgh2>@jbgh2</denchmark-link>
  I tried reproducing the issue on Colab with Tensorflow 2.0.0-dev20190709 but i got the below error  . Please help us to reproduce the issue. Thanks!
 		",2.0,jbgh2,2019-07-11T08:15:12Z,"
 		<denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
  You need to have at least one .wav file in your working directory, otherwise it will obviously not run.
 		",3.0,jbgh2,2019-07-12T06:39:07Z,"
 		<denchmark-link:https://github.com/jbgh2>@jbgh2</denchmark-link>
  I tried executing the .wav file but I am getting the following error
 `
 
 ValueError: sample_rate was a non-constant Tensor. Must be a Python float or a constant Tensor.
 
 `. Thanks!
 		",03ff87bfdeec43b9d3a208746ae19ebf9c139c14,RJ Skerry-Ryan,2019-09-16 14:07:30-07:00,MODIFY,7,tensorflow\python\kernel_tests\signal\mel_ops_test.py,tensorflow\python\kernel_tests\signal\mel_ops_test.py,1.0,"137,138,139,140,141,142,143,144,145,146,147,148,149,150,151","137,138,139,140,141,142,143,144,145,146,147,148,149,150,151",MODIFY,0.0,tensorflow\python\ops\signal\mel_ops.py,tensorflow\python\ops\signal\mel_ops.py,4.0,jbgh2,2019-07-12T14:00:23Z,"
 		Bug repro in Colab using TF Versions: 2.0.0-beta1
 <denchmark-link:https://colab.research.google.com/drive/139qvpTBQH079_z-RFfeTcbJqixV2oL_G>https://colab.research.google.com/drive/139qvpTBQH079_z-RFfeTcbJqixV2oL_G</denchmark-link>
 
 As far as I can tell the bug is because _validate_arguments expects Python types not Tensors and decode_wav returns sample_rate as a Tensor.
 I've included code to repro both problems:
 
 Sample_Rate from decode_wav is an int32 tensor that is compared to a Python 0.0
 Cast sample_rate to float32 tensor causes bool comparison error
 
 		",5.0,jbgh2,2019-09-15T06:21:10Z,"
 		Thanks for the report! I have a fix for this out for review.
 		",6.0,jbgh2,2019-09-19T16:07:57Z,"
 		Fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/03ff87bfdeec43b9d3a208746ae19ebf9c139c14>03ff87b</denchmark-link>
 .
 		",0.0,"81,82,83,84,85,86,87,130,131,132,133,148,167","75,76,83,84,85,86,129,130,131,132,147,148,158,159,160,170",,,,,,,,,,,,,,,,,,,test_matches_reference_implementation,self,137,151,1.0,"153,154,155,156,157","153,155,156,157",test_dtypes,self,153,157,1.0,"187,188,193","183,188,189,190,191,192,193",test_constant_folding,self,183,193,1.0,"188,193,194,195,196,197,198,199","188,189,190,191,192,193",test_constant_folding,"self,dtype",188,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,jbgh2,2019-09-19T16:07:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30574>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30574>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"159,160,161,163,164","169,170,171",test_error,self,159,181,1.0,"147,148,149","147,148,149",test_matches_reference_implementation,"self,num_mel_bins,num_spectrogram_bins,sample_rate,use_tensor_sample_rate,lower_edge_hertz,upper_edge_hertz,dtype",147,149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"161,163,164",,test_dtypes,"self,dtype",161,164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30642,SeekerYb,2019-07-12T11:12:52Z,2019-09-13T19:45:58Z,‘scatter_nd_update’ doesn't work with string,"
 
 I Reproduced this issue in newest <denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow>tensorflow official docker image</denchmark-link>
 .
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
 TensorFlow installed from (source or binary):binary
 TensorFlow version (use command below):1.14.0
 Python version:2.7.15+
 
 
 In my model, I need to maintain an extremely long 2-D variable tensor，which has several columns and many rows, and its dtype is string. In every training step, I need to update only several individual rows of that tensor. <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/scatter_nd_update>tf.scatter_nd_update</denchmark-link>
  meets my requirements perfectly,
 except that it doesn't work with string in fact. As a contrast,  <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/scatter_nd>tf.scatter_nd</denchmark-link>
  does work. Since the document doesn't mention that  can't be string, I think it may be a bug.
 Describe the expected behavior
 I hope tf.scatter_nd_update support string ref,and I really need this feature in my project. So if it can't be fixed quickly, any walk-around suggestions (include modify some source code) is also welcome.
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 ref = tf.Variable([‘qq’,’ww’,’ee’,’rr’,’’,’’,’’,’’])
 indices = tf.constant([[4], [3], [1] ,[7]])
 updates = tf.constant(['aa', 'dd', 'cc', 'bb'])
 update = tf.scatter_nd_update(ref, indices, updates)
 with tf.Session() as sess:
     sess.run(tf.initialize_all_variables())
     print(sess.run(update))
 </denchmark-code>
 
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File ""<stdin>"", line 2, in <module>
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 950, in run
     run_metadata_ptr)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1173, in _run
     feed_dict_tensor, options, run_metadata)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1350, in _do_run
     run_metadata)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ScatterNdUpdate' used by node ScatterNdUpdate (defined at <stdin>:1) with these attrs: [_class=[""loc:@Variable""], use_locking=true, Tindices=DT_INT32, T=DT_STRING]
 Registered devices: [CPU, XLA_CPU]
 Registered kernels:
   device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_COMPLEX128]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_COMPLEX64]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_DOUBLE]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_BFLOAT16]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_HALF]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_HALF]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_INT8]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_INT8]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_UINT8]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_INT16]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_INT16]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_UINT16]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]
   device='CPU'; T in [DT_INT64]; Tindices in [DT_INT64]
   device='CPU'; T in [DT_INT64]; Tindices in [DT_INT32]
 
 	 [[ScatterNdUpdate]]
 </denchmark-code>
 
 	",1.0,SeekerYb,2019-07-15T13:08:11Z,"
 		Could reproduce the issue with TF version 1.14.Thanks
 		",2.0,SeekerYb,2019-09-13T19:45:59Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30642>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30642>No</denchmark-link>
 
 		",,,,,c3e32b03e187fc2854c34add42ee3d1fe1f17628,Alexandre Passos,2019-09-13 12:45:40-07:00,MODIFY,0,tensorflow\core\kernels\scatter_nd_op.cc,tensorflow\core\kernels\scatter_nd_op.cc,0.0,385,,MODIFY,1.0,tensorflow\python\kernel_tests\scatter_nd_ops_test.py,tensorflow\python\kernel_tests\scatter_nd_ops_test.py,,,,,,,,,,,,,1.0,"171,172,173,174,175,176,177,178",,testString,self,171,178,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
30685,mdanatg,2019-07-13T18:49:53Z,2019-07-24T17:14:09Z,`TensorArray` objects used as `Dataset.reduce` state lose inferred shapes,"
 System information
 
 TensorFlow version (use command below): 2.0
 Python version: 3
 
 Describe the current behavior
 TensorArray objects passed as accumulators to Dataset.reduce lose inferred shapes. Subsequent calls to TensorArray.concat returns a fully unknown shape.
 Describe the expected behavior
 The element shape of the TensorArray should be partially known, consistent with the behavior of an equivalent tf.while_loop.
 Code to reproduce the issue
 <denchmark-code>@tf.function
 def compute():
     arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
     def body(i, arr):
         real_logits = tf.random.normal([5, 1])
         arr = arr.write(tf.cast(i, tf.int32), real_logits)
         i += 1
         return i, arr
     def cond(i, arr):
       return i < 10
     _, arr = tf.while_loop(cond, body, (0, arr))
 
     c = arr.concat()
     tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
     return c
 
 @tf.function
 def compute_ds():
     arr = tf.TensorArray(tf.float32, 1, dynamic_size=True)
     def body(state, _):
         i, arr = state
         real_logits = tf.random.normal([5, 1])
         arr = arr.write(tf.cast(i, tf.int32), real_logits)
         i += 1
         return i, arr
     en_ds = tf.data.Dataset.range(10).enumerate()
     _, arr = en_ds.reduce((0, arr), body)
 
     c = arr.concat()
     tf.print('TensortArray.concat() shape:', c.shape, 'rank:', c.shape.rank)
     return c
 
 print('*** With tf.while_loop')
 _ = compute()
 print()
 print('*** With tf.Dataset.reduce')
 _ = compute_ds()
 </denchmark-code>
 
 <denchmark-code>*** With tf.while_loop
 TensortArray.concat() shape: TensorShape([None, 1]) rank: 2
 
 *** With tf.Dataset.reduce
 TensortArray.concat() shape: TensorShape(None) rank: None
 </denchmark-code>
 
 	",1.0,mdanatg,2019-07-13T18:56:26Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  could you triage the issue?
 		",2.0,mdanatg,2019-07-22T17:40:44Z,"
 		<denchmark-link:https://github.com/aaudiber>@aaudiber</denchmark-link>
  could you please take a look?
 		",3.0,mdanatg,2019-07-23T16:45:47Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  Thank you for reporting this and for including simple repro instructions!
 The issue is with how we use TensorArraySpec to convert the TensorArray to/from Tensor components across the reduce boundary. As you noticed, it loses the shape information. I have a CL out to fix it, and will update once it is merged
 		",6cd69820a7ec68363647bf918d312b5d10e0e07a,Andrew Audibert,2019-07-23 15:56:58-07:00,MODIFY,2,tensorflow\python\data\util\structure_test.py,tensorflow\python\data\util\structure_test.py,1.0,"384,385,386,387,388,389,390,391",,MODIFY,1.0,tensorflow\python\ops\tensor_array_ops.py,tensorflow\python\ops\tensor_array_ops.py,4.0,mdanatg,2019-07-24T17:14:09Z,"
 		Now that <denchmark-link:https://github.com/tensorflow/tensorflow/commit/6cd69820a7ec68363647bf918d312b5d10e0e07a>6cd6982</denchmark-link>
  has merged, inferred  shapes will be preserved across all tf.data operations.
 		",5.0,mdanatg,2019-07-24T17:14:11Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=30685>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=30685>No</denchmark-link>
 
 		",,,,,1.0,1321,1321,_from_components,"self,tensor_list",1313,1322,,,,,,,,,,,,,,,testPreserveInferredTensorArrayShape,self,384,391,1.0,"376,377,378,379,380,381,382",,testPreserveTensorArrayShape,self,376,382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31596,michaelbenayoun,2019-08-13T22:26:37Z,2020-05-28T17:10:49Z,TFLiteConverter fails with tf.gather when the params argument is a layer attribute,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS
 TensorFlow installed from (source or binary): conda
 TensorFlow version (use command below): 2.0.0-dev20190807
 Python version: 3.6.8
 CUDA/cuDNN version: 10.0
 GPU model and memory: 8 x Tesla P100-PCIE-16GB
 
 Describe the current behavior
 I am not able to convert a SavedModel to a FlatBuffer using TFLiteConverter when the corresponding tf.keras.Model contains a layer with a tf.gather op for which the params argument comes from a variable that was initialized in the build method of that said layer.
 When the params argument is from a locally defined variable, or when using tf.nn.embedding_lookup instead of tf.gather, everything works perfectly fine.
 It also applies to tf.gather_nd.
 Describe the expected behavior
 I expect tf.gather to work for the case in which the params argument is an attribute of the tf.keras.layers.Layer, just as it does for the other cases mentioned.
 Code to reproduce the issue
 I wrote a toy example to reproduce the issue, it might be clearer than the description above.
 <denchmark-code>import numpy as np
 import tensorflow as tf
 print(tf.__version__)
 
 class Embedding(tf.keras.layers.Layer):
     def __init__(self, vocab_size, hidden_size):
         super(Embedding, self).__init__()
         self.vocab_size = vocab_size
         self.hidden_size = hidden_size
     
     def build(self, input_shape):
         self.shared_weights = self.add_weight(
             ""weights"",
             shape=(self.vocab_size, self.hidden_size),
             dtype=tf.float32,
             initializer=tf.random_normal_initializer(
                 mean=0.0, 
                 stddev=self.hidden_size ** (-0.5)
             )
         )
     
     def call(self, input_):
         # return tf.nn.embedding_lookup(self.shared_weights, input_)
         # return tf.gather(tf.zeros(shape=(self.vocab_size, self.hidden_size)), input_)
         return tf.gather(self.shared_weights, input_)
 
 
 class SimpleModel(tf.keras.Model):
     def __init__(self, vocab_size, hidden_size):
         super(SimpleModel, self).__init__()
         self.embedding_layer = Embedding(vocab_size, hidden_size)
     
     @tf.function(input_signature=[tf.TensorSpec(shape=(None, ), dtype=tf.int64, name='input')])
     def call(self, input_):
         return self.embedding_layer(input_)
 
 vocab_size = 20000
 hidden_size = 300
 
 # Building the model.
 model = SimpleModel(vocab_size, hidden_size)
 input_ = tf.random.uniform(shape=(20, ), dtype=tf.int64, maxval=100)
 model(input_)
 
 # Exporting to SavedModel.
 saved_model_dir = 'simple_model/'
 tf.saved_model.save(model, saved_model_dir)
 
 # TFLite conversion.
 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
 tflite_model = converter.convert()
 </denchmark-code>
 
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File ""/home/michael/.conda/envs/tf20/bin/toco_from_protos"", line 10, in <module>
     sys.exit(main())
   File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
     app.run(main=execute, argv=[sys.argv[0]] + unparsed)
   File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
   File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py"", line 300, in run
     _run_main(main, args)
   File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
     sys.exit(main(argv))
   File ""/home/michael/.conda/envs/tf20/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
     enable_mlir_converter)
 Exception: Placeholder statefulpartitionedcall_args_1 should be specied by input_arrays.
 
 </denchmark-code>
 
 	",1.0,michaelbenayoun,2019-08-14T03:29:16Z,"
 		Issue replicating with TF nightly-2.0-preview.
 		",2.0,michaelbenayoun,2019-08-19T14:56:42Z,"
 		Follow-up: another (temporary) solution I found is to use tf.identity,  return tf.gather(tf.identity(self.shared_weights), input_) works fine.
 It might not be the most efficient way of fixing the issue (especially for large tensors) but it works.
 		",3.0,michaelbenayoun,2019-08-26T20:26:57Z,"
 		This should be fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/b42547e3bca7ab796f40f20726f4b09bf552e833>b42547e</denchmark-link>
 . Please reopen if the original code doesn't work in the nightly.
 		",252e6183523d226e50137c06a101df0aa5d4d5d9,Reed Wanderman-Milne,2019-12-23 15:48:33-08:00,MODIFY,2,tensorflow\python\keras\saving\metrics_serialization_test.py,tensorflow\python\keras\saving\metrics_serialization_test.py,1.0,223,223,,,,,4.0,michaelbenayoun,2019-08-26T20:26:59Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=31596>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=31596>No</denchmark-link>
 
 		",5.0,michaelbenayoun,2019-09-04T14:51:57Z,"
 		Updated tensorflow 2.0 preview to the 20190903 nightly and still facing an issue.
 <denchmark-h:h4>Code to reproduce</denchmark-h>
 
 The code is basically the same as before: it runs when using tf.nn.embedding_lookup or wrapping the shared weights with a tf.identity op, but fails in the regular case with a tf.gather.
 <denchmark-code>import numpy as np
 import tensorflow as tf
 print(tf.__version__)
 
 import os
 
 class Embedding(tf.keras.layers.Layer):
     def __init__(self, vocab_size, hidden_size):
         super(Embedding, self).__init__()
         self.vocab_size = vocab_size
         self.hidden_size = hidden_size
     
     def build(self, input_shape):
         self.shared_weights = self.add_weight(
             ""weights"",
             shape=(self.vocab_size, self.hidden_size),
             dtype=tf.float32,
             initializer=tf.random_normal_initializer(
                 mean=0.0, 
                 stddev=self.hidden_size ** (-0.5)
             )
         )
     
     def call(self, input_, mode=""embedding""):
         # return tf.gather(tf.identity(self.shared_weights), input_)
         # return tf.nn.embedding_lookup(self.shared_weights, input_)
         return tf.gather(self.shared_weights, input_)
 
 
 class SimpleModel(tf.keras.Model):
     def __init__(self, vocab_size, hidden_size):
         super(SimpleModel, self).__init__()
         self.embedding_layer = Embedding(vocab_size, hidden_size)
     
     @tf.function(input_signature=[tf.TensorSpec(shape=(None, 25), dtype=tf.int64, name='input')])
     def call(self, input_):
         return self.embedding_layer(input_)
 
 vocab_size = 20000
 hidden_size = 300
 
 # Building the model.
 model = SimpleModel(vocab_size, hidden_size)
 input_ = tf.random.uniform(shape=(10, 25), dtype=tf.int64, maxval=100)
 model(input_)
 
 # Exporting to SavedModel.
 saved_model_dir = 'simple_model/'
 tf.saved_model.save(model, saved_model_dir)
 
 # TFLite conversion.
 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
 tflite_model = converter.convert()
 </denchmark-code>
 
 <denchmark-h:h4>Other info / logs</denchmark-h>
 
 Although it is not the same error message as before (ResourceGather related), I think it might be related to the initial issue.
 <denchmark-code>2019-09-04 10:49:32.751617: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 6 operators, 9 arrays (0 quantized)
 2019-09-04 10:49:32.751814: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 6 operators, 9 arrays (0 quantized)
 2019-09-04 10:49:32.751891: F tensorflow/lite/toco/graph_transformations/resolve_gather_attributes.cc:47] Check failed: axis_data.size() == 1 (2 vs. 1)Multidimensional gather not supported on {Gather operator with output StatefulPartitionedCall/embedding_4/Gather}
 Fatal Python error: Aborted
 </denchmark-code>
 
 		",6.0,michaelbenayoun,2020-05-14T16:01:07Z,"
 		
 Updated tensorflow 2.0 preview to the 20190903 nightly and still facing an issue.
 
 <denchmark-link:https://github.com/michaelbenayoun>@michaelbenayoun</denchmark-link>
 ,
 I was able to run the code without any issues with the latest TF-nightly i.e. 2.3.0-dev20200514. Please find the gist of it <denchmark-link:https://colab.research.google.com/gist/amahendrakar/dfb9e315b72999f54e8e6f258d155195/31596-tf-nightly.ipynb>here</denchmark-link>
 . Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,test_serializing_model_with_metric_with_custom_objects,"self,value",218,264,1.0,223,223,test_serializing_model_with_metric_with_custom_objects.get_instance,x,220,225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,michaelbenayoun,2020-05-21T16:26:15Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,michaelbenayoun,2020-05-28T17:10:45Z,"
 		Closing as stale. Please reopen if you'd like to work on this further.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,michaelbenayoun,2020-05-28T17:10:50Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31596>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31596>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
31952,David-Mao,2019-08-25T09:51:25Z,2019-12-02T23:30:48Z,[TF 2.0] tf.gather doesn't work alongside @tf.function,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Darwin Kernel Version 18.6.0
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 N/A
 TensorFlow installed from (source or binary):
 binary
 TensorFlow version (use command below):
 2.0.0-dev20190730
 Python version:
 Python 3.6.8 :: Anaconda, Inc.
 Bazel version (if compiling from source):
 N/A
 GCC/Compiler version (if compiling from source):
 N/A
 CUDA/cuDNN version:
 N/A
 GPU model and memory:
 N/A
 
 Describe the current behavior
 It seems that when tf.gather() is called after a tf.function, the gradient cannot be calculated. The example code blow shows the bug. The code itself raises the following error message:
 
 AssertionError: Expected all args to be Tensors or Variables; but got CompositeTensor
 
 The code will work if we remove the tf.function decorator, or  put the tf.gather line inside the tf.funtion graph.
 Code to reproduce the issue
 <denchmark-code>import numpy as np
 import tensorflow as tf
 
 x = tf.cast(np.random.randn(100, 100), tf.float32)
 z = tf.cast(np.random.randn(1, 100), tf.float32)
 
 layer = tf.keras.layers.Dense(100)
 
 @tf.function  # <- removing this and the code works fine
 def fun(x, layer):
     y = layer(x)
     return y
 
 with tf.GradientTape() as tape:
     y = fun(x, layer)
     y = tf.gather(y, [0])  # if we put this line inside the function it works fine
     loss = tf.norm(y - z)
 
 grads = tape.gradient(loss, layer.trainable_variables)
 </denchmark-code>
 
 	",1.0,David-Mao,2019-08-26T05:58:13Z,"
 		I have tried on Colab with TF version 2.0.0-dev20190730, recent nightly version 2.0.0-dev20190825 and was able to reproduce the issue.Please, find the <denchmark-link:https://colab.research.google.com/drive/1P4tejxmGIxKmXM4TFoesuaXTZUEsPpdx>gist</denchmark-link>
  here.Thanks!
 		",2.0,David-Mao,2019-11-27T13:11:30Z,"
 		Hi,
 We encountered the same bug, which currently prevents our migration from TF1 to TF2.
 As David-mao said, it works perfectly well in Eager mode.
 The problem arises only when calling tf.gather on tensors returned from a tf.function, and then calculating its gradients.
 		",3.0,David-Mao,2019-11-28T15:26:18Z,"
 		<denchmark-link:https://github.com/diNatale>@diNatale</denchmark-link>
  I found a very ugly workaround for this. You wrap  into a graph function:
 <denchmark-code>
 @tf.function
 def gather(x, ind):
     return tf.gather(x + 0, ind)
 </denchmark-code>
 
 and use this gather instead of tf.gather in your code.  I know it's absurd (the most absurd part is to have + 0 inside it, which is necessary for reasons unclear to me), but it works in my cases.
 		",d5ee347de231b55f8ef7c11402db1673ff111d53,Alexandre Passos,2019-12-02 15:16:16-08:00,MODIFY,2,tensorflow\python\eager\backprop_test.py,tensorflow\python\eager\backprop_test.py,1.0,"313,314",,MODIFY,2.0,tensorflow\python\eager\function.py,tensorflow\python\eager\function.py,4.0,David-Mao,2019-11-28T15:43:02Z,"
 		Wow!
 ""+ 0"" - of course, how didn't we guess that :)
 It does work for me if I use both the wrapper and the +0
 Is there any explanation for this behaviour?
 thanks
 		",5.0,David-Mao,2019-12-02T23:30:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/31952>No</denchmark-link>
 
 		",,,,,1.0,"1227,1228,1229,1230,1231,1232",,_wrap_backward_function._backward_function_wrapper,args,1219,1251,,,,,,,,,,,,,,,testFunctionIndexedSlicesGradient.f,x,313,314,1.0,"310,311,312,313,314,315,316,317,318,319,320,321",,testFunctionIndexedSlicesGradient,self,310,321,,,,,,,,,,,,,,,1.0,"1227,1228,1229,1230,1231,1232",,_wrap_backward_function,"self,forward_graph,backward,outputs",1177,1253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32029,roebel,2019-08-28T00:13:46Z,2020-02-20T23:00:36Z,tensorflow.keras.Model.compute_output_shape gives wrong results,"
 System information
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 yes
 
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 linux Ubuntu 18.04
 
 
 TensorFlow installed from (source or binary):
 conda
 
 
 TensorFlow version (use command below):
 tried with 1.12.0 and 1.14.0
 
 
 Python version:
 3.6
 
 
 Describe the current behavior
 using a keras model (stored in a variable mm) in tensorflow.keras I would like to calculate the output_shape for a given input. This works correctly only the first time I call mm.compute_output_shape(), the subsequent results for calling the same function with different shapes are inconsistent.
 Using standard keras methods I get different and consistent results.
 An example for the problem is implemented in the tf_bug.py script that you find in the zip
 if you call it without parameters it loads a fully convolutional model
 from a json file (provided in the zip) and does
 <denchmark-code>import json
 import tensorflow.keras as keras
 with open(""model_tf_bug.json"", ""r"") as fi:  
     kk=json.load(fi)  
     mm=keras.models.model_from_json(json.dumps(kk)) 
 
 for n in range(999, 1020):  
      ss=[(1,n,1,1)]
      print(ss,mm.compute_output_shape(input_shape=ss)) 
 </denchmark-code>
 
 the result displaying the input and corresponding output shape on each line is
 <denchmark-code>[(1, 999, 1, 1)] (1, 481, 1, 1)
 [(1, 1000, 1, 1)] (1, 481, 1, 1)
 [(1, 1001, 1, 1)] (1, 482, 1, 1)
 [(1, 1002, 1, 1)] (1, 482, 1, 1)
 [(1, 1003, 1, 1)] (1, 483, 1, 1)
 [(1, 1004, 1, 1)] (1, 483, 1, 1)
 [(1, 1005, 1, 1)] (1, 484, 1, 1)
 [(1, 1006, 1, 1)] (1, 484, 1, 1)
 [(1, 1007, 1, 1)] (1, 482, 1, 1)
 [(1, 1008, 1, 1)] (1, 485, 1, 1)
 ...
 </denchmark-code>
 
 I kept only the relevant lines. You see that after the first lines that are correct
 starting with input shape 1007 the output shape decreases and starts to produce erratic behavior, while for the fully convolutional model it should increase monotonously with the input size.
 Describe the expected behavior
 Running the same script with argument keras uses the vanilla keras version 2.2.4
 and in this case the output shape increases  as expected
 <denchmark-code>[(1, 999, 1, 1)] (1, 481, 1, 1)
 [(1, 1000, 1, 1)] (1, 481, 1, 1)
 [(1, 1001, 1, 1)] (1, 482, 1, 1)
 [(1, 1002, 1, 1)] (1, 482, 1, 1)
 [(1, 1003, 1, 1)] (1, 483, 1, 1)
 [(1, 1004, 1, 1)] (1, 483, 1, 1)
 [(1, 1005, 1, 1)] (1, 484, 1, 1)
 [(1, 1006, 1, 1)] (1, 484, 1, 1)
 [(1, 1007, 1, 1)] (1, 485, 1, 1)
 [(1, 1008, 1, 1)] (1, 485, 1, 1)
 ...
 </denchmark-code>
 
 Note that I can get a correct result with tf.keras as well if I clear the model._output_shape_cache before I compute the output_shape.
 Running the script with argument clear uses a modified loop as follows
 <denchmark-code>for n in range(999, 1020):  
      ss=[(1,n,1,1)]
      if len(sys.argv) > 1 and sys.argv[1] == ""clear"":
          mm._output_shape_cache.clear()
      print(ss,mm.compute_output_shape(input_shape=ss)) 
 </denchmark-code>
 
 The results are correct as expected.
 Looking into the function mm.compute_output_shape
 I found that compared to keras you changed the cache_key generation
 where keras does
 <denchmark-code>cache_key = ', '.join([str(x) for x in input_shapes])
 </denchmark-code>
 
 tf.keras does
 <denchmark-code>    cache_key = generic_utils.object_list_uid(input_shape)
 </denchmark-code>
 
 It appears that the cache_key in tf.keras confuses different input shapes as the same and returns wrong results from the cache.
 Code to reproduce the issue
 You find the script, model and output files in the zip
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/3548512/tf_compute_output_shape_bug.zip>tf_compute_output_shape_bug.zip</denchmark-link>
 
 	",1.0,roebel,2019-09-05T17:27:28Z,"
 		Was able to reproduce the issue. Please find the attachment of github gist <denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/b4519fb3fb254cd7daaecc7095070456/untitled127.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,roebel,2019-11-04T15:16:00Z,"
 		<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
  Did you find the solution for this?
 I have similar problem: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33785>#33785</denchmark-link>
 
 		",3.0,roebel,2020-02-20T23:00:37Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32029>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32029>No</denchmark-link>
 
 		",3ba8bd697faf4b831f78c3fa547d7956f1b1a0aa,Scott Zhu,2020-02-20 14:53:02-08:00,MODIFY,1,tensorflow\python\keras\engine\network.py,tensorflow\python\keras\engine\network.py,1.0,"723,724,725",723,MODIFY,1.0,tensorflow\python\keras\engine\network_test.py,tensorflow\python\keras\engine\network_test.py,,,,,,,,,,,,,1.0,"1872,1873,1874,1875,1876,1877,1878,1879,1880",,test_compute_output_shape_cache,self,1872,1880,MODIFY,1.0,tensorflow\python\keras\utils\generic_utils.py,tensorflow\python\keras\utils\generic_utils.py,1.0,,"759,760,761,762",,,,,,,,compute_output_shape,"self,input_shape",711,783,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,object_list_uid,object_list,759,762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32049,guillaumekln,2019-08-28T13:57:06Z,2019-09-04T23:07:59Z,Creating a boolean constant prints a deprecation warning,"
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution: Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0rc0
 Python version: 3.6
 
 Describe the current behavior
 Creating a boolean constant prints a deprecation warning:
 
 W0828 15:45:36.142576 139852094695168 deprecation.py:323] From /lib/python3.6/site-packages/tensorflow_core/python/framework/constant_op.py:253: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.identity instead.
 
 Describe the expected behavior
 No deprecation warning.
 Code to reproduce the issue
 import tensorflow as tf
 tf.zeros([10], dtype=tf.bool)
 	",1.0,guillaumekln,2019-08-29T05:34:23Z,"
 		Please find the Gist of Colab for <denchmark-link:https://colab.sandbox.google.com/gist/oanush/14d18b393e1bd59d77307a28ec2b0c19/32049.ipynb>TF 2.0rc0</denchmark-link>
 .Thanks!
 		",2.0,guillaumekln,2019-09-04T23:08:00Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32049>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32049>No</denchmark-link>
 
 		",3.0,guillaumekln,2020-01-06T11:55:47Z,"
 		I am still getting this warning on the current version of Tensorflow (I just installed it yesterday). When will this be fixed for mainstream users?
 		",8d4ecb0f24c4f9fc18c248838d2496b8410961f6,Alexandre Passos,2019-09-04 16:07:46-07:00,MODIFY,0,tensorflow\python\framework\constant_op.py,tensorflow\python\framework\constant_op.py,0.0,253,253,,,,,4.0,guillaumekln,2020-09-15T07:23:51Z,"
 		Although far from satisfactory, I managed to avoid the warning through
 tf.cast(tf.zeros((10, )), tf.bool)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32162,csukuangfj,2019-09-03T03:21:37Z,2019-09-10T00:36:53Z,[lite doc] broken link in`TensorFlow Lite and TensorFlow operator compatibility`,"
 The link for  on page <denchmark-link:https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations>https://www.tensorflow.org/lite/guide/ops_compatibility#compatible_operations</denchmark-link>
 
 is broken.
 	",1.0,csukuangfj,2019-09-09T22:35:59Z,"
 		Thanks. Looks like this only exists in TF2: <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/transpose>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/transpose</denchmark-link>
 
 It will (eventually) sort itself out, but it shouldn't be linked in TF1
 		",2.0,csukuangfj,2019-09-09T22:47:55Z,"
 		Ehh, the links on that page should all use backticks for API symbols and not absolute URLs. I'll fix it
 		",3.0,csukuangfj,2019-09-10T00:36:54Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32162>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32162>No</denchmark-link>
 
 		",50f0bb045ca2483c516938a867df806d12b6ee49,Billy Lamberta,2019-09-09 17:36:36-07:00,MODIFY,0,tensorflow\lite\g3doc\guide\ops_compatibility.md,tensorflow\lite\g3doc\guide\ops_compatibility.md,0.0,"19,32,38,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,126,128,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157","19,32,38,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,136,138,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32487,cadama,2019-09-13T06:50:22Z,2019-10-03T00:04:27Z,[TF -2] Multi gpu training error,"
 I am trying to train a keras model on two k80.
 **
 
 Have I written custom code
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): SMP Debian 4.9.144-3.1
 TensorFlow version (use command below): 2.0.0-rc0
 Python version: 3.6.9
 CUDA/cuDNN version: 10.1
 GPU model and memory: Tesla K80
 
 Here is the the keras model that I am trying to fit:
 <denchmark-code>import tensorflow as tf
 import numpy as np
 
 class SparseSlice(tf.keras.layers.Layer):
     def __init__(self, feature_column):
         super(SparseSlice, self).__init__()
         self.fc = feature_column
 
     def build(self, input_shape):
 
         self.kernel = self.add_weight('{}_kernel'.format(self.fc.name), shape=(self.fc.num_buckets, ), dtype=tf.float32)
 
     def call(self, input):
         ids = self.fc._transform_input_tensor(input)
         return tf.expand_dims(tf.gather(self.kernel, ids.values), axis=1)
 
 
 strategy = tf.distribute.MirroredStrategy()
 with strategy.scope():
 
     batch_size = 10
     sparse_col = tf.feature_column.categorical_column_with_hash_bucket('sparse_col', 10000, dtype=tf.int64)
     dense_col = tf.feature_column.numeric_column('dense_col', dtype=tf.float32)
     example_spec = tf.feature_column.make_parse_example_spec([sparse_col, dense_col])
 
     sparse_inputs = tf.keras.layers.Input(name=sparse_col.name, shape=(None, ), batch_size=batch_size, sparse=True, dtype=tf.int64)
     dense_inputs = {dense_col.name: tf.keras.layers.Input(name=dense_col.name, shape=(1, ), dtype=tf.float32)}
 
     sparse_out = SparseSlice(sparse_col)(sparse_inputs)
     output = tf.keras.layers.Dense(1, activation='sigmoid')(sparse_out)
     num = tf.keras.layers.DenseFeatures(dense_col)(dense_inputs)
 
     concats = tf.keras.layers.Concatenate()([output, num])
     output = tf.keras.layers.Dense(1, activation='sigmoid')(concats)
 
     model = tf.keras.Model([dense_inputs, {'sparse_output': sparse_inputs}], output)
 
     model.compile(optimizer='adam',
                   loss='mse')
 
     np.random.random(())
 
     features = {dense_col.name: tf.constant(np.random.random((batch_size, )))}
     features.update({sparse_col.name: tf.sparse.SparseTensor(indices=[[i, 0] for i in range(batch_size)], values=np.random.randint(0, 1000, (batch_size, )), dense_shape=(batch_size, 1))})
     ys = tf.constant(np.random.rand(batch_size), dtype=tf.float32)
 
     dataset = tf.data.Dataset.from_tensor_slices((features, ys)).batch(batch_size)
 
     model.fit(x=dataset,
               epochs=1
               )
 </denchmark-code>
 
 but I am getting the following error:
 <denchmark-code>2019-09-13 06:48:10.524592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.525159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
 name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
 pciBusID: 0000:00:04.0
 2019-09-13 06:48:10.525252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.525673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
 name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235
 pciBusID: 0000:00:05.0
 2019-09-13 06:48:10.525737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
 2019-09-13 06:48:10.525763: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
 2019-09-13 06:48:10.525798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
 2019-09-13 06:48:10.525835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
 2019-09-13 06:48:10.525869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
 2019-09-13 06:48:10.525904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
 2019-09-13 06:48:10.525937: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 2019-09-13 06:48:10.526033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.526541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.527021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.527491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.527907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
 2019-09-13 06:48:10.528002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
 2019-09-13 06:48:10.528023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
 2019-09-13 06:48:10.528036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N Y 
 2019-09-13 06:48:10.528054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   Y N 
 2019-09-13 06:48:10.528240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.528714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.529244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.529670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)
 2019-09-13 06:48:10.529763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1006] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 2019-09-13 06:48:10.530226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/device:GPU:1 with 10805 MB memory) -> physical GPU (device: 1, name: Tesla K80, pci bus id: 0000:00:05.0, compute capability: 3.7)
       1/Unknown - 0s 75ms/stepTraceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3326, in run_code
     exec(code_obj, self.user_global_ns, self.user_ns)
   File ""<ipython-input-7-9c71ae70d829>"", line 33, in <module>
     epochs=1
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 734, in fit
     use_multiprocessing=use_multiprocessing)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 324, in fit
     total_epochs=epochs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 123, in run_one_epoch
     batch_outs = execution_function(iterator)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 86, in execution_function
     distributed_function(input_fn))
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 427, in __call__
     self._initialize(args, kwds, add_initializers_to=initializer_map)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 370, in _initialize
     *args, **kwds))
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 1847, in _get_concrete_function_internal_garbage_collected
     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2147, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2038, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 320, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 66, in distributed_function
     model, input_iterator, mode)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 112, in _prepare_feed_values
     inputs, targets, sample_weights = _get_input_from_iterator(inputs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 132, in _get_input_from_iterator
     next_element = next(iterator)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 275, in __next__
     return self.get_next()
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 304, in get_next
     global_has_value, replicas = _get_next_as_optional(self, self._strategy)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 200, in _get_next_as_optional
     iterator._iterators[i].get_next_as_list(new_name))  # pylint: disable=protected-access
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in get_next_as_list
     lambda: _dummy_tensor_fn(data.value_structure))
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
     return func(*args, **kwargs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/cond_v2.py"", line 91, in cond_v2
     op_return_value=pred)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 878, in <lambda>
     lambda: _dummy_tensor_fn(data.value_structure))
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 801, in _dummy_tensor_fn
     result.append(create_dummy_tensor(feature_shape, feature_type))
   File ""/home/cdalmaso/.local/lib/python3.6/site-packages/tensorflow_core/python/distribute/input_lib.py"", line 784, in create_dummy_tensor
     for dim in feature_shape.dims:
 TypeError: 'NoneType' object is not iterable
 </denchmark-code>
 
 Everything runs fine if I exclude the with strategy.scope()
 	",1.0,cadama,2019-09-13T09:30:44Z,"
 		Issue replicating with TF version-2.0rc0, please find the <denchmark-link:https://colab.research.google.com/gist/oanush/fa05103b0c9f82e26949f3ede67ffaf5/32487.ipynb>gist</denchmark-link>
  of the colab.Thanks!
 		",2.0,cadama,2019-09-30T19:34:39Z,"
 		We have an internal fix for this pending. Meanwhile, adding
 <denchmark-code>strategy = tf.distribute.MirroredStrategy()
 strategy.extended.experimental_enable_get_next_as_optional = False
 </denchmark-code>
 
 should work.
 		",3.0,cadama,2019-10-02T14:45:49Z,"
 		It works indeed. Thanks.
 		",144e0ebb1c9ede81886e215904a4e4598cd8b0b0,Philip Pham,2019-10-02 17:04:12-07:00,MODIFY,1,tensorflow\core\common_runtime\copy_tensor.cc,tensorflow\core\common_runtime\copy_tensor.cc,1.0,311,"311,312,313,314,315,316",MODIFY,0.0,tensorflow\python\distribute\BUILD,tensorflow\python\distribute\BUILD,4.0,cadama,2019-10-03T00:04:29Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32487>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32487>No</denchmark-link>
 
 		",5.0,cadama,2019-11-20T11:21:33Z,"
 		I also met this problem, and use above the internal fix, thanks !
 still I have few questions:
 
 is this a bug and will be fixed in the later version or this is the way to use it?
 what causes this ?
 if i don't set this option, but to do some changes to the dataset, will it work as expected ?
 
 		",,,,,0.0,"446,449,708,709,711,712,713,722,723",,,,,,MODIFY,5.0,tensorflow\python\distribute\input_lib.py,tensorflow\python\distribute\input_lib.py,1.0,913,"883,884",MODIFY,7.0,tensorflow\python\distribute\input_lib_test.py,tensorflow\python\distribute\input_lib_test.py,1.0,"516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604",,tensorflow::WrappedTensorDeviceCopy,"from,to,copy",308,318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get_next_as_list,"self,name",882,919,testRaggedSparse,"self,distribution,input_type,drop_remainder,defun",516,604,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,801,"782,784,790,791,792,793,794,795,796",_dummy_tensor_fn.create_dummy_tensor,"feature_shape,feature_type",782,801,1.0,"801,803,804,805,806,807,808,809,810,811,812,818,819,820,821,824,825,826,827,828,829,832,833,834,835,836,837,838,841","803,804,805,806,807,808,809,810,811",_dummy_tensor_fn,value_structure,798,841,1.0,"331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349",330,get_next,"self,name",281,352,1.0,"583,584,585,586,587",,testRaggedSparse.sum_for_loop,dataset,583,587,1.0,"566,567,568,569,570,571,572,573,574,575,576,577",,testRaggedSparse.sum_batch,per_replica_features,566,577,1.0,"579,580,581",,testRaggedSparse._reduce,"state,batch",579,581,1.0,"569,570,571,572,573,574,575",,testRaggedSparse.testRaggedSparse.sum_batch.map_fn,per_replica_values,569,575,1.0,"589,590,591,592,593,594,595",,testRaggedSparse.sum_while_loop,"iterator,reduce_fn",589,595,1.0,"524,525,526,527,528,529,530,531,532,533,534,535,536",,testRaggedSparse.dataset_fn,ctx,524,536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"801,803,804,805,806,807,808,809,810,811,812,818,819,820,821,824,825,826,827,828,829,832,833,834,835,836,837,838","803,804,805,806,807,808,809,810,811",_dummy_tensor_fn.create_dummy_tensor,type_spec,801,839,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32501,drasmuss,2019-09-13T15:55:45Z,2019-09-17T21:31:07Z,Error when using stateful RNN with multiple inputs,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0rc0
 Python version: 3.6.8
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: 10.0.130/7.6.0
 GPU model and memory: GTX 980 Ti
 
 Describe the current behavior
 The stock example of RNNs with multiple inputs from here <denchmark-link:https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs>https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs</denchmark-link>
  produces an error if you set .  This seems to be a problem with any multi-input RNN with stateful=True.
 Describe the expected behavior
 There should be no error, multi-input RNNs with stateful=True should work the same as with stateful=False (other than preserving state).
 Code to reproduce the issue
 Note, this code is copied from <denchmark-link:https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs>https://www.tensorflow.org/beta/guide/keras/rnn#rnns_with_listdict_inputs_or_nested_inputs</denchmark-link>
 , with the exception that I changed the line
 <denchmark-code>rnn = tf.keras.layers.RNN(cell)
 </denchmark-code>
 
 to
 <denchmark-code>rnn = tf.keras.layers.RNN(cell, stateful=True)
 </denchmark-code>
 
 import collections
 
 import tensorflow as tf
 
 NestedInput = collections.namedtuple(""NestedInput"", [""feature1"", ""feature2""])
 NestedState = collections.namedtuple(""NestedState"", [""state1"", ""state2""])
 
 
 class NestedCell(tf.keras.layers.Layer):
     def __init__(self, unit_1, unit_2, unit_3, **kwargs):
         self.unit_1 = unit_1
         self.unit_2 = unit_2
         self.unit_3 = unit_3
         self.state_size = NestedState(
             state1=unit_1, state2=tf.TensorShape([unit_2, unit_3])
         )
         self.output_size = (unit_1, tf.TensorShape([unit_2, unit_3]))
         super(NestedCell, self).__init__(**kwargs)
 
     def build(self, input_shapes):
         # expect input_shape to contain 2 items, [(batch, i1), (batch, i2, i3)]
         input_1 = input_shapes.feature1[1]
         input_2, input_3 = input_shapes.feature2[1:]
 
         self.kernel_1 = self.add_weight(
             shape=(input_1, self.unit_1), initializer=""uniform"", name=""kernel_1""
         )
         self.kernel_2_3 = self.add_weight(
             shape=(input_2, input_3, self.unit_2, self.unit_3),
             initializer=""uniform"",
             name=""kernel_2_3"",
         )
 
     def call(self, inputs, states):
         # inputs should be in [(batch, input_1), (batch, input_2, input_3)]
         # state should be in shape [(batch, unit_1), (batch, unit_2, unit_3)]
         input_1, input_2 = tf.nest.flatten(inputs)
         s1, s2 = states
 
         output_1 = tf.matmul(input_1, self.kernel_1)
         output_2_3 = tf.einsum(""bij,ijkl->bkl"", input_2, self.kernel_2_3)
         state_1 = s1 + output_1
         state_2_3 = s2 + output_2_3
 
         output = [output_1, output_2_3]
         new_states = NestedState(state1=state_1, state2=state_2_3)
 
         return output, new_states
 
 
 unit_1 = 10
 unit_2 = 20
 unit_3 = 30
 
 input_1 = 32
 input_2 = 64
 input_3 = 32
 batch_size = 64
 num_batch = 100
 timestep = 50
 
 cell = NestedCell(unit_1, unit_2, unit_3)
 rnn = tf.keras.layers.RNN(cell, stateful=True)
 
 inp_1 = tf.keras.Input((None, input_1))
 inp_2 = tf.keras.Input((None, input_2, input_3))
 
 outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))
 
 model = tf.keras.models.Model([inp_1, inp_2], outputs)
 
 model.compile(optimizer=""adam"", loss=""mse"", metrics=[""accuracy""])
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File "".../tmp2.py"", line 70, in <module>
     outputs = rnn(NestedInput(feature1=inp_1, feature2=inp_2))
   File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 623, in __call__
     return super(RNN, self).__call__(inputs, **kwargs)
   File ""...\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 777, in __call__
     self._maybe_build(inputs)
   File ""...\site-packages\tensorflow_core\python\keras\engine\base_layer.py"", line 2099, in _maybe_build
     self.build(input_shapes)
   File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 561, in build
     self.reset_states()
   File ""...\site-packages\tensorflow_core\python\keras\layers\recurrent.py"", line 809, in reset_states
     spec_shape = None if self.input_spec is None else self.input_spec[0].shape
 AttributeError: 'NestedInput' object has no attribute 'shape'
 </denchmark-code>
 
 	",1.0,drasmuss,2019-09-16T09:03:54Z,"
 		Issue replicating for TF version-2.0rc0 and also 2.0rc1, please find the <denchmark-link:https://colab.sandbox.google.com/gist/oanush/8b07187ad22ba68f2bdd7f45a058d851/32501.ipynb>gist</denchmark-link>
  of the colab.Thanks!
 		",2.0,drasmuss,2019-09-16T12:40:42Z,"
 		This issue is present in 1.14.0 as well (and probably earlier, but the RNN api was different so the example doesn't run for other reasons).
 		",3.0,drasmuss,2019-09-17T17:07:51Z,"
 		Thanks for reporting the issue. Let me take a look.
 		",38b748907e04fb212c1183b4999425d768de0233,Scott Zhu,2019-09-17 12:26:17-07:00,MODIFY,1,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,"822,823,824",822,MODIFY,2.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,4.0,drasmuss,2019-09-17T21:31:08Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32501>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32501>No</denchmark-link>
 
 		",,,,,,,,,1.0,"1021,1028,1030,1031,1032,1037,1038,1049,1050,1054,1056,1057,1062,1063,1075,1076","1027,1029,1030,1035,1036,1047,1051,1053,1054,1059,1060,1072",test_nested_input_output,"self,stateful",1021,1076,,,,,,,,,,,,,,,reset_states,"self,states",819,873,,,,,,,,,,,,,,,,,,,,,,1.0,"1020,1021,1028,1030,1031,1032,1037,1038,1049,1050,1054,1056,1057,1062,1063","1020,1027,1029,1030,1035,1036,1047,1051,1053,1054,1059,1060,1072",test_nested_input_output,self,1020,1072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32543,ExpectationMax,2019-09-15T16:26:56Z,2019-09-25T20:24:34Z,RNN layer does not reset dropout masks of RNNCell,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave 10.14.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below):  v1.12.1-9392-gf3c7314d83 1.15.0-rc0
 Python version: 3.7.4
 
 Describe the current behavior
 The RNN layer with an RNNCell does not reset the states of dropout masks compared to the layer implementations of the cells. Thus the behavior of tf.keras.layers.GRU(10) != tf.keras.layers.RNN(tf.keras.layers.GRUCell(10)).
 This is especially problematic, because the <denchmark-link:https://www.tensorflow.org/beta/guide/keras/rnn#rnn_layers_and_rnn_cells>Keras RNN API tutorial</denchmark-link>
  states both approaches are mathematically equivalent.
 Describe the expected behavior
 The RNN layer should check the type of the RNNCell and, if it is a subclass of DropoutRNNCellMixin, reset the dropout masks after each call. By calling cell.reset_recurrent_dropout_mask() and cell.reset_dropout_mask.
 
 Partially copied from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/29391>#29391</denchmark-link>
 
 from __future__ import absolute_import, division, print_function
 import numpy as np
 
 import tensorflow as tf
 tf.enable_eager_execution()
 
 
 tf.enable_eager_execution()
 print(tf.__version__)
 data = np.random.normal(0, 1, (1, 10, 2)).astype(np.float32)
 rnn = tf.keras.layers.GRU(units=10, dropout=0.5,
                                    recurrent_dropout=0.5)
 print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))
 
 rnn_cell = tf.keras.layers.GRUCell(units=10, dropout=0.5,
                                    recurrent_dropout=0.5)
 rnn = tf.keras.layers.RNN(rnn_cell)
 print(set([rnn(data, training=True).numpy()[0, 0] for _ in range(5)]))
 Output:
 <denchmark-code>WARNING:tensorflow:From check_dropout.py:5: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.
 
 1.15.0-rc0
 {0.04537238, 0.15487108, 0.0, 0.08881481, 0.055508718}  # Different dropout mask was used for each call
 {-0.34464198}  # Same dropout mask was used for each call
 </denchmark-code>
 
 	",1.0,ExpectationMax,2019-09-16T06:20:45Z,"
 		I have tried on colab with TF 1.15.0-rc0 , nightly versions and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/d29fc400d2933fbf3efedfe1311f9a83/untitled186.ipynb>here</denchmark-link>
 .Thanks!
 		",2.0,ExpectationMax,2019-09-24T23:13:17Z,"
 		Thanks for reporting the issue, let me fix it.
 		",3.0,ExpectationMax,2019-09-25T20:24:35Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32543>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32543>No</denchmark-link>
 
 		",2eb6dc0f2e7f5455d368c59c35458709eef03a55,Scott Zhu,2019-09-25 13:13:18-07:00,MODIFY,1,tensorflow\python\keras\layers\convolutional_recurrent.py,tensorflow\python\keras\layers\convolutional_recurrent.py,1.0,934,"934,935",MODIFY,2.0,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,,,,,,,,,,,,,1.0,1534,,call,"self,inputs,mask,training,initial_state",1533,1536,MODIFY,2.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,1.0,"824,825,826,827,828,829,830,831,833,834,835,836,837,838,839,840,841,842,843,844",824,MODIFY,1.0,tensorflow\python\keras\layers\recurrent_v2.py,tensorflow\python\keras\layers\recurrent_v2.py,1.0,388,"388,389",call,"self,inputs,mask,training,initial_state",933,938,,,,,,,,,,,,,,,,,,,,,,1.0,"874,875,876,877",,_maybe_reset_cell_dropout_mask,"self,cell",874,877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_dropout_mask_reuse.verify,rnn_layer,824,844,call,"self,inputs,mask,training,initial_state",370,425,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847","798,799,800,801,802,803,804,805,806,807,808,809,810,811,813,814,815,816,817,818,819,820,821,822,823,824",test_dropout_mask_reuse,self,794,847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32570,neonrights,2019-09-17T01:59:08Z,2019-09-27T16:34:11Z,Assertion error when using mask with unrolled stacked LSTM,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): 1.14
 Python version: 3.6
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: no GPU, 32 GB ram
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 I receive an assertion error when creating a forward pass for an unrolled multi-layer LSTM while using a mask.
 Describe the expected behavior
 No assertion error, or at least a better explanation as to the cause.
 Code to reproduce the issue
 import tensorflow as tf
 
 inputs = tf.placeholder(tf.float32, (3,4,5))
 mask = tf.placeholder(tf.bool, (3,4))
 
 single_cells = [tf.keras.layer.LSTMCell(10) for _ in range(3)]
 multi_cell = tf.keras.layers.StackedRNNCell(cells=single_cells)
 lstm = tf.keras.layers.RNN(cell=multi_cell, unroll=True)
 output, state = lstm(inputs=inputs, mask=mask)  # <- assertion error occurs here
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,neonrights,2019-09-17T10:03:33Z,"
 		<denchmark-link:https://github.com/neonrights>@neonrights</denchmark-link>
 
 I tried in colab with TF version 1.14 and 1.15.0-rc0 and i am getting the below error . I am attaching the <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/9603c7f3d9814aa73513ef4db56f94b0/untitled193.ipynb>gist </denchmark-link>
 for your reference. Thanks!
 		",2.0,neonrights,2019-09-17T19:06:52Z,"
 		Sorry I have a typo in my code, try this instead
 import tensorflow as tf
 
 inputs = tf.placeholder(tf.float32, (3,4,5))
 mask = tf.placeholder(tf.bool, (3,4))
 
 single_cells = [tf.keras.layers.LSTMCell(10) for _ in range(3)]
 multi_cell = tf.keras.layers.StackedRNNCells(cells=single_cells)
 lstm = tf.keras.layers.RNN(cell=multi_cell, unroll=True)
 output, state = lstm(inputs=inputs, mask=mask)  # <- assertion error occurs here
 		",3.0,neonrights,2019-09-18T08:57:53Z,"
 		I have tried on colab with TF version 1.14 ,1.15.0-rc0 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e202ba52a64cfd2a3fb1f51bd50f9b67/untitled200.ipynb>here</denchmark-link>
 .Thanks!
 		",4d011793076f6c62a560085b8fe03fbf732d67e6,Scott Zhu,2019-09-25 14:16:57-07:00,MODIFY,1,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,1.0,"3909,3910,3911,3912","3909,3910",MODIFY,4.0,tensorflow\python\keras\layers\lstm_test.py,tensorflow\python\keras\layers\lstm_test.py,4.0,neonrights,2019-09-27T16:34:12Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32570>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32570>No</denchmark-link>
 
 		",,,,,,,,,1.0,"152,153,160,161","151,158",test_masking_with_stacking_LSTM,self,151,164,,,,,,,,,,,,,,,_expand_mask,"mask_t,input_t,fixed_dim",3908,3917,,,,,,,,,,,,,,,,,,,,,,1.0,"137,144",143,test_with_masking_layer_LSTM,"self,unroll",137,150,1.0,"136,137,144","136,143",test_with_masking_layer_LSTM,self,136,149,1.0,"153,160,161",158,test_masking_with_stacking_LSTM,"self,unroll",153,167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32586,guillaumekln,2019-09-17T12:35:38Z,2019-09-17T22:09:08Z,RNN does not forward the training flag to StackedRNNCells,"
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution: Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: 2.0.0rc0
 Python version: 3.6.6
 
 Describe the current behavior
 When using tf.keras.layers.StackedRNNCells with tf.keras.layers.RNN, the RNN layer does not forward the training flag to the cell. This is because the RNN code checks that cell explictly defines the training flag as argument, which tf.keras.layers.StackedRNNCells does not.
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py#L709-L710>https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/layers/recurrent.py#L709-L710</denchmark-link>
 
 Describe the expected behavior
 The training flag should be passed to tf.keras.layers.StackedRNNCells, and to each stacked cell.
 Code to reproduce the issue
 The code below should not raise the AssertionError:
 import tensorflow as tf
 
 class CellWrapper(tf.keras.layers.AbstractRNNCell):
 
     def __init__(self, cell):
         super(CellWrapper, self).__init__()
         self.cell = cell
 
     @property
     def state_size(self):
         return self.cell.state_size
 
     @property
     def output_size(self):
         return self.cell.output_size
 
     def get_initial_state(self, inputs=None, batch_size=None, dtype=None):
         return self.cell.get_initial_state(
             inputs=inputs, batch_size=batch_size, dtype=dtype)
 
     def call(self, inputs, states, training=None, **kwargs):
         assert training is not None
 
 
 cell = tf.keras.layers.LSTMCell(32)
 cell = CellWrapper(cell)
 cell = tf.keras.layers.StackedRNNCells([cell])
 
 rnn = tf.keras.layers.RNN(cell)
 inputs = tf.random.uniform([4, 7, 16])
 rnn(inputs, training=True)
 	",1.0,guillaumekln,2019-09-17T18:50:04Z,"
 		Was able to reproduce this issue. Please find the attachment of github gist <denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/6e1f7a532de56ec863606e7f1657f1c3/untitled144.ipynb>here</denchmark-link>
 .
 		",2.0,guillaumekln,2019-09-17T22:09:09Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=32586>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=32586>No</denchmark-link>
 
 		",,,,,df2b252fa380994cd9236cc56b06557bcf12a9d3,Scott Zhu,2019-09-17 15:06:57-07:00,MODIFY,2,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,"122,135,136,137,138",122,MODIFY,7.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,,,,,,,,,,,,,1.0,"872,873,874",,test_stacked_rnn_with_training_param.build,"self,input_shape",872,874,MODIFY,0.0,tensorflow\tools\api\golden\v1\tensorflow.keras.layers.-stacked-r-n-n-cells.pbtxt,tensorflow\tools\api\golden\v1\tensorflow.keras.layers.-stacked-r-n-n-cells.pbtxt,0.0,155,155,MODIFY,0.0,tensorflow\tools\api\golden\v2\tensorflow.keras.layers.-stacked-r-n-n-cells.pbtxt,tensorflow\tools\api\golden\v2\tensorflow.keras.layers.-stacked-r-n-n-cells.pbtxt,0.0,155,155,call,"self,inputs,states,constants,kwargs",122,143,1.0,"122,135,136,137,138",122,call,"self,inputs,states,constants,training,kwargs",122,147,,,,,,,,,,,,,,,1.0,"860,861,862",,test_stacked_rnn_with_training_param.__init__,"self,cell",860,862,1.0,"880,881,882",,test_stacked_rnn_with_training_param.call,"self,inputs,states,training,kwargs",880,882,1.0,"855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890",,test_stacked_rnn_with_training_param,self,855,890,1.0,"869,870",,test_stacked_rnn_with_training_param.output_size,self,869,870,1.0,"876,877,878",,test_stacked_rnn_with_training_param.get_initial_state,"self,inputs,batch_size,dtype",876,878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"865,866",,test_stacked_rnn_with_training_param.state_size,self,865,866,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32755,MattConley,2019-09-24T02:09:01Z,2019-10-04T04:39:19Z,"Bincount Op test ""test_negative"" fails with TF 2.0","
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): Binary
 TensorFlow version (use command below): 2.0.0-rc1
 Python version: 3.6
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: 10.0
 GPU model and memory: RTX 2080 Ti, 11G
 
 Describe the current behavior
 The test_negative test in tensorflow/python/kernel_tests/bincount_op_test.py fails, as the bincount call with negative values does not throw an InvalidArgumentError.
 This behavior might be the result of the op being called on the GPU, as only the CPU call is expected to throw the error, as per the comment here: <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/kernel_tests/bincount_op_test.py#L107>https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/kernel_tests/bincount_op_test.py#L107</denchmark-link>
 .
 Setting CUDA_VISIBLE_DEVICES to be empty forces the op to run on CPU and the test passes (the invalid argument error is successfully thrown), but passing use_gpu=False as an option to the session wrapper does not have this effect.
 Describe the expected behavior
 The test_negative test should pass, as the call to bincount with a negative input value is expected to throw an InvalidArgumentError.
 Code to reproduce the issue
 Run the python test tensorflow/python/kernel_tests/bincount_op_test.py.
 	",1.0,MattConley,2019-09-25T08:04:58Z,"
 		<denchmark-link:https://github.com/MattConley>@MattConley</denchmark-link>
 , Will it be possible to provide the sample standalone code to replicate the issue. Thanks!
 		",2.0,MattConley,2019-09-25T20:56:32Z,"
 		<denchmark-link:https://github.com/gadagashwini>@gadagashwini</denchmark-link>
  Unfortunately the minimal repro I have at the moment is the bincount_op_test itself, specifically  (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/bincount_op_test.py#L106-L110>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/bincount_op_test.py#L106-L110</denchmark-link>
 ).
 The issue might be with the test framework itself, as internal session wrappers are used even with TF 2.0.
 Also, setting CUDA_VISIBLE_DEVICES to be empty forces the op to run on CPU and the test passes (the invalid argument error is successfully thrown), but passing use_gpu=False as an option to the session wrapper does not have this effect.
 		",3.0,MattConley,2019-09-27T23:11:55Z,"
 		I was able to reproduce the error reported in TF 2.0 nightly version '2.0.0-dev20190924'
 OP_REQUIRES failed at bincount_op.cc:111 : Invalid argument: Input arr must be non-negative!
 		",9b5f9aabd84b0d1db782286143c560792d711449,Reed Wanderman-Milne,2019-10-03 21:39:01-07:00,MODIFY,0,tensorflow\python\kernel_tests\BUILD,tensorflow\python\kernel_tests\BUILD,0.0,170,170,MODIFY,1.0,tensorflow\python\kernel_tests\bincount_op_test.py,tensorflow\python\kernel_tests\bincount_op_test.py,4.0,MattConley,2019-10-02T21:04:49Z,"
 		<denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
  can you take a look at this?
 		",5.0,MattConley,2019-10-04T04:39:20Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32755>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/32755>No</denchmark-link>
 
 		",,,,,1.0,109,108,test_negative,self,107,111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3277,rueberger,2016-07-11T22:00:32Z,2016-07-13T21:04:06Z,Broadcast 0-rank tensors when computing gradients for tf.nn.relu,"
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: OSX (macOS....?), CPU only, version 0.9.0
 Perhaps this is desired behavior, but I would have much appreciated a more descriptive warning at least, which would have saved much debugging.
 I haven't found a small reproducible case for this: but in the code I originally found this bug, no error is raised as the other variables are trained, leaving me scratching my head as to why the linear rectified variable was not being trained.
 The same issue also occurs for tf.nn.softplus, and perhaps other methods as well.
 <denchmark-h:h3>Steps to reproduce</denchmark-h>
 
 <denchmark-code>import tensorflow as tf
 sess = tf.Session()
 
 x = tf.Variable(100.)
 y = tf.nn.relu(x)
 loss = y ** 2
 optimizer = tf.train.AdamOptimizer(learning_rate=0.1)
 train_op = optimizer.minimize(loss)
 sess.run(tf.initialize_all_variables())
 
 sess.run(train_op)
 
 ---------------------------------------------------------------------------
 InvalidArgumentError                      Traceback (most recent call last)
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
     729     try:
 --> 730       return fn(*args)
     731     except errors.OpError as e:
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
     711                                  feed_dict, fetch_list, target_list,
 --> 712                                  status, run_metadata)
     713 
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/contextlib.py in __exit__(self, type, value, traceback)
      65             try:
 ---> 66                 next(self.gen)
      67             except StopIteration:
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/errors.py in raise_exception_on_not_ok_status()
     449           compat.as_text(pywrap_tensorflow.TF_Message(status)),
 --> 450           pywrap_tensorflow.TF_GetCode(status))
     451   finally:
 
 InvalidArgumentError: We only handle up to Tensor::dims() up to 8, not 0
      [[Node: gradients_29/Relu_5_grad/ReluGrad = ReluGrad[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients_29/pow_9_grad/tuple/control_dependency, Relu_5)]]
 
 During handling of the above exception, another exception occurred:
 
 InvalidArgumentError                      Traceback (most recent call last)
 <ipython-input-51-ea082b2869a4> in <module>()
 ----> 1 sess.run(train_op)
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
     380     try:
     381       result = self._run(None, fetches, feed_dict, options_ptr,
 --> 382                          run_metadata_ptr)
     383       if run_metadata:
     384         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
     653     movers = self._update_with_movers(feed_dict_string, feed_map)
     654     results = self._do_run(handle, target_list, unique_fetches,
 --> 655                            feed_dict_string, options, run_metadata)
     656 
     657     # User may have fetched the same tensor multiple times, but we
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
     721     if handle is None:
     722       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
 --> 723                            target_list, options, run_metadata)
     724     else:
     725       return self._do_call(_prun_fn, self._session, handle, feed_dict,
 
 /Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
     741         except KeyError:
     742           pass
 --> 743       raise type(e)(node_def, op, message)
     744 
     745   def _extend_graph(self):
 
 InvalidArgumentError: We only handle up to Tensor::dims() up to 8, not 0
      [[Node: gradients_29/Relu_5_grad/ReluGrad = ReluGrad[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""](gradients_29/pow_9_grad/tuple/control_dependency, Relu_5)]]
 Caused by op 'gradients_29/Relu_5_grad/ReluGrad', defined at:
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
     ""__main__"", mod_spec)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/runpy.py"", line 85, in _run_code
     exec(code, run_globals)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/__main__.py"", line 3, in <module>
     app.launch_new_instance()
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/traitlets/config/application.py"", line 596, in launch_instance
     app.start()
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/kernelapp.py"", line 442, in start
     ioloop.IOLoop.instance().start()
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/zmq/eventloop/ioloop.py"", line 162, in start
     super(ZMQIOLoop, self).start()
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tornado/ioloop.py"", line 883, in start
     handler_func(fd_obj, events)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
     return fn(*args, **kwargs)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
     self._handle_recv()
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
     self._run_callback(callback, msg)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
     callback(*args, **kwargs)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
     return fn(*args, **kwargs)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 276, in dispatcher
     return self.dispatch_shell(stream, msg)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 228, in dispatch_shell
     handler(stream, idents, msg)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/kernelbase.py"", line 391, in execute_request
     user_expressions, allow_stdin)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/ipykernel/ipkernel.py"", line 199, in do_execute
     shell.run_cell(code, store_history=store_history, silent=silent)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2723, in run_cell
     interactivity=interactivity, compiler=compiler, result=result)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2825, in run_ast_nodes
     if self.run_code(code, result):
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2885, in run_code
     exec(code_obj, self.user_global_ns, self.user_ns)
   File ""<ipython-input-49-21a2f038fac9>"", line 5, in <module>
     train_op = optimizer.minimize(loss)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py"", line 193, in minimize
     grad_loss=grad_loss)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/training/optimizer.py"", line 250, in compute_gradients
     colocate_gradients_with_ops=colocate_gradients_with_ops)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/ops/gradients.py"", line 482, in gradients
     in_grads = _AsList(grad_fn(op, *out_grads))
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/ops/nn_grad.py"", line 233, in _ReluGrad
     return gen_nn_ops._relu_grad(grad, op.outputs[0])
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1374, in _relu_grad
     features=features, name=name)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
     op_def=op_def)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2297, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1231, in __init__
     self._traceback = _extract_stack()
 
 ...which was originally created as op 'Relu_5', defined at:
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
     ""__main__"", mod_spec)
 [elided 17 identical lines from previous traceback]
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/IPython/core/interactiveshell.py"", line 2885, in run_code
     exec(code_obj, self.user_global_ns, self.user_ns)
   File ""<ipython-input-49-21a2f038fac9>"", line 2, in <module>
     y = tf.nn.relu(x)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1312, in relu
     result = _op_def_lib.apply_op(""Relu"", features=features, name=name)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
     op_def=op_def)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2297, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/Users/andrew/anaconda/envs/tf_dev/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1231, in __init__
     self._traceback = _extract_stack()
 </denchmark-code>
 
 <denchmark-h:h3>What have you tried?</denchmark-h>
 
 The problem is resolved by expanding the dimensions of x:
 <denchmark-code>import tensorflow as tf
 sess = tf.Session()
 
 x = tf.Variable(100.)
 y = tf.nn.relu(tf.expand_dims(x, 0))
 loss = y ** 2
 optimizer = tf.train.AdamOptimizer(learning_rate=0.1)
 train_op = optimizer.minimize(loss)
 sess.run(tf.initialize_all_variables())
 
 sess.run(train_op)
 # runs fine
 </denchmark-code>
 
 I wonder if it would be possible to do this automatically?
 	",1.0,rueberger,2016-07-12T00:50:51Z,"
 		So I checked this out, does seem to be a bug. The good news is it looks like it might be really easy to fix.
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/numeric_op.h#L90>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/numeric_op.h#L90</denchmark-link>
 
 Just add NDIM_CASE(0); on line 90 and the template magic should take care of the rest.
 I'll work on getting this submitted.
 		",2.0,rueberger,2016-07-13T15:49:32Z,"
 		Looks like the fix got pushed. Could you check whether this solves the issue with your more complicated example?
 Thank you for the awesome bug report!
 		",3.0,rueberger,2016-07-14T14:29:16Z,"
 		Yup! All is well. Thanks for quickly dispatching this one!
 		",5d5db35ed2e9e90b95ab27f8b37898fd4543457f,A. Unique TensorFlower,2016-07-12 16:17:59-07:00,MODIFY,1,tensorflow\core\framework\numeric_op.h,tensorflow\core\framework\numeric_op.h,1.0,91,,MODIFY,1.0,tensorflow\python\kernel_tests\relu_op_test.py,tensorflow\python\kernel_tests\relu_op_test.py,,,,,,,,,,,,,1.0,"127,128,129,130,131,132,133,134,135,136",,testGradientScalar,self,127,136,,,,,,,,,,,,,,,tensorflow::BinaryElementWiseOp::Compute,context,72,107,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
32923,sharvil,2019-09-30T18:13:33Z,2019-10-09T05:51:49Z,No documentation for tf.strings.reduce_join,"
 Thank you for submitting a TensorFlow documentation issue. Per our GitHub
 policy, we only address code/doc bugs, performance issues, feature requests, and
 build/installation issues on GitHub.
 The TensorFlow docs are open source! To get involved, read the documentation
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/strings/reduce_join>https://www.tensorflow.org/api_docs/python/tf/strings/reduce_join</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 Add documentation for this method.
 <denchmark-h:h3>Clear description</denchmark-h>
 
 <denchmark-h:h3>Correct links</denchmark-h>
 
 Is the link to the source code correct?
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Are all parameters defined and formatted correctly?
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Are return values defined?
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Are the errors defined? For example,
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises</denchmark-link>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Is there a usage example?
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 Are there currently visuals? If not, will it clarify the content?
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Are you planning to also submit a pull request to fix the issue? See the docs
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
  and the
 docs style guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_style>https://www.tensorflow.org/community/contribute/docs_style</denchmark-link>
 
 	",1.0,sharvil,2019-10-02T20:45:23Z,"
 		Thanks for the report.
 <denchmark-link:https://github.com/sharvil>@sharvil</denchmark-link>
  this page4 is generated from the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/string_ops.py#L347-L363>source here</denchmark-link>
 .
 Any chance you could send me a PR to fix it?
 Note that we support <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>python doctest for code examples</denchmark-link>
  now.
 		",2.0,sharvil,2019-10-04T06:20:26Z,"
 		Sorry Mark, I have neither the setup nor the process know-how to send a PR for this issue. It would be more efficient for someone else to take this on.
 On another note, you mentioned in another doc issue that it's not worth the effort to go back and fix documentation bugs for non-current TF releases. Instead, is it possible to add a regression test to make sure that documentation doesn't get dropped in new TF releases (like what happened in this issue and  <denchmark-link:https://github.com/tensorflow/tensorflow/issues/32528>#32528</denchmark-link>
 )?
 		",3.0,sharvil,2019-10-04T14:35:02Z,"
 		I'll take care of this one.
 For simple things you can often just hit the little ""edit"" button (pencil button). Spend 5 or 10 minutes trying it out to find useful examples. Paste them in using doctest format. Follow the pattern of other docstrings.
 Do try it sometime.
 
 add a regression test to make sure that documentation doesn't get dropped in new TF releases.
 
 The problem with this sort of test, how can you tell what's right?
 When I caused the error in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/32528>#32528</denchmark-link>
 , I rebuilt the docs: ~1000 files changed. The computer can't tell what's right. Not many use a  constructor. I didn't see it.
 		",de38438ba192145ec7d5a04257e0935b96b2053f,Mark Daoust,2019-10-08 22:51:32-07:00,MODIFY,0,tensorflow\python\ops\string_ops.py,tensorflow\python\ops\string_ops.py,0.0,"355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33094,luvwinnie,2019-10-06T23:45:43Z,2019-10-25T18:28:58Z,Tensorflow 2.0.0 / tf.keras.layers.TimeDistributed layer can't be save to saved Model,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution : Colaboratory (GPU Runtime)
 tensorflow version: 2.0.0
 python version: 3.6.8
 
 Describe the current behavior
 The model defined which has tf.keras.layers.timeDistributed layer cannot be save by model.save() function.
 It shows the error below
 <denchmark-code>ValueError                                Traceback (most recent call last)
 /usr/lib/python3.6/inspect.py in getfullargspec(func)
    1125                                        skip_bound_arg=False,
 -> 1126                                        sigcls=Signature)
    1127     except Exception as ex:
 
 41 frames
 ValueError: no signature found for builtin <tensorflow.python.keras.saving.saved_model.save_impl.LayerCall object at 0x7f74467284a8>
 
 The above exception was the direct cause of the following exception:
 
 TypeError                                 Traceback (most recent call last)
 /usr/lib/python3.6/inspect.py in getfullargspec(func)
    1130         # else. So to be fully backwards compatible, we catch all
    1131         # possible exceptions here, and reraise a TypeError.
 -> 1132         raise TypeError('unsupported callable') from ex
    1133 
    1134     args = []
 
 TypeError: unsupported callable
 </denchmark-code>
 
 Describe the expected behavior
 In tensorflow 2.0.0 supposed the model.save model default to be saved in SavedModel format.
 Code to reproduce the issue
 <denchmark-code>
 def get_data():
   datasets, ds_info = tfds.load(name='mnist', with_info=True, as_supervised=True)
   mnist_train, mnist_test = datasets['train'], datasets['test']
 
   BUFFER_SIZE = 10000
 
   BATCH_SIZE_PER_REPLICA = 64
   BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync
 
   def scale(image, label):
     image = tf.cast(image, tf.float32)
     image /= 255
 
     return image, label
 
   train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
   eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)
 
   return train_dataset, eval_dataset
 
 def get_model():
   with mirrored_strategy.scope():
     model = tf.keras.Sequential([
         tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
         tf.keras.layers.MaxPooling2D(),
         # tf.keras.layers.Flatten(),
         tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='softmax')),
         # tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(64, activation='softmax')),
         # tf.keras.layers.Dense(10, activation='softmax')
     ])
 
     model.compile(loss='sparse_categorical_crossentropy',
                   optimizer=tf.keras.optimizers.Adam(),
                   metrics=['accuracy'])
     return model
 
 model = get_model()
 model.save(""test_save"")
 </denchmark-code>
 
 Other info / logs
 The not only can be reproduce in Colaboratory, but also in normal Ubuntu machien which installed with tensorflow-gpu 2.0.0
 	",1.0,luvwinnie,2019-10-09T11:19:32Z,"
 		Could reproduce this issue with TF Version 2.0. Here is the <denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/c698628e49bce4d456daac7c2ddf3bea/33094.ipynb>Gist</denchmark-link>
 .
 		",2.0,luvwinnie,2019-10-18T21:39:02Z,"
 		Also running into this. Here's a concise test case:
 <denchmark-code>model = tf.keras.Sequential([tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1), input_shape=(None, 1))])
 model.save('test_save')
 </denchmark-code>
 
 		",3.0,luvwinnie,2019-10-23T01:48:41Z,"
 		does this issues got any progress?
 		",66b9b602af695fddb76c113d823d2fa4d0646c04,Katherine Wu,2019-10-25 11:27:57-07:00,MODIFY,1,tensorflow\python\keras\saving\saved_model\saved_model_test.py,tensorflow\python\keras\saving\saved_model\saved_model_test.py,1.0,"620,621,622,623,624,625,626,627,628,629,630,631",,MODIFY,0.0,tensorflow\python\keras\saving\saved_model\utils.py,tensorflow\python\keras\saving\saved_model\utils.py,4.0,luvwinnie,2019-10-24T23:52:21Z,"
 		Any update?
 		",5.0,luvwinnie,2019-10-25T00:38:22Z,"
 		This appears to be an error with Python 3. Submitting a fix to resolve this.
 		",6.0,luvwinnie,2019-10-25T18:29:00Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33094>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33094>No</denchmark-link>
 
 		",0.0,"45,47,48",45,,,,,,,,,,,,,,,,,,,testSaveTimeDistributedLayer,self,620,631,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,luvwinnie,2019-10-26T08:36:28Z,"
 		so is this been fixed to tensorflow-gpu 2.0 stable build? or i have to install another build?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,luvwinnie,2019-10-29T15:46:31Z,"
 		<denchmark-link:https://github.com/luvwinnie>@luvwinnie</denchmark-link>
  It was fixed in  which is development version of  (will be released in
 the near future). Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33148,mimxrt,2019-10-08T15:25:27Z,2020-07-14T16:40:54Z,Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code: Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.2 LTS
 TensorFlow installed from (source or binary): Binary, pip
 TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
 Python version: Python 3.7.3
 CUDA/cuDNN version: CUDA=10.0, CUDNN=7.6.2.24-1
 GPU model and memory: Quadro RTX 6000 major: 7 minor: 5 memoryClockRate(GHz): 1.77
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 It seems there is an issue with the CuDNN LSTM implementation when using a tf.keras.layers.Masking layer.
 <denchmark-code>batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 </denchmark-code>
 
 Similar to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33069>#33069</denchmark-link>
  I receive this error during training and I have strictly right-padded data (I am doing trimming and right-padding manually). However, in contrast to this issue, I confirmed that I do not have any inputs containing only zeroes via the following snippet:
 <denchmark-code>for i, e in enumerate(ds_train):
     res = []
     f, l = [x.numpy() for x in e]
     for j in range(f.shape[0]):
         if not (f[j] == 0.0).all():
             res.append(1)
         else:
             res.append(0)
     fin = [res[0]]
     for e in res[1:]:
         if e != fin[-1]:
             fin.append(e)
     print(""i {}: {}"".format(i, fin))
 
 # Result:
 i 0: [1, 0]
 i 1: [1, 0]
 i 2: [1, 0]
 i 3: [1, 0]
 i 4: [1]
 i 5: [1, 0]
 ...
 </denchmark-code>
 
 If I remove the Masking-layer, the error does not occur. I confirmed this by running a complete epoch (2324 batches), however, the training is probably pretty pointless when including the padded data.
 Is there any other pitfall that I am missing that could cause this issue?
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 Python output:
 <denchmark-code>Epoch 1/1000
 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: 
 
 
 CancelledErrorTraceback (most recent call last)
 <ipython-input-7-1c503c2dd55c> in <module>
 ----> 1 m.fit(train=True)
 
 /ws/tf/vol_local/_model_lstm.py in fit(self, train, verbose)
     315             ]
     316             self.model.fit(ds_train, epochs=num_epochs, verbose=verbose, shuffle=False,
 --> 317                                 validation_data=ds_val, validation_steps=None, callbacks=cbs)
     318             #self.model.save(sess_hdf5_path)
     319             self.model.save_weights(self.sess_h5_path.as_posix())
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
     726         max_queue_size=max_queue_size,
     727         workers=workers,
 --> 728         use_multiprocessing=use_multiprocessing)
     729 
     730   def evaluate(self,
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)
     322                 mode=ModeKeys.TRAIN,
     323                 training_context=training_context,
 --> 324                 total_epochs=epochs)
     325             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
     326 
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
     121         step=step, mode=mode, size=current_batch_size) as batch_logs:
     122       try:
 --> 123         batch_outs = execution_function(iterator)
     124       except (StopIteration, errors.OutOfRangeError):
     125         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py in execution_function(input_fn)
      84     # `numpy` translates Tensors to values in Eager mode.
      85     return nest.map_structure(_non_none_constant_value,
 ---> 86                               distributed_function(input_fn))
      87 
      88   return execution_function
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
     455 
     456     tracing_count = self._get_tracing_count()
 --> 457     result = self._call(*args, **kwds)
     458     if tracing_count == self._get_tracing_count():
     459       self._call_counter.called_without_tracing()
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
     518         # Lifting succeeded, so variables are initialized and we can run the
     519         # stateless function.
 --> 520         return self._stateless_fn(*args, **kwds)
     521     else:
     522       canon_args, canon_kwds = \
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)
    1821     """"""Calls a graph function specialized to the inputs.""""""
    1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
 -> 1823     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
    1824 
    1825   @property
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)
    1139          if isinstance(t, (ops.Tensor,
    1140                            resource_variable_ops.BaseResourceVariable))),
 -> 1141         self.captured_inputs)
    1142 
    1143   def _call_flat(self, args, captured_inputs, cancellation_manager=None):
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
    1222     if executing_eagerly:
    1223       flat_outputs = forward_function.call(
 -> 1224           ctx, args, cancellation_manager=cancellation_manager)
    1225     else:
    1226       gradient_name = self._delayed_rewrite_functions.register()
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in call(self, ctx, args, cancellation_manager)
     509               inputs=args,
     510               attrs=(""executor_type"", executor_type, ""config_proto"", config),
 --> 511               ctx=ctx)
     512         else:
     513           outputs = execute.execute_with_cancellation(
 
 /ws/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
      65     else:
      66       message = e.message
 ---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
      68   except TypeError as e:
      69     keras_symbolic_tensors = [
 
 /ws/miniconda3/lib/python3.7/site-packages/six.py in raise_from(value, from_value)
 
 CancelledError:  [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]] [Op:__inference_distributed_function_172102]
 
 Function call stack:
 distributed_function
 </denchmark-code>
 
 Command line log:
 <denchmark-code>2019-10-08 14:38:27.367875: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_169668_171093' and '__inference___backward_cudnn_lstm_with_fallback_169668_171093_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_172102' both implement 'lstm_dce676f4-acdd-4bb5-88d9-e8dd57573aba' but their signatures do not match.
 2019-10-08 14:38:27.536666: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
 2019-10-08 14:38:39.982582: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 2019-10-08 14:38:41.215567: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
 2019-10-08 14:38:41.215616: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
 	 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 2019-10-08 14:38:41.215638: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 	 [[loss/activation_loss/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_106/has_invalid_dims/concat/_28]]
 2019-10-08 14:38:41.215693: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Cancelled: [_Derived_]RecvAsync is cancelled.
 	 [[{{node metrics/accuracy/broadcast_weights/assert_broadcastable/AssertGuard/else/_36/Assert/data_2/_62}}]]
 </denchmark-code>
 
 	",1.0,mimxrt,2019-10-09T09:28:09Z,"
 		<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>
  ,
 Thanks for reporting the issue, can you please provide simple and standalone code to reproduce the issue?
 		",2.0,mimxrt,2019-10-09T14:46:37Z,"
 		I am trying to create a standalone example, however, I am facing some other errors that prevent me from finishing it. I created the following example which can produce the error using one .tfrecord file of my dataset. In the example, I generate a .tfrecord file from random data (for reproducibility) but TensorFlow fails to parse the file afterwards (actually parsing works but I still get the error):
 <denchmark-code>import numpy as np
 import tensorflow as tf
 
 gpus = tf.config.experimental.list_physical_devices('GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 assert tf.executing_eagerly()
 
 batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 
 #n_files = 3320
 n_files = 10
 num_epochs = 1000
 
 seq_len_max_trunc = batch_size * num_tsteps
 flen = 3728
 
 ### Create TFRecord
 
 X = np.random.rand(flen + 1, num_features)
 n_label0 = int((flen + 1) * 0.2)
 Y = np.concatenate((
     np.zeros((n_label0, 1)), # label 0
     np.ones((flen - n_label0 + 1, 1)), # label 1
 ), axis=0)
 ds_out = tf.data.Dataset.from_tensor_slices((X, Y))
 ds_ser = ds_out.map(lambda *x: 
    tf.reshape(tf.py_function(lambda *v: 
        tf.train.Example(features=tf.train.Features(feature={
            ""features"": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),
            ""label"": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),
        })).SerializeToString(), x, tf.string
    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE
 )
 for i, e in enumerate(ds_ser):
     ex = tf.train.Example.FromString(e.numpy())
     print(""{}: {}"".format(i, ex).replace("" "", """").replace(""\n"", "" "")[:100])
 writer = tf.data.experimental.TFRecordWriter(""temp.tfrecord"")
 writer.write(ds_ser)
 
 ### Read TFRecord and train
 
 files = [""temp.tfrecord""] * n_files
 #files = [""data/myfile.tfrecord""] * n_files
 
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])
 
 
 def _prep_ds_file(file):
     _ds = tf.data.TFRecordDataset(file)
     for i, e in enumerate(_ds):
         ex = tf.train.Example.FromString(e.numpy())
         print(""{}: {}"".format(i, ex).replace("" "", """").replace(""\n"", "" "")[:100])
     print(""\n\n\n\n\n\n\n"")
     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {
         ""features"": tf.io.FixedLenFeature([132], tf.float32),
         ""label"": tf.io.FixedLenFeature([1], tf.float32),
     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)
 
     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[""features""][2:], v[""label""])))
 
     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)
     _ds = _ds.take(_trunc)
 
     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps
     if _c_pad >= 0:
         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc
         _ds_pad = tf.data.Dataset.from_tensors((
             tf.constant(0.0, shape=[num_features,]),
             tf.constant(0.0, shape=[1,])))
         _ds_pad = _ds_pad.repeat(_c_pad)
         _ds = _ds.concatenate(_ds_pad) # pad to correct size
 
     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)
     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))
 
     _ds = _ds.batch(batch_size, drop_remainder=True)
     
     return _ds
 
 
 ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)
 fs_train = ds_fs.take(int(n_files * 0.7))
 fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))
 
 ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]
 for f in fs_train.skip(1):
     ds_train = ds_train.concatenate(_prep_ds_file(f))
 ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 ds_val = [_prep_ds_file(f) for f in fs_val.take(1)][0]
 for f in fs_val.skip(1):
     ds_val = ds_val.concatenate(_prep_ds_file(f))
 ds_val = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 cbs = [
     tf.keras.callbacks.EarlyStopping(monitor=""val_loss"", patience=10, restore_best_weights=True),
 ]
 model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
           validation_data=ds_val, validation_steps=None, callbacks=cbs)
 </denchmark-code>
 
 For me this code produces the following error:
 <denchmark-code>InvalidArgumentError: 2 root error(s) found.
   (0) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.
 	 [[{{node ParseSingleExample/ParseSingleExample}}]]
 	 [[IteratorGetNext]]
 	 [[IteratorGetNext/_2]]
   (1) Invalid argument:  {{function_node __inference_Dataset_map_<lambda>_32068}} Key: features.  Can't parse serialized Example.
 	 [[{{node ParseSingleExample/ParseSingleExample}}]]
 	 [[IteratorGetNext]]
 0 successful operations.
 0 derived errors ignored. [Op:__inference_distributed_function_64891]
 
 Function call stack:
 distributed_function -> distributed_function
 </denchmark-code>
 
 This is weird because it does iterate through the full file and prints the values on the screen (see the dataset iteration both when generating and when parsing the .tfrecord file in _prep_ds_file()). I can see that elements 0 to 3728 are printed. When switching to the file myfile.tfrecord (commented out) I can also see the elements until 3728 (same amount) and finally receive the error mentioned in the issue above.
 Do you have any idea what causes this error? If not I could provile the myfile.tfrecord file but of course that would not be ideal for reproducibility.
 		",3.0,mimxrt,2019-10-10T05:49:11Z,"
 		Issue replicating with TF-2.0, kindly find the <denchmark-link:https://colab.sandbox.google.com/gist/oanush/11dcf7d43fb3a2ba2c1d97e22ff145d2/33148.ipynb>gist</denchmark-link>
  of colab.Thanks!
 		",4d582a660b4e84fb283eba598127ae40fdd8d1ed,Scott Zhu,2020-07-13 21:27:11-07:00,MODIFY,1,tensorflow\python\keras\layers\gru_v2_test.py,tensorflow\python\keras\layers\gru_v2_test.py,1.0,"615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641",,MODIFY,1.0,tensorflow\python\keras\layers\lstm_v2_test.py,tensorflow\python\keras\layers\lstm_v2_test.py,4.0,mimxrt,2019-10-10T09:00:05Z,"
 		I found my mistake and updated the code accordingly. Please update the gist to represent the corrected and less verbose version below. The error happens in TF-2.0 and does not happen in TF-1.0 (1.14.0). Please also try to removing the Masking layer to confirm the issue only exists with masking.
 <denchmark-code>import numpy as np
 import tensorflow as tf
 
 # NOTE: Comment the block below for testing with TF-1.0
 gpus = tf.config.experimental.list_physical_devices('GPU')
 for gpu in gpus:
     tf.config.experimental.set_memory_growth(gpu, True)
 
 # NOTE: Uncomment the block below for testing with TF-1.0
 # tf.compat.v1.enable_eager_execution()
 # config = tf.compat.v1.ConfigProto()
 # config.gpu_options.allow_growth = True
 # sess = tf.compat.v1.Session(config=config)
 # tf.compat.v1.keras.backend.set_session(sess)
 
 assert tf.executing_eagerly()
 
 batch_size = 256
 num_tsteps = 144
 num_features = 130
 num_units = 88
 
 #n_files = 3320
 n_files = 10
 num_epochs = 1000
 
 seq_len_max_trunc = batch_size * num_tsteps
 flen = 3728
 
 ### Create TFRecord
 
 X = np.random.rand(flen + 1, num_features + 2)
 n_label0 = int((flen + 1) * 0.2)
 Y = np.concatenate((
     np.zeros((n_label0, 1)), # label 0
     np.ones((flen - n_label0 + 1, 1)), # label 1
 ), axis=0)
 ds_out = tf.data.Dataset.from_tensor_slices((X, Y))
 ds_ser = ds_out.map(lambda *x: 
    tf.reshape(tf.py_function(lambda *v: 
        tf.train.Example(features=tf.train.Features(feature={
            ""features"": tf.train.Feature(float_list=tf.train.FloatList(value=v[0].numpy())),
            ""label"": tf.train.Feature(float_list=tf.train.FloatList(value=v[1].numpy())),
        })).SerializeToString(), x, tf.string
    ), ()), num_parallel_calls=tf.data.experimental.AUTOTUNE
 )
 writer = tf.data.experimental.TFRecordWriter(""temp.tfrecord"")
 writer.write(ds_ser)
 
 ### Read TFRecord and train
 
 files = [""temp.tfrecord""] * n_files
 
 model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=True, stateful=False),
     tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),
     tf.keras.layers.Activation('sigmoid'),
 ])
 model.compile(optimizer='adam',
               loss='binary_crossentropy',
               metrics=['accuracy'])
 
 
 def _prep_ds_file(file):
     _ds = tf.data.TFRecordDataset(file)
     _ds = _ds.map(lambda x: tf.io.parse_single_example(x, {
         ""features"": tf.io.FixedLenFeature([132], tf.float32),
         ""label"": tf.io.FixedLenFeature([1], tf.float32),
     }), num_parallel_calls=tf.data.experimental.AUTOTUNE)
         
     _ds = _ds.flat_map(lambda v: tf.data.Dataset.from_tensors((v[""features""][2:], v[""label""])))
 
     _trunc = min(seq_len_max_trunc, ((flen + 1) // num_tsteps) * num_tsteps)
     _ds = _ds.take(_trunc)
 
     _c_pad = (batch_size - ((flen + 1) // num_tsteps)) * num_tsteps
     if _c_pad >= 0:
         assert _c_pad + ((flen + 1) // num_tsteps * num_tsteps) == seq_len_max_trunc
         _ds_pad = tf.data.Dataset.from_tensors((
             tf.constant(0.0, shape=[num_features,]),
             tf.constant(0.0, shape=[1,])))
         _ds_pad = _ds_pad.repeat(_c_pad)
         _ds = _ds.concatenate(_ds_pad) # pad to correct size
 
     _ds = _ds.window(size=num_tsteps, shift=None, stride=1, drop_remainder=True)
     _ds = _ds.flat_map(lambda x, y: tf.data.Dataset.zip((x.batch(num_tsteps), y.batch(num_tsteps))))
 
     _ds = _ds.batch(batch_size, drop_remainder=True)
     
     return _ds
 
 
 ds_fs = tf.data.Dataset.list_files(files, shuffle=True, seed=1)
 fs_train = ds_fs.take(int(n_files * 0.7))
 fs_val = ds_fs.skip(int(n_files * 0.7)).take(int(n_files * 0.1))
 
 ds_train = [_prep_ds_file(f) for f in fs_train.take(1)][0]
 for f in fs_train.skip(1):
     ds_train = ds_train.concatenate(_prep_ds_file(f))
 ds_train = ds_train.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
 cbs = [
     tf.keras.callbacks.EarlyStopping(monitor=""val_loss"", patience=10, restore_best_weights=True),
 ]
 model.fit(ds_train, epochs=num_epochs, verbose=1, shuffle=False,
           validation_data=None, validation_steps=None, callbacks=cbs)
 </denchmark-code>
 
 		",5.0,mimxrt,2019-10-10T16:57:21Z,"
 		<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>
  I could reproduce the issue with . However, if I use only cpu then there is no error (with and without masking) as shown in the original post. I think root-cause may be related to . <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/3a768611de0a28330d047f12e120f931/untitled553.ipynb>Here</denchmark-link>
  is the gist for your reference. Thanks!
 		",6.0,mimxrt,2019-10-11T07:17:23Z,"
 		Thank you for your reply. I'm not sure what you are saying though: did you try removing tf.config.experimental.set_memory_growth(gpu, True) at all? I tried and for me the same error happens. For me, the only difference is that TF occupies all available GPU memory.
 Could you please try again if the error also happens without memory growth? If so, I guess the issue could be in the GPU LSTM implementation (cuDNN)?
 		",1.0,"816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842",,test_with_fully_masked_inputs,self,816,842,MODIFY,11.0,tensorflow\python\keras\layers\recurrent_v2.py,tensorflow\python\keras\layers\recurrent_v2.py,1.0,1172,1172,,,,,,,,test_with_fully_masked_inputs,self,615,641,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,mimxrt,2019-10-11T20:14:23Z,"
 		<denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>
  I mentioned two things.
 
 I could reproduce the issue with tf-nightly-gpu
 Just tried to point some root-cause. I ran your code as it is in a cpu and I don't see any error. so I guess root-cause may be related to tf.config.experimental.set_memory_growth(gpu, True). I may be wrong.
 
 Thanks!
 		",call,"self,inputs,mask,training,initial_state",1100,1204,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,mimxrt,2019-10-14T07:12:44Z,"
 		I see, thank you for the clarification. So in summary we can say that this issue does not happen on the CPU and that it is related to the GPU implementation. As I tried running it without tf.config.experimental.set_memory_growth(gpu, True) and the error still occurs, it should probably be save to rule that out as the cause/issue.
 		",,,,,,,,,,,,,,,1.0,738,738,cudnn_gru_fn,,738,748,1.0,"1564,1581,1593,1594","1564,1581,1582,1583,1588,1589",is_sequence_right_padded,"mask,time_major",1564,1594,1.0,"1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603",,has_fully_masked_sequence,mask,1593,1603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,mimxrt,2019-11-12T01:34:44Z,"
 		same problem here. did you find any solution?
 		",10.0,mimxrt,2019-11-12T07:33:03Z,"
 		No sorry, still waiting for any help myself.
 		",11.0,mimxrt,2019-11-12T08:12:30Z,"
 		I changed my model from:
         self.model = Sequential([
             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1],mask_zeros=True),
             LSTM(self.lstm_out),
             Dense(len(self.item_map)-1),
         ])
 to:
         self.model = Sequential([
             Embedding(len(self.item_map), self.embed_dim, input_length = X.shape[1]),
             Masking(mask_value=0),
             LSTM(self.lstm_out),
             Dense(len(self.item_map)-1),
         ])
 And solved my isssue
 I know <denchmark-link:https://github.com/mimxrt>@mimxrt</denchmark-link>
 's code has the same model and I dont know why it works for me,
 but im adding this for anyone else comes here with the issue and maybe it can help with debugging
 		",12.0,mimxrt,2019-11-12T09:35:41Z,"
 		<denchmark-link:https://github.com/ynsgnr>@ynsgnr</denchmark-link>
  if you are running on CPU then it works properly, the problem is when you run it on GPU. By the way, you do not use the Timedistirbuted layer in your code since this problem shows up when you use Timedistributed with LSTM.
 		",13.0,mimxrt,2019-11-12T10:16:45Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
 , I tried your suggestion but it still produces the same error. I am new to GitHub , how to tell this problem to someone from TensorFlow? do they see our conversation ?
 		",14.0,mimxrt,2019-11-12T10:20:14Z,"
 		In my experience and example the issue stems from the Masking + LSTM combination, not from the  layer. Please try the example in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342>#33148 (comment)</denchmark-link>
  without it, i.e.
 <denchmark-code>model = tf.keras.Sequential([
     tf.keras.layers.InputLayer(input_shape=(num_tsteps, num_features), batch_size=batch_size),
     tf.keras.layers.Masking(mask_value=0.0, input_shape=(num_tsteps, num_features)),
     tf.keras.layers.LSTM(num_units,  batch_input_shape=(batch_size, num_tsteps, num_features), return_sequences=False, stateful=False),
     tf.keras.layers.Dense(1),
     tf.keras.layers.Activation('sigmoid'),
 ])
 </denchmark-code>
 
 (change return_sequences to False and remove TimeDistributed).
 You can also try to remove the Masking layer in the example above and the error will go away.
 		",15.0,mimxrt,2019-11-12T11:40:52Z,"
 		but I have this model:
        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=True,mask_zero=True) model.add(embeding_layer) model.add(layers.LSTM(50)) model.add(layers.Dropout(0.5)) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.RMSprop(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy']) self.model=model
 which works fine and with no error. As you can see I used masking and LSTM at the same time. so in my experience the problem stem from TimeDistributed and masking.
 		",16.0,mimxrt,2019-11-12T11:51:23Z,"
 		Interesting. Can you provide a complete example like mine so I can try your code as well?
 		",17.0,mimxrt,2019-11-12T12:01:51Z,"
 		the above example is a simple classifier, you can make a random dataset and feed the model with it, so you can see that it will work with no problem.
 this is where the model does not work and produce an error while training :
        model=tf.keras.Sequential() embeding_layer=layers.Embedding(self.vocab_size,self.word_vector_dim,weights=[word_embeding_matrix],trainable=False,mask_zero=True) model.add(TimeDistributed(embeding_layer)) model.add(TimeDistributed(tf.keras.layers.LSTM(50))) model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(50))) # model.add(tf.keras.layers.Bidirectional(costumized_lstm.Costumized_LSTM(100))) model.add(layers.Dense(3,activation='softmax')) opt=tf.keras.optimizers.Adam(learning_rate=0.001) model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])
 in this example, if I set mask_zero=true at the embedding layer then, it crushes at the begging or sometimes at the end of the epoc when evaluating the validation loss. and this is the error message :
  C:\Users\jalil\PycharmProjects\untitled1\venv\Scripts\python.exe C:/Users/jalil/PycharmProjects/untitled1/main_file.py
 2019-11-14 14:11:36.983144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
 2019-11-14 14:11:45.638679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
 2019-11-14 14:11:46.216495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
 name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
 pciBusID: 0000:01:00.0
 2019-11-14 14:11:46.216676: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
 2019-11-14 14:11:46.217282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
 2019-11-14 14:11:50.885396: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
 2019-11-14 14:11:51.214275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties:
 name: GeForce GTX 970M major: 5 minor: 2 memoryClockRate(GHz): 1.038
 pciBusID: 0000:01:00.0
 2019-11-14 14:11:51.214484: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
 2019-11-14 14:11:51.218182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
 2019-11-14 14:11:51.905201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
 2019-11-14 14:11:51.905307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165] 0
 2019-11-14 14:11:51.905366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0: N
 2019-11-14 14:11:51.906228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4757 MB memory) -> physical GPU (device: 0, name: GeForce GTX 970M, pci bus id: 0000:01:00.0, compute capability: 5.2)
 WARNING:tensorflow:From C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\backend.py:3983: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
 Instructions for updating:
 Use tf.where in 2.0, which has the same broadcast rule as np.where
 Train on 35000 samples, validate on 6447 samples
 Epoch 1/1000
 2019-11-14 14:12:33.178251: W tensorflow/core/grappler/optimizers/implementation_selector.cc:310] Skipping optimization due to error while loading function libraries: Invalid argument: Functions '__inference___backward_cudnn_lstm_with_fallback_671418_672877' and '__inference___backward_cudnn_lstm_with_fallback_671418_672877_specialized_for_StatefulPartitionedCall_at___inference_distributed_function_675292' both implement 'lstm_81cdaa4a-fa6f-4675-abbb-02fb4cd0189b' but their signatures do not match.
 2019-11-14 14:12:33.544669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
 2019-11-14 14:12:34.397677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
 32/35000 [..............................] - ETA: 2:03:422019-11-14 14:12:35.151804: W tensorflow/core/framework/op_kernel.cc:1622] OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
 2019-11-14 14:12:35.152137: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 2019-11-14 14:12:35.152541: W tensorflow/core/common_runtime/base_collective_executor.cc:216] BaseCollectiveExecutor::StartAbort Unknown: {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} {{function_node __forward_cudnn_lstm_with_fallback_672876_specialized_for_sequential_time_distributed_1_lstm_StatefulPartitionedCall_at___inference_distributed_function_675292}} CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]]
 32/35000 [..............................] - ETA: 2:25:41Traceback (most recent call last):
 File ""C:/Users/jalil/PycharmProjects/untitled1/main_file.py"", line 102, in
 main_model_instance.train_model(train_batch_data,train_batch_labels,test_batch_data,test_batch_labels)
 File ""C:\Users\jalil\PycharmProjects\untitled1\main_model.py"", line 103, in train_model
 history = self.model.fit(x=np.array(train_batch_data),y=np.array(train_batch_labels),validation_data=(np.array(test_batch_data),np.array(test_batch_labels)),epochs=1000,callbacks=[tensorboard_callback])
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 734, in fit
 use_multiprocessing=use_multiprocessing)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 324, in fit
 total_epochs=epochs)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 123, in run_one_epoch
 batch_outs = execution_function(iterator)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 86, in execution_function
 distributed_function(input_fn))
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 439, in call
 return self._stateless_fn(*args, *kwds)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1822, in call
 return graph_function._filtered_call(args, kwargs) # pylint: disable=protected-access
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1141, in _filtered_call
 self.captured_inputs)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1224, in _call_flat
 ctx, args, cancellation_manager=cancellation_manager)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\function.py"", line 511, in call
 ctx=ctx)
 File ""C:\Users\jalil\PycharmProjects\untitled1\venv\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
 six.raise_from(core._status_to_exception(e.code, message), None)
 File """", line 3, in raise_from
 tensorflow.python.framework.errors_impl.UnknownError: [Derived] CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1424): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void)&padding_fill)'
 [[{{node cond_64/then/_0/CudnnRNNV3}}]]
 [[sequential/time_distributed_1/lstm/StatefulPartitionedCall]] [Op:__inference_distributed_function_675292]
 Function call stack:
 distributed_function -> distributed_function -> distributed_function
 `
 		",18.0,mimxrt,2020-05-02T18:58:32Z,"
 		import tensorflow as tf
 tf.compat.v1.disable_eager_execution()
 Dissable eager execution and everything is running fine without the fused rnn kernel. Thx for the help guys :)
 		",19.0,mimxrt,2020-06-01T12:39:43Z,"
 		I have a work around that seems to work:  force TF to use the non CuDNN implementation by selecting a sigmoid activation instead of TANH
 layers.LSTM(...,activation='sigmoid')
 Outputs
 WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
 This forces TF to use a generic GPU kernel in place of CuDNN.  It's slower but a slower implementation is a lot faster than not working at all ;p
 		",20.0,mimxrt,2020-06-09T07:27:49Z,"
 		If you really want that as a workaround, you can find the requirements for the cuDNN implementation <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM?hl=en#used-in-the-notebooks_1>here</denchmark-link>
 .
 
 The requirements to use the cuDNN implementation are:
 
 activation == tanh
 recurrent_activation == sigmoid
 recurrent_dropout == 0
 unroll is False
 use_bias is True
 Inputs are not masked or strictly right padded.
 
 
 I guess changing any one of these will result in the CPU non-cuDNN implementation being used.
 		",21.0,mimxrt,2020-06-12T19:32:36Z,"
 		That's right mimxrt.  I guess the interesting point here is that the inputs to my LSTM are always masked, but I have to force the non-cudnn implementation using the activation function.  Might be a clue for someone who can fix this.
 		",22.0,mimxrt,2020-06-26T15:47:49Z,"
 		I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.
 		",23.0,mimxrt,2020-07-03T10:55:45Z,"
 		Facing this same issue and reported here: <denchmark-link:https://github.com/tensorflow/tensorflow/issues/40982>#40982</denchmark-link>
 .
 		",24.0,mimxrt,2020-07-06T16:47:51Z,"
 		
 I'd be nice to have a flag on LSTM layers which allows to disable the use of the CuDNN implementation instead of either disabling eager execution or using non-default activation functions.
 
 There is a private field you can set to disable the cudnn kernel once the layer is created.
 <denchmark-code>layer = tf.keras.layers.LSTM(4)
 layer. _could_use_gpu_kernel = False
 </denchmark-code>
 
 This will focus the lstm layer to use the non-cudnn implementation, even landed on GPU.
 		",25.0,mimxrt,2020-07-06T16:50:37Z,"
 		<denchmark-link:https://github.com/queirozfcom>@queirozfcom</denchmark-link>
  what would you suggest to use between this option and disabling eager execution as when eager execution is disabled it does work?
 		",26.0,mimxrt,2020-07-06T16:57:32Z,"
 		Disable eager will have larger side effects since it change the runtime to execute in graph/session context. We don't recommend user to fallback to graph unless they really need some feature in graph/session.
 		",27.0,mimxrt,2020-07-06T17:13:03Z,"
 		Thanks :)
 		",28.0,mimxrt,2020-07-10T18:40:02Z,"
 		I am still facing this issue using TF 2.2.0. I also found the same workaround of forcing the LSTM to not use the cuDNN implementation to work, however it is nearly prohibitively slow. I found the generic GPU implementation took ~30 times longer to train per epoch than the cuDNN version. I hope this can be fixed soon.
 		",29.0,mimxrt,2020-07-14T16:40:54Z,"
 		I have disable the cudnn code path if there is fully masked data in the batch. It will have some performance dip since it fallback to generic kernel, but won't error out.
 		",30.0,mimxrt,2020-07-14T16:40:56Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33148>No</denchmark-link>
 
 		",31.0,mimxrt,2020-07-22T00:57:42Z,"
 		facing this issue using TF 2.2.0 while evaluating
 		",32.0,mimxrt,2020-07-22T02:36:50Z,"
 		Please try with the latest nightly version.
 		",33.0,mimxrt,2020-08-07T06:42:40Z,"
 		
 @qlzh727 Not yet. We might need to wait for the next major release for the fix.
 
 cudnn 8 is here, is there any info about this issue?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.0,mimxrt,2020-09-25T09:44:01Z,"
 		<denchmark-link:https://github.com/geetachavan1>@geetachavan1</denchmark-link>
  I am using `tensorflow==2.3.0 but for this network:
 def call(self, inputs, training=None, memory_states=None, **kwargs):
 
     x = self.embedding(inputs, training=training)
 
     if memory_states is None:
         memory_states = self.get_initial_state(shape_list(x)[0])
 
     next_memory_states = []
 
     for i, (lstm, norm) in enumerate(self.lstm_stack):
         x = norm(x, training=training)
         outputs = lstm(x, training=training, initial_state=memory_states[i])
         x, state = outputs[0], outputs[1:]
         next_memory_states.append(state)
 
     return x, next_memory_states
 where we have:
 self.embedding = layers.Embedding(vocab_size, embedding_size, mask_zero=True)
 self.lstm_stack = list()
 for _ in range(num_layers):
     lstm = layers.LSTM(
         units=lstm_units,
         return_sequences=True,
         return_state=True,
         dropout=dropout
     )
     norm = layers.LayerNormalization()
     self.lstm_stack.append((lstm, norm))
 I am getting
 <denchmark-code>Traceback (most recent call last):
   File ""/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py"", line 153, in <module>
     main()
   File ""/home/sfalk/tmp/speech-v2/asr/bin/eval_transducer.py"", line 85, in main
     y_greedy = model.greedy_decode(encoder_inputs)[0]
   File ""/home/sfalk/tmp/speech-v2/asr/model/transducer.py"", line 351, in greedy_decode
     init_predict_network_state = PredictNetworkState(*self.predict_network(init_yseq))
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 985, in __call__
     outputs = call_fn(inputs, *args, **kwargs)
   File ""/home/sfalk/tmp/speech-v2/asr/model/transducer.py"", line 118, in call
     outputs = lstm(x, training=training, initial_state=memory_states[i])
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 720, in __call__
     return super(RNN, self).__call__(inputs, **kwargs)
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 985, in __call__
     outputs = call_fn(inputs, *args, **kwargs)
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py"", line 1176, in call
     last_output, outputs, new_h, new_c, runtime = gpu_lstm(
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent_v2.py"", line 1401, in gpu_lstm
     outputs, h, c, _, _ = gen_cudnn_rnn_ops.cudnn_rnnv3(
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py"", line 1914, in cudnn_rnnv3
     return cudnn_rnnv3_eager_fallback(
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/gen_cudnn_rnn_ops.py"", line 2011, in cudnn_rnnv3_eager_fallback
     _result = _execute.execute(b""CudnnRNNV3"", 5, inputs=_inputs_flat,
   File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
 tensorflow.python.framework.errors_impl.UnknownError: CUDNN_STATUS_BAD_PARAM
 in tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]
 </denchmark-code>
 
 Shouldn't this be fixed?
 		",35.0,mimxrt,2020-10-15T20:28:54Z,"
 		Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33148#issuecomment-540472342>#33148 (comment)</denchmark-link>
 .
 		",36.0,mimxrt,2020-11-28T18:48:49Z,"
 		When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.
 		",37.0,mimxrt,2020-11-30T06:33:20Z,"
 		
 When will this release happen .....looks like without CuDNN the implementation of LSTM runs very slow.
 
 cudnn 8.0.5 is already out <denchmark-link:https://docs.nvidia.com/deeplearning/cudnn/archives/index.html>https://docs.nvidia.com/deeplearning/cudnn/archives/index.html</denchmark-link>
 
 		",38.0,mimxrt,2020-12-07T11:02:11Z,"
 		
 Some update: the cudnn v8.0.5 (next release) should be able to fix the issue exposed by #33148 (comment).
 
 <denchmark-link:https://github.com/kaixih>@kaixih</denchmark-link>
  where did you read it?
 I'd like to give it a try.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,750,750,input_not_right_padded,,750,761,1.0,738,738,input_right_padded,,738,748,1.0,1509,1509,cudnn_lstm_fn,,1509,1520,1.0,"1564,1581","1564,1581,1582,1583,1588,1589",is_sequence_right_padded,mask,1564,1590,1.0,"1606,1607,1608,1609,1610,1611,1612",,is_cudnn_supported_inputs,"mask,time_major",1606,1612,1.0,750,750,standard_gru_fn,,750,761,1.0,1522,1522,stardard_lstm_fn,,1522,1534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33340,off99555,2019-10-14T15:13:52Z,2020-01-15T05:51:39Z,Significant prediction slowdown after model.compile(),"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 TensorFlow installed from (source or binary): pip install tensorflow
 TensorFlow version: 2.0.0
 Python version: 3.7
 CUDA/cuDNN version: CUDA=10.0, cuDNN=7.6.4
 GPU model and memory: GTX 1060 6GB
 
 Describe the current behavior
 The prediction speed is slowed down a lot after model.compile() call.
 Describe the expected behavior
 Speed should not be affected. Predict function is used by users assuming that it will work fast because we use it all the time in production. It should not cause surprise to users.
 
 <denchmark-link:https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true>https://nbviewer.jupyter.org/github/off99555/TensorFlowExperiments/blob/master/test-prediction-speed-after-compile.ipynb?flush_cache=true</denchmark-link>
 
 <denchmark-link:https://user-images.githubusercontent.com/15215732/66762282-e3dc0900-eecf-11e9-8d93-82c8bcc5325b.png></denchmark-link>
 
 	",1.0,off99555,2019-10-14T16:30:26Z,"
 		<denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58378941#58378941>Relevant SO</denchmark-link>
 , and another minimal reproducible example:
 from tensorflow.keras.layers import Input, Dense
 from tensorflow.keras.models import Model
 import numpy as np
 from time import time
 
 def timeit(func, arg, iterations):
     t0 = time()
     for _ in range(iterations):
         func(arg)
     print(""%.4f sec"" % (time() - t0))
 
 ipt   = Input(shape=(4,))
 x     = Dense(2, activation='relu')(ipt)
 out   = Dense(1, activation='sigmoid')(x)
 model = Model(ipt, out)
 
 X = np.random.randn(32,4)
 
 timeit(model.predict, X, 1000)
 model.compile('adam', loss='binary_crossentropy')
 timeit(model.predict, X, 1000)
 model._make_train_function()  # build optimizer
 timeit(model.predict, X, 1000)
 Outputs:
 0.9891 sec
 29.785 sec
 29.521 sec
 That's a 30-fold slowdown. Worse yet, building the optimizer does not elicit any further slowdowns - so ""graph size"" may not be the main explanation here.
 		",2.0,off99555,2019-10-15T00:01:01Z,"
 		<denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156>Solved</denchmark-link>
 .
 		",3.0,off99555,2019-10-15T07:49:13Z,"
 		<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  ,
 Can you confirm if the issue is resolved?Thanks!
 		",42f469be0f3e8c36624f0b01c571e7ed15f75faf,Zhenyu Tan,2020-01-14 20:01:47-08:00,MODIFY,0,tensorflow\python\keras\engine\training.py,tensorflow\python\keras\engine\training.py,0.0,"837,838,839,840,841,842",837,,,,,4.0,off99555,2019-10-15T11:57:39Z,"
 		It does not seem to me that it is resolved. It's more like we know how the issue occurs but we don't have a solution, just workaround. I need to compare timing between compiled and non-compiled version and see which is faster. But I don't think a user will be aware of this in general. So we should make a better solution.
 In this case, should I close the issue or keep it open?
 		",5.0,off99555,2019-10-15T13:46:08Z,"
 		<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  I'd agree to request a documentation improvement from TensorFlow to notify users of this, but I doubt any code-level changes will be implemented to address this as it'd require revamping a massive portion of TF graph. It's up to the user to be aware of functionality differences and adjust accordingly - but admittedly, while this isn't the only issue where a workaround is required, other cases are at least documented.
 		",6.0,off99555,2019-10-18T21:10:25Z,"
 		<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  I cannot reproduce the issue. When I ran it in colab, computation time is little more but not as much as you reported. I have checked some other models also. Please check the <denchmark-link:https://colab.sandbox.google.com/gist/jvishnuvardhan/d91713318125c34768c5f8841e60b928/untitled582.ipynb>gist here</denchmark-link>
 .
 <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>
  I could reproduce your issue. The reason is that when the model is very small, it takes more time to predict after compilation. I have checked other similar small models and I noticed it. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,off99555,2019-10-18T21:25:32Z,"
 		
 The reason is that when the model is very small, it takes more time to predict after compilation
 
 <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  That is the problem, yes, but not its explanation - I've done the latter <denchmark-link:https://stackoverflow.com/questions/58378374/why-does-keras-model-predict-slower-after-compile/58385156#58385156>here</denchmark-link>
 . Further, it's not the model size, but model size  data size.
 As noted by <denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  , the solution only works for those aware of the problem and its workaround - this issue, however, isn't documented or mentioned in a docstring. To remedy, some form of mention should be made in documentation or  docstring as a disclaimer.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,off99555,2019-12-28T19:11:44Z,"
 		As OverLordGoldDragon mentioned, you can disable the experimental_run_tf_function flag for a Keras model and force it to the old v1 execution path (I think). It's a bit nicer than disabling eager execution globally.
 The good news is that you can do this by simply passing the param to the compile() function, so you don't need to make any private calls. Like so: model.compile(loss=loss, optimizer=optimizer, experimental_run_tf_function=False)
 This restores the performance for me (I experienced more than 10x slowdown). Let me second everybody in that I'm surprised this is not a big issue - the parameter is on the public interface of a major release, but it's not even mentioned in the docstring.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,off99555,2019-12-28T19:51:41Z,"
 		Noone assigned; if it doesn't cause a compile error, doesn't mean it isn't a worthwhile problem. Has anyone made a docstring PR on this yet? If not, I could
 		",10.0,off99555,2019-12-28T23:12:17Z,"
 		Hi. Let me try to address some of the questions here and see if that helps.
 
 Has anyone made a docstring PR on this yet?
 
 experimental_run_tf_function is an implementation detail, and that flag is mostly there as a debug during the transition. We don't plan to document it because it will be removed at some point in the future and the True behavior will be the only behavior.
 Now I expect that you may be surprised (or aghast) that it's going to be always on given the discussion in this thread. What experimental_run_tf_function does is funnel all calls to fit, evaluate, and predict through a central adapter which creates a Dataset and performs a variety of checks and input validation. This is generally desirable because it makes everything more robust, but there is some overhead to spinning up this machinery which is not amortized by small models with little data.
 Code to profile the step:
 <denchmark-code>import cProfile
 import pstats
 
 profiler = cProfile.Profile()
 profiler.enable()
 for _ in range(5):
   model.predict(x)
 profiler.disable()
 
 stats = pstats.Stats(profiler)
 stats.sort_stats(""cumtime"").print_stats(20)
 </denchmark-code>
 
 In this case most of the extra time is spent creating the dataset; there is machinery in there which makes sense, but it's surplus to requirements for the degenerate case of a single batch. (<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  in case you want to look into the init time, but it's not obvious that it's unreasonable given the pipeline.) Really, this is not what  is for. That endpoint is for predictions on lots of data where the batching and aggregation machinery in that endpoint is necessary. For single batch prediction there is , which doesn't invoke all of that machinery and just directly calls into the model function. I tested it, and it is identical in v1 and v2. (And faster than even v1 )
 However this seems to be a common pitfall; I see lots of issues around single batch . (<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  increment your counter...) From a documentation standpoint, I think the most valuable contribution would be to document  in the  docstring, and probably also warn in  when the batch cardinality is one. <denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>
  <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
   Can you remind me to bring this up at the next triage? And <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>
  if you want to take a crack at a PR that would be great; feel free to tag me and I'll try to provide some assistance.
 		",11.0,off99555,2019-12-28T23:12:52Z,"
 		Oh, and <denchmark-link:https://github.com/goldiegadde>@goldiegadde</denchmark-link>
  for tracking 2.2 performance, of course.
 		",12.0,off99555,2019-12-29T00:47:17Z,"
 		<denchmark-link:https://github.com/robieta>@robieta</denchmark-link>
  Solid response, thanks - I'll look into the PR sometime. As for , so long as a faster alternative remains (i.e. ) and is noted in a docstring, it'd make an excellent resolution.
 		",13.0,off99555,2019-12-29T14:17:29Z,"
 		Thanks for the clarification, and for the quick help! model.predict_on_batch speeds things up, but is still significantly slower on the v2 path. Here is the cumsum using your profile snippet (using tf2.0.0, calling predict 100 times instead of 5) for a small DQN model on a small batch of data:
 
 
 
 experimental_run_tf_function
 predict()
 predict_on_batch()
 
 
 
 
 False / v1
 0.209s
 0.078s
 
 
 True / v2 (default)
 3.720s
 0.246s
 
 
 
 So, anyone with a single batch should switch to predict_on_batch, but an even faster option exists (v1 predict_on_batch), which is going to be deprecated if I understood correctly.
 Is this use case truly so exotic that we simply should not use the Keras API for it? I understand that we should of course look into batching, but for anyone who just writes quick prototypes it would be nice to have a fast light-weight way of evaluating things. Anyway, warning single-batch users of predict() will surely help most users, so that sounds great.
 Also, here's a detail that may be important to people who migrate their code: When running in v2 mode, predict_on_batch will not return a numpy array (contrary to the docstring), but an EagerTensor instead. The caller may want to wrap the result -- as in np.array(model.predict_on_batch(...)) -- to guarantee the same behavior for both v1 and v2. predict however returns a numpy array in both cases.  If you like, I can make a PR for the docstring.
 		",14.0,off99555,2020-01-03T14:47:30Z,"
 		<denchmark-link:https://github.com/robieta>@robieta</denchmark-link>
  Actually I'll pass on making a PR; currently rather occupied - may try later if noone has done it
 		",15.0,off99555,2020-01-13T11:48:25Z,"
 		<denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>
   I tried to reproduce this with tf nightly but it doesn't seem to occur anymore. Or equivalently even without compile it becomes pretty slow as well.
 Now this is more of a tensorflow 1.15 much faster than tensorflow v2 -- can you confirm?
 		",16.0,off99555,2020-01-14T11:00:50Z,"
 		<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>
  Can confirm for nightly. The v1 path fallback has been removed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/c73c99ca3e0bacf2bca313f270bb3eae28869530#diff-de9b96ac2d81503324cbbbe21732031f>c73c99c#diff-de9b96ac2d81503324cbbbe21732031f</denchmark-link>
  , so everything now executes on v2, regardless of model.compile()
 It's still possible to get the old speed back behavior by using tf.compat.v1.disable_eager_execution(). In any case, for most users moving to predict_on_batch() should be recommended, and also makes the slowdown less drastic (see numbers above) so the issue is less extreme.
 That said, sorry for taking so long on the PR for warning single-batch predict() users.
 		",17.0,off99555,2020-01-15T02:44:40Z,"
 		<denchmark-link:https://github.com/ttbrunner>@ttbrunner</denchmark-link>
  Ah yeah that was the commit, thanks.
 Ok so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)
 <denchmark-link:https://github.com/off99555>@off99555</denchmark-link>
  <denchmark-link:https://github.com/OverLordGoldDragon>@OverLordGoldDragon</denchmark-link>
  <denchmark-link:https://github.com/ttbrunner>@ttbrunner</denchmark-link>
  So here's what I would recommend going forward:
 
 
 you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no ""conversion to dataset"" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)
 
 
 I will make a docstring to recommend users to use model call and explain predict is for large dataset.
 
 
 SG?
 		",18.0,off99555,2020-01-15T03:02:37Z,"
 		<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>
  I haven't benchmarked any of it, but if it is as you say - sounds like an excellent resolution indeed. Do let me know when it's done so I update my SO answer -- thanks for following through with this to the end.
 		",19.0,off99555,2020-01-15T05:51:39Z,"
 		I have updated the doc, also tested the performance for model(x) in nightly. Closing it for now. Thanks all for reporting and collaborative work!
 		",20.0,off99555,2020-01-15T06:04:14Z,"
 		<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>
  Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)
 		",21.0,off99555,2020-01-15T06:05:21Z,"
 		
 @tanzhenyu Great. To clarify, is the speedup for TF 2.1+ only, or also 2.0? (if latter, is 2.1 even faster?)
 
 I believe this should be universal to 2.x versions
 		",22.0,off99555,2020-01-15T09:06:29Z,"
 		Thanks for the docstring update, also for the explanation. I'm always interested!
 Can confirm that model(x) has the same runtime as predict_on_batch(x), i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.
 Another note for users: it's possible to specify model.run_eagerly = False before compiling. With this and the model(x) call, I am getting almost the same performance as in v1, without globally disabling eager execution.
 P.S.: Sorry for the many edits of this post.
 		",23.0,off99555,2020-01-16T05:54:40Z,"
 		
 Thanks for the docstring update, also for the explanation. I'm always interested!
 Can confirm that model(x) has the same runtime as predict_on_batch(x), i.e. the v2 path is still slightly slower. It's OK for my use case though, so thanks again.
 Another note for users: it's possible to specify model.run_eagerly = False before compiling. With this and the model(x) call, I am getting almost the same performance as in v1, without globally disabling eager execution.
 P.S.: Sorry for the many edits of this post.
 
 You can also compile(..., run_eagerly=False)
 		",24.0,off99555,2020-01-16T06:12:48Z,"
 		<denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>
  Side-ish question: what does  do in TF2, which runs in eager by default? From inspecting a fair chunk of the source code, it seems to change only a few execution paths for certain less-usual dataset types. Likewise, any difference with  vs. disabling eager?
 		",25.0,off99555,2020-03-27T21:50:32Z,"
 		
 @ttbrunner Ah yeah that was the commit, thanks.
 Ok so we follow the adapter pattern for convert numpy and dataframes to dataset first, and has a single path for execution. Apparently the speed down is mainly two things: 1) the construction of dataset. 2) creating tf.function for predict. (Check TensorLikeDataAdapter under /python/keras/engine/data_adapter.py if you're interested)
 @off99555 @OverLordGoldDragon @ttbrunner So here's what I would recommend going forward:
 
 you can predict the output using model call, not model predict, i.e., calling model(x) would make this much faster because there are no ""conversion to dataset"" part, and also it's directly calling a cached tf.function. However be aware that if you have batch norm layers or any other layers that behaves differently between training and inference, make sure to call it with model(x, training=False)
 I will make a docstring to recommend users to use model call and explain predict is for large dataset.
 
 SG?
 
 <denchmark-link:https://github.com/tanzhenyu>@tanzhenyu</denchmark-link>
  <denchmark-link:https://github.com/robieta>@robieta</denchmark-link>
  Could you please clarify what the difference between model.predict_on_batch(x) and model(x) is?
 The call stack and execution times (as captured by statistical profiler pyinstrument) are extremely different.
 Here are the details for model.predict_on_batch(x):
 <denchmark-code>0.017 <module>  code1.py:1
 └─ 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training.py:1220
    └─ 0.017 predict_on_batch  tensorflow_core/python/keras/engine/training_v2_utils.py:514
       ├─ 0.005 _standardize_user_data  tensorflow_core/python/keras/engine/training.py:2247
       │  ├─ 0.000 run_eagerly  tensorflow_core/python/keras/engine/training.py:508
       │  │  └─ 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126
       │  └─ 0.005 _standardize_tensors  tensorflow_core/python/keras/engine/training.py:2385
       │     ├─ 0.001 standardize_input_data  tensorflow_core/python/keras/engine/training_utils.py:460
       │     │  └─ 0.000 <listcomp>  tensorflow_core/python/keras/engine/training_utils.py:526
       │     │     └─ 0.000 standardize_single_array  tensorflow_core/python/keras/engine/training_utils.py:439
       │     ├─ 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471
       │     │  └─ 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431
       │     │     ├─ 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396
       │     │     │  └─ 0.000 _yield_value  tensorflow_core/python/util/nest.py:174
       │     │     │     └─ 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179
       │     │     └─ 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119
       │     ├─ 0.002 map_structure  tensorflow_core/python/util/nest.py:507
       │     │  ├─ 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568
       │     │  │  └─ 0.001 _type_spec_from_value  tensorflow_core/python/keras/engine/training.py:2431
       │     │  │     └─ 0.000 __init__  tensorflow_core/python/framework/tensor_spec.py:39
       │     │  └─ 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471
       │     │     └─ 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431
       │     │        ├─ 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396
       │     │        │  └─ 0.000 _yield_value  tensorflow_core/python/util/nest.py:174
       │     │        │     └─ 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179
       │     │        └─ 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119
       │     │           └─ 0.000 [self]  
       │     └─ 0.000 wrapped  tensorflow_core/python/training/tracking/layer_utils.py:126
       ├─ 0.001 cast_to_model_input_dtypes  tensorflow_core/python/keras/engine/training_utils.py:1363
       │  └─ 0.001 map_structure  tensorflow_core/python/util/nest.py:507
       │     ├─ 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471
       │     │  └─ 0.000 _pack_sequence_as  tensorflow_core/python/util/nest.py:431
       │     │     └─ 0.000 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396
       │     ├─ 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568
       │     │  └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
       │     │     └─ 0.001 cast  tensorflow_core/python/ops/math_ops.py:649
       │     │        └─ 0.000 cast  tensorflow_core/python/ops/gen_math_ops.py:1944
       │     └─ 0.000 pack_sequence_as  tensorflow_core/python/util/nest.py:471
       ├─ 0.000 _get_or_make_on_batch_function  tensorflow_core/python/keras/engine/training_v2_utils.py:103
       └─ 0.010 __call__  tensorflow_core/python/eager/def_function.py:551
          └─ 0.010 _call  tensorflow_core/python/eager/def_function.py:590
             └─ 0.010 __call__  tensorflow_core/python/eager/function.py:2359
                ├─ 0.000 _maybe_define_function  tensorflow_core/python/eager/function.py:2637
                │  └─ 0.000 _cache_key  tensorflow_core/python/eager/function.py:2496
                └─ 0.010 _filtered_call  tensorflow_core/python/eager/function.py:1593
                   └─ 0.009 _call_flat  tensorflow_core/python/eager/function.py:1613
                      ├─ 0.008 call  tensorflow_core/python/eager/function.py:505
                      │  └─ 0.008 quick_execute  tensorflow_core/python/eager/execute.py:33
                      │     └─ 0.008 [self]  
                      └─ 0.001 _build_call_outputs  tensorflow_core/python/eager/function.py:1909
                         └─ 0.001 pack_sequence_as  tensorflow_core/python/util/nest.py:471
                            └─ 0.001 _pack_sequence_as  tensorflow_core/python/util/nest.py:431
                               ├─ 0.001 _packed_nest_with_indices  tensorflow_core/python/util/nest.py:396
                               │  └─ 0.000 _yield_value  tensorflow_core/python/util/nest.py:174
                               │     └─ 0.000 _yield_sorted_items  tensorflow_core/python/util/nest.py:179
                               └─ 0.000 _sequence_like  tensorflow_core/python/util/nest.py:119
 </denchmark-code>
 
 And here are the details for the much slower model(x):
 <denchmark-code>0.041 <module>  code2.py:1
 └─ 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628
    └─ 0.041 call  tensorflow_core/python/keras/engine/network.py:693
       └─ 0.041 _run_internal_graph  tensorflow_core/python/keras/engine/network.py:791
          └─ 0.041 __call__  tensorflow_core/python/keras/layers/recurrent.py:637
             └─ 0.041 __call__  tensorflow_core/python/keras/engine/base_layer.py:628
                ├─ 0.001 helper  contextlib.py:237
                │  └─ 0.001 __init__  contextlib.py:81
                ├─ 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:2689
                │  └─ 0.039 call  tensorflow_core/python/keras/layers/recurrent.py:699
                │     ├─ 0.001 _process_inputs  tensorflow_core/python/keras/layers/recurrent.py:804
                │     │  └─ 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:613
                │     │     └─ 0.001 get_initial_state  tensorflow_core/python/keras/layers/recurrent.py:2455
                │     │        └─ 0.001 _generate_zero_filled_state_for_cell  tensorflow_core/python/keras/layers/recurrent.py:2891
                │     │           └─ 0.001 _generate_zero_filled_state  tensorflow_core/python/keras/layers/recurrent.py:2898
                │     │              └─ 0.001 map_structure  tensorflow_core/python/util/nest.py:507
                │     │                 └─ 0.001 <listcomp>  tensorflow_core/python/util/nest.py:568
                │     │                    └─ 0.001 create_zeros  tensorflow_core/python/keras/layers/recurrent.py:2905
                │     │                       └─ 0.001 zeros  tensorflow_core/python/ops/array_ops.py:2399
                │     │                          └─ 0.001 fill  tensorflow_core/python/ops/array_ops.py:198
                │     │                             └─ 0.001 fill  tensorflow_core/python/ops/gen_array_ops.py:3193
                │     └─ 0.038 rnn  tensorflow_core/python/keras/backend.py:3808
                │        ├─ 0.001 <listcomp>  tensorflow_core/python/keras/backend.py:4023
                │        │  └─ 0.001 _slice_helper  tensorflow_core/python/ops/array_ops.py:759
                │        │     └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
                │        ├─ 0.001 [self]  
                │        └─ 0.036 while_loop  tensorflow_core/python/ops/control_flow_ops.py:2482
                │           ├─ 0.018 _step  tensorflow_core/python/keras/backend.py:4096
                │           │  ├─ 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │           │  ├─ 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │           │  │  └─ 0.003 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │           │  │     ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │           │  │     │  └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           │  │     │     └─ 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396
                │           │  │     ├─ 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508
                │           │  │     │  └─ 0.001 bias_add  tensorflow_core/python/ops/nn_ops.py:2699
                │           │  │     │     └─ 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396
                │           │  │     └─ 0.001 dot  tensorflow_core/python/keras/backend.py:1614
                │           │  │        └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
                │           │  │           └─ 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617
                │           │  │              └─ 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576
                │           │  ├─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4132
                │           │  │  └─ 0.001 wrapped  tensorflow_core/python/util/tf_should_use.py:234
                │           │  │     └─ 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:1139
                │           │  │        └─ 0.001 write  tensorflow_core/python/ops/tensor_array_ops.py:829
                │           │  │           └─ 0.001 _write  tensorflow_core/python/ops/tensor_array_ops.py:779
                │           │  │              └─ 0.001 numpy  tensorflow_core/python/framework/ops.py:918
                │           │  │                 └─ 0.001 _numpy  tensorflow_core/python/framework/ops.py:905
                │           │  ├─ 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │           │  │  └─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070
                │           │  │     └─ 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889
                │           │  │        └─ 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662
                │           │  ├─ 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │           │  │  └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │           │  │     ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │           │  │     │  └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           │  │     │     └─ 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189
                │           │  │     │        └─ 0.001 __eq__  tensorflow_core/python/framework/dtypes.py:261
                │           │  │     └─ 0.001 dot  tensorflow_core/python/keras/backend.py:1614
                │           │  │        └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
                │           │  │           └─ 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617
                │           │  │              └─ 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576
                │           │  ├─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           │  │  └─ 0.001 __exit__  tensorflow_core/python/framework/ops.py:6396
                │           │  ├─ 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │           │  │  └─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │           │  │     └─ 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │           │  │        └─ 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │           │  │           └─ 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                │           │  │              └─ 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236
                │           │  │                 └─ 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263
                │           │  │                    └─ 0.001 _autopacking_conversion_function  tensorflow_core/python/ops/array_ops.py:1355
                │           │  │                       └─ 0.001 _should_not_autopack  tensorflow_core/python/ops/array_ops.py:1345
                │           │  │                          └─ 0.001 <genexpr>  tensorflow_core/python/ops/array_ops.py:1351
                │           │  ├─ 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │           │  │  ├─ 0.001 [self]  
                │           │  │  └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │           │  │     ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │           │  │     │  └─ 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246
                │           │  │     │     └─ 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134
                │           │  │     │        └─ 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720
                │           │  │     └─ 0.001 dot  tensorflow_core/python/keras/backend.py:1614
                │           │  │        └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
                │           │  │           └─ 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617
                │           │  │              └─ 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576
                │           │  ├─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           │  │  └─ 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189
                │           │  │     └─ 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451
                │           │  ├─ 0.001 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │           │  │  └─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │           │  │     └─ 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │           │  │        └─ 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │           │  │           └─ 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                │           │  │              └─ 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236
                │           │  │                 └─ 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263
                │           │  └─ 0.003 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │           │     ├─ 0.001 [self]  
                │           │     └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │           │        ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │           │        │  └─ 0.001 sigmoid  tensorflow_core/python/keras/activations.py:246
                │           │        │     └─ 0.001 sigmoid  tensorflow_core/python/ops/math_ops.py:3134
                │           │        │        └─ 0.001 sigmoid  tensorflow_core/python/ops/gen_math_ops.py:8720
                │           │        └─ 0.001 dot  tensorflow_core/python/keras/backend.py:1614
                │           │           └─ 0.001 wrapper  tensorflow_core/python/util/dispatch.py:177
                │           │              └─ 0.001 matmul  tensorflow_core/python/ops/math_ops.py:2617
                │           │                 └─ 0.001 mat_mul  tensorflow_core/python/ops/gen_math_ops.py:5576
                │           ├─ 0.001 [self]  
                │           ├─ 0.004 _step  tensorflow_core/python/keras/backend.py:4096
                │           │  ├─ 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │           │  │  └─ 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │           │  │     └─ 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │           │  │        └─ 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │           │  │           └─ 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                │           │  │              ├─ 0.001 quick_execute  tensorflow_core/python/eager/execute.py:33
                │           │  │              └─ 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236
                │           │  │                 └─ 0.001 convert_to_tensor  tensorflow_core/python/framework/ops.py:1263
                │           │  │                    └─ 0.001 get  tensorflow_core/python/framework/tensor_conversion_registry.py:114
                │           │  └─ 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │           │     └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │           │        ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │           │        │  └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           │        │     └─ 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197
                │           │        │        └─ 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093
                │           │        └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │           ├─ 0.001 assert_same_structure  tensorflow_core/python/util/nest.py:293
                │           └─ 0.012 _step  tensorflow_core/python/keras/backend.py:4096
                │              ├─ 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │              │  └─ 0.002 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │              │     └─ 0.002 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │              │        └─ 0.002 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │              │           └─ 0.002 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                │              ├─ 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │              │  └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │              │     ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │              │     │  └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │              │     │     └─ 0.001 _mul_dispatch  tensorflow_core/python/ops/math_ops.py:1197
                │              │     │        └─ 0.001 mul  tensorflow_core/python/ops/gen_math_ops.py:6093
                │              │     └─ 0.001 binary_op_wrapper  tensorflow_core/python/ops/math_ops.py:899
                │              │        └─ 0.001 _add_dispatch  tensorflow_core/python/ops/math_ops.py:1189
                │              │           └─ 0.001 add_v2  tensorflow_core/python/ops/gen_math_ops.py:451
                │              ├─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108
                │              │  └─ 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127
                │              │     └─ 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746
                │              ├─ 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │              │  ├─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070
                │              │  │  └─ 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889
                │              │  │     └─ 0.001 select_v2  tensorflow_core/python/ops/gen_math_ops.py:8662
                │              │  └─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │              │     └─ 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │              │        └─ 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │              │           └─ 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                │              │              └─ 0.001 args_to_matching_eager  tensorflow_core/python/eager/execute.py:236
                │              │                 └─ 0.001 <listcomp>  tensorflow_core/python/eager/execute.py:271
                │              ├─ 0.002 step  tensorflow_core/python/keras/layers/recurrent.py:765
                │              │  └─ 0.002 call  tensorflow_core/python/keras/layers/recurrent.py:2355
                │              │     ├─ 0.001 _compute_carry_and_output_fused  tensorflow_core/python/keras/layers/recurrent.py:2346
                │              │     │  └─ 0.001 tanh  tensorflow_core/python/keras/activations.py:224
                │              │     │     └─ 0.001 tanh  tensorflow_core/python/ops/gen_math_ops.py:10284
                │              │     └─ 0.001 bias_add  tensorflow_core/python/keras/backend.py:5508
                │              ├─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4108
                │              │  └─ 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:1127
                │              │     └─ 0.001 read  tensorflow_core/python/ops/tensor_array_ops.py:746
                │              └─ 0.002 compute_masked_output  tensorflow_core/python/keras/backend.py:4065
                │                 ├─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4070
                │                 │  └─ 0.001 where_v2  tensorflow_core/python/ops/array_ops.py:3889
                │                 └─ 0.001 <genexpr>  tensorflow_core/python/keras/backend.py:4067
                │                    └─ 0.001 _expand_mask  tensorflow_core/python/keras/backend.py:3913
                │                       └─ 0.001 tile  tensorflow_core/python/ops/gen_array_ops.py:10344
                │                          └─ 0.001 tile_eager_fallback  tensorflow_core/python/ops/gen_array_ops.py:10431
                └─ 0.001 _set_mask_metadata  tensorflow_core/python/keras/engine/base_layer.py:1918
                   └─ 0.001 flatten  tensorflow_core/python/util/nest.py:242
 </denchmark-code>
 
 Out of the two suggestions you guys provided in this thread, the one that ended up being put in the docstring is the slowest one (even slower than model.predict(x)) for my simple LSTM encoder being run on Tensorflow 2.1.0 on with GPU support, and other users like me might be confused.
 Also, can you please clarify why calling model(x) runs the model step by step even though run_eagerly is set to False and, as per tanzhenyu's comment, it should instead call a cached tf.function?
 Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33365,tb438,2019-10-15T03:37:06Z,2020-03-10T20:49:02Z,No float64 support with batch normalization in Tensorflow 2.0?,"
 Stock Ubuntu 19.04 with Cuda 10.0, Tensorflow 2.0.0 installed via pip3, Python 3.7.3, GTX1060.
 I have a float64 valued dataset with a simple conv2d network that includes tf.keras.layers.BatchNormalization() which is where the error is being thrown I think.
 The first set of issues:
 <denchmark-code>WARNING:tensorflow:Layer conv2d is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.
 
 If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.
 
 To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. 
 </denchmark-code>
 
 After setting tf.keras.backend.set_floatx('float64'), next set of errors:
 <denchmark-code>Traceback (most recent call last):
   File ""/home/aj/ga.py"", line 183, in <module>
     encoder = make_encoder_model(z_dim)
   File ""/home/aj/ga.py"", line 138, in make_encoder_model
     x = tf.keras.layers.BatchNormalization()(x)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 842, in __call__
     outputs = call_fn(cast_inputs, *args, **kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 659, in call
     outputs = self._fused_batch_norm(inputs, training=training)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 517, in _fused_batch_norm
     training, _fused_batch_norm_training, _fused_batch_norm_inference)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
     pred, true_fn=true_fn, false_fn=false_fn, name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py"", line 59, in smart_cond
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
     return func(*args, **kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py"", line 84, in cond_v2
     op_return_value=pred)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 503, in _fused_batch_norm_training
     data_format=self._data_format)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py"", line 1509, in fused_batch_norm
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 4620, in fused_batch_norm_v3
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 631, in _apply_op_helper
     param_name=input_name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
     "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
 TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
 </denchmark-code>
 
 So is there perhaps another (hopefully drop in) method of batch normalization that supports float64? I don't want to go hacking at allowed_list and all that.
 	",1.0,tb438,2019-10-16T06:56:58Z,"
 		<denchmark-link:https://github.com/tb438>@tb438</denchmark-link>
 
 In order to expedite the trouble-shooting process, please provide a minimal standalone code to reproduce the issue reported here. Thanks!
 		",2.0,tb438,2019-10-25T10:43:17Z,"
 		<denchmark-link:https://github.com/tb438>@tb438</denchmark-link>
 
 Please, let us know any update on this issue. Thanks!
 		",3.0,tb438,2019-10-25T21:43:45Z,"
 		Yes give me another day and I'll upload sanitized code.
 		",2ebc291f6a94163c49fc835d3afc93892e645e45,Reed Wanderman-Milne,2020-03-10 13:46:51-07:00,MODIFY,1,tensorflow\python\keras\layers\normalization.py,tensorflow\python\keras\layers\normalization.py,1.0,"254,255,256,257,276,277,278,279,280",,MODIFY,1.0,tensorflow\python\keras\layers\normalization_test.py,tensorflow\python\keras\layers\normalization_test.py,4.0,tb438,2019-10-29T15:23:57Z,"
 		Hi. I have a similar problem.
 I used this keras code <denchmark-link:https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py>https://github.com/keras-team/keras/blob/master/examples/cifar10_resnet.py</denchmark-link>
 .
 And it works fine with TF2.
 But if I set  K.set_floatx('float64')
 It writed ""Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32""
 Are there working examples, where batch normalization works with float64 in TF 2?
 		",5.0,tb438,2019-10-30T02:20:58Z,"
 		Hi Ravikyram,
 Here is the code and exception:
 Python 3.7.5rc1 (default, Oct  8 2019, 16:47:45)
 [GCC 9.2.1 20191008] on linux
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 
 
 
 import numpy as np
 import tensorflow as tf
 print(tf.version)
 2.0.0
 tf.keras.backend.set_floatx('float64')
 inputs = tf.keras.layers.Input(shape=(28, 28, 1))
 x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
 [...]
 x = tf.keras.layers.LeakyReLU(0.2)(x)
 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
 x = tf.keras.layers.BatchNormalization()(x)
 Traceback (most recent call last):
 File """", line 1, in 
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 842, in call
 outputs = call_fn(cast_inputs, *args, **kwargs)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 659, in call
 outputs = self._fused_batch_norm(inputs, training=training)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 517, in _fused_batch_norm
 training, _fused_batch_norm_training, _fused_batch_norm_inference)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
 pred, true_fn=true_fn, false_fn=false_fn, name=name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py"", line 59, in smart_cond
 name=name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
 return func(*args, **kwargs)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
 return cond_v2.cond_v2(pred, true_fn, false_fn, name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py"", line 84, in cond_v2
 op_return_value=pred)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 915, in func_graph_from_py_func
 func_outputs = python_func(*func_args, **func_kwargs)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 503, in _fused_batch_norm_training
 data_format=self._data_format)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py"", line 1509, in fused_batch_norm
 name=name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 4620, in fused_batch_norm_v3
 name=name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 631, in _apply_op_helper
 param_name=input_name)
 File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 60, in _SatisfiesTypeConstraint
 "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
 TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
 
 
 
 		",6.0,tb438,2019-10-30T10:03:43Z,"
 		I have tried on colab with TF version 2.0 , 2.1.0-dev20191029 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/e6adf13651041e582a832373063de997/untitled319.ipynb>here</denchmark-link>
 .Thanks!
 		",1.0,"72,73,74,75,76,77,78,79,80,81",,test_basic_batchnorm,self,48,81,,,,,,,,,,,,,,,_raise_if_fused_cannot_be_used,self,248,280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,tb438,2019-10-30T15:09:38Z,"
 		Hi Ravikyram,
 Thank you for the prompt response. Using your gist above on Colab, I am still getting the same error?
 2.0.0:
 <denchmark-code>---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
 <ipython-input-12-fffaff38b358> in <module>()
       5 x = tf.keras.layers.LeakyReLU(0.2)(x)
       6 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
 ----> 7 x = tf.keras.layers.BatchNormalization()(x)
 
 13 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
      58           ""allowed values: %s"" %
      59           (param_name, dtypes.as_dtype(dtype).name,
 ---> 60            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
      61 
      62 
 
 TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
 </denchmark-code>
 
 And with 2.1.0:
 <denchmark-code>---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
 <ipython-input-3-fffaff38b358> in <module>()
       5 x = tf.keras.layers.LeakyReLU(0.2)(x)
       6 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
 ----> 7 x = tf.keras.layers.BatchNormalization()(x)
 
 13 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py in _SatisfiesTypeConstraint(dtype, attr_def, param_name)
      59           ""allowed values: %s"" %
      60           (param_name, dtypes.as_dtype(dtype).name,
 ---> 61            "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
      62 
      63 
 
 TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,tb438,2019-10-30T15:28:08Z,"
 		I've updated my development workstation to 2.1.0 as well, same issue:
 <denchmark-code>>>> import tensorflow as tf
 >>> import numpy as np
 >>> tf.version
 '2.1.0-dev20191029'
 >>> tf.keras.backend.set_floatx('float64')
 >>> inputs = tf.keras.layers.Input(shape=(28, 28, 1))
 >>> x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
 >>> x = tf.keras.layers.LeakyReLU(0.2)(x)
 >>> x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
 >>> x = tf.keras.layers.BatchNormalization()(x)
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 773, in __call__
     outputs = call_fn(cast_inputs, *args, **kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 695, in call
     outputs = self._fused_batch_norm(inputs, training=training)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 553, in _fused_batch_norm
     training, _fused_batch_norm_training, _fused_batch_norm_inference)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/utils/tf_utils.py"", line 59, in smart_cond
     pred, true_fn=true_fn, false_fn=false_fn, name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/smart_cond.py"", line 59, in smart_cond
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func
     return func(*args, **kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/control_flow_ops.py"", line 1174, in cond
     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/cond_v2.py"", line 83, in cond_v2
     op_return_value=pred)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/func_graph.py"", line 958, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/layers/normalization.py"", line 539, in _fused_batch_norm_training
     data_format=self._data_format)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py"", line 1502, in fused_batch_norm
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/gen_nn_ops.py"", line 4248, in fused_batch_norm_v3
     name=name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 576, in _apply_op_helper
     param_name=input_name)
   File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/op_def_library.py"", line 61, in _SatisfiesTypeConstraint
     "", "".join(dtypes.as_dtype(x).name for x in allowed_list)))
 TypeError: Value passed to parameter 'x' has DataType float64 not in list of allowed values: float16, bfloat16, float32
 </denchmark-code>
 
 Can you try running the same code with GPU-enabled Colab? Perhaps this would only fixed for the CPU branch and not tensorflow-gpu 2.1.0?
 Thanks again for your help.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10.0,tb438,2019-11-25T05:16:42Z,"
 		LOOK, AGAIN, YOUR REDUCED PRECISION INT8 AND INT16 AINT MEAN NOTHING TO US, DO YOU UNDERSTAND GOOGLE? YOUR GOOGLE TPU CAIN'T TRAIN, SO IMA GO WITH CUDA128 JUST AS SOON AS YOU BRING MY FLOAT64 BATCH NORMALIZATION BABY BACK
 		",11.0,tb438,2020-01-08T01:03:18Z,"
 		Hello,
 Is there a solution for this?
 i am getting it when i call the model.predict on boolean data.
 it also causes a segmentation fault.
 i am using tf2`s keras
 		",12.0,tb438,2020-01-10T13:54:55Z,"
 		Hi all, supporting float64 would be really cool indeed :)
 <denchmark-link:https://github.com/TrailBlazerAI>@TrailBlazerAI</denchmark-link>
  maybe consider a change of tone when asking for other people to help you?
 		",13.0,tb438,2020-01-11T02:05:07Z,"
 		
 Hello,
 Is there a solution for this?
 i am getting it when i call the model.predict on boolean data.
 it also causes a segmentation fault.
 i am using tf2`s keras
 
 I changed the data to np.float32 and it worked well with tf2, maybe it may help someone still having this issue.
 Thanks
 		",14.0,tb438,2020-02-11T14:15:56Z,"
 		Hello,
 Can anyone provide info on the matter?
 Is it gonna be fixed? If no, why? Is there a timeline? Are there any blockers?
 Many thanks,
 
 Julien
 
 		",15.0,tb438,2020-03-02T11:09:15Z,"
 		Are there any updates to this? I'm facing the same problem and I'm having to set BatchNorm's dtype to float32 for now but it'd be nice to be able to use float64
 		",16.0,tb438,2020-03-10T16:11:38Z,"
 		This appears to be an issue with fused batch norm in particular, and if you disable the fused kernel, the code above does not raise an error:
 <denchmark-code>import numpy as np
 import tensorflow as tf
 print(tf.version)
 tf.keras.backend.set_floatx('float64')
 inputs = tf.keras.layers.Input(shape=(28, 28, 1))
 x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same')(inputs)
 x = tf.keras.layers.LeakyReLU(0.2)(x)
 x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(x)
 x = tf.keras.layers.BatchNormalization(fused=False)(x)
 </denchmark-code>
 
 <denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
  / <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  -- this is an unfortunate hard edge for BatchNorm. Can we either add fp64 support to the fused kernel or not default to fusing for unsupported datatypes?
 		",17.0,tb438,2020-03-10T17:40:14Z,"
 		I'll start by not defaulting to fused for unsupported datatypes. We can add fp64 support to the fused kernel later.
 		",18.0,tb438,2020-03-10T20:49:04Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33365>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33365>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33376,rggjan,2019-10-15T13:32:32Z,2019-12-17T21:41:47Z,importing tensorflow inside a function/object causes a memory leak,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.15
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 TensorFlow installed from (source or binary): pip install tensorflow==1.14
 TensorFlow version (use command below): 1.14.0
 Python version: 3.6.8
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: -
 GPU model and memory: -
 
 Describe the current behavior
 When importing tensorflow from a function or object, the import statement somehow keeps a reference to the function and increasing it's reference count. The full import stacktrace is never freed, making it impossible for the object (and anything referenced from that object or function) to be freed from memory.
 Describe the expected behavior
 It should be possible to free the function calling import tensorflow. This is not an issue with any other imports (like import logger).
 Code to reproduce the issue
 <denchmark-code>import gc
 
 
 class TFImporter:
     def __init__(self, name):
         self._name = name
         print(f""TFImporter init {self._name}"")
 
     def get_tf(self):
         print(f""import tensorflow {self._name}"")
         import tensorflow
         print(tensorflow.version.VERSION)
 
     def get_other_module(self):
         print(f""import logging {self._name}"")
         import logging
         logging.info(""Message"")
 
     def __del__(self):
         print(f""TFImporter delete {self._name}"")
 
 
 def main():
     importer1 = TFImporter(1)
     importer1.get_other_module()
     del importer1
     print(""importer1 deleted"")
 
     importer2 = TFImporter(2)
     importer2.get_tf()
     del importer2
     print(""importer2 deleted"")
 
     importer3 = TFImporter(3)
     importer3.get_tf()
     del importer3
     print(""importer3 deleted"")
 
     print(f""Garbage collection: {gc.collect()}"")
 
     print(f""Waiting for input:"")
     input()
 
 
 main()
 </denchmark-code>
 
 this outputs:
 <denchmark-code>/Users/jan/miniconda/envs/foo/bin/python /Users/jan/code/tensorflow_error.py
 TFImporter init 1
 import logging 1
 TFImporter delete 1
 importer1 deleted
 TFImporter init 2
 import tensorflow 2
 1.14.0
 importer2 deleted
 TFImporter init 3
 import tensorflow 3
 1.14.0
 TFImporter delete 3
 importer3 deleted
 Garbage collection: 22
 Waiting for input:
 foo
 TFImporter delete 2
 
 Process finished with exit code 0
 </denchmark-code>
 
 So importer2 is only freed after the python application finishes. Neither gc.collect nor deleting the object causes it to be released in python.
 This is not an issue in this toy example, but importer2 could have a reference to a large number of other objects that take considerable space in memory in reality.
 Also, this only happens for the first import. importer3 can be freed without issues.
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/3729501/tf_env.txt>tf_env.txt</denchmark-link>
 
 	",1.0,rggjan,2019-10-16T06:34:23Z,"
 		Issue replicating for Tf 1.14, kindly for the <denchmark-link:https://colab.sandbox.google.com/gist/oanush/e224d0fa208b982d84e5cb18c1317ce9/33376.ipynb>gist</denchmark-link>
 .ThThanks!
 		",2.0,rggjan,2019-12-16T21:11:18Z,"
 		<denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  is this related to lazy loaders or estimator/keras integration?
 		",3.0,rggjan,2019-12-17T00:30:38Z,"
 		It appears to be due to saving error when importing portpicker here:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L44>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/test_util.py#L44</denchmark-link>
 
 We can probably save the error text instead or import portpicker inside the function that creates a cluster.
 		",413d0fa9d75e99e02b74bb079465bea728eb3a44,Anna R,2019-12-17 13:40:41-08:00,MODIFY,0,tensorflow\python\framework\test_util.py,tensorflow\python\framework\test_util.py,0.0,3088,"40,41,42,43,44,45,46,47,3096,3097",,,,,4.0,rggjan,2019-12-17T01:08:20Z,"
 		Yeah, it seems weird that we'd survive the import, only to error out later.
 Let's either import it inside the cluster creation, or print the error
 directly on import (but still continue), and later error with a note to
 look for the earlier error.
 
 Honestly, I think the local import is preferable in this case.
 		",5.0,rggjan,2019-12-17T21:41:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33376>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33376>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33425,racinmat,2019-10-16T12:25:29Z,2020-03-13T22:19:12Z,"Tensorflow eager execution not working with tf.math.unsorted_segment_max, Gradient output is null","
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Professional Edition
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary, installed using conda
 TensorFlow version (use command below): unknown, 1.14.0
 Python version: 3.7.3
 CUDA/cuDNN version: 10.0, 7.6
 GPU model and memory: T1000, 4GB VRAM
 
 Describe the current behavior
 When using tf.math.unsorted_segment_max with Tensorflow eager execution and Gradient Tape, the source code (see below) produces following error:
 <denchmark-code>Traceback (most recent call last):
   File ""C:/Projects/iotmap/py/segmented_max_error.py"", line 80, in <module>
     grads = tape.gradient(loss_value, model.trainable_weights)
   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\backprop.py"", line 980, in gradient
     unconnected_gradients=unconnected_gradients)
   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\imperative_grad.py"", line 76, in imperative_grad
     compat.as_str(unconnected_gradients.value))
   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\eager\backprop.py"", line 137, in _gradient_function
     return grad_fn(mock_op, *out_grads)
   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 349, in _UnsortedSegmentMaxGrad
     return _UnsortedSegmentMinOrMaxGrad(op, grad)
   File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 326, in _UnsortedSegmentMinOrMaxGrad
     _GatherDropNegatives(op.outputs[0], op.inputs[1])
 TypeError: 'NoneType' object is not subscriptable
 </denchmark-code>
 
 Operations tf.math.segment_max, tf.math.segment_mean and tf.math.unsorted_segment_mean are working ok, though.
 I need the unsorted version because in more complex code bases, I a using several segmented aggregations and concatenating them, so I need to have fixed sizes.
 Describe the expected behavior
 It should work without throwing error.
 
 The code is here:
 <denchmark-link:https://gist.github.com/racinmat/9a95cac7db36d5f0b6b33e9c35678ca2>https://gist.github.com/racinmat/9a95cac7db36d5f0b6b33e9c35678ca2</denchmark-link>
 
 Other info / logs
 Exception thrown is mentioned above.
 	",1.0,racinmat,2019-10-17T07:01:31Z,"
 		<denchmark-link:https://github.com/racinmat>@racinmat</denchmark-link>
 
 I tried reproducing the issue with TF 1.14 on colab. However i am seeing the different error.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/2f6897da2b30cbaba0bdd43ec9a84aa3/untitled279.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,racinmat,2019-10-17T09:53:57Z,"
 		Yes, my bad, it is fixed now in the gist and produces the abovementioned error.
 Here is the google colab notebook with fixed code: <denchmark-link:https://colab.research.google.com/gist/racinmat/057bb526253484884f3f484f62cb1f0a/untitled279.ipynb>https://colab.research.google.com/gist/racinmat/057bb526253484884f3f484f62cb1f0a/untitled279.ipynb</denchmark-link>
 
 		",3.0,racinmat,2019-10-18T10:36:56Z,"
 		I have tried on colab with TF 1.14,1.15.0-rc3 and was able to reproduce the issue
 Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/c69ad2af68dbb3ff0747b0d4ed0e7df4/untitled279.ipynb#scrollTo=YVFBL130Y2w7>here</denchmark-link>
 . Thanks!
 		",cb9319253d81374e6c9b0dc27c28fe8f5ba2ebb1,Akshay Modi,2019-10-28 19:34:08-07:00,MODIFY,0,tensorflow\python\eager\pywrap_tfe_src.cc,tensorflow\python\eager\pywrap_tfe_src.cc,0.0,,2892,,,,,4.0,racinmat,2019-10-20T08:09:50Z,"
 		Just a small question, should this still have the awaiting response label?
 		",5.0,racinmat,2019-10-29T02:35:06Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425>No</denchmark-link>
 
 		",6.0,racinmat,2019-10-29T09:17:25Z,"
 		I see the issue is fixed in master, will there be a 1.X version with the fix released sometimes?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,racinmat,2020-02-24T19:15:23Z,"
 		I'm seeing this same problem in TensorFlow 2.1.  My model uses tf.math.unsorted_segment_max().  When I call tape.gradient() in eager mode I get the error
 <denchmark-code>  File ""/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 1014, in gradient
     unconnected_gradients=unconnected_gradients)
   File ""/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py"", line 76, in imperative_grad
     compat.as_str(unconnected_gradients.value))
   File ""/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py"", line 138, in _gradient_function
     return grad_fn(mock_op, *out_grads)
   File ""/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py"", line 455, in _UnsortedSegmentMaxGrad
     return _UnsortedSegmentMinOrMaxGrad(op, grad)
   File ""/Users/peastman/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py"", line 432, in _UnsortedSegmentMinOrMaxGrad
     _GatherDropNegatives(op.outputs[0], op.inputs[1])
 TypeError: 'NoneType' object is not subscriptable
 </denchmark-code>
 
 Which versions is this supposed to be fixed in?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,racinmat,2020-03-04T12:48:24Z,"
 		The issue is still not resolved in v1.15.2, <denchmark-link:https://github.com/ravikyram>@ravikyram</denchmark-link>
   why has it been closed? The bug is still there.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,racinmat,2020-03-04T17:40:04Z,"
 		Also, in TensorFlow 2.1 it appears this is no longer restricted to eager mode.  Even if I wrap the calculation in tf.function it still fails.  Can someone reopen this so it will get fixed?
 		",10.0,racinmat,2020-03-13T22:19:12Z,"
 		Actually I cannot reproduce this against nightly TF, so I think it has been fixed since. Sorry for the noise.
 		",11.0,racinmat,2020-03-13T22:19:14Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33425>No</denchmark-link>
 
 		",12.0,racinmat,2020-05-10T08:58:42Z,"
 		<denchmark-link:https://github.com/ravikyram>@ravikyram</denchmark-link>
  This problem still happened in tensorflow-gpu==2.0.0. But, it's ok in tensorflow-gpu==2.1.0.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33526,MeghnaNatraj,2019-10-19T06:22:11Z,2020-05-04T18:27:52Z,Error while trying to use tf.broadcast_weights,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Google Colab)
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Google Colab
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0
 Python version: 3.x
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: -
 GPU model and memory: -
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Unable to import tf.broadcast_weights in TF 2.0.
 Describe the expected behavior
 Should be able to import tf.broadcast_weights in TF 2.0
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 Method 1: Plain python + TF 2.0
 <denchmark-code>import tensorflow as tf     # version 2.0
 tf.broadcast_weights
 </denchmark-code>
 
 throws ** AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'**
 Method 2: Codelab
 I found this error in a recent TF 2.0 + Keras tutorial - <denchmark-link:https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO>https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO</denchmark-link>
 
 
 Search for ""broadcast_weights"" in this codelab.
 Run all cells before this.
 Modify code ""m.update_state([0, 1, 1, 1], [0, 1, 0, 0])"" to ""m.update_state([0, 1, 1, 1], [0, 1, 0, 0]), sample_weight=[0.1,0.2,0.3,0.4]""
 Run this cell
 throws AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,MeghnaNatraj,2019-10-21T06:59:18Z,"
 		I have tried on colab with TF version 2.0  and was able to reproduce the issue. Please, find the gist here.<denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/a2a526b2e1d95d7a70d6361efdc04da9/tensorflow-2-0-keras-crash-course.ipynb>Method1</denchmark-link>
  and <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/dc8967174fec9d63a6630c2bff684c10/untitled288.ipynb>Method2</denchmark-link>
 .Thanks!
 		",2.0,MeghnaNatraj,2019-10-22T19:51:20Z,"
 		<denchmark-link:https://github.com/MeghnaNatraj>@MeghnaNatraj</denchmark-link>
  Thanks for finding the bug in the doc. Could you point us to the source of the notebook. Where did you find this notebook? Thanks!
 		",3.0,MeghnaNatraj,2019-10-22T21:03:41Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  I found this issue while trying out a recent tutorial published by François Chollet
 <denchmark-link:http://link.oreilly.com/P000Q3rFWM0iPY010sCSs30>http://link.oreilly.com/P000Q3rFWM0iPY010sCSs30</denchmark-link>
 
 (Original Link: <denchmark-link:https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO>https://colab.sandbox.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO</denchmark-link>
 )
 Search for ""tf.broadcast_weights"" in this tutorial.
 Usage 2:
 This API is also used in the TF website here: <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric>https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric</denchmark-link>
 
 Definition:
 
 
 
 tensorflow/tensorflow/python/ops/weights_broadcast_ops.py
 
 
          Line 136
       in
       9590c4c
 
 
 
 
 
 
  def broadcast_weights(weights, values): 
 
 
 
 
 
 		",5dd0b6a589f4f2c66b66a0d022a3d753588bca80,Mark Daoust,2020-05-04 11:25:40-07:00,MODIFY,0,tensorflow\python\keras\metrics.py,tensorflow\python\keras\metrics.py,0.0,141,141,,,,,4.0,MeghnaNatraj,2019-10-23T22:22:16Z,"
 		<denchmark-link:https://github.com/MeghnaNatraj>@MeghnaNatraj</denchmark-link>
  Thanks for the links. First link is already updated by the author. We will work on updating link in the . Thanks!
 		",5.0,MeghnaNatraj,2019-10-23T22:31:22Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  I think we need to update <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric>this page</denchmark-link>
 . The issue is in this line
 sample_weight = tf.broadcast_weights(sample_weight, values)
 where tf.broadcast_weights throws AttributeError: module 'tensorflow' has no attribute 'broadcast_weights'.
 I don't think this tf.broadcast_weights is exposed to the public. Thank you!
 		",6.0,MeghnaNatraj,2019-11-16T08:39:35Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  I confirmed all the codes of the tf.broadcast_weights in 
 ,
 
 
 
 tensorflow/tensorflow/python/ops/metrics.py
 
 
          Line 23
       in
       9590c4c
 
 
 
 
 
 
  from tensorflow.python.ops.metrics_impl import * 
 
 
 
 
 ,
 
 
 
 tensorflow/tensorflow/python/ops/metrics_impl.py
 
 
          Line 35
       in
       9590c4c
 
 
 
 
 
 
  from tensorflow.python.ops import weights_broadcast_ops 
 
 
 
 
 
 And these all seem fine. But still, tf.broadcast_weights throughs Attribute error.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,MeghnaNatraj,2020-02-06T16:38:30Z,"
 		tf.broadcast_weights does not work and is still referenced here (<denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric>https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric</denchmark-link>
 ).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,MeghnaNatraj,2020-05-03T20:38:33Z,"
 		The issue is that tf.broadcast_weights is not exposed through tf_export, wondering if it makes sense to expose this API given it is referenced in multiple places?
 /cc <denchmark-link:https://github.com/orgs/tensorflow/teams/api-owners>@tensorflow/api-owners</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,MeghnaNatraj,2020-05-04T14:54:21Z,"
 		I think we need to remove the reference to tf.broadcast_weights from the documentation.
 		",10.0,MeghnaNatraj,2020-05-04T17:45:54Z,"
 		I take a look at the usage of tf.broadcast_weights in docstring. The related docstring is just an example,  and broadcast_weights by itself is just to make sure it follow a subset of broadcast rules than tf.multiply. It actually makes sense to remove this line as it does not add much to be an example in docs:
          sample_weight = tf.cast(sample_weight, self.dtype)
 -        sample_weight = tf.broadcast_weights(sample_weight, values)
          values = tf.multiply(values, sample_weight)
 Created a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/39161>#39161</denchmark-link>
  for the fix.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33572,dreamibor,2019-10-21T13:18:38Z,2019-11-22T19:50:09Z,[tflite] Support INT8 quantization for PACK with TFLITE_BUILTINS_INT8 OpsSet,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 1.14
 Python version: 3.6
 
 
 Similar to the UNPACK node issue in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31902>#31902</denchmark-link>
 , the new TFLiteConverter post-training quantisation flow, as described in <denchmark-link:https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations>https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations</denchmark-link>
 , does not support quantization of PACK/STACK operation when only integer operations are requested in the output model. When such conversion is attempted the following error is reported:
 
 RuntimeError: Quantization not yet supported for op: PACK
 
 Code to reproduce the issue
 For example, the script below:
 import tensorflow as tf
 import numpy as np
 
 def representative_dataset_gen():
 	input_1 = np.ones([1, 10],dtype=np.float32)
 	input_2 = np.ones([1, 10],dtype=np.float32)
 	for _ in range(10):
 		yield [input_1, input_2]
 
 # tf Graph Input
 foo = tf.compat.v1.placeholder(""float32"", [1, 10])
 bar = tf.compat.v1.placeholder(""float32"", [1, 10])
 out_stacked = tf.stack([foo, bar], axis=0)
 
 with tf.compat.v1.Session() as sess:
 	tf.io.write_graph(tf.compat.v1.get_default_graph(), '.','pack.pb', as_text=False)
 
 input_name = [""Placeholder"", ""Placeholder_1""]
 output_name = [""stack""]
 
 tflite_model_name = ""int8_pack.tflite""
 converter = tf.lite.TFLiteConverter.from_frozen_graph(""pack.pb"", input_name, output_name)
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 converter.representative_dataset = representative_dataset_gen
 tflite_model = converter.convert()
 open(tflite_model_name, ""wb"").write(tflite_model)
 
 # Load TFLite model and allocate tensors.
 interpreter = tf.lite.Interpreter(tflite_model_name)
 interpreter.allocate_tensors()
 
 # Get input and output tensors.
 input_details = interpreter.get_input_details()
 output_details = interpreter.get_output_details()
 
 # Test model on random input data.
 input_shape = input_details[0]['shape']
 input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
 interpreter.set_tensor(input_details[0]['index'], input_data)
 interpreter.invoke()`
 produces errors as follows:
 <denchmark-code>2019-10-21 14:02:33.682706: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-10-21 14:02:33.708278: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3408000000 Hz
 2019-10-21 14:02:33.708892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x33f5a60 executing computations on platform Host. Devices:
 2019-10-21 14:02:33.708924: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
 2019-10-21 14:02:33.717107: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
 2019-10-21 14:02:33.717228: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
 INFO: Initialized TensorFlow Lite runtime.
 Traceback (most recent call last):
   File ""pack_example.py"", line 26, in <module>
     tflite_model = converter.convert()
   File ""/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 908, in convert
     inference_output_type)
   File ""/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/lite.py"", line 200, in _calibrate_quantize_model
     inference_output_type, allow_float)
   File ""/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 78, in calibrate_and_quantize
     np.dtype(output_type.as_numpy_dtype()).num, allow_float)
   File ""/home/jaszha02/Work/venvs/audio/lib/python3.6/site-packages/tensorflow/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py"", line 115, in QuantizeModel
     return _tensorflow_lite_wrap_calibration_wrapper.CalibrationWrapper_QuantizeModel(self, input_py_type, output_py_type, allow_float)
 RuntimeError: Quantization not yet supported for op: PACK
 </denchmark-code>
 
 Both kTfLiteUInt8 and kTfLiteInt8 version of the PACK operator is already implemented in TFLite (see <denchmark-link:https://github.com/tensorflow/tensorflow/blob/186e794d71c17b52deb52ace151ec5add8525f2c/tensorflow/lite/kernels/pack.cc>pack.cc</denchmark-link>
 ), so it should be straightforward to support PACK as well in the TFLite Converter.
 	",1.0,dreamibor,2019-11-08T14:16:09Z,"
 		Any updates?
 		",2.0,dreamibor,2019-11-20T18:26:48Z,"
 		Hi, I have a CL in progress to fix this.
 		",3.0,dreamibor,2019-11-22T19:50:10Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33572>No</denchmark-link>
 
 		",d8668d9e03b65c4a6d9ecb08e74b0b67798fbbab,Suharsh Sivakumar,2019-11-22 11:49:22-08:00,MODIFY,1,tensorflow\lite\tools\optimize\operator_property.cc,tensorflow\lite\tools\optimize\operator_property.cc,1.0,"289,290,291,292,293,294",,,,,,4.0,dreamibor,2020-07-31T09:04:38Z,"
 		Hey, I'm facing the same issue when converting  (see <denchmark-link:https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md>model zoo</denchmark-link>
 ) to quantized tflite.
 Simple conversion to  tflite works just fine.
 With TF 2 I can't even convert the model to tflite proper.
 Using tensorflow_gpu version 1.5.3 on google Colab, Ubuntu 18.04.
 Any help would be much appreciated 🙏
 Conversion script looks like this:
 import tensorflow as tf
 import pathlib
 import os
 import cv2
 from PIL import Image
 import json
 import numpy as np
 
 frozen_model = './mobilenet_varroa_w_inputshape/frozen_inference_graph_00/tflite_graph.pb'
 # TF 1
 input_shapes = {'normalized_input_image_tensor':(1,896,896,3)}
 input_arrays = ['normalized_input_image_tensor']
 output_arrays = ['TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3']
 converter = tf.lite.TFLiteConverter.from_frozen_graph(frozen_model, input_arrays, output_arrays, input_shapes)
 
 converter.experimental_new_converter = False
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 
 def representative_dataset_gen():
     directory_images = ""./rep_dataset/""
     for img in os.listdir(directory_images):
       tmp_img = Image.open(f""{directory_images}/{img}"")
       tmp_img = np.array(tmp_img.getdata()).reshape((807, 807, 3)).astype(np.uint8)
       tmp_img = cv2.resize(tmp_img, (896, 896), interpolation=cv2.INTER_AREA)
       input_data = tmp_img.astype('float32')/255.0
       input_data = tf.expand_dims(input_data, 0)
       input_data = tf.convert_to_tensor(input_data, dtype=tf.float32)
       yield [input_data]
 
 converter.representative_dataset = representative_dataset_gen
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 converter.allow_custom_ops = True
 converter.inference_input_type = tf.int8 
 converter.inference_output_type = tf.int8 
 tflite_quant_model = converter.convert() # crashes on this line
 open(""mobilenet_test.tflite"", ""wb"").write(tflite_quant_model)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tflite::optimize::operator_property::GetOperatorProperty,"model,subgraph_index,op_index",41,397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33724,omoindrot,2019-10-25T15:33:43Z,2020-02-03T19:21:56Z,Infinite loop with generators wrapping a dataset in tf.function,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
 Python version: 3.7.4
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.0 / 7.6.3
 GPU model and memory: TITAN Xp, 12196MiB
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 My use case was to use tqdm to track progress on a training loop over a tf.data.Dataset:
 @tf.function
 def train_one_epoch(model, dataset):
     for x in tqdm(dataset):
         train_step(model, x)
 However, when the function train_one_epoch is wrapped in a tf.function, the AutoGraph is stuck in an infinite loop.
 Describe the expected behavior
 The expected behavior would be that using tf.function results in the same behavior than the eager mode.
 The current issue is that AutoGraph doesn't recognize tqdm(dataset) as a tf.data.Dataset (which is normal). However, iterating infinitely over the dataset in AutoGraph is weird and shouldn't happen. Maybe it should give an exception.
 Maybe the easiest fix would be to prevent dataset.__iter__ being called inside tf.function if it is not in a loop.
 So for x in dataset would be fine, but for x in tqdm(dataset) wouldn't.
 Code to reproduce the issue
 import tensorflow as tf
 
 
 class Iterable():
     def __init__(self, iterable):
         self.iterable = iterable
 
     def __iter__(self):
         for obj in self.iterable:
             yield obj
 
 @tf.function
 def f(dataset):
     for x in Iterable(dataset):
         print(x)
 
 dataset = tf.data.Dataset.range(5)
 f(dataset)
 The minimal Iterable class can be replaced by tqdm (from tqdm import tqdm), and this should yield the same results.
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 Without the @tf.function, the wrapped dataset Iterable(dataset) is iterated over in eager mode:
 <denchmark-code>tf.Tensor(0, shape=(), dtype=int64)
 tf.Tensor(1, shape=(), dtype=int64)
 tf.Tensor(2, shape=(), dtype=int64)
 tf.Tensor(3, shape=(), dtype=int64)
 tf.Tensor(4, shape=(), dtype=int64)
 </denchmark-code>
 
 With the @tf.function, the AutoGraph mode doesn't recognize the wrapped Iterable(dataset) as a tf.data.Dataset, so it tries to iterate over it in python to trace the graph. However, it looks like the Iterable(dataset).__iter__ infinitely yields a next element named IteratorGetNext.
 This results in an infinite iteration over the dataset:
 <denchmark-code>Tensor(""IteratorGetNext:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_1:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_2:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_3:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_4:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_5:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_6:0"", shape=(), dtype=int64)
 Tensor(""IteratorGetNext_7:0"", shape=(), dtype=int64)
 ...
 </denchmark-code>
 
 	",1.0,omoindrot,2019-12-24T13:07:28Z,"
 		Indeed, AutoGraph does not recognize Iterator or the generators executed by tqdm. I'll explain in more detail below, but in general the objects are not recognized because they are not subclasses of a dataset or a dataset iterator.
 AutoGraph only transforms objects of types it recognizes - everything else is executed in Python, at function tracing time. For this reason, when running the code above, it's as if you ran it without AutoGraph (try executing , it should behave identically). To better understand this, have a look at the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/operators/control_flow.py#L331>dispatch logic for for loops</denchmark-link>
 . Since  is not an instance of any of the classes listed there, AutoGraph reverts to running the for loop in Python. In Python, it will cycle infinitely because the  objects that the hidden  iterator returns evaluate to True.  never returns  in graph mode - it can't, because it doesn't know how much data there is at graph construction.
 You might wonder why AutoGraph doesn't detect that the for loop inside  which does use a Dataset. The answer is because AutoGraph doesn't touch generators, however <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/core/unsupported_features_checker.py#L40>the warning it generates is silenced by default</denchmark-link>
 . I think we'll need to make that warning more obvious.
 Now, AutoGraph could detect that a for loop is trying to iterate over a generator and disallow it in graph mode. However, that could break legitimate uses of generators (e.g. such as those used by Keras), but again, it looks like a warning would be useful.
 Separately from all that, tqdm itself contains console output logic and other operations that are highly dependent on the Python runtime and which are not very compatible with TF graphs. So it would be tricky to run in graph mode regardless (though it could be possible to create a more graph-friendly version). When running training loops inside tf.function, you can currently only use summaries and tf.print and tensorboard to monitor the training progress.
 I think adding a graph-compatible tqdm (one that uses TF Iterators and tf.print) would be a useful API, although we probably won't have the chance to build one soon (it would be a welcome contribution though).
 		",2.0,omoindrot,2020-02-03T19:21:56Z,"
 		There is now a verification in autograph that outputs a warning in situations such as this. We could add a verification that looks specifically for tqdm, but that won't work work the custom generator in the OP so it would be of limited help.
 Here is an alternative that might be useful for adding a progress bar to a dataset - it modifies a dataset to print tqdm messages whenever it is being iterated, using tf.py_function:
 <denchmark-code>def tf_tqdm(ds):
 
   # Suppress printing the initial status message - it creates extra newlines, for some reason.
   bar = tqdm.tqdm(file=io.StringIO())
 
   def advance_tqdm(e):
     def fn():
       bar.update(1)
       # Print the status update manually.
       print('\r', end='')
       print(repr(bar), end='')
     tf.py_function(fn, [], [])
     return e
 
   return ds.map(advance_tqdm)
 
 
 @tf.function
 def f(ds):
   for x in tf_tqdm(ds):
     pass
 
 ds = tf.data.Dataset.from_tensor_slices(tf.range(3))
 f(ds)
 </denchmark-code>
 
 Another similar alternative is to use Dataset.from_generator and supply it with a custom generator that itself includes tqdm. This will work whenever you are constructing the dataset from a Python object (it will not work when using built-in datasets like TFRecordDataset, though):
 <denchmark-code>data = tf.range(3)
 
 def f():
   bar = tqdm.tqdm(file=io.StringIO())
   for i in data:
     bar.update(1)
     # Print the status update manually.
     print('\r', end='')
     print(repr(bar), end='')
     yield i
 
 ds = tf.data.Dataset.from_generator(f, tf.int32)
 
 
 @tf.function
 def f():
   for x in ds:
     pass
 
 f()
 </denchmark-code>
 
 		",3.0,omoindrot,2020-02-03T19:21:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33724>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33724>No</denchmark-link>
 
 		",eba45c548371e29cd141c32d367b582b9ca656be,Dan Moldovan,2020-01-24 05:04:49-08:00,MODIFY,0,tensorflow\python\autograph\g3doc\reference\common_errors.md,tensorflow\python\autograph\g3doc\reference\common_errors.md,0.0,"27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64",,MODIFY,0.0,tensorflow\python\autograph\g3doc\reference\control_flow.md,tensorflow\python\autograph\g3doc\reference\control_flow.md,,,,,,,,,,,,,0.0,"364,365,369,370,371,372,373,374,375,376","364,365",,,,,MODIFY,0.0,tensorflow\python\autograph\g3doc\reference\functions.md,tensorflow\python\autograph\g3doc\reference\functions.md,0.0,"57,58",,MODIFY,9.0,tensorflow\python\autograph\operators\control_flow.py,tensorflow\python\autograph\operators\control_flow.py,1.0,767,"767,768",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_check_unroll_limits,self,766,768,MODIFY,7.0,tensorflow\python\autograph\operators\control_flow_test.py,tensorflow\python\autograph\operators\control_flow_test.py,1.0,"670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694","679,680",test_python_while_large_unroll_warning,self,670,694,1.0,"656,657,658,659,660,661,662,663,664,665,666,667,668",656,test_python_for_infinite,self,656,668,1.0,"656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680","656,679,680",test_python_long_loop_unroll_warning,self,656,680,1.0,643,643,test_python_while_infinite,self,643,654,1.0,"696,719",,test_python_for_large_unroll_warning,self,696,719,1.0,643,643,test_python_infinite_loop,self,643,654,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"371,372,373,374,375,376,377,378,379,380,381,382,383",,_py_for_stmt,"iter_,extra_test,body,get_state,set_state",367,393,1.0,"829,830,831,832",,_py_while_stmt.protected_body,,829,832,1.0,"787,788,789,790,791,792,793",787,_verify_ineffcient_unroll,self,775,794,1.0,"824,825,826,827,828,829,830,831,832,833",,_py_while_stmt,"test,body,get_state,set_state,opts",818,836,1.0,757,,__init__,self,756,761,1.0,"378,379,380,381",,_py_for_stmt.protected_body,protected_iter,378,381,1.0,809,"804,805,807,808",after_iteration,self,803,815,1.0,772,"770,771",_stop_checking_inefficient_unroll,self,770,773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"679,680,681,682","679,680",test_python_while_large_unroll_warning.custom_iterator,,679,682,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33776,EgorLakomkin,2019-10-28T09:26:19Z,2020-02-03T23:55:36Z,LSTMCell name is ignored in trainable_variables when wrapped in keras.layers.RNN,"
 I created a model with several LSTMCell cells, wrapped in keras.layers.RNN.
 When I print trainable_variables, cell names are ignored, which results into duplicate variable names (several same recurrent_kernel/etc), which confuses Tensorboard for example.
 Collab notebook to reproduce: <denchmark-link:https://colab.research.google.com/drive/11W6ntLFGj4gqh5sS09CeVgb0LMNb_Fiu>https://colab.research.google.com/drive/11W6ntLFGj4gqh5sS09CeVgb0LMNb_Fiu</denchmark-link>
 
 Environment: tensorflow 2 release
 	",1.0,EgorLakomkin,2019-10-29T04:59:16Z,"
 		Issue replicating for the given code - TF-2.0.
 		",2.0,EgorLakomkin,2019-10-29T21:39:56Z,"
 		I would like to work on this one. <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
 
 		",3.0,EgorLakomkin,2019-10-29T22:15:52Z,"
 		Thanks for reporting this issue. I think this is a historical issue since the we directly invoke cell.call() method rather than cell.call(). This cause the name scope in the call to be ignored, which result into the variable name doesn't have the prefix.
 The fix might need some special care since it affects the existing saved model (since the variable name changed, as well as the ops under the cell, they will now have a prefix of the cell's name). For anyone who want to try a fix, I would suggest to make sure the save model and checkpoint is explicitly verified.
 		",91bdf64a8b3778d70213446418b2a3d2d7a04a68,Scott Zhu,2020-02-03 15:54:24-08:00,MODIFY,1,tensorflow\python\keras\integration_test.py,tensorflow\python\keras\integration_test.py,1.0,"236,237,238,239,240,241,242,243,244,245,246,247,248,249","235,236,237,238,239,240,241,242,243,244,245,246",MODIFY,3.0,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,4.0,EgorLakomkin,2019-10-30T13:47:13Z,"
 		<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  thank you! Maybe you have an idea of a simple workaround (not sure if calling  explicitly for each cell would help) until the proper fix is done?
 		",5.0,EgorLakomkin,2019-10-31T17:17:34Z,"
 		<denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  How do you recommend to manually set the LSTM weights before the first call of  ?
 The  method will say that the layer was expecting 0 weights, but was provided with a list of N weights.
 		",6.0,EgorLakomkin,2019-10-31T18:31:28Z,"
 		The weights of the layer is initialized in build(), so you have to build the layer first with proper input_shape, so that it can init the weights with proper shape. then set_weights() should work fine.
 		",1.0,769,770,step,"inputs,states",764,773,MODIFY,3.0,tensorflow\python\keras\layers\recurrent_v2.py,tensorflow\python\keras\layers\recurrent_v2.py,1.0,420,420,MODIFY,3.0,tensorflow\python\keras\layers\rnn_cell_wrapper_v2_test.py,tensorflow\python\keras\layers\rnn_cell_wrapper_v2_test.py,1.0,"124,125","115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139",test_timeseries_classification_sequential_tf_rnn,self,227,257,,,,,,,,,,,,,,,,,,,,,,1.0,"167,168,170","164,165",build,"self,input_shape",163,179,1.0,"149,150,151,153,154,156","150,151,153",call,"self,inputs,states,constants,training,kwargs",132,160,,,,,,,,,,,,,,,,,,,,,,7.0,EgorLakomkin,2019-11-06T12:14:56Z,"
 		I tried to build each cell, but the names are still duplicated
 		",call,"self,inputs,mask,training,initial_state",399,453,testWrapperV2VariableNames,"self,wrapper",115,139,MODIFY,1.0,tensorflow\python\keras\saving\save_test.py,tensorflow\python\keras\saving\save_test.py,1.0,"196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214",,test_saving_h5_for_rnn_layers,self,196,214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,EgorLakomkin,2020-02-03T23:55:37Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33776>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33776>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,1.0,1123,1123,call.step,"inputs,states",1122,1123,1.0,420,420,call.step,"cell_inputs,cell_states",419,420,,,,,,,,1.0,"124,125","116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134",testWrapperWeights,"self,wrapper",116,134,1.0,"124,125","118,119,120,121,122,123,124,125,126,127,128,129,130,131",testWrapperV2VariableNames._rnn_input,"apply_wrapper,name",118,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33888,sonu1-p,2019-10-31T16:23:59Z,2020-01-29T00:03:54Z,Bug in saving model in hdf5 format,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs Mojave version 10.14.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0
 Python version:
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 When try to save the below model in keras format, we get the following error:
 ValueError: Unable to create group (name already exists)
 This happens as this model has three layers with name as below:
 tf_op_layer_Pad/paddings/0
 tf_op_layer_Pad/paddings
 tf_op_layer_Pad
 Such name causes error in keras as described here - <denchmark-link:https://github.com/keras-team/keras/issues/12195>keras-team/keras#12195</denchmark-link>
 
 Describe the expected behavior
 Model saving should not fail.
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 from tensorflow import keras
 
 x = keras.Input(shape=(None,10), dtype=""int32"", name=""input"")
 T = tf.shape(x)[0]
 to_pad = -T % 2
 y = tf.pad(x, [[0, to_pad], [0, 0], [0, 0]])
 model = keras.Model(inputs=[x,], outputs=[y,])
 model.save(""model.h5"")
 </denchmark-code>
 
 
 This fix for this has been checked into keras few days ago it seems - <denchmark-link:https://github.com/keras-team/keras/commit/7dee298ebec503c6b0e1727dfd49b89a3fb002d7>keras-team/keras@7dee298</denchmark-link>
 
 But it seems TF has its own copy of this hdf5 saving, so it seems this fix will also have to be made there -
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/hdf5_format.py#L624>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/saving/hdf5_format.py#L624</denchmark-link>
 
 	",1.0,sonu1-p,2019-11-01T19:59:36Z,"
 		Saving the model to TensorFlow format (tf) works where as hdf5 saving fails with TF nightly version '2.1.0-dev20191101'
 import tensorflow as tf
 from tensorflow import keras
 
 x = keras.Input(shape=(None,10), dtype=""int32"", name=""input"")
 T = tf.shape(x)[0]
 to_pad = -T % 2
 y = tf.pad(x, [[0, to_pad], [0, 0], [0, 0]])
 model = keras.Model(inputs=[x,], outputs=[y,])
 model.save(""models.tf"")
 		",2.0,sonu1-p,2019-11-04T09:37:45Z,"
 		We need to backport <denchmark-link:https://github.com/keras-team/keras/pull/13477>keras-team/keras#13477</denchmark-link>
 
 		",3.0,sonu1-p,2019-11-04T09:38:43Z,"
 		I believe this is a duplicate of <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33565>#33565</denchmark-link>
 
 		",0e884391beabe2fadb6398b1fc5f48a9662c333c,A. Unique TensorFlower,2020-01-28 15:53:05-08:00,MODIFY,1,tensorflow\python\keras\saving\hdf5_format.py,tensorflow\python\keras\saving\hdf5_format.py,1.0,"624,625,626",624,MODIFY,1.0,tensorflow\python\keras\saving\hdf5_format_test.py,tensorflow\python\keras\saving\hdf5_format_test.py,4.0,sonu1-p,2020-01-22T02:57:09Z,"
 		I have the same problem using TF 2.1.0. Calling keras from tensorflow will lead to the same error. As mentioned by <denchmark-link:https://github.com/sonu1-p>@sonu1-p</denchmark-link>
  , TF hasn't updated its own copy of this hdf5 saving.
 		",5.0,sonu1-p,2020-01-29T00:03:55Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33888>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/33888>No</denchmark-link>
 
 		",,,,,1.0,"755,756,757,758,759,760,761,762,763,764,765,766,767,768,769",,test_saving_group_naming_h5py,self,755,769,,,,,,,,,,,,,,,save_weights_to_hdf5_group,"f,layers",610,638,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33974,bodin-e,2019-11-04T13:27:34Z,2019-11-14T16:45:28Z,assert_shapes broken code in documentation,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes>https://www.tensorflow.org/api_docs/python/tf/debugging/assert_shapes</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 The source code in the example is incorrect.
 Is:
 tf.assert_shapes([
 (x: ('N', 'Q')),
 (y: ('N', 'D')),
 (param: ('Q',)),
 (scalar: ()),
 ])
 Should be:
 tf.assert_shapes([
 (x, ('N', 'Q')),
 (y, ('N', 'D')),
 (param, ('Q',)),
 (scalar, ()),
 ]).
 Note that "":"" is not allowed in Python to form 2-tuples.
 	",1.0,bodin-e,2019-11-06T18:43:09Z,"
 		Thanks for the report! Can you make a PR? That file is here: <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/check_ops.py#L1697-L1747>https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/ops/check_ops.py#L1697-L1747</denchmark-link>
  (link at top of page on tf.org)
 		",2.0,bodin-e,2019-11-07T16:27:03Z,"
 		<denchmark-link:https://github.com/lamberta>@lamberta</denchmark-link>
   I have opened a pull request making the relevant changes to the documentation folder.
 		",3.0,bodin-e,2019-11-07T18:14:11Z,"
 		Thanks <denchmark-link:https://github.com/dubesar>@dubesar</denchmark-link>
 , then its sorted.
 		",beef1a7bc10883e142813dfbd68da113a94cd6b7,Sarvesh Dubey,2019-11-11 09:45:39-08:00,MODIFY,0,tensorflow\python\ops\check_ops.py,tensorflow\python\ops\check_ops.py,0.0,"1597,1598,1599,1600,1601","1597,1598,1599,1600,1601",,,,,4.0,bodin-e,2019-11-14T07:21:31Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
   this issue can be closed now as it's solved
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34020,kaisuke,2019-11-05T21:51:18Z,2019-11-08T01:28:31Z,Checkpoint.restore doesn't restore Dataset iterator state when Dataset contains shuffle(),"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
 Python version: 3.5.2
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: not installed
 GPU model and memory: no GPU
 
 Describe the current behavior
 The code at the end results in the following output.
 <denchmark-code>2019-11-05 13:54:16.140994: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-11-05 13:54:16.149389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2397225000 Hz
 2019-11-05 13:54:16.151608: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50ebc70 executing computations on platform Host. Devices:
 2019-11-05 13:54:16.151636: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
 tf.Tensor(72, shape=(), dtype=int64)
 tf.Tensor(83, shape=(), dtype=int64)
 tf.Tensor(19, shape=(), dtype=int64)
 Saved to /tmp/x/ckpt-1
 tf.Tensor(74, shape=(), dtype=int64)
 tf.Tensor(33, shape=(), dtype=int64)
 tf.Tensor(93, shape=(), dtype=int64)
 Restored from /tmp/x/ckpt-1
 tf.Tensor(21, shape=(), dtype=int64)
 tf.Tensor(0, shape=(), dtype=int64)
 tf.Tensor(8, shape=(), dtype=int64)
 </denchmark-code>
 
 Describe the expected behavior
 After restoring from the checkpoint, I expect the iterator to return the same elements as it did after saving the checkpoint.  I.e.
 <denchmark-code>2019-11-05 13:54:16.140994: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2019-11-05 13:54:16.149389: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2397225000 Hz
 2019-11-05 13:54:16.151608: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50ebc70 executing computations on platform Host. Devices:
 2019-11-05 13:54:16.151636: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
 tf.Tensor(72, shape=(), dtype=int64)
 tf.Tensor(83, shape=(), dtype=int64)
 tf.Tensor(19, shape=(), dtype=int64)
 Saved to /tmp/x/ckpt-1
 tf.Tensor(74, shape=(), dtype=int64)
 tf.Tensor(33, shape=(), dtype=int64)
 tf.Tensor(93, shape=(), dtype=int64)
 Restored from /tmp/x/ckpt-1
 tf.Tensor(74, shape=(), dtype=int64)
 tf.Tensor(33, shape=(), dtype=int64)
 tf.Tensor(93, shape=(), dtype=int64)
 </denchmark-code>
 
 Code to reproduce the issue
 import tensorflow as tf
 ds = tf.data.Dataset.range(100).shuffle(100, seed=42, reshuffle_each_iteration=False)
 it = iter(ds)
 ckpt = tf.train.Checkpoint(foo=it)
 mgr = tf.train.CheckpointManager(ckpt, '/tmp/x', max_to_keep=3)
 for _ in range(3): print(next(it))
 mgr.save()
 print(""Saved to {}"".format(mgr.latest_checkpoint))
 for _ in range(3): print(next(it))
 ckpt.restore(mgr.latest_checkpoint)
 print(""Restored from {}"".format(mgr.latest_checkpoint))
 for _ in range(3): print(next(it))
 FWIW, I get the expected result if I remove .shuffle(...).
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,kaisuke,2019-11-06T10:48:02Z,"
 		Issue replicating for TF-2.0, kindly find the <denchmark-link:https://colab.sandbox.google.com/gist/oanush/571af19a2605e0b63938207f55803361/untitled26.ipynb>gist</denchmark-link>
  of colab.Thanks!
 		",2.0,kaisuke,2019-11-06T18:54:40Z,"
 		<denchmark-link:https://github.com/kaisuke>@kaisuke</denchmark-link>
  Checkpoint.save and Checkpoint.restore write and read object-based checkpoints, in contrast to TensorFlow 1.x's tf.compat.v1.train.Saver which writes and reads variable.name based checkpoints. Object-based checkpointing saves a graph of dependencies between Python objects (Layers, Optimizers, Variables, etc.) with named edges, and this graph is used to match variables when restoring a checkpoint. It can be more robust to changes in the Python program, and helps to support restore-on-create for variables.
 This is the reason why you are observing changes when you are using .shuffle() method. For more information, you can go through the following <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint>link</denchmark-link>
 . Thanks!
 		",3.0,kaisuke,2019-11-06T19:59:34Z,"
 		Thanks <denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
  for the explanation, but I couldn't see the logical connection between your paragraph 1 & 2...
 In the TensorFlow C++ codebase, I see support for saving/restoring iterators for a shuffling Dataset ( &  etc. in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/4fae137/tensorflow/core/kernels/data/shuffle_dataset_op.cc>https://github.com/tensorflow/tensorflow/blob/4fae137/tensorflow/core/kernels/data/shuffle_dataset_op.cc</denchmark-link>
 ).  So I thought that iterators qualify as  as mentioned in the <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint>tf.train.Checkpoint documentation</denchmark-link>
 .
 <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
 : It looks like you've been working on tf.data, including .  May I ask what your view on this is?
 		",53d244502fe0de438c939438d00e86f0409b2cb5,Andrew Audibert,2019-11-07 17:27:38-08:00,MODIFY,1,tensorflow\core\kernels\data\shuffle_dataset_op.cc,tensorflow\core\kernels\data\shuffle_dataset_op.cc,1.0,328,,,,,,4.0,kaisuke,2019-11-07T17:37:31Z,"
 		<denchmark-link:https://github.com/aaudiber>@aaudiber</denchmark-link>
  could you please take a look? thanks
 		",5.0,kaisuke,2019-11-07T22:43:22Z,"
 		I was able to reproduce the issue. It appears that some state is not properly reset when restoring an active shuffle iterator from a checkpoint. I have a fix in-flight.
 		",6.0,kaisuke,2019-11-08T01:28:32Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34020>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34020>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::data::ShuffleDatasetOpBase::ShuffleDatasetBase::Iterator::RestoreInternal,"ctx,reader",296,358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34055,keithchugg,2019-11-07T00:34:22Z,2019-11-19T17:24:57Z,model.reset_states() does not work for bidirectional-RNNs in tf.keras.,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.0.0
 Python version: 3.7.4
 GPU model and memory: none (MacBook Pro, Core i5, Iris Graphics 6100, 1.5 GB)
 
 Describe the current behavior
 State handling in RNNs with a Bidirectional wrapper has changed in tf.keras from keras with TF 1.x.  In the old keras with TF 1.x, using stateful=True in a bidi-RNN had no effect -- i.e., all bidi-RNN models behaved as if  stateful=False.  Therefore model.reset_states() did not do anything.
 In the new tf.keras, stateful=True in a bidi-RNN does have an effect -- the fwd-RNN is stateful and the bwd-RNN is stateful.  This is a good change IMO -- even though stateful bidi-RNNs are unusual, this is the best way to implement.  However, in tf.keras, the model.reset_states() does not do anything for bidi-RNN models (SimpleRNN, GRU, LSTM).
 Describe the expected behavior
 For the minimal example script provided below, here is the output:
 <denchmark-code>FWD::
 non_stateful: [ 1.   -0.5   0.25]
 stateful: [ 1.   -0.5   0.25]
 delta: [0. 0. 0.]
 BWD::
 non_stateful: [1. 0. 0.]
 stateful: [1. 0. 0.]
 delta: [0. 0. 0.]
 FWD::
 non_stateful: [ 1.   -0.5   0.25]
 stateful: [ 0.875   -0.4375   0.21875]
 delta: [-0.125    0.0625  -0.03125]
 BWD::
 non_stateful: [1. 0. 0.]
 stateful: [ 0.875  0.25  -0.5  ]
 delta: [-0.125  0.25  -0.5  ]
 
 ** RESETING STATES in STATEFUL MODEL **
 
 FWD::
 non_stateful: [ 1.   -0.5   0.25]
 stateful: [ 0.890625   -0.4453125   0.22265625]
 delta: [-0.109375    0.0546875  -0.02734375]
 BWD::
 non_stateful: [1. 0. 0.]
 stateful: [ 0.890625  0.21875  -0.4375  ]
 delta: [-0.109375  0.21875  -0.4375  ]
 </denchmark-code>
 
 The results after the STATE RESET  should be the same as the first set of results -- i.e., the last (third) set of results should produce the same result for the stateful and non-stateful models (same as the first set of results).
 Code to reproduce the issue
 import numpy as np
 TF2 = True
 if TF2:
 	### currently, there is a bug in tf.keras: model.reset_states() does not work
 	from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
 	from tensorflow.keras.models import Model
 else:
 	### in the old keras, bidi-RNNs with stateful=True behave smae as stateful=False
 	from keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
 	from keras.models import Model
 
 sequence_length = 3
 feature_dim = 1
 features_in = Input(batch_shape=(1, sequence_length, feature_dim)) 
 
 rnn_out = Bidirectional( SimpleRNN(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in)
 stateless_model = Model(inputs=[features_in], outputs=[rnn_out])
 
 stateful_rnn_out = Bidirectional( SimpleRNN(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in)
 stateful_model = Model(inputs=features_in, outputs=stateful_rnn_out)
 
 toy_weights = [ np.asarray([[1.0]], dtype=np.float32), np.asarray([[-0.5]], dtype=np.float32), np.asarray([[1.0]], dtype=np.float32), np.asarray([[-0.5]], dtype=np.float32)]
 
 stateless_model.set_weights(toy_weights)
 stateful_model.set_weights(toy_weights)
 
 x_in = np.zeros(sequence_length)
 x_in[0] = 1
 x_in = x_in.reshape( (1, sequence_length, feature_dim) )
 
 def print_bidi_out(non_stateful_out, stateful_out):
 	fb = ['FWD::', 'BWD::']
 
 	for i in range(2):
 		print(fb[i])
 		print(f'non_stateful: {non_stateful_out.T[i]}')
 		print(f'stateful: {stateful_out.T[i]}')
 		print(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')
 
 
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 print('\n** RESETING STATES in STATEFUL MODEL **\n')
 stateful_model.reset_states()
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 	",1.0,keithchugg,2019-11-07T20:29:55Z,"
 		Note: this is an issue with tf.keras vs. keras (not TF 1.x vs TF 2.0)
 		",2.0,keithchugg,2019-11-08T08:14:13Z,"
 		I could replicate issue on colab with Tf 2.0.
 Please take a look at <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/0d4771e1d00b215c8f06577915bf7fef/untitled246.ipynb>gist</denchmark-link>
 . Thanks!
 		",3.0,keithchugg,2019-11-19T17:24:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34055>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34055>No</denchmark-link>
 
 		",7cea9a4edfd181771da97db944e89551b62195ce,Scott Zhu,2019-11-18 17:38:12-08:00,MODIFY,0,tensorflow\python\keras\layers\wrappers.py,tensorflow\python\keras\layers\wrappers.py,0.0,"424,425,426,427,428,429","462,463,464,465,466",MODIFY,1.0,tensorflow\python\keras\layers\wrappers_test.py,tensorflow\python\keras\layers\wrappers_test.py,,,,,,,,,,,,,1.0,"554,555,556,557,559,560,561,562,563,564,565","554,555",test_bidirectional_statefulness,self,539,567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34124,VoVAllen,2019-11-09T16:07:33Z,2019-11-14T13:07:11Z,[Docs] Doc example of DeviceSpec doesn't work with tf 2.0,"
 Doc link: <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/DeviceSpec>https://www.tensorflow.org/api_docs/python/tf/DeviceSpec</denchmark-link>
 
 import tensorflow as tf
 device_spec = tf.DeviceSpec(job=""ps"", device_type=""CPU"", device_index=0)
 with tf.device(device_spec):
   pass
 Got error:
 <denchmark-code>---------------------------------------------------------------------------
 KeyError                                  Traceback (most recent call last)
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py in __enter__(self)
    1508     try:
 -> 1509       new_device_name, new_device_spec = _device_parsing_cache[cache_key]
    1510     except TypeError:
 
 KeyError: ('', <tensorflow.python.framework.device_spec.DeviceSpecV2 object at 0x7f34c74de528>)
 
 During handling of the above exception, another exception occurred:
 
 ValueError                                Traceback (most recent call last)
 1 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/context.py in __enter__(self)
    1517         if not isinstance(new_device_name, six.string_types):
    1518           raise ValueError(""Expecting a string device name. Got %s(%s)"" %
 -> 1519                            (type(new_device_name), new_device_name))
    1520         device_spec = pydev.DeviceSpec.from_string(new_device_name)
    1521         if old_device_name:
 
 ValueError: Expecting a string device name. Got <class 'tensorflow.python.framework.device_spec.DeviceSpecV2'>(<tensorflow.python.framework.device_spec.DeviceSpecV2 object at 0x7f34c74de528>)
 </denchmark-code>
 
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): 2.0.0
 TensorFlow version (use command below):
 Python version:
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Describe the expected behavior
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,VoVAllen,2019-11-09T18:50:55Z,"
 		Syntax works fine on TensorFlow 1.15.0 (all 1.x): <denchmark-link:https://colab.research.google.com/gist/nikochiko/7cefa862d487ff9d6d31c9c2034a3cb8/34124.ipynb#scrollTo=6jMLFtVeeJpX>gist</denchmark-link>
 
 Throws error in TensorFlow 2.x: <denchmark-link:https://colab.research.google.com/drive/1nkoSsW4hINcp03EPwLC0NFQhpuJURUbW#scrollTo=tB54LogAgL1D>gist</denchmark-link>
 
 		",2.0,VoVAllen,2019-11-10T11:34:08Z,"
 		Update: found out the problem lies in using this command with eager execution.
 Find <denchmark-link:https://colab.research.google.com/gist/nikochiko/0de00bb23b936f1ce714bf969aae5f1b/34124_fix.ipynb>gist</denchmark-link>
  here. Must be called with  function in eager execution.
 Will update the docs. Issue can be assigned to me. Will put a PR soon.
 		",3.0,VoVAllen,2019-11-10T11:47:35Z,"
 		Thank you <denchmark-link:https://github.com/VoVAllen>@VoVAllen</denchmark-link>
  for bringing this issue to attention.
 		",5006295cf7a20de9ef9087127569e9d58b28022d,Kaustubh Maske Patil,2019-11-11 23:16:14+05:18,MODIFY,0,tensorflow\python\framework\device_spec.py,tensorflow\python\framework\device_spec.py,0.0,"57,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,83,84,86","50,54,56,60,65,70,72,73,75,78,81",,,,,4.0,VoVAllen,2019-11-11T18:52:28Z,"
 		This issue is fixed with latest version TF nightly build '2.1.0-dev20191111'. Thanks!
 		",5.0,VoVAllen,2019-11-14T13:07:08Z,"
 		However I would consider it as an implementation bug instead of a documentation error. Thanks for your help!
 		",6.0,VoVAllen,2019-11-14T13:07:13Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34124>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34124>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34165,saikumarGadde,2019-11-11T19:06:57Z,2020-10-07T19:55:47Z,tf.keras.backend.sqrt(tf.constant(-1.0)) is 0 which is misleading and tf.sqrt(tf.constant(-1.0)) is 'nan' which is the way it should be.,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): 1.15
 Python version: 3.7
 Bazel version (if compiling from source): No
 GCC/Compiler version (if compiling from source): No
 CUDA/cuDNN version: No
 GPU model and memory: Run in CPU
 Describe the current behavior
 tf.keras.backend.sqrt(tf.constant(-1.0)) returns 0 as clip_by_value is being done in the source code (Which is highly misleading, as can be seen only in the source and not in the function document) whereas tf.sqrt(tf.constant(-1.0)) returns 'nan' which is the expected behavior of any sqrt function. This causes some bugs which are very difficult to track.
 
 Describe the expected behavior
 Make sqrt functions return only the expected behavior and remove the clip_by_value.
 Code to reproduce the issue
 import tensorflow as tf
 tf.enable_eager_execution()
 tf.keras.backend.sqrt(tf.constant(-1.0)).numpy()
 tf.sqrt(tf.constant(-1.0)).numpy()
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,saikumarGadde,2019-11-11T22:12:39Z,"
 		As you can see from the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/keras/backend.py#L2104>source code</denchmark-link>
  tf.keras.backend.sqrt clips the input value to zero if its less than 0 and to infinity if its equal to infinity as shown by the function <denchmark-link:https://github.com/tensorflow/tensorflow/blob/590d6eef7e91a6a7392c8ffffb7b58f2e0c8bc6b/tensorflow/python/keras/backend.py#L2104>here</denchmark-link>
 . This is the reason why the output is zero
 But in tf.sqrt the sqrt function is from gen_math_ops.py which has a different behaviour i.e., behaviour similar to numpy and thats the reason why its value is defaulted to NaN
 		",2.0,saikumarGadde,2019-11-11T23:45:54Z,"
 		But developers usually expect the tensorflow to return nan when tf.sqrt( negative numbers) is done, as thats the norm unlike returning 0, which defies the name of the function. Or, I would request you to update the documentation just to avoid the confusion as tensorflow is moving towards keras.
 		",3.0,saikumarGadde,2019-11-12T00:37:14Z,"
 		Sure <denchmark-link:https://github.com/saikumarGadde>@saikumarGadde</denchmark-link>
 
 		",8134c918424e5f99683617ca5afa8303e5c90642,A. Unique TensorFlower,2020-10-07 10:59:21-07:00,MODIFY,1,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,1.0,"2431,2432,2433",,,,,,4.0,saikumarGadde,2020-01-15T10:23:29Z,"
 		<denchmark-link:https://github.com/MarkDaoust>@MarkDaoust</denchmark-link>
  Is this issue is solved? Thanks.
 		",5.0,saikumarGadde,2020-10-07T19:55:47Z,"
 		<denchmark-link:https://github.com/saikumarGadde>@saikumarGadde</denchmark-link>
 : Yes, you have a point but: we need to be very conservative about changing the behavior of existing functions. Also and what is the goal of using  at all?
 At this point tf.keras.backend should be considered an implementation detail. It was a tools for writing keras to be backend-agnostic. TensorFlow is effectively the only backend now.
 That's why fchollet removed the documentation for these in:
 <denchmark-link:https://github.com/tensorflow/tensorflow/commit/91e5ad0fad9bbf8462a797ddd7183df1c15f6832#diff-e329ed6b8d30dca9a441689005047035>91e5ad0#diff-e329ed6b8d30dca9a441689005047035</denchmark-link>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/backend?version=nightly>https://www.tensorflow.org/api_docs/python/tf/keras/backend?version=nightly</denchmark-link>
 
 The functions will still be available in the package but they are undocumented to communicate that they should not be used.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,sqrt,x,2428,2443,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34194,danijar,2019-11-12T09:56:51Z,2019-11-18T18:27:53Z,tf.size() has no documentation,"
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/size>https://www.tensorflow.org/api_docs/python/tf/size</denchmark-link>
 
 Example: ""Returns the number of elements in the tensor. It equals the length of the flattened tensor.""
 	",1.0,danijar,2019-11-16T09:09:20Z,"
 		<denchmark-link:https://github.com/danijar>@danijar</denchmark-link>
 , I want to contribute to this issue. Can you help me in doing so?
 I am completely new to this world of open source.
 		",2.0,danijar,2019-11-18T06:16:28Z,"
 		Sorry, I can't assist you with the workflow. <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md>CONTRIBUTING.md</denchmark-link>
  would be a good place to get started.
 		",3.0,danijar,2019-11-18T07:21:59Z,"
 		<denchmark-link:https://github.com/lamberta>@lamberta</denchmark-link>
  <denchmark-link:https://github.com/goldiegadde>@goldiegadde</denchmark-link>
  Is this issue being worked on? If not, can I submit a PR?
 		",b70b3e7032cfbbd6d237dc01cdbd1399378b8351,Srishti Yadav,2019-11-18 10:38:55-08:00,MODIFY,1,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,1.0,"606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631",,,,,,4.0,danijar,2019-11-18T08:18:25Z,"
 		Submitted a PR <denchmark-link:https://github.com/danijar>@danijar</denchmark-link>
  <denchmark-link:https://github.com/lamberta>@lamberta</denchmark-link>
 
 		",5.0,danijar,2019-11-18T18:27:53Z,"
 		Thank you <denchmark-link:https://github.com/copperwiring>@copperwiring</denchmark-link>
  
 		",,,,,,,,,,,,,,,,,,,,,,,,,,size_v2,"input,out_type,name",604,632,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34297,MikeOfZen,2019-11-15T01:59:48Z,2019-11-21T23:39:00Z,tf.function hangs on ragged tensor input,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):No
 TensorFlow installed from (source or binary):Colab
 TensorFlow version (use command below):2.0.0
 Python version:3.6
 GPU model and memory:None
 
 Describe the current behavior
 When using tf.function with a number of for loops on a RaggedTensor, the function call hangs (I stopped waiting after half hour).
 -when running the function directly (no tf.function decorator), the function executes immediately
 -when converting the ragged tensor to dense tensor, the function executes immediately.
 I couldn't pinpoint the exact combination of operations that causes this Autograph behavior, but I tried to reduce my code to the simplest combination that still causes this behavior.
 I struggled for hours with my own code, trying to get tf.function to work, until I figured it was due to the ragged tensor + for loops + tf.function hanging the kernel
 I observed similar behavior on my machine and a colab machine as well.
 Describe the expected behavior
 Should execute in a comparable time to a dense tensor.
 Code to reproduce the issue
 <denchmark-code>%tensorflow_version 2.x
 import tensorflow as tf
 import numpy as np
 
 inp=tf.ragged.constant(np.arange(1000,dtype=np.float32).reshape(10,10,10))
 
 @tf.function    #if not using tf.function it runs well
 def ragged_example(r_tensor):
   s=tf.constant(0.0)
   for i in tf.range(10):
     inner=r_tensor[i]
     for x in inner:
       b=tf.reduce_sum(x)
       s=s+b
   return s
 
 #inp=inp.to_tensor() #if this is uncommented, it runs well
 
 ragged_example(inp) #this hangs
 </denchmark-code>
 
 Other observations
 Also noted very large memory footprint as a result (9gb and growing)
 	",1.0,MikeOfZen,2019-11-18T11:22:31Z,"
 		Could reproduce the issue with TF Version 2.0. Here is the <denchmark-link:https://colab.sandbox.google.com/gist/rmothukuru/3a286f4b25e62d041d0030e80a4e1cb2/34297.ipynb>Gist</denchmark-link>
 . Thanks!
 		",2.0,MikeOfZen,2019-11-18T21:05:00Z,"
 		This bug is a combination of several problems that we'll need to resolve, fopefully in 2.2. In the mean time, a workaround is to avoid writing for loops over ragged tensors (e.g. for x in ragged_tensor). This means rewriting your code like so:
 <denchmark-code>def ragged_example(r_tensor):
   s = tf.constant(0.0)
   for i in tf.range(10):
     inner = r_tensor[i]
     for j in tf.range(inner.row_lengths()[0]):  #  `for x in inner` doesn't work yet.
       x = inner[j]
       b = tf.reduce_sum(x)
       s = s + b
   return s
 </denchmark-code>
 
 A few more details on the cause of the bug --
 Autograph doesn't currently support iterating over , and it thinks they are normal Python objects. We'll fix that in autograph, hopefully by TF 2.2.
 Now, this should have normally generated an error (something in the lines of ""cannot directly iterate over RaggedTensor in graph mode""). But it seems that the  class is iterable and the iterator it returns hangs when consumed in graph mode. <denchmark-link:https://github.com/edloper>@edloper</denchmark-link>
  I think this iterator should error out in  instead?
 Edit: fixed the index in the example
 		",3.0,MikeOfZen,2019-11-18T21:19:38Z,"
 		Thank you for the update and the details.
 		",75af7b4750cd757cd82b32896eb98414e1b38898,Dan Moldovan,2019-11-21 15:38:00-08:00,MODIFY,3,tensorflow\python\autograph\operators\control_flow.py,tensorflow\python\autograph\operators\control_flow.py,1.0,"425,426,427",,MODIFY,5.0,tensorflow\python\autograph\operators\control_flow_test.py,tensorflow\python\autograph\operators\control_flow_test.py,4.0,MikeOfZen,2019-11-21T23:39:01Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34297>No</denchmark-link>
 
 		",5.0,MikeOfZen,2019-11-22T14:11:17Z,"
 		The PR that just got merged should fix the issue in tf.function.
 As a side note, we should still disallow RaggedTensor.iter in graph code (or make it return a proper graph-based iterator).
 		",,,,,1.0,"285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305",,test_tf_ragged_tensor_no_loop_vars,self,285,305,,,,,,,,,,,,,,,_tf_ragged_for_stmt,"iter_,extra_test,body,get_state,set_state,init_vars,basic_symbol_names,composite_symbol_names",425,427,1.0,"450,451,452,453,454",,while_cond,"iterate_index,loop_vars",450,454,1.0,"437,438,439,440,441,442,443,444,445,446,447,448",,while_body,"iterate_index,loop_vars",437,448,,,,,,,,1.0,"294,295,296,297,298,299,300,301",,test_tf_ragged_tensor_no_loop_vars.test_fn,,294,301,1.0,"261,262,263,264,265,266,267,268,269",,test_tf_ragged_tensor,self,261,269,1.0,"289,290",,test_tf_ragged_tensor_no_loop_vars.stateless_with_side_effects,i,289,290,1.0,"271,272,273,274,275,276,277,278,279,280,281,282,283",,test_tf_ragged_tensor_higher_dimensional,self,271,283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34701,stakemura,2019-11-29T10:57:55Z,2019-12-02T18:57:51Z,"std::uniform_int_distribution&lt;int8_t&gt; is undefined in the C++17 standard, but TFLite violates this limitation.","
 As <denchmark-link:https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution>the C++ reference</denchmark-link>
  mentioned, std::uniform_int_distribution<int8_t> is undefined in the C++17.
 Therefore microsoft visual C++ 2017 will give the following build error when the code includes <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc#L496>benchmark_tflite_model.cc #L496</denchmark-link>
 .
 In fact, the recent TFLite model benchmark couldn't build on windows as <denchmark-link:https://dev.azure.com/mlops/tensorflow/_build/results?buildId=548&view=logs&j=e1e4dfe0-fc62-5ca1-9c02-b15972c8e9c4&t=9da95ac0-03a9-5448-4a9d-063bdd2c2605&l=1058>my CI environment</denchmark-link>
  shows.
 bazel build -c opt --verbose_failures //tensorflow/lite/tools/benchmark:benchmark_model
 <denchmark-code>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\include\random(2401): error C2338: invalid template argument for uniform_int_distribution: N4659 29.6.1.1 [rand.req.genl]/1e requires one of short, int, long, long long, unsigned short, unsigned int, unsigned long, or unsigned long long
 tensorflow/lite/tools/benchmark/benchmark_tflite_model.cc(496): note: see reference to class template instantiation 'std::uniform_int_distribution<uint8_t>' being compiled
 C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.16.27023\include\random(2401): error C2338: note: char, signed char, unsigned char, int8_t, and uint8_t are not allowed
 </denchmark-code>
 
 Would you like to modify benchmark_tflite_model.cc?
 	",1.0,stakemura,2019-12-02T17:05:13Z,"
 		<denchmark-link:https://github.com/lrdxgm>@lrdxgm</denchmark-link>
  can you fix? We can just use a cast from the proper min/max range.
 		",2.0,stakemura,2019-12-02T17:13:12Z,"
 		Sure. On vacation, will be back on Wednesday.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Mon, Dec 2, 2019, 9:05 AM Jared Duke ***@***.***> wrote:
  @lrdxgm <https://github.com/lrdxgm> can you fix? We can just use a cast
  from the proper min/max range.
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#34701?email_source=notifications&email_token=AATL3TNWOIPXJRMHHARRLT3QWU545A5CNFSM4JS6CSCKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEFUFU2I#issuecomment-560487017>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/AATL3TMHR2MCVEEA3EU3VY3QWU545ANCNFSM4JS6CSCA>
  .
 
 
 
 		",3.0,stakemura,2019-12-02T18:57:52Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34701>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34701>No</denchmark-link>
 
 		",c16614ef36b990ce9633e25aa00467ea4ce85844,Jared Duke,2019-12-02 10:56:46-08:00,MODIFY,1,tensorflow\lite\tools\benchmark\benchmark_tflite_model.cc,tensorflow\lite\tools\benchmark\benchmark_tflite_model.cc,1.0,"495,497,501,503","496,501",MODIFY,1.0,tensorflow\lite\tools\benchmark\benchmark_tflite_model.h,tensorflow\lite\tools\benchmark\benchmark_tflite_model.h,,,,,,,,,,,,,1.0,"93,94,95","93,94",tflite::benchmark::BenchmarkTfLiteModel::CreateInputTensorData,"num_elements,distribution",88,102,,,,,,,,,,,,,,,tflite::benchmark::BenchmarkTfLiteModel::PrepareInputData,,424,514,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34789,guillaumekln,2019-12-03T10:07:02Z,2020-01-08T01:32:55Z,GRUCell is not compatible with its own initial state,"
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution: Ubuntu 16.04
 TensorFlow installed from: binary
 TensorFlow version: 2.1.0rc0
 Python version: 3.5.2
 
 Describe the current behavior
 The initial state returned by tf.keras.layers.GRUCell.get_initial_state() can not be passed to the first cell call without error. It raises an InvalidArgumentError error.
 Describe the expected behavior
 RNN cells should accept their own initial states.
 Code to reproduce the issue
 import tensorflow as tf
 batch_size = 4
 cell = tf.keras.layers.GRUCell(20)
 initial_state = cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)
 output, state = cell(tf.random.uniform([batch_size, 10]), initial_state)
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File ""test/gru_incompat.py"", line 5, in <module>
     output, state = cell(tf.random.uniform([batch_size, 10]), initial_state)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 822, in __call__
     outputs = self.call(cast_inputs, *args, **kwargs)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/layers/recurrent.py"", line 1846, in call
     matrix_inner = K.dot(h_tm1, self.recurrent_kernel)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/keras/backend.py"", line 1678, in dot
     out = math_ops.matmul(x, y)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/util/dispatch.py"", line 180, in wrapper
     return target(*args, **kwargs)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/ops/math_ops.py"", line 2797, in matmul
     a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/ops/gen_math_ops.py"", line 5631, in mat_mul
     _ops.raise_from_not_ok_status(e, name)
   File ""/lib/python3.5/site-packages/tensorflow_core/python/framework/ops.py"", line 6598, in raise_from_not_ok_status
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""<string>"", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: In[0] is not a matrix. Instead it has shape [20] [Op:MatMul] name: gru_cell/MatMul/
 </denchmark-code>
 
 	",1.0,guillaumekln,2019-12-28T11:54:21Z,"
 		<denchmark-link:https://github.com/guillaumekln>@guillaumekln</denchmark-link>
  hi, I met this error too, and I think here is a bug. I changed two line of codes, everything is ok.
 
 
 
 tensorflow/tensorflow/python/keras/layers/recurrent.py
 
 
          Line 1773
       in
       4235c01
 
 
 
 
 
 
  h_tm1 = states[0]  # previous memory 
 
 
 
 
 
 should be h_tm1 = states  # previous memory
 
 
 
 tensorflow/tensorflow/python/keras/layers/recurrent.py
 
 
          Line 1871
       in
       4235c01
 
 
 
 
 
 
  return h, [h] 
 
 
 
 
 
 should be return h, h
 		",2.0,guillaumekln,2020-01-06T08:33:17Z,"
 		Sorry for the breakage, will fix it very soon.
 		",3.0,guillaumekln,2020-01-08T01:32:57Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34789>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34789>No</denchmark-link>
 
 		",48b920246f9f06a645a9b864c39171c5b0c2c4ef,Scott Zhu,2020-01-07 17:29:46-08:00,MODIFY,1,tensorflow\python\keras\layers\gru_test.py,tensorflow\python\keras\layers\gru_test.py,1.0,"215,216,217,218,219,220,221,222",,MODIFY,1.0,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,,,,,,,,,,,,,1.0,1326,1326,call,"self,inputs,states,training",1325,1344,MODIFY,1.0,tensorflow\python\keras\layers\simplernn_test.py,tensorflow\python\keras\layers\simplernn_test.py,1.0,"222,223,224,225,226,227,228,229",,,,,,,,,test_get_initial_states,self,215,222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_get_initial_states,self,222,229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34873,bilalsoomro,2019-12-05T16:14:44Z,2020-01-13T11:15:31Z,Using GPU delegate causes app to crash,"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 TensorFlow installed from (source or binary): binary
 TensorFlow version (or github SHA if from source): 'org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'
 
 Command used to run the converter or code if you’re using the Python API
 <denchmark-code>model = tf.keras.models.load_model('my_conv.h5')
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_spec.supported_types = [tf.float16]
 tflite_model = converter.convert()
 open(""custom_cnn_f16.tflite"", ""wb"").write(tflite_model)
 </denchmark-code>
 
 The output from the converter invocation
 <denchmark-code>It successfully converts the model to TFlite f16
 </denchmark-code>
 
 Also, please include a link to the saved model or GraphDef
 <denchmark-code>[Link](https://drive.google.com/file/d/1-0R5fPIWIM7N3Kv442QGTxVJ5Dz0ebZ-/view?usp=sharing)
 
 </denchmark-code>
 
 The model is a simple CNN which takes a 50x50x1 grayscale image and outputs probabilities for 10 classes.
 Failure details
 I want to run the Float16 version of the model using TFLite on the GPU (Samsung S10+) however using the GPU delegate on this model causes the app to crash. I have tested on other devices and this model crashes on all phones when running on the GPU.
 Any other info / logs
 <denchmark-code>2019-12-05 18:06:30.715 3830-3830/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Build fingerprint: 'xxxxxxxxxxxxxxxx/release-keys'
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Revision: '26'
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: ABI: 'arm64'
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: pid: 2979, tid: 3198, name: inference  >>> com.test.app <<<
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x0
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG: Cause: null pointer dereference
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x0  0000000000000000  x1  0000000000000000  x2  00000070516c44e0  x3  00000070516c43f0
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x4  00000000000000ba  x5  00000070546a5288  x6  000000704d0bb540  x7  000000704d0bb560
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x8  185cb8064dde48fc  x9  185cb8064dde48fc  x10 0000000000000000  x11 00000070546a51d0
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x12 000000704d0bb580  x13 000000704d0bb5a0  x14 00000000ffffffff  x15 0000000000000000
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x16 00000070f06f3bd0  x17 00000070f068898c  x18 00000000ffffffff  x19 000000706424f000
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x20 00000070516c43b0  x21 00000070516c43f0  x22 00000070516c44e0  x23 00000070642a9fe0
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x24 00000070516c4430  x25 00000070516c7588  x26 0000007064253b88  x27 00000070516c7588
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     x28 000000704d1803c0  x29 00000070516c4390
 2019-12-05 18:06:30.716 3830-3830/? A/DEBUG:     sp  00000070516c42c0  lr  0000007039a729f0  pc  0000007039a729f4
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG: backtrace:
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #00 pc 00000000000c49f4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #01 pc 000000000001ba8c  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #02 pc 000000000017e52c  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #03 pc 000000000017e0a4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #04 pc 000000000017de68  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #05 pc 000000000001b5d4  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_gpu_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #06 pc 000000000017fb08  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #07 pc 0000000000182fa0  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #08 pc 000000000000f214  /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_applyDelegate+40)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #09 pc 00000000005545e0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #10 pc 000000000054b84c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #11 pc 00000000000d00b8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #12 pc 000000000027ec54  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #13 pc 0000000000279da4  /system/lib64/libart.so (bool art::interpreter::DoCall<true, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+752)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #14 pc 000000000051d854  /system/lib64/libart.so (MterpInvokeStaticRange+148)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #15 pc 000000000053e014  /system/lib64/libart.so (ExecuteMterpImpl+15380)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #16 pc 000000000046472e  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates+122)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #17 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #18 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #19 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #20 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #21 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #22 pc 00000000004649a4  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.init+140)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #23 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #24 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #25 pc 0000000000279d88  /system/lib64/libart.so (bool art::interpreter::DoCall<true, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+724)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #26 pc 000000000051d6b4  /system/lib64/libart.so (MterpInvokeDirectRange+244)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #27 pc 000000000053df94  /system/lib64/libart.so (ExecuteMterpImpl+15252)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #28 pc 000000000046468c  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.<init>+128)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #29 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #30 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #31 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #32 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #33 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #34 pc 0000000000464002  /dev/ashmem/dalvik-classes.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk_2979_2979 (deleted) (org.tensorflow.lite.Interpreter.<init>+10)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #35 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #36 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #37 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #38 pc 000000000051be90  /system/lib64/libart.so (MterpInvokeDirect+296)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #39 pc 000000000053dc94  /system/lib64/libart.so (ExecuteMterpImpl+14484)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #40 pc 0000000000044cf2  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk!classes2.dex_2979_2979 (deleted) (com.test.app.tflite.Classifier.runModel+302)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #41 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #42 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #43 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #44 pc 000000000051ab60  /system/lib64/libart.so (MterpInvokeVirtual+584)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #45 pc 000000000053db94  /system/lib64/libart.so (ExecuteMterpImpl+14228)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #46 pc 000000000003fe80  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/com.test.app-EdFQ7ADOagBLvNg1917Lag==/base.apk!classes2.dex_2979_2979 (deleted) (com.test.app.home.HomeFragment$run$1.run+128)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #47 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #48 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #49 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #50 pc 000000000051bacc  /system/lib64/libart.so (MterpInvokeInterface+1392)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #51 pc 000000000053dd94  /system/lib64/libart.so (ExecuteMterpImpl+14740)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #52 pc 0000000000dbd78c  /system/framework/boot-framework.vdex (android.os.Handler.handleCallback+4)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #53 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #54 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #55 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #56 pc 000000000051c054  /system/lib64/libart.so (MterpInvokeStatic+204)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #57 pc 000000000053dd14  /system/lib64/libart.so (ExecuteMterpImpl+14612)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #58 pc 0000000000c658a8  /system/framework/boot-framework.vdex (android.os.Handler.dispatchMessage+8)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #59 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #60 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #61 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #62 pc 000000000051ab60  /system/lib64/libart.so (MterpInvokeVirtual+584)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #63 pc 000000000053db94  /system/lib64/libart.so (ExecuteMterpImpl+14228)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #64 pc 0000000000c6e4ee  /system/framework/boot-framework.vdex (android.os.Looper.loop+406)
 2019-12-05 18:06:30.739 3830-3830/? A/DEBUG:     #65 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #66 pc 0000000000258374  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #67 pc 0000000000278c78  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+920)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #68 pc 000000000051c054  /system/lib64/libart.so (MterpInvokeStatic+204)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #69 pc 000000000053dd14  /system/lib64/libart.so (ExecuteMterpImpl+14612)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #70 pc 0000000000c6540c  /system/framework/boot-framework.vdex (android.os.HandlerThread.run+56)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #71 pc 0000000000252c14  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.3612284370+488)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #72 pc 000000000050b850  /system/lib64/libart.so (artQuickToInterpreterBridge+1032)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #73 pc 00000000005546fc  /system/lib64/libart.so (art_quick_to_interpreter_bridge+92)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #74 pc 000000000054b588  /system/lib64/libart.so (art_quick_invoke_stub+584)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #75 pc 00000000000d0098  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+200)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #76 pc 0000000000454970  /system/lib64/libart.so (art::(anonymous namespace)::InvokeWithArgArray(art::ScopedObjectAccessAlreadyRunnable const&, art::ArtMethod*, art::(anonymous namespace)::ArgArray*, art::JValue*, char const*)+104)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #77 pc 0000000000455a3c  /system/lib64/libart.so (art::InvokeVirtualOrInterfaceWithJValues(art::ScopedObjectAccessAlreadyRunnable const&, _jobject*, _jmethodID*, jvalue*)+424)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #78 pc 00000000004807f0  /system/lib64/libart.so (art::Thread::CreateCallback(void*)+1260)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #79 pc 0000000000084148  /system/lib64/libc.so (__pthread_start(void*)+64)
 2019-12-05 18:06:30.740 3830-3830/? A/DEBUG:     #80 pc 0000000000023b28  /system/lib64/libc.so (__start_thread+68)
 </denchmark-code>
 
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,bilalsoomro,2019-12-06T22:07:54Z,"
 		I don't get a segfault but this instead:
 <denchmark-code>INFO: Initialized TensorFlow Lite runtime.
 INFO: Created TensorFlow Lite delegate for GPU.
 INFO: Replacing 87 node(s) with delegate (TfLiteGpuDelegate) node, yielding 1 partitions.
 ERROR: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error
 INTERNAL ERROR: no main() function!
 ERROR: 1 compilation errors.  No code generated.
 
 
 ERROR: Node number 87 (TfLiteGpuDelegate) failed to prepare.
 
 ERROR: tensorflow/lite/kernels/depthwise_conv.cc:109 filter->type != data_type (10 != 1)
 ERROR: Node number 2 (DEPTHWISE_CONV_2D) failed to prepare.
 
 F1206 14:03:13.223900   10663 jet_pilot.cc:72] Check failed: gpu_inference->ModifyGraphWithDelegate(gpu_delegate) == kTfLiteOk (1 vs. 0)
 </denchmark-code>
 
 Is your network well-formed?  Tensor shapes are quite strict in the GPU runtime.  Even if TFLite runs it fine on CPU because elements are contiguous in memory, TFLite GPU can fail.
 		",2.0,bilalsoomro,2019-12-09T07:01:49Z,"
 		<denchmark-link:https://github.com/impjdi>@impjdi</denchmark-link>
 , I will double check my network and get back to you.
 		",3.0,bilalsoomro,2019-12-10T08:29:02Z,"
 		<denchmark-link:https://github.com/impjdi>@impjdi</denchmark-link>
  Here's a simple example of a CNN that causes a segfault when it is converted to F16 for TFLite. Maybe this could help useful in debugging. I don't see anything wrong with the network itself.
 <denchmark-code>import tensorflow as tf # 2.0
 from tensorflow import keras
 from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input, MaxPooling2D, Dropout
 import numpy as np
 
 data = np.random.uniform(0, 1, (1000, 100, 100, 1)).astype(np.float32)
 labels = np.random.randint(0, 9, (1000,)).astype(np.int32)
 dataset = tf.data.Dataset.from_tensor_slices((data, labels))
 
 layers = []
 layers.append(Conv2D(64, 3, padding='same', activation='relu', name='conv_1', input_shape=(100, 100, 1)))
 layers.append(MaxPooling2D(2, name='max_pool_1'))
 layers.append(Dropout(0.2, name='dropout_1'))
 layers.append(Conv2D(128, 3, padding='same', activation='relu', name='conv_2'))
 layers.append(MaxPooling2D(2, name='max_pool_2'))
 layers.append(Dropout(0.2, name='dropout_2'))
 layers.append(Conv2D(256, 3, padding='same', activation='relu', name='conv_3'))
 layers.append(MaxPooling2D(2, name='max_pool_3'))
 layers.append(Dropout(0.2, name='dropout_3'))
 layers.append(Flatten(name='flatten'))
 layers.append(Dense(128, activation='relu', name='dense_1'))
 layers.append(Dense(64, activation='relu', name='dense_2'))
 layers.append(Dense(10, activation='softmax', name='dense_3'))
 model = keras.Sequential(layers, name='my_cnn')
 model.summary()
 
 dataset = dataset.batch(8)
 model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
 model.fit(dataset, epochs=1)
 
 # Convert to TFLite with Float16 quantization
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_spec.supported_types = [tf.float16]
 tflite_model = converter.convert()
 open(""my_f16_cnn.tflite"", ""wb"").write(tflite_model)
 
 </denchmark-code>
 
 Running this ""my_f16_cnn.tflite"" model on the device with the GPU Delegate will cause the app to crash.
 		",f48446d6d301d261f4fa7d73baa80e593e965abf,Juhyun Lee,2019-12-11 15:40:33-08:00,MODIFY,1,tensorflow\lite\delegates\gpu\common\model_builder.cc,tensorflow\lite\delegates\gpu\common\model_builder.cc,1.0,1214,,,,,,4.0,bilalsoomro,2019-12-10T23:44:03Z,"
 		As you said, this didn't work as expected.  Some error around
 
 INFO: Replacing 22 node(s) with delegate (TfLiteGpuDelegate) node, yielding 1 partitions.
 ERROR: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error
 INTERNAL ERROR: no main() function!
 ERROR: 1 compilation errors.  No code generated.
 ERROR: Node number 22 (TfLiteGpuDelegate) failed to prepare.
 ERROR: third_party/tensorflow/lite/kernels/depthwise_conv.cc:109 filter->type != data_type (10 != 1)
 ERROR: Node number 2 (DEPTHWISE_CONV_2D) failed to prepare.
 
 So... I tried to get rid of the data_type 10 which is float16.  In your script, I changed it to:
 
 converter.target_spec.supported_types = [tf.float16]
 
 Then I ran into:
 
 ERROR: Next operations are not supported by GPU delegate:
 FULLY_CONNECTED: Max version supported: 1. Requested version 3.
 First 6 operations will run on the GPU, and the remaining 4 on the CPU.
 ERROR: TfLiteGpuDelegate Prepare: CONV_2D: Unsupported data type for float32 tensor
 
 Let's ignore the FULLY_CONNECTED version check for the moment.
 ERROR: TfLiteGpuDelegate Prepare: CONV_2D: Unsupported data type for float32 tensor is coming from the fact that the tensor.type = kTfLiteInt8.  Unfortunately, I don't know enough about Keras or TFLiteConverter to tell why it's creating quantized weights =/
 		",5.0,bilalsoomro,2019-12-11T00:44:07Z,"
 		Found out about the quantized weights:
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 was replaced with:
 converter.optimizations = []
 Then I get:
 
 ERROR: TfLiteGpuDelegate Prepare: Shader compilation failed: ERROR: 0:6: 'data' : Syntax error:  syntax error
 INTERNAL ERROR: no main() function!
 ERROR: 1 compilation errors.  No code generated.
 
 I will find out what's failing.
 		",6.0,bilalsoomro,2020-01-10T11:01:05Z,"
 		Any update on this?
 		",,,,,,,,,,,,,,,,,,,,,,tflite::gpu::FullyConnectedOperationParser::Parse,"tflite_node,registration,graph,reader",1182,1231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,bilalsoomro,2020-01-10T20:04:18Z,"
 		I think I did everything on my end.  Is there any remaining action items for me?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,bilalsoomro,2020-01-13T11:15:31Z,"
 		<denchmark-link:https://github.com/impjdi>@impjdi</denchmark-link>
  Hi, I just built tensorflow-lite from source and the crash doesn't happen there anymore. So I will close the issue. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,bilalsoomro,2020-01-13T11:15:33Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34873>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/34873>No</denchmark-link>
 
 		",10.0,bilalsoomro,2020-06-03T19:54:11Z,"
 		
 @impjdi Hi, I just built tensorflow-lite from source and the crash doesn't happen there anymore. So I will close the issue. Thanks!
 
 <denchmark-link:https://github.com/bilalsoomro>@bilalsoomro</denchmark-link>
  Could you provide instructions for the same ? Thanks.
 		",11.0,bilalsoomro,2020-06-03T20:22:02Z,"
 		<denchmark-link:https://github.com/NikhilBartwal>@NikhilBartwal</denchmark-link>
  The issue has been fixed and does not happen in the nightly builds for me. If you want to also try building from source, I use the instructions provided in this <denchmark-link:https://www.tensorflow.org/lite/guide/android#build_tensorflow_lite_locally>link</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34975,shun-lin,2019-12-09T21:42:36Z,2020-04-03T00:32:06Z,[TF 2.0 API Docs] `tf.keras.callbacks.LearningRateScheduler` (Very small update),"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 Please provide a link to the documentation entry, for example:
 Doc Link:
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler>https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler</denchmark-link>
 
 Code Link:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py#L1311-L1358>https://github.com/tensorflow/tensorflow/blob/r2.0/tensorflow/python/keras/callbacks.py#L1311-L1358</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Parameters</denchmark-h>
 
 The next API for scheduler parameter of LearningRateScheduler takes in 2 parameters, epoch and lr (learning rate) instead of just epoch, this is evident in the on_epoch_begin of the LearningRateScheduler method.
 The documentation for this method is still outdated, the docs and the example code still shows the scheduler function takes in only epoch instead of both epoch and lr. I think the doc should be updated to reflect the new API.
 Proposed change to the doc:
 
 update the description of scheduler:
 
 schedule: a function that takes an epoch index as input (integer, indexed from 0) and current learning rate and returns a new learning rate as output (float).
 (copied from the doc from keras.io)
 
 update the example usage to include a scheduler that utilize the current learning rate as well.
 
 I hope this is helpful! Happy to contribute if needed.
 <denchmark-h:h3>Submit a pull request? Yes, if this should be updated.</denchmark-h>
 
 Are you planning to also submit a pull request to fix the issue? See the docs
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 ,
 docs API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
  and the
 docs style guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_style>https://www.tensorflow.org/community/contribute/docs_style</denchmark-link>
 
 	",1.0,shun-lin,2020-04-03T00:32:06Z,"
 		fixed :)
 		",,,,,,,,,ec684ae119051481f4435ecfa7d0cc7c06eb0fa8,A. Unique TensorFlower,2020-04-02 17:24:39-07:00,MODIFY,1,tensorflow\python\keras\callbacks.py,tensorflow\python\keras\callbacks.py,1.0,"1643,1644","1642,1643",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,schedule,verbose",1639,1650,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35100,olk,2019-12-13T22:06:00Z,2019-12-20T00:22:24Z,Error occurred when finalizing GeneratorDataset iterator,"
 System information
 
 OS Platform and Distribution: Arch Linux, 5.4.2-arch1-1-ARCH
 TensorFlow installed from: binary
 TensorFlow version: 2.1.0rc0-1
 Keras version: 2.2.4-tf
 Python version: 3.8
 GPU model and memory: 2x GTX 1080 Ti 11GB""`
 
 Describe the current behavior
 executing Tensorflow's MNIST handwriting example produces error:
 the error dissapears if the code doesn't use OneDeviceStrategy or MirroredStrategy
 
 W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
  import tensorflow_datasets as tfds
  import time
  
  from tensorflow.keras.optimizers import Adam
  
  def build_model():
      filters = 48
      units = 24
      kernel_size = 7
      learning_rate = 1e-4
      model = tf.keras.Sequential([
        tf.keras.layers.Conv2D(filters=filters, kernel_size=(kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),
        tf.keras.layers.MaxPooling2D(),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(units, activation='relu'),
        tf.keras.layers.Dense(10, activation='softmax')
      ])
      model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])
      return model
  
  datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)
  mnist_train, mnist_test = datasets['train'], datasets['test']
  
  num_train_examples = info.splits['train'].num_examples
  num_test_examples = info.splits['test'].num_examples
  
  strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')
  
  BUFFER_SIZE = 10000
  BATCH_SIZE = 32
  
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label
  
  train_dataset = mnist_train.map(scale).shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  eval_dataset = mnist_test.map(scale).repeat().batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
  
  with strategy.scope():
    model = build_model()
  
  epochs=5
  start = time.perf_counter()
  model.fit(
          train_dataset,
          validation_data=eval_dataset,
          steps_per_epoch=num_train_examples/epochs,
          validation_steps=num_test_examples/epochs,
          epochs=epochs)
  elapsed = time.perf_counter() - start
  print('elapsed: {:0.3f}'.format(elapsed))
 </denchmark-code>
 
 	",1.0,olk,2019-12-16T08:20:34Z,"
 		<denchmark-link:https://github.com/olk>@olk</denchmark-link>
 , I tried reproducing the reported issue but it worked as expected. Please take a look at the <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/0f769c920b16c68d0b2c7d238256e0c9/untitled311.ipynb>gist</denchmark-link>
 . Thanks!
 		",2.0,olk,2019-12-16T22:04:34Z,"
 		upgraded to TensorFlow version: 2.1.0-rc1 - still get errors
 please note that I execute the example at real hardware (not colab)
 		",3.0,olk,2019-12-17T14:33:12Z,"
 		I guess this issue is related to using Tensorflow with Python-3.8.
 		",b6edd34c5858ab0ab4380da774e7e2fd81a92da0,Jiri Simsa,2019-12-19 16:21:22-08:00,MODIFY,10,tensorflow\core\kernels\data\captured_function.cc,tensorflow\core\kernels\data\captured_function.cc,1.0,"852,853","852,854",MODIFY,0.0,tensorflow\core\kernels\data\captured_function.h,tensorflow\core\kernels\data\captured_function.h,4.0,olk,2019-12-17T18:30:53Z,"
 		I've downgraded my system:
 
 Python 3.7.4
 Tensorflow-2.1.0-rc1
 
 Still facing the error:
 
 Train for 30000.0 steps, validate for 5000.0 steps
 Epoch 1/2
 2019-12-17 19:21:54.361240: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
 2019-12-17 19:21:55.824790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
 2019-12-17 19:21:56.980785: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Not found: ./bin/ptxas not found
 Relying on driver to perform ptx compilation. This message will be only logged once.
 30000/30000 [==============================] - 115s 4ms/step - loss: 0.0856 - accuracy: 0.9761 - val_loss: 0.0376 - val_accuracy: 0.9879
 Epoch 2/2
 29990/30000 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.99582019-12-17 19:25:28.372294: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 30000/30000 [==============================] - 111s 4ms/step - loss: 0.0152 - accuracy: 0.9958 - val_loss: 0.0375 - val_accuracy: 0.9889
 2019-12-17 19:25:40.010887: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 2019-12-17 19:25:40.031138: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 elapsed: 226.391
 
 seams to be related to tensorflow-2.1.0-rc1
 		",5.0,olk,2019-12-17T23:18:41Z,"
 		I have the same issue.  Originally I was using:
 tensorflow/tensorflow:nightly-gpu-py3
 which has:
 2.1.0-dev20191106
 Then I tried upgrading tensorflow in the container with:
 <denchmark-link:https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl>https://files.pythonhosted.org/packages/a9/fa/8ac34cf1369deb4f523a80eeb86ec0be3dd44139bfb42c45dd3829d6aff5/tf_nightly_gpu-2.1.0.dev20191217-cp36-cp36m-manylinux2010_x86_64.whl</denchmark-link>
 
 I still have the same issue.
 		",6.0,olk,2019-12-18T00:12:21Z,"
 		<denchmark-link:https://github.com/guptapriya>@guptapriya</denchmark-link>
  <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  this seems to be an issue related to tf.distribute + tf.keras. In particular, as far as I can tell, the user code does not use  but the error indicates that GeneratorDataset is used. Could you please triage? Thanks.
 		",0.0,"112,113,193,200,259","101,113,114,116,195,202,261",,,,,MODIFY,1.0,tensorflow\core\kernels\data\generator_dataset_op.cc,tensorflow\core\kernels\data\generator_dataset_op.cc,1.0,"146,148,149","146,147,149,150",MODIFY,1.0,tensorflow\core\kernels\data\iterator_ops.h,tensorflow\core\kernels\data\iterator_ops.h,1.0,"77,79","77,79",tensorflow::data::CapturedFunction::CapturedFunction,"metadata,captured_inputs",851,854,1.0,484,484,tensorflow::data::CapturedFunction::Create,"ctx,metadata,argument_name,out_function",482,491,1.0,484,484,tensorflow::data::CapturedFunction::Create,"ctx,metadata,argument_name,out_function",482,491,1.0,729,731,tensorflow::data::InstantiatedCapturedFunction::RunInstantiated,"args,rets",714,750,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,olk,2019-12-18T00:36:51Z,"
 		The error log suggests that the training completed fine, but something at the end caused this error. Neither the training or validation dataset are using generators, so it does seem weird that there is a generator related error. Also it seems like it's just a warning - since the user's print statement at the end ""elapsed.."" did get printed as well.
 <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  is tf.data.Dataset.from_generator the only time generator_dataset_op is used? Or could there be something else that could trigger it?
 <denchmark-link:https://github.com/rchao>@rchao</denchmark-link>
  could it be something related to any of the fault tolerance callbacks?
 		",tensorflow::data::GeneratorDatasetOp::Dataset::Iterator::GetNextInternal,"ctx,out_tensors,end_of_sequence",120,153,tensorflow::data::IteratorResource::State::State,"flib_def,pflr,flr,iterator",73,81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,olk,2019-12-18T15:18:07Z,"
 		I can verify this error with python 3.8 and python-tensorflow-opt-cuda 2.1.0rc1-2 on arch linux.  This error is weirdly not present if you import only the generator from tensorflow, and everything else from Keras.
 		",1.0,622,"623,628",tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction,"lib,f_handle,ret_types,runner,cancellation_manager,captured_func",620,629,1.0,496,496,tensorflow::data::CapturedFunction::Create,"ctx,metadata,captured_inputs,out_function",494,502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,olk,2019-12-19T01:16:10Z,"
 		<denchmark-link:https://github.com/guptapriya>@guptapriya</denchmark-link>
  I realized that generator dataset is used in multi-device iterator. This seems related to newly added support for cancellation in tf.data.
 The good news is that, as you pointed out, the warning is superfluous. The bad news is that, as far as I can tell, this warning will be present for all tf.distribute jobs in TF 2.1 (given how tf.data cancellation is implemented). I will look into having a fix for this cherrypicked into TF 2.1.
 		",10.0,olk,2019-12-19T01:21:35Z,"
 		Ah, great, thanks <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
 .
 		",11.0,olk,2019-12-20T00:22:26Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35100>No</denchmark-link>
 
 		",12.0,olk,2020-02-12T02:54:54Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  Any update on this? I'm getting this exact message and it looks like my model.fit is not doing it's thing on validation dataset during training.
 		",13.0,olk,2020-02-13T22:40:16Z,"
 		I'm also experiencing this on the official Google Cloud Platform tf2-gpu.2-1.m42 image with Python 3.5.3.
 		",14.0,olk,2020-03-05T00:47:14Z,"
 		It might help to point out that this error is being printed out once per active GPU.
 2020-03-05 00:40:55.703069: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled 
 		",15.0,olk,2020-03-08T18:58:20Z,"
 		Problem description
 I am using TensorFlow 2.1.0 for image classification under Centos Linux. As my image training data set is growing, I have to start using a Generator as I do not have enough RAM to hold all pictures. I have coded the Generator based on this <denchmark-link:https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly>tutorial</denchmark-link>
 .
 It seems to work fine, until my program all the sudden gets killed without an error message:
 <denchmark-code>Epoch 6/30
 2020-03-08 13:28:11.361785: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 43/43 [==============================] - 54s 1s/step - loss: 5.6839 - accuracy: 0.4669
 Epoch 7/30
 2020-03-08 13:29:05.511813: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
  7/43 [===>..........................] - ETA: 1:04 - loss: 4.3953 - accuracy: 0.5268Killed
 </denchmark-code>
 
 Looking at the growing memory consumption with linux's top, I suspect a memory leak?
 What I have tried
 
 
 The above suggestion to switch to TF nightly build version. For me it did not help, also downgrading to TF2.0.1 did not help
 
 
 There is a discussion  suggesting that it is important, that 'steps_per_epoch' and 'batch size' correspond (whatever this exactly means) - I played with it without finding any improvement.
 
 
 Trying to narrow down by looking at the size development of all variables in my Generator
 
 
 Relevant code snippets
 <denchmark-code>class DataGenerator(tf.keras.utils.Sequence):
     'Generates data for Keras'
     def __init__(self, list_IDs, labels, dir, n_classes):
         'Initialization'
         config = configparser.ConfigParser()
         config.sections()
         config.read('config.ini')
 
         self.dim = (int(config['Basics']['PicHeight']),int(config['Basics']['PicWidth']))
         self.batch_size = int(config['HyperParameter']['batchsize'])
         self.labels = labels
         self.list_IDs = list_IDs
         self.dir = dir
         self.n_channels = 3
         self.n_classes = n_classes
         self.on_epoch_end()        
 
 
     def __len__(self):
         'Denotes the number of batches per epoch'
         return math.floor(len(self.list_IDs) / self.batch_size)
 
     def __getitem__(self, index):
         'Generate one batch of data'
         # Generate indexes of the batch
         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]
 
         # Find list of IDs
         list_IDs_temp = [self.list_IDs[k] for k in indexes]
 
         # Generate data
         X, y = self.__data_generation(list_IDs_temp)
 
         return X, y, [None]
 </denchmark-code>
 
 being called by
 <denchmark-code>        training_generator = datagenerator.DataGenerator(train_files, labels, dir, len(self.class_names))
         self.model.fit(x=training_generator,
                     use_multiprocessing=False,
                     workers=6, 
                     epochs=self._Epochs, 
                     steps_per_epoch = len(training_generator),
                     callbacks=[LoggingCallback(self.logger.debug)])
 </denchmark-code>
 
 I have tried running the exact same code under Windows 10, which gives me the following error:
 <denchmark-code>Epoch 9/30
 2020-03-08 20:49:37.555692: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 41/41 [==============================] - 75s 2s/step - loss: 2.0167 - accuracy: 0.3133
 Epoch 10/30
 2020-03-08 20:50:52.986306: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
  1/41 [..............................] - ETA: 2:36 - loss: 1.6237 - accuracy: 0.39062020-03-08 20:50:57.689373: W tensorflow/core/framework/op_kernel.cc:1655] OP_REQUIRES failed at matmul_op.cc:480 : Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
 2020-03-08 20:50:57.766163: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Resource exhausted: OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
          [[{{node MatMul_6}}]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 
  2/41 [>.............................] - ETA: 2:02 - loss: 1.6237 - accuracy: 0.3906Traceback (most recent call last):
   File ""run.py"", line 83, in <module>
     main()
   File ""run.py"", line 70, in main
     accuracy, num_of_classes = train_Posture(unique_name)
   File ""run.py"", line 31, in train_Posture
     acc = neuro.train(picdb, train_ids, test_ids, ""Posture"")
   File ""A:\200307 3rd Try\neuro.py"", line 161, in train
     callbacks=[LoggingCallback(self.logger.debug)])
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
     use_multiprocessing=use_multiprocessing)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 342, in fit
     total_epochs=epochs)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 128, in run_one_epoch
     batch_outs = execution_function(iterator)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 98, in execution_function
     distributed_function(input_fn))
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 568, in __call__
     result = self._call(*args, **kwds)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 599, in _call
     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2363, in __call__
     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1611, in _filtered_call
     self.captured_inputs)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1692, in _call_flat
     ctx, args, cancellation_manager=cancellation_manager))
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\function.py"", line 545, in call
     ctx=ctx)
   File ""C:\Users\Frank\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""<string>"", line 3, in raise_from
 tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[1279200,322] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
          [[node MatMul_6 (defined at A:\200307 3rd Try\neuro.py:161) ]]
 Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
  [Op:__inference_distributed_function_764]
 
 Function call stack:
 distributed_function
 
 2020-03-08 20:51:00.785175: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 </denchmark-code>
 
 		",16.0,olk,2020-03-09T21:03:42Z,"
 		<denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>
  I seem have the same issue. Should this be reopened? Why is it closed anyway?
 On my side it seems to happen when early stoppping triggers. So it does not cancel the training.
 And I get no OOM message.
 <denchmark-code>156/156 [==============================] - 86s 550ms/step - loss: 0.0676 - acc: 0.9790 - val_loss: 0.7805 - val_acc: 0.8569
 Epoch 17/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.0711 - acc: 0.9748 - val_loss: 0.4852 - val_acc: 0.8875
 Epoch 18/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.0638 - acc: 0.9772 - val_loss: 1.1247 - val_acc: 0.8371
 2020-03-09 20:41:21.425818: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled
 [0.8876654] [5]
 WARNING:tensorflow:sample_weight modes were coerced from
   {'output': '...'}
     to
   ['...']
 Train for 156 steps, validate on 11715 samples
 Epoch 1/1000
 156/156 [==============================] - 88s 566ms/step - loss: 0.4006 - acc: 0.8377 - val_loss: 1.3430 - val_acc: 0.5214
 Epoch 2/1000
 156/156 [==============================] - 86s 550ms/step - loss: 0.1554 - acc: 0.9437 - val_loss: 0.7877 - val_acc: 0.8219
 Epoch 3/1000
 </denchmark-code>
 
 		",17.0,olk,2020-03-10T12:35:50Z,"
 		<denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>
  <denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>
  <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  the same problem here
 		",18.0,olk,2020-03-11T20:33:57Z,"
 		Since more and more people (<denchmark-link:https://github.com/PhilipMay>@PhilipMay</denchmark-link>
 , <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
 , <denchmark-link:https://github.com/drsantos89>@drsantos89</denchmark-link>
 , <denchmark-link:https://github.com/4doge>@4doge</denchmark-link>
 , <denchmark-link:https://github.com/Tuxius>@Tuxius</denchmark-link>
 , ...) report to also still have the same issue I have reopened it <denchmark-link:https://github.com/tensorflow/tensorflow/issues/37515>#37515</denchmark-link>
 
 		",19.0,olk,2020-03-17T21:38:46Z,"
 		I have the same problem (using fit_generator)
 I'm using windows 10, python 3.6 and tensorflow 2.1 (cuda 10.1, cudnn 7.6.5)
 		",20.0,olk,2020-03-24T07:57:02Z,"
 		same issue here, with CPU, tf 2.1 and python 3.7
 		",21.0,olk,2020-03-24T18:10:43Z,"
 		same issue here, scales with validation_freq (occurs on those epochs for which validation is performed.)
 I'm using the tensorflow image tensorflow/tensorflow:2.1.0-gpu-py3 from docker hub: <denchmark-link:https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1>https://hub.docker.com/r/tensorflow/tensorflow/tags/?page=1</denchmark-link>
 
 Within the image, the system is 18.04.3 LTS, cuda 10.1.243, python Python 3.6.9
 		",22.0,olk,2020-03-25T15:03:18Z,"
 		I'm using keras Sequence and saw this issue too. However it's weird if I only use one worker, it works well. I didn't use multiprocessing tho. 2 worker thread will leave the training hang and data show this error
 		",23.0,olk,2020-03-25T15:55:33Z,"
 		There was a bug for Keras sequence multi-processing implementation that was fixed in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/e918c6e6fab5d0005fcde83d57e92b70343d3553>e918c6e</denchmark-link>
 . This will be available in TF 2.2 and should be already available in TF nightly.
 		",24.0,olk,2020-03-26T03:35:14Z,"
 		<denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  thanks for the heads up! I updated to tf-nightly-gpu and the error:
  went away.
 		",25.0,olk,2020-03-26T15:38:54Z,"
 		Good to know. Note that the warning and the memory leak are unrelated.
 		",26.0,olk,2020-03-28T19:01:37Z,"
 		
 @jsimsa thanks for the heads up! I updated to tf-nightly-gpu and the error:
 W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Cancelled: Operation was cancelled went away.
 
 I updated today. I confirm that.
 		",27.0,olk,2020-03-28T19:04:26Z,"
 		but the training result   seems not stable.   train/val  loss/accuracy up and downs too much.
 		",28.0,olk,2020-03-28T23:53:47Z,"
 		For me the results aren't reproducible from run to run either, even with
 tf.random.set_seed(), but I suspect it has to do with multiple workers for
 my image augmentation generator.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Sat, Mar 28, 2020, 12:04 PM flydragon2018 ***@***.***> wrote:
  but the training result seems not stable. train/val loss/accuracy up and
  downs too much.
 
  —
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  <#35100 (comment)>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/ABI2DNKEM2P6L6YZLNMF3FLRJZC4XANCNFSM4J2WWO2A>
  .
 
 
 
 		",29.0,olk,2020-06-12T15:55:38Z,"
 		Had the same problem. Memory leak and crash after some number of epochs. Looks like the ModelCheckpoint callback is a culprit. Removing it solved the issue.
 		",30.0,olk,2020-07-28T16:21:59Z,"
 		
 I guess this issue is related to using Tensorflow with Python-3.8.
 
 It is not related to Python 3.8. I have the same problem with Python 3.7.4
 		",31.0,olk,2020-08-01T17:15:53Z,"
 		I found a reason for the problem on my computer - YMMV. I was using the ModelCheckpoint callback to save the best model, and if there was a model with that name already in the folder, I got the error. Removing or renaming the model with that name fixed the issue. Windows 10 system, Python 3.7.4.
 		",32.0,olk,2020-08-10T18:06:18Z,"
 		Adding this code snippet fixes this issue for me when using RTX GPUs:
 <denchmark-code>devices = tf.config.experimental.list_physical_devices('GPU')
 tf.config.experimental.set_memory_growth(devices[0], True)
 </denchmark-code>
 
 This is something I have to do in my training scripts as well. Might help someone 👍
 		",33.0,olk,2020-09-07T01:48:15Z,"
 		<denchmark-link:https://github.com/610265158/face_landmark/issues/34#issuecomment-615152235>610265158/face_landmark#34 (comment)</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,34.0,olk,2020-10-03T09:02:54Z,"
 		Ideas from stackoverflow.  I just directly copy the code from deeplearning.ai in colab. A part of it goes like this:
 `train_generator = train_datagen.flow_from_directory(
 'horse-or-human/',  # This is the source directory for training images
 target_size=(300, 300),  # All images will be resized to 300x300
 batch_size=128,
 # Since we use binary_crossentropy loss, we need binary labels
 class_mode='binary')
 history = model.fit(
 train_generator,
 steps_per_epoch=8,
 epochs=15,
 verbose=1)`
 and there are 1027 images. 128*8=1024, less than 1027. I set steps_per_epoch to 9, the error disappear.
 So, for me the .
 At least this is one of the cases for the error.
 Here is the original answer <denchmark-link:url>https://stackoverflow.com/questions/60000573/error-occurred-when-finalizing-generatordataset-iterator-cancelled-operation-w</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,496,496,tensorflow::data::CapturedFunction::Create,"ctx,metadata,captured_inputs,out_function",494,502,,,,,,,,1.0,622,623,tensorflow::data::InstantiatedCapturedFunction::InstantiatedCapturedFunction,"lib,f_handle,ret_types,runner,captured_func",619,627,1.0,"850,852,853",852,tensorflow::data::CapturedFunction::CapturedFunction,"metadata,captured_inputs",849,853,1.0,605,"605,606",tensorflow::data::CapturedFunction::Instantiate,"ctx,instantiated_captured_function",527,607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35306,gmrhub,2019-12-20T11:38:08Z,2020-02-20T00:18:57Z,A possible bug in ConvRNN2D __call__,"
 Referring to
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L294-L341>ConvRNN2D.call</denchmark-link>
 
 L308  kwargs['initial_state'] = initial_state and L317 kwargs['constants'] = constants should be added in the else block at L340. The current situation contradicts with the use of full_input at L337.
 I am not sure if we can simply replicate the code from its parent <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/recurrent.py#L640-L700>RNN.call</denchmark-link>
 .
 I went ahead and tried it, but after loading the saved model weights in a complete new python session the results on validation data doesn't match at all.
 I would appreciate a quick fix for local edit at least.
 Thanks
 	",1.0,gmrhub,2019-12-23T09:57:32Z,"
 		Can you please  help us with simple standalone code to reproduce the issue in our environment. It helps us in localizing the issue faster. Which TensorFlow version you are using?.Thanks!
 		",2.0,gmrhub,2019-12-23T10:18:31Z,"
 		Please refer the linked issue. That support is never resolved, people tried their own ways.
 So while I was skimming through the code I noticed the bug and suggested the line numbers.
 It seems that feature is never tested. After fixing the code it works but has another problem of non reproducible results when re loading the weights (in a new run). I think somehow it behaves like stateful model, which is not.
 I'm using tf 2.0, but the code in master branch is same.
 Since you asked, an example model:
 `
 from tensorflow.keras import layers
 import tensorflow.keras as keras
 encoder_inputs = layers.Input((None,32,32,3))
 encoder = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=""same"", return_sequences=False, return_state=True)
 encoder_outputs, state_h, state_c = encoder(encoder_inputs)
 encoder_states = [state_h, state_c]
 decoder_inputs = layers.Input((None,32,32,4))
 decoder_lstm = layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), padding=""same"", return_sequences=False, return_state=True)
 decoder_outputs, _, _ = decoder_lstm([decoder_inputs,state_h, state_c])
 output = layers.Conv2D(1, (3,3), padding=""same"", activation=""relu"")(decoder_outputs)
 model = keras.Model([encoder_inputs, decoder_inputs], output)
 `
 		",3.0,gmrhub,2019-12-24T06:49:37Z,"
 		<denchmark-link:https://github.com/gmrhub>@gmrhub</denchmark-link>
 
 I have tried on colab with TF version 2.0,2.1.0-rc1 and i am seeing .Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/47bd873e15b693985657da79121bc541/untitled498.ipynb>here</denchmark-link>
 .Is this the expected behavior?
 		",74c9e141067c804bb9a5f94df9342d270cc01f75,Scott Zhu,2020-02-19 16:08:44-08:00,MODIFY,2,tensorflow\python\keras\layers\convolutional_recurrent.py,tensorflow\python\keras\layers\convolutional_recurrent.py,1.0,"318,319,320","317,318,319,320",MODIFY,1.0,tensorflow\python\keras\layers\convolutional_recurrent_test.py,tensorflow\python\keras\layers\convolutional_recurrent_test.py,4.0,gmrhub,2019-12-24T07:56:23Z,"
 		Yes, that error is due to the lines I mentioned above.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Tue, 24 Dec 2019 at 12:19 PM, ravikyram ***@***.***> wrote:
  @gmrhub <https://github.com/gmrhub>
  I have tried on colab with TF version 2.0,2.1.0-rc1 and i am seeing AssertionError:
  .Please, find the gist here
  <https://colab.sandbox.google.com/gist/ravikyram/47bd873e15b693985657da79121bc541/untitled498.ipynb>.Is
  this the expected behavior?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#35306?email_source=notifications&email_token=ACSHLKBXQLZHJZ7ZEJWO5DLQ2GWIPA5CNFSM4J54YP5KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEHSUCTI#issuecomment-568672589>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/ACSHLKDD5JLODB467XTV62LQ2GWIPANCNFSM4J54YP5A>
  .
 
 
 
 		",5.0,gmrhub,2020-01-15T18:26:18Z,"
 		Related stackoverflow with a workaround that allowed me to train successfully (tf 1.14): <denchmark-link:https://stackoverflow.com/questions/50253138/convlstm2d-initial-state-assertion-error>https://stackoverflow.com/questions/50253138/convlstm2d-initial-state-assertion-error</denchmark-link>
 
 In line 331 in  ConvRNN2D.__call__() : change
    if K.is_keras_tensor(additional_inputs[0]):
 to
   if False and K.is_keras_tensor(additional_inputs[0]):
 preventing the if statement from executing.
 		",6.0,gmrhub,2020-02-15T00:25:05Z,"
 		Sorry for the very late reply. I will take a look and fix the issue.
 		",1.0,"205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233",,test_conv_lstm_with_initial_state,self,205,233,,,,,,,,,,,,,,,step,"inputs,states",317,320,1.0,"302,303,318,319,320","295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342",__call__,"self,inputs,initial_state,constants,kwargs",295,342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,gmrhub,2020-02-20T00:18:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35306>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35306>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,gmrhub,2020-03-08T04:25:58Z,"
 		tf.version=1.15.0
 tf.keras.version = 2.2.4-tf
 I encounter a similar problem when using tf.keras.layers.convLSTM2D with initial_state being input. The code raises the problem is
 <denchmark-code>de_convlstm = ConvLSTM2D(filters=64, kernel_size=(5, 5), name='de_convlstm',
                              return_state=True, padding='same', return_sequences=True)
 decoder_convlstm, h, c = de_convlstm(inputs=[decoder_conv2d_2, h, c])
 </denchmark-code>
 
 Following a possible solution in <denchmark-link:https://github.com/keras-team/keras/issues/9761#issuecomment-381058540>keras-team/keras#9761 (comment)</denchmark-link>
 , I remove
 <denchmark-code>inputs, initial_state, constants = self._standardize_args(
             inputs, initial_state, constants)
 </denchmark-code>
 
 from tensorflow_core/python/keras/layers/convolutional_recurrent.py. However, the following ValueError is reported
 ValueError: Variable <tf.Variable 'en3_trans/kernel:0' shape=(1, 1, 5, 8) dtype=float32> has None for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval
 BTW, I am really sure that my code runs well without initial_state being assigned, such as
 <denchmark-code>de_convlstm = ConvLSTM2D(filters=64, kernel_size=(5, 5), name='de_convlstm',
                              return_state=True, padding='same', return_sequences=True)
 decoder_convlstm, h, c = de_convlstm(inputs=decoder_conv2d_2)
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35335,leandro-gracia-gil,2019-12-22T01:32:27Z,2020-01-23T18:37:13Z,Dataset scan loses variable modifications,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, providing source.
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15.2, most likely irrelevant.
 TensorFlow installed from (source or binary): binary from pip
 TensorFlow version (use command below): v1.12.1-21171-g9798f84fa9 2.1.0-dev20191221 (installed via pip install tf-nightly==2.1.0dev20191221)
 Python version: 3.7.2
 CUDA/cuDNN version: using CPU only.
 
 Describe the current behavior
 While writing a unit test I created a function that iterates a tf.data.Dataset and accumulates the values in a local variable. This worked fine using eager mode, but then I noticed that the returned result was zero when using tf.function.
 I've produced a small simple code that reproduces the problem. In particular, returning the accumulator variable produces a result of 0, but accessing the variable directly works fine. Also, using tf.print on the accumulator while iterating the dataset shows the correct value, but printing it after the iteration still within the method shows 0, suggesting perhaps some kind of scoping problem.
 Please see the attached source to understand better what I mean.
 Describe the expected behavior
 The result should be the same when using eager mode and tf.function. Also, when using tf.function the result should be the same when returning the variable and when accessing it directly.
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/3992032/tf_function_variable.py.txt>tf_function_variable.py.txt</denchmark-link>
 
 	",1.0,leandro-gracia-gil,2019-12-23T07:08:39Z,"
 		I could replicate the issue with Tf 2.1.
 Please find the gist <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/70fca4acb63107bfdb216b4024f57373/untitled319.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,leandro-gracia-gil,2019-12-24T13:29:27Z,"
 		<denchmark-link:https://github.com/leandro-gracia-gil>@leandro-gracia-gil</denchmark-link>
  Please use the following workaround until we get this fixed: .
 <denchmark-link:https://github.com/jsimsa>@jsimsa</denchmark-link>
  it looks like  loses variable updates. Repro below. Note that  alone works, but we need the  addition due to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/32138>#32138</denchmark-link>
 . I'm tempted to drop the use of  altogether and use iterators throughout, since they seem to be more stable.
 <denchmark-code>acc = tf.Variable(0.0, dtype=tf.float32, trainable=False)
 
 @tf.function(autograph=False)
 def sum_dataset(dataset):
   def body(dummy, t):
     acc.assign_add(t)
 
     # Prints the actual value in all cases.
     tf.print('in loop', acc)
 
     return (dummy, dummy)
 
   def reduce_body(_, scan_outputs):
     return scan_outputs
 
   tr = tf.data.experimental.scan((tf.constant(0),), body)
   dataset = dataset.apply(tr)
   dataset.reduce((tf.constant(0),), reduce_body)
 
   # Prints 0.0 when tf.function is used, but the actual value in eager mode.
   tf.print('after loop', acc)
 
   return acc
 
 records = np.random.uniform(size=(10,)).astype(np.float32)
 dataset = tf.data.Dataset.from_tensor_slices(records)
 result = sum_dataset(dataset)
 
 # Fails when tf.function is used in sum_dataset because result is 0.0.
 assert result.numpy() == acc.numpy()
 </denchmark-code>
 
 		",3.0,leandro-gracia-gil,2019-12-30T21:44:27Z,"
 		I believe that this is another instance of the ""datasets do not propagate"" control dependencies bug (b/142341957) that and I plan to fix as soon as possible.
 		",c4ec9389364cb8d1bff451ab8baf55d25cabdd1f,Jiri Simsa,2020-01-23 10:35:51-08:00,MODIFY,4,tensorflow\python\data\kernel_tests\iterator_test.py,tensorflow\python\data\kernel_tests\iterator_test.py,1.0,"1002,1003,1004",,MODIFY,7.0,tensorflow\python\data\kernel_tests\reduce_test.py,tensorflow\python\data\kernel_tests\reduce_test.py,4.0,leandro-gracia-gil,2020-01-23T18:37:15Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35335>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35335>No</denchmark-link>
 
 		",,,,,,,,,1.0,170,170,testSideEffect,self,160,177,MODIFY,3.0,tensorflow\python\data\ops\dataset_ops.py,tensorflow\python\data\ops\dataset_ops.py,1.0,"4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339",,,,,,,,,testNestedAutomaticControlDependencies.map_fn,x,1002,1004,1.0,"1006,1007",,testNestedAutomaticControlDependencies.dataset_fn,,1006,1007,1.0,"999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017",,testNestedAutomaticControlDependencies,self,999,1017,1.0,"1010,1011,1012,1013,1014",,testNestedAutomaticControlDependencies.fn,,1010,1014,1.0,"208,209,210",,testNestedAutomaticControlDependencies.map_fn,x,208,210,1.0,150,150,testDatasetSideEffect,self,137,157,1.0,"205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222",,testNestedAutomaticControlDependencies,self,205,222,1.0,194,194,testAutomaticControlDependencies,self,180,202,1.0,"212,213",,testNestedAutomaticControlDependencies.dataset_fn,,212,213,,,,,_collect_resource_inputs._process,"op_queue,seen_ops",4325,4339,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379",,_resource_resolver,"op,resource_inputs",4351,4379,1.0,"4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347",,_collect_resource_inputs,op,4322,4347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"216,217,218,219",,testNestedAutomaticControlDependencies.fn,,216,219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35379,bearsroom,2019-12-24T06:21:18Z,2019-12-27T01:59:40Z,Cannot export keras model to SavedModel if mixed-precision policy is enabled,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): starting from 2.0, nightly version tested
 Python version: 3.5
 
 Describe the current behavior
 When keras mixed-precision policy ""mixed_float16"" is in use, we can't save the keras model in SavedModel format with method keras.models.Model.save without a specific signatures. It seems like a mismatch between input signature inferred by the model itself and the auto-casted inputs:
 <denchmark-code>ValueError: Python inputs incompatible with input_signature: inputs ((<tf.Tensor 'conv1_pad/Cast:0' shape=(None, 224, 224, 3) dtype=float16>,)), input_signature ((TensorSpec(shape=(None, None, None, None), dtype=tf.float32, name=None),))
 </denchmark-code>
 
 Although we can use graph_rewrite as mixed-precision training method to bypass this autocasting issue, but graph_rewrite is not working in some cases (e.g. train a subclassed model with tf.GradientTape) thus it is not recommended by tensorflow official guide. For flexibility we do hope to use mixed-precision policy in mixed-precision training, and directly exporting mixed-precision trained model to SavedModel for deployment is straightforward in production pipeline.
 Code to reproduce the issue
 We can reproduce this bug by using the official image classification training example from <denchmark-link:https://github.com/tensorflow/models/tree/master/official/vision/image_classification>https://github.com/tensorflow/models/tree/master/official/vision/image_classification</denchmark-link>
 
 <denchmark-code>""""""
 Test mixed-precision policy model saving
 """"""
 import logging
 import os
 
 from absl import app as absl_app
 import tensorflow as tf
 
 from official.vision.image_classification.resnet_model import resnet50
 
 
 def main(argv):
   tf.compat.v1.enable_eager_execution()
   
   # setup mixed-precision policy
   # the policy enables the autocasting behavior in keras layers
   policy = tf.keras.mixed_precision.experimental.Policy(
         'mixed_float16', loss_scale=128)
   tf.keras.mixed_precision.experimental.set_policy(policy)
 
   model = resnet50(1000)
   model_dir = 'temp/saved_model_test'
 
   if not os.path.isdir(model_dir):
     os.makedirs(model_dir)
   model.save(model_dir,
              save_format='tf')
   logging.info('Exported trained model to directory {}'.format(
       model_dir))
 
 
 if __name__ == '__main__':
   absl_app.run(main)
 </denchmark-code>
 
 	",1.0,bearsroom,2019-12-24T06:24:41Z,"
 		Gently ping <denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
 ; does this look familiar to you?
 		",2.0,bearsroom,2019-12-26T06:33:51Z,"
 		When tried replicating the issue, I got the error as per the colab <denchmark-link:https://colab.sandbox.google.com/gist/oanush/1cc1d54c6411da3cdc2d5009b59775b1/35379.ipynb>gist</denchmark-link>
 .Thanks!
 		",3.0,bearsroom,2019-12-27T00:23:03Z,"
 		Thank you for filing this and the short example to reproduce. I will have a fix soon. In the future, I plan on testing the official models with SavedModel to ensure issues like this do not occur in the future.
 		",cd6184047e9e497955c473b88387b54818ff23a0,Reed Wanderman-Milne,2019-12-26 17:58:07-08:00,MODIFY,1,tensorflow\python\keras\mixed_precision\experimental\keras_test.py,tensorflow\python\keras\mixed_precision\experimental\keras_test.py,1.0,498,,MODIFY,1.0,tensorflow\python\keras\mixed_precision\experimental\test_util.py,tensorflow\python\keras\mixed_precision\experimental\test_util.py,4.0,bearsroom,2019-12-27T01:59:42Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35379>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35379>No</denchmark-link>
 
 		",5.0,bearsroom,2019-12-27T05:05:51Z,"
 		<denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
  thanks for the prompt fix!
 <denchmark-link:https://github.com/goldiegadde>@goldiegadde</denchmark-link>
  shall we apply the fix above to r2.1? Mixed precision along with XLA improves training performance dramatically and we’d really like to use it with our e2e training/inference pipeline.
 		",6.0,bearsroom,2020-01-22T19:42:15Z,"
 		Is this working for anyone? I updated to 2.1 and I no longer get this same error but now I get
 TypeError: Input 'filter' of 'Conv2D' Op has type float16 that does not match type float32 of argument 'input'.
 I tried this first on my models and then ran the given snippet and got the same deal.
 		",1.0,127,127,assert_input_types,"self,inputs",120,127,MODIFY,2.0,tensorflow\python\keras\saving\saved_model\save_impl.py,tensorflow\python\keras\saving\saved_model\save_impl.py,1.0,368,368,,,,,,,,test_model,"self,strategy_fn,use_operator,use_regularizer,policy_name,get_config,save_format,use_input_spec,experimental_run_tf_function",491,499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,bearsroom,2020-01-22T21:48:56Z,"
 		Unfortunately the fix did not make it into 2.1 :(. It will be in 2.2 however, and the fix is already in tf-nightly
 I'm not sure why the error message is different. Note that Model.save_weights still works, just not Model.save.
 As a workaround, you can call , rebuild the model in fp32, call  on the fp32 model then call . But understandably, this is very irritating, and it doesn't save a mixed precision model. Using the old <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/train/experimental/enable_mixed_precision_graph_rewrite>tf.train.experimental.enable_mixed_precision_graph_rewrite</denchmark-link>
  API also works but note we plan on deprecating then removing it (it will remain in  for a long time though).
 		",_generate_input_signature.to_tensor_spec_or_none,x,367,375,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,368,368,_generate_input_signature,"self,layer",349,381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35547,wchargin,2020-01-02T18:27:18Z,2020-01-03T20:50:10Z,“Cloud TPU” console spam on every TensorFlow import,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): gLinux (like Debian)
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v1.12.1-21487-g2e8d5e5 2.1.0-dev20200102
 Python version: Python 3.7.5rc1
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 Importing TensorFlow prints an unnecessary and unhelpful warning:
 
 WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
 
 Describe the expected behavior
 Importing TensorFlow should not print any messages about Cloud TPUs.
 This is a normal desktop installation that doesn’t have anything to do
 with TPUs, and doesn’t need them.
 Code to reproduce the issue
 python -c 'import tensorflow' 2>&1 | diff -u /dev/null -
 Other info / logs
 Likely introduced by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/5364121e858be715439c328c5f67de8441050f4b>5364121</denchmark-link>
 .
 	",1.0,wchargin,2020-01-03T20:50:10Z,"
 		Thanks for your report! It should be fixed starting with the nightly build tomorrow morning (2.1.0-dev20200104).
 		",2.0,wchargin,2020-01-03T20:50:12Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35547>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35547>No</denchmark-link>
 
 		",3.0,wchargin,2020-01-03T21:18:38Z,"
 		Awesome; thanks! Thanks for fixing the grammar, too. :-)
 		",3ff0960fee4927db7baea2f354e60c3328b066fe,Frank Chen,2020-01-03 12:35:16-08:00,MODIFY,0,tensorflow\python\distribute\cluster_resolver\tpu_cluster_resolver.py,tensorflow\python\distribute\cluster_resolver\tpu_cluster_resolver.py,0.0,"34,35,36","34,35,36",MODIFY,0.0,tensorflow\python\distribute\cluster_resolver\tpu_cluster_resolver_test.py,tensorflow\python\distribute\cluster_resolver\tpu_cluster_resolver_test.py,,,,,,,,,,,,,0.0,"41,42,43","41,42,43",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35551,am15h,2020-01-02T21:33:26Z,2020-01-08T00:48:42Z,TFLite expermimental_new_converter error with tf.keras Bidirectional Wrapper or attribute go_backwards=True,"
 System information
 
 OS Platform and Distribution: Linux Ubuntu  18.04
 TensorFlow installed from (source or binary):
 TensorFlow version (or github SHA if from source):
 Used to build model: 2.0.0
 Used to run converter : 2.1.0-dev20191227
 
 Command used to run the converter or code if you’re using the Python API
 <denchmark-code>import tensorflow as tf
 import os
 os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
 
 print(tf.__version__)
 
 model = tf.keras.models.load_model(""/home/amish/PycharmProjects/myproject/scripts/temp.h5"")
 model.summary()
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                        tf.lite.OpsSet.SELECT_TF_OPS]
 converter.experimental_new_converter = True  # Add this line
 
 tflite_model = converter.convert()
 </denchmark-code>
 
 The output from the converter invocation
 <denchmark-code>WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
 2.1.0-dev20191227
 2020-01-03 02:45:22.187710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
 2020-01-03 02:45:22.193838: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
 2020-01-03 02:45:22.193908: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: enigma
 2020-01-03 02:45:22.193928: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: enigma
 2020-01-03 02:45:22.207575: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0
 2020-01-03 02:45:22.207706: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0
 2020-01-03 02:45:22.207730: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0
 2020-01-03 02:45:22.208264: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2020-01-03 02:45:22.384583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2496000000 Hz
 2020-01-03 02:45:22.391769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55cbc700ede0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
 2020-01-03 02:45:22.391868: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
 Model: ""sequential_14""
 _________________________________________________________________
 Layer (type)                 Output Shape              Param #   
 =================================================================
 masking_14 (Masking)         (None, 50, 36)            0         
 _________________________________________________________________
 bidirectional_6 (Bidirection (None, 50, 256)           168960    
 _________________________________________________________________
 dropout_16 (Dropout)         (None, 50, 256)           0         
 _________________________________________________________________
 dense_16 (Dense)             (None, 50, 10)            2570      
 =================================================================
 Total params: 171,530
 Trainable params: 171,530
 Non-trainable params: 0
 _________________________________________________________________
 2020-01-03 02:45:24.787867: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
 2020-01-03 02:45:24.788386: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
 2020-01-03 02:45:25.204179: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
 2020-01-03 02:45:25.204249: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 255 nodes (0), 310 edges (0), time = 121.027ms.
 2020-01-03 02:45:25.204268: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: Graph size after: 255 nodes (0), 310 edges (0), time = 9.261ms.
 2020-01-03 02:45:25.204281: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_body_3164
 2020-01-03 02:45:25.204300: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.004ms.
 2020-01-03 02:45:25.204316: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
 2020-01-03 02:45:25.204332: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_cond_3163
 2020-01-03 02:45:25.204345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
 2020-01-03 02:45:25.204360: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
 2020-01-03 02:45:25.204376: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_body_3360
 2020-01-03 02:45:25.204391: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
 2020-01-03 02:45:25.204406: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
 2020-01-03 02:45:25.204422: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_cond_3359
 2020-01-03 02:45:25.204440: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
 2020-01-03 02:45:25.204460: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   function_optimizer: function_optimizer did nothing. time = 0ms.
 2020-01-03 02:45:25.373374: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
 2020-01-03 02:45:25.373537: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
 2020-01-03 02:45:25.447623: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: graph_to_optimize
 2020-01-03 02:45:25.447671: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 177 nodes (-78), 225 edges (-85), time = 50.718ms.
 2020-01-03 02:45:25.447678: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 177 nodes (0), 225 edges (0), time = 3.89ms.
 2020-01-03 02:45:25.447702: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_body_3360_frozen
 2020-01-03 02:45:25.447707: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (-1), 96 edges (0), time = 2.225ms.
 2020-01-03 02:45:25.447730: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (0), 96 edges (0), time = 1.116ms.
 2020-01-03 02:45:25.447752: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_body_3164_frozen
 2020-01-03 02:45:25.447757: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (-1), 96 edges (0), time = 2.118ms.
 2020-01-03 02:45:25.447761: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 71 nodes (0), 96 edges (0), time = 1.179ms.
 2020-01-03 02:45:25.447766: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_backward_lstm_56_while_cond_3359_frozen
 2020-01-03 02:45:25.447771: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.339ms.
 2020-01-03 02:45:25.447795: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.214ms.
 2020-01-03 02:45:25.447800: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:815] Optimization results for grappler item: sequential_14_bidirectional_6_forward_lstm_56_while_cond_3163_frozen
 2020-01-03 02:45:25.447817: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.311ms.
 2020-01-03 02:45:25.447822: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:817]   constant_folding: Graph size after: 17 nodes (0), 4 edges (0), time = 0.213ms.
 Traceback (most recent call last):
   File ""convert.py"", line 15, in <module>
     tflite_model = converter.convert()
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 490, in convert
     **converter_kwargs)
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 476, in toco_convert_impl
     enable_mlir_converter=enable_mlir_converter)
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 215, in toco_convert_protos
     raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
 tensorflow.lite.python.convert.ConverterError: See console for info.
 WARNING:tensorflow:Falling back to tensorflow client, its recommended to install the cloud tpu client directly with pip install cloud-tpu-client .
 2020-01-03 02:45:27.257206: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:108] Ignored output_format.
 2020-01-03 02:45:27.257267: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:114] Ignored drop_control_dependency.
 2020-01-03 02:45:27.478616: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2020-01-03 02:45:27.500564: I tensorflow/core/platform/profile_utils/cpu_utils.cc:101] CPU Frequency: 2496000000 Hz
 2020-01-03 02:45:27.500991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555ae8caec30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
 2020-01-03 02:45:27.501033: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
 2020-01-03 02:45:27.502780: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
 2020-01-03 02:45:27.505700: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
 2020-01-03 02:45:27.505725: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: enigma
 2020-01-03 02:45:27.505732: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: enigma
 2020-01-03 02:45:27.505775: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 430.50.0
 2020-01-03 02:45:27.505796: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 430.50.0
 2020-01-03 02:45:27.505801: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 430.50.0
 loc(callsite(""sequential_14/bidirectional_6/backward_lstm_56/ReverseV2_1""(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"":853:0) at callsite(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"":947:0 at callsite(""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"":409:0 at ""convert.py"":9:0)))): error: 'tfl.reverse_v2' op operand #0 must be tensor of 32-bit float or 16-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<50x1x1xi1>'
 Traceback (most recent call last):
   File ""/home/amish/anaconda3/bin/toco_from_protos"", line 8, in <module>
     sys.exit(main())
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
     app.run(main=execute, argv=[sys.argv[0]] + unparsed)
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
     _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
     _run_main(main, args)
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
     sys.exit(main(argv))
   File ""/home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
     enable_mlir_converter)
 Exception: /home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:853:9: error: 'tfl.reverse_v2' op operand #0 must be tensor of 32-bit float or 16-bit integer or 32-bit integer or 64-bit integer values, but got 'tensor<50x1x1xi1>'
         self._initialize(args, kwargs, add_initializers_to=initializers)
         ^
 /home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:947:5: note: called from
     concrete = self._get_concrete_function_garbage_collected(*args, **kwargs)
     ^
 /home/amish/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py:409:5: note: called from
     concrete_func = func.get_concrete_function()
     ^
 convert.py:9:1: note: called from
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 
 </denchmark-code>
 
 This issue caused due to Bidirectional Wrapper. Also same error occurs when go_backwards=True in the LSTM layer.
 Please tell a workaround(if any) for this so that I can temporarily fix this.
 	",1.0,am15h,2020-01-03T23:23:23Z,"
 		<denchmark-link:https://github.com/am15h>@am15h</denchmark-link>
  Can you please share the model or the code to create that model. Thanks!
 		",2.0,am15h,2020-01-04T13:10:57Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  The code to create model.
 <denchmark-code>model = tf.keras.Sequential()
 model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))
 
 #model.add(layers.LSTM(128, return_sequences=True)) #Works fine without Birdirectional
 model.add(layers.Dense(20, activation='softmax'))
 
 model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])
 
 model.summary()
 
 model.fit(x_train, y_train, batch_size=32, epochs=13, shuffle=True, validation_data=(x_test, y_test))
 </denchmark-code>
 
 		",3.0,am15h,2020-01-06T23:05:55Z,"
 		I couldn't reproduce your issue. I used the below code to build the model and convert:
 model = tf.keras.Sequential()
 model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True), input_shape=(128, 128)))
 model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))
 model.add(tf.keras.layers.Dense(20, activation='softmax'))
 model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.experimental_new_converter = True
 tflite_model = converter.convert()
 This could convert with no error reported. Could you confirm if this model is the same as yours? Also, is there a Masking layer as the first layer? And what's the input data type/shape?
 		",e8f6431f53f49f8cab7e15bef24ab2ee775f2ed9,Haoliang Zhang,2020-01-07 16:47:28-08:00,MODIFY,0,tensorflow\compiler\mlir\lite\ir\tfl_ops.td,tensorflow\compiler\mlir\lite\ir\tfl_ops.td,0.0,"2130,2139,2144","2130,2139,2144",MODIFY,1.0,tensorflow\lite\kernels\register.cc,tensorflow\lite\kernels\register.cc,4.0,am15h,2020-01-07T10:34:13Z,"
 		<denchmark-link:https://github.com/haozha111>@haozha111</denchmark-link>
  Yes there is a masking layer, I accidentally remove that line while sharing code,
 <denchmark-code>model = tf.keras.Sequential()
 model.add(layers.Masking(input_shape=(50,36)))
 model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))
 model.add(layers.Dropout(0.2, noise_shape=None, seed=None))
 model.add(layers.Dense(20, activation='softmax'))
 
 model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])
 
 model.summary()
 
 model.fit(x_train, y_train, batch_size=32, epochs=13, shuffle=True, validation_data=(x_test, y_test))
 
 </denchmark-code>
 
 		",5.0,am15h,2020-01-07T21:27:34Z,"
 		Thanks!
 I can reproduce the error now. The issue is that tflite's reverse op doesn't support bool type. So the conversion fails. I'm working on a fix for that.
 		",6.0,am15h,2020-01-08T00:07:54Z,"
 		I've completed a fix for this issue and the code will be merged into tf-nightly soon. Please try again with the tf-nightly (tomorrow's version should contain the fix) and then try if your issue is resolved.
 Cheers.
 		",1.0,"263,264,265",263,tflite::ops::builtin::BuiltinOpResolver::BuiltinOpResolver,,43,292,MODIFY,2.0,tensorflow\lite\kernels\reverse.cc,tensorflow\lite\kernels\reverse.cc,1.0,"106,107,108,109,110,111",,MODIFY,3.0,tensorflow\lite\testing\op_tests\reverse_v2.py,tensorflow\lite\testing\op_tests\reverse_v2.py,1.0,"31,46,47,48,53,54","46,51",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,am15h,2020-01-08T00:48:43Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35551>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35551>No</denchmark-link>
 
 		",tflite::ops::builtin::reverse::Eval,"context,node",67,120,make_reverse_v2_tests,options,27,58,MODIFY,1.0,tensorflow\lite\toco\tflite\op_version.cc,tensorflow\lite\toco\tflite\op_version.cc,1.0,233,,toco::tflite::GetMinimumRuntimeVersionForModel,model,46,260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\lite\tools\versioning\op_version.cc,tensorflow\lite\tools\versioning\op_version.cc,1.0,"270,271,272,273,274",,tflite::GetBuiltinOperatorVersion,op_sig,29,318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,43,43,tflite::ops::builtin::reverse::Prepare,"context,node",32,65,,,,,,,,,,,,,,,1.0,"53,54",,make_reverse_v2_tests.build_inputs,"parameters,sess,inputs,outputs",52,56,1.0,"46,47,48",46,make_reverse_v2_tests.build_graph,parameters,44,50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35765,emailweixu,2020-01-11T06:54:34Z,2020-01-20T13:54:24Z,Autograph failure with `\`,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
 Python version: 3.6
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory: Nvidia RTX 2080
 
 Describe the current behavior
 Tensorflow shows warning about  failure of autograph
 <denchmark-code>WARNING:tensorflow:AutoGraph could not transform <bound method C.f of <__main__.C object at 0x7f4a83904668>> and will run it as-is.
 Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
 Cause: expected exactly one node node, found [<gast.gast.FunctionDef object at 0x7f4a47e0d5c0>, <gast.gast.Return object at 0x7f4a47e0df28>]
 </denchmark-code>
 
 The warning seems to be caused by the backslash ""\"".
 Describe the expected behavior
 There should be no such warning
 Code to reproduce the issue
 import tensorflow as tf
 
 class C(object):
     def f(self):
         # error disappear if \ in the following line is removed
         a = \
             1
         return a
 
 obj = C()
 
 @tf.function
 def func():
     mem =  obj.f()
     return mem
 
 def main():
     print(func())
 
 if __name__ == ""__main__"":
     main()
 	",1.0,emailweixu,2020-01-13T20:53:00Z,"
 		Just ran into this as well, removing the slash fixed it.
 		",2.0,emailweixu,2020-01-13T21:57:07Z,"
 		I am closing this as the related PR fix merged. Thanks!
 		",3.0,emailweixu,2020-01-14T19:47:42Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35765>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35765>No</denchmark-link>
 
 		",8e3adf77b2a148fb2c6fea8fea2d0217bca3339f,Dan Moldovan,2020-01-20 05:47:15-08:00,MODIFY,2,tensorflow\python\autograph\pyct\parser.py,tensorflow\python\autograph\pyct\parser.py,1.0,"46,47,48",,MODIFY,2.0,tensorflow\python\autograph\pyct\parser_test.py,tensorflow\python\autograph\pyct\parser_test.py,4.0,emailweixu,2020-01-14T22:39:14Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  which PR fixed this bug?
 		",5.0,emailweixu,2020-01-14T23:44:19Z,"
 		<denchmark-link:https://github.com/emailweixu>@emailweixu</denchmark-link>
  Sorry. I thought it was merged into  branch in . It was my mistake. I am reopening it. Thanks!
 		",6.0,emailweixu,2020-01-15T23:59:26Z,"
 		Looks like a bug in the parser. dedent_block seems to be confused by the backslash continuation:
 <denchmark-code>s = r'''
     def f():
         a = \
             1
         return a
 '''
 
 print(dedent_block(s))
 </denchmark-code>
 
 <denchmark-code>def f():
     a = \
     1
 return a
 </denchmark-code>
 
 		",1.0,"132,133,134,135,136,137,138,139,140,141,142",,test_dedent_block_continuation,self,132,142,,,,,,,,,,,,,,,_unfold_continuations,code_string,46,48,1.0,"54,55",,dedent_block,code_string,51,116,,,,,,,,,,,,,,,1.0,"144,145,146,147,148,149,150,151,152,153,154",,test_dedent_block_continuation_in_string,self,144,154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,emailweixu,2020-01-17T21:40:01Z,"
 		For a workaround until this is fixed, note that you can use parentheses to break expressions on multiple lines:
 <denchmark-code>a = (
     1)
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,emailweixu,2020-01-20T13:54:25Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35765>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35765>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
35980,Intellicode,2020-01-17T11:07:41Z,2020-01-28T19:10:51Z,Tensorflow predict call crashes when loading a model with gevent enabled,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14.6, also tried it on the Docker container nvidia/cuda:10.1-cudnn7-runtime
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
 Python version: 3.7.4 on Mac, Python 3.6.9 :: Anaconda, Inc. in Docker
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 Tensorflow crashes after calling predict on the model, this happens with gevent 1.4.0 and also 1.5a2
 Describe the expected behavior
 Tensorflow doesn't crash
 Code to reproduce the issue
 <denchmark-code>from gevent import monkey
 monkey.patch_all()
 
 import numpy as np
 import tensorflow as tf
 
 classifier = tf.keras.models.load_model('tensorflow_model_dir')
 classifier.predict(np.array(
     np.zeros((1, 12623))
 ))
 </denchmark-code>
 
 Other info / logs
 <denchmark-code>Traceback (most recent call last):
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 430, in eager_learning_phase_scope
     _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value
   File ""/opt/conda/envs/py36/lib/python3.6/weakref.py"", line 407, in __setitem__
     self.data[ref(key, self._remove)] = value
 TypeError: cannot create weak reference to 'gevent._local.local' object
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""scripts/gevent_load_classifier.py"", line 9, in <module>
     np.zeros((1, 12623))
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1013, in predict
     use_multiprocessing=use_multiprocessing)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 498, in predict
     workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 475, in _model_iteration
     total_epochs=1)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
     batch_outs = execution_function(iterator)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
     distributed_function(input_fn))
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
     result = self._call(*args, **kwds)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
     self._initialize(args, kwds, add_initializers_to=initializers)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
     *args, **kwds))
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 85, in distributed_function
     per_replica_function, args=args)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 763, in experimental_run_v2
     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1819, in call_for_each_replica
     return self._call_for_each_replica(fn, args, kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2164, in _call_for_each_replica
     return fn(*args, **kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
     return func(*args, **kwargs)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 212, in _predict_on_batch
     result = predict_on_batch(model, x)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 555, in predict_on_batch
     with backend.eager_learning_phase_scope(0):
   File ""/opt/conda/envs/py36/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/opt/conda/envs/py36/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 437, in eager_learning_phase_scope
     del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
   File ""/opt/conda/envs/py36/lib/python3.6/weakref.py"", line 391, in __delitem__
     del self.data[ref(key)]
 TypeError: cannot create weak reference to 'gevent._local.local' object
 
 </denchmark-code>
 
 	",1.0,Intellicode,2020-01-23T10:33:46Z,"
 		Tried replicating the issue from<denchmark-link:https://colab.sandbox.google.com/gist/oanush/6e224557fe243a2e53b7421176afbfa0/35980.ipynb> given code</denchmark-link>
 ,it just keeps running without any output.Thanks!
 		",2.0,Intellicode,2020-01-23T12:22:42Z,"
 		Thanks for checking <denchmark-link:https://github.com/oanush>@oanush</denchmark-link>
 , I tried the notebook, but I think the notebook environment loads tensorflow before doing the monkey patch:
 <denchmark-code>/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/usr/local/lib/python3.6/dist-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/usr/local/lib/python3.6/dist-packages/urllib3/util/ssl_.py)']. 
 </denchmark-code>
 
 This is the same warning I get when I import tensorflow before importing gevent. Also, to reproduce the error you probably need to load an actual tensorflow model (replace tensorflow_model_dir)
 		",3.0,Intellicode,2020-01-23T23:02:48Z,"
 		Looks like an error in the environment itself. Can you try this in a new virtual environment and let me know if you are facing the same issue?
 		",75286a79e7cdf9fdc27b15919f453786eee8936d,Igor Ganichev,2020-01-28 11:09:31-08:00,MODIFY,6,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,1.0,"462,467,469",,,,,,4.0,Intellicode,2020-01-24T09:58:26Z,"
 		<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
  I've tried creating a seperate containter for this purpose:
 <denchmark-code>FROM python:3.6-slim
 
 RUN apt-get update \
     && apt-get install -y --no-install-recommends build-essential \
     && rm -rf /var/lib/apt/lists/*
 
 RUN pip install tensorflow gevent
 
 COPY tf_test.py /opt/test/tf_test.py
 COPY model_dir /opt/test/model_dir
 
 CMD python /opt/test/tf_test.py
 </denchmark-code>
 
 where tf_test.py contains the following code:
 <denchmark-code>from gevent import monkey
 monkey.patch_all()
 
 import numpy as np
 import tensorflow as tf
 
 classifier = tf.keras.models.load_model('/opt/test/model_dir')
 classifier.predict(np.array(
     np.zeros((1, 12623))
 ))
 </denchmark-code>
 
 This is the complete output:
 <denchmark-code>2020-01-24 09:56:31.304096: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory
 2020-01-24 09:56:31.304280: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory
 2020-01-24 09:56:31.304324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
 2020-01-24 09:56:31.867084: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
 2020-01-24 09:56:31.867143: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)
 2020-01-24 09:56:31.867175: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b3b6a3003434): /proc/driver/nvidia/version does not exist
 2020-01-24 09:56:31.867464: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
 2020-01-24 09:56:31.874365: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2208000000 Hz
 2020-01-24 09:56:31.875321: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a8216a44f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
 2020-01-24 09:56:31.875375: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
 Traceback (most recent call last):
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 430, in eager_learning_phase_scope
     _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value
   File ""/usr/local/lib/python3.6/weakref.py"", line 407, in __setitem__
     self.data[ref(key, self._remove)] = value
 TypeError: cannot create weak reference to 'gevent._local.local' object
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""/opt/test/tf_test.py"", line 9, in <module>
     np.zeros((1, 12623))
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1013, in predict
     use_multiprocessing=use_multiprocessing)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 498, in predict
     workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 475, in _model_iteration
     total_epochs=1)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py"", line 128, in run_one_epoch
     batch_outs = execution_function(iterator)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 98, in execution_function
     distributed_function(input_fn))
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
     result = self._call(*args, **kwds)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
     self._initialize(args, kwds, add_initializers_to=initializers)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
     *args, **kwds))
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 85, in distributed_function
     per_replica_function, args=args)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 763, in experimental_run_v2
     return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 1819, in call_for_each_replica
     return self._call_for_each_replica(fn, args, kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/distribute/distribute_lib.py"", line 2164, in _call_for_each_replica
     return fn(*args, **kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/autograph/impl/api.py"", line 292, in wrapper
     return func(*args, **kwargs)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 212, in _predict_on_batch
     result = predict_on_batch(model, x)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 555, in predict_on_batch
     with backend.eager_learning_phase_scope(0):
   File ""/usr/local/lib/python3.6/contextlib.py"", line 81, in __enter__
     return next(self.gen)
   File ""/usr/local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py"", line 437, in eager_learning_phase_scope
     del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
   File ""/usr/local/lib/python3.6/weakref.py"", line 391, in __delitem__
     del self.data[ref(key)]
 TypeError: cannot create weak reference to 'gevent._local.local' object
 </denchmark-code>
 
 		",5.0,Intellicode,2020-01-24T10:10:23Z,"
 		You can also reproduce the error with the code from the <denchmark-link:https://www.tensorflow.org/tutorials/quickstart/beginner>official beginner tutorial</denchmark-link>
 :
 <denchmark-code>from gevent import monkey
 monkey.patch_all()
 import tensorflow as tf
 
 mnist = tf.keras.datasets.mnist
 
 (x_train, y_train), (x_test, y_test) = mnist.load_data()
 x_train, x_test = x_train / 255.0, x_test / 255.0
 
 model = tf.keras.models.Sequential([
     tf.keras.layers.Flatten(input_shape=(28, 28)),
     tf.keras.layers.Dense(128, activation='relu'),
     tf.keras.layers.Dropout(0.2),
     tf.keras.layers.Dense(10, activation='softmax')
 ])
 
 model.compile(optimizer='adam',
               loss='sparse_categorical_crossentropy',
               metrics=['accuracy'])
 
 model.fit(x_train, y_train, epochs=5)
 
 model.evaluate(x_test, y_test, verbose=2)
 </denchmark-code>
 
 		",6.0,Intellicode,2020-01-27T20:59:54Z,"
 		I can confirm this issue. I spent some time browsing the code today, and discovered the following.
 _GRAPH_LEARNING_PHASES  is a weakref.WeakKeyDictionary, and at some point a learning phase is added with the key _DUMMY_EAGER_GRAPH, which is a threading.local() object. Because Gevent monkey patching replaces this local object with a gevent._local.local, which cannot be weakly referenced, lookups for _DUMMY_EAGER_GRAPH will throw the exception above.
 I am not sure the use of  is really necessary here, or perhaps there are alternatives. <denchmark-link:https://github.com/iganichev>@iganichev</denchmark-link>
  I see that you introduced them a while back, perhaps you can shed some light on this?
 		",,,,,,,,,,,,,,,,,,,,,,eager_learning_phase_scope,value,442,469,1.0,"137,138,139,140,141,142",,__init__,self,137,142,1.0,"419,430,431,432","398,399,400,430,435,437",learning_phase_scope,value,397,438,1.0,"330,333",308,learning_phase,,304,336,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,Intellicode,2020-01-27T21:48:43Z,"
 		WeakKeyDictionary is definitely needed. There are a couple of issues here and I can submit a fix. Can somebody quickly test if something like this would work with gevent?
 <denchmark-code>diff --git a/google3/third_party/tensorflow/python/keras/backend.py b/google3/third_party/tensorflow/python/keras/backend.py
 --- a/google3/third_party/tensorflow/python/keras/backend.py
 +++ b/google3/third_party/tensorflow/python/keras/backend.py
 @@ -110,7 +110,14 @@ py_any = any
  # _DUMMY_EAGER_GRAPH is used as a key in _GRAPH_LEARNING_PHASES.
  # We keep a separate reference to it to make sure it does not get removed from
  # _GRAPH_LEARNING_PHASES.
 -_DUMMY_EAGER_GRAPH = threading.local()
 +class DummyEagerGraph(threading.local):
 +  class Foo(object):
 +    pass
 +  def __init__(self):
 +    super(DummyEagerGraph, self).__init__()
 +    self.key = Foo()
 +
 +
 +_DUMMY_EAGER_GRAPH = DummyEagerGraph()
  
  # This boolean flag can be set to True to leave variable initialization
  # up to the user.
 @@ -295,17 +302,17 @@ def learning_phase():
      # will always execute non-eagerly using a function-specific default
      # subgraph.
      if context.executing_eagerly():
 -      if _DUMMY_EAGER_GRAPH not in _GRAPH_LEARNING_PHASES:
 +      if _DUMMY_EAGER_GRAPH.key not in _GRAPH_LEARNING_PHASES:
          # Fallback to inference mode as default.
          return 0
 -      return _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
 +      return _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]
      learning_phase = symbolic_learning_phase()
      _mark_func_graph_as_unsaveable(graph, learning_phase)
      return learning_phase
  
  
  def global_learning_phase_is_set():
 -  return _DUMMY_EAGER_GRAPH in _GRAPH_LEARNING_PHASES
 +  return _DUMMY_EAGER_GRAPH.key in _GRAPH_LEARNING_PHASES
  
  
  def _mark_func_graph_as_unsaveable(graph, learning_phase):
 @@ -356,7 +363,7 @@ def set_learning_phase(value):
      if context.executing_eagerly():
        # In an eager context, the learning phase values applies to both the eager
        # context and the internal Keras graph.
 -      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value
 +      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value
      _GRAPH_LEARNING_PHASES[get_graph()] = value
  
  
 @@ -384,7 +391,7 @@ def learning_phase_scope(value):
    with ops.init_scope():
      if context.executing_eagerly():
        previous_eager_value = _GRAPH_LEARNING_PHASES.get(
 -          _DUMMY_EAGER_GRAPH, None)
 +          _DUMMY_EAGER_GRAPH.key, None)
      previous_graph_value = _GRAPH_LEARNING_PHASES.get(get_graph(), None)
  
    try:
 @@ -395,9 +402,9 @@ def learning_phase_scope(value):
      with ops.init_scope():
        if context.executing_eagerly():
          if previous_eager_value is not None:
 -          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_eager_value
 -        elif _DUMMY_EAGER_GRAPH in _GRAPH_LEARNING_PHASES:
 -          del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
 +          _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_eager_value
 +        elif _DUMMY_EAGER_GRAPH.key in _GRAPH_LEARNING_PHASES:
 +          del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]
  
        graph = get_graph()
        if previous_graph_value is not None:
 @@ -427,14 +434,14 @@ def eager_learning_phase_scope(value):
    if global_learning_phase_was_set:
      previous_value = learning_phase()
    try:
 -    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = value
 +    _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = value
      yield
    finally:
      # Restore learning phase to initial value or unset.
      if global_learning_phase_was_set:
 -      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH] = previous_value
 +      _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key] = previous_value
      else:
 -      del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH]
 +      del _GRAPH_LEARNING_PHASES[_DUMMY_EAGER_GRAPH.key]
  
  
  def _current_graph(op_input_list):
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,Intellicode,2020-01-27T23:16:11Z,"
 		Yes, this seems to resolve the issue. I did have to modify the patch to reference 'Foo' correctly: self.key = DummyEagerGraph.Foo().
 		",1.0,391,387,set_learning_phase,value,374,392,1.0,340,,global_learning_phase_is_set,,339,340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,Intellicode,2020-01-27T23:40:25Z,"
 		Great. I will make the patch and send it out internally (a little easier). I should make it to github tomorrow night or a bit later.
 		",10.0,Intellicode,2020-01-27T23:43:31Z,"
 		That's amazing, thanks for the quick response!
 		",11.0,Intellicode,2020-01-28T19:10:52Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35980>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/35980>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36067,byronyi,2020-01-20T14:38:28Z,2020-01-20T18:48:51Z,saved_model_cli breaks nightly packages,"
 Our in-house nightly builds were broken since 2020-01-16 when auditwheel tries to repair my nightly packages. The reason under the hood seems to be an incorrect link from the recent change of adding XLA support to  in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/9959c04433623e0b7ebf6248e0f75bc7a24bd7cb>9959c04</denchmark-link>
 .
 Install the latest nightly, and navigate to the directory of tensorflow_core/compiler/aot:
 <denchmark-code>$ ldd _pywrap_tfcompile.so
 	linux-vdso.so.1 (0x00007ffc5e064000)
 	libtensorflow_framework.so.2 => /usr/local/lib/python3.7/dist-packages/tensorflow_core/compiler/aot/./../../libtensorflow_framework.so.2 (0x00007fa798bba000)
 	_pywrap_tensorflow_internal.so => not found
 	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fa798b77000)
 	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fa798b56000)
 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fa7989d1000)
 	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fa79884d000)
 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fa798833000)
 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa798672000)
 	/lib64/ld-linux-x86-64.so.2 (0x00007fa79afbc000)
 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fa798668000)
 </denchmark-code>
 
 Obviously it links to _pywrap_tensorflow_internal.so but it is not found with the relative path.
 PS: we are using auditwheel==3.0.0 to produce manylinux2014 builds, but the official tf-nightly uses an older version which fails to catch this.
 PPS: directly using saved_model_cli does not give this error as _pywrap_tensorflow_internal.so seems to be preloaded. But I am pretty sure this is a bug that we need to fix.
 Ping <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  <denchmark-link:https://github.com/mihaimaruseac>@mihaimaruseac</denchmark-link>
 .
 	",1.0,byronyi,2020-01-20T14:45:09Z,"
 		The reason seems to be that tf_python_pybind_extension should only be used for packages in //tensorflow/python/... but it is used in //tensorflow/compiler/.. in this case.
 Ping <denchmark-link:https://github.com/av8ramit>@av8ramit</denchmark-link>
  to double check.
 See <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L2573-L2577>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorflow.bzl#L2573-L2577</denchmark-link>
 .
 		",2.0,byronyi,2020-01-20T15:02:04Z,"
 		I think we can move pywrap_tfcompile to python/
 		",3.0,byronyi,2020-01-20T18:01:43Z,"
 		I have a solution; gonna get it through internal reviews and then will ping you when it's available.
 		",0a57a64a022a180abf7a95584a9570e9f126c42e,Eugene Brevdo,2020-01-20 10:39:37-08:00,MODIFY,0,tensorflow\compiler\aot\BUILD,tensorflow\compiler\aot\BUILD,0.0,"98,99,100,101,102,103,104,105,106,107,108,109,110","3,4,5,6,7,8,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106",MODIFY,0.0,tensorflow\python\BUILD,tensorflow\python\BUILD,4.0,byronyi,2020-01-20T20:08:17Z,"
 		<denchmark-link:https://github.com/byronyi>@byronyi</denchmark-link>
  let me know if this doesn't work for you.
 		",,,,,,,,,0.0,"13,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,1143",1118,,,,,RENAME,0.0,tensorflow\compiler\aot\tfcompile_wrapper.cc,tensorflow\python\tfcompile_wrapper.cc,,,,MODIFY,0.0,tensorflow\python\tools\saved_model_cli.py,tensorflow\python\tools\saved_model_cli.py,0.0,74,74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36198,yourtheron,2020-01-25T06:10:44Z,2020-12-03T18:55:44Z,model.summary() Does not Work in Some Cases,"
 Tf Version 2.1
 Having been suggested by <denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
  I am filing this bug. Please see the last comment in issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/35441>#35441</denchmark-link>
  for details.
 /CC <denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
 
 	",1.0,yourtheron,2020-01-27T09:01:37Z,"
 		<denchmark-link:https://github.com/yourtheron>@yourtheron</denchmark-link>
  ,
 Thank you for raising bug, can you please provide us an example to reproduce the issue?
 		",2.0,yourtheron,2020-01-28T02:27:51Z,"
 		<denchmark-link:https://github.com/oanush>@oanush</denchmark-link>
  . As I mentioned above, please refer to the last comment in issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/35441>#35441</denchmark-link>
  and talk to <denchmark-link:https://github.com/reedwm>@reedwm</denchmark-link>
  , who actually found out the bug and suggested me about raising it in a new issue. How the bug was found out was too long and unnecessary to be repeated.
 		",3.0,yourtheron,2020-01-28T03:39:54Z,"
 		Ah I see the issue. You never used either of your Concatenate layers. Here is a smaller example to reproduce:
 import tensorflow as tf
 
 class MyModel(tf.keras.Model):
   def __init__(self):
     super(MyModel, self).__init__()
     self.dense = tf.keras.layers.Dense(16)
     self.concat = tf.keras.layers.Concatenate(axis=1)
 
   def call(self, inputs):
     return self.dense(inputs)
 
 model = MyModel()
 model.build((16, 16))
 model.summary()
 The error is
 <denchmark-code>ValueError: You tried to call `count_params` on concatenate, but the layer isn't built. You can build it manually via: `concatenate.build(batch_input_shape)`.
 </denchmark-code>
 
 This isn't a bug, but perhaps we could improve the error message?
 		",bb2e09ad7207c504296962192fa5f1b7ec53a659,Reed Wanderman-Milne,2020-12-03 10:54:34-08:00,MODIFY,1,tensorflow\python\keras\tests\model_subclassing_test.py,tensorflow\python\keras\tests\model_subclassing_test.py,1.0,"345,353,354,355,356,357,358,359,360,361,362,363","345,353",MODIFY,2.0,tensorflow\python\keras\utils\layer_utils.py,tensorflow\python\keras\utils\layer_utils.py,4.0,yourtheron,2020-02-22T00:28:54Z,"
 		<denchmark-link:https://github.com/tensorflowbutler>@tensorflowbutler</denchmark-link>
  Sorry for the delay due to the hotch-potch in China, I am also confused, is it not a bug related to model.summary() in tf2.1?
 		",5.0,yourtheron,2020-12-03T18:55:45Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36198>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36198>No</denchmark-link>
 
 		",,,,,1.0,"210,211,212,213,214,215,216",210,print_summary.print_layer_summary,layer,196,217,,,,,,,,,,,,,,,test_summary,self,329,363,,,,,,,,,,,,,,,,,,,,,,1.0,"210,211,212,213,214,215,216",210,print_summary,"model,line_length,positions,print_fn",111,276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3624,sbrodeur,2016-08-03T15:32:28Z,2016-10-25T18:04:29Z,Basic Element-wise Complex Number Calculations Not Available On GPU,"
 Basic element-wise addition, subtraction, multiplication or division for any Tensor of type tf.complex64 is not implemented on GPU.
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: Centos 7,  3.10.0-327.22.2.el7.x86_64
 Installed version of CUDA and cuDNN:  CUDA 7.5 and cuDNN 7.0-v4
 -rw-r--r--. 1 root root 189170 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudadevrt.a
 lrwxrwxrwx. 1 root root     16 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so -> libcudart.so.7.5
 lrwxrwxrwx. 1 root root     19 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5 -> libcudart.so.7.5.18
 -rwxr-xr-x. 1 root root 311596 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart.so.7.5.18
 -rw-r--r--. 1 root root 558020 Jul 22 16:14 /usr/local/cuda-7.5/lib/libcudart_static.a
 Tensorflow installed from source:
 
 Commit hash 00700f0
 Bazel information:
 Build label: 0.3.0-2016-07-22 (@ca36b06)
 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Fri Jul 22 19:23:10 2016 (1469215390)
 Build timestamp: 1469215390
 Build timestamp as int: 1469215390
 
 <denchmark-h:h3>Steps to reproduce</denchmark-h>
 
 
 Add, subtract, multiply or divide any Tensor of type tf.complex64. A code example is shown here for element-wise addition:
 
 <denchmark-code>import tensorflow as tf
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = a + b
 
         with tf.Session() as sess:
             c = sess.run(c)
 </denchmark-code>
 
 The code returns the following output if run on GPU (works well on CPU):
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so.7.5 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so.4.0.7 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so.7.5 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally
 I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so.7.5 locally
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:
 name: Tesla K40c
 major: 3 minor: 5 memoryClockRate (GHz) 0.745
 pciBusID 0000:02:00.0
 Total memory: 12.00GiB
 Free memory: 11.90GiB
 W tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x5168890
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties:
 name: GeForce GT 610
 major: 2 minor: 1 memoryClockRate (GHz) 1.62
 pciBusID 0000:01:00.0
 Total memory: 1023.19MiB
 Free memory: 396.98MiB
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N
 I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K40c, pci bus id: 0000:02:00.0)
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:814] Ignoring gpu device (device: 1, name: GeForce GT 610, pci bus id: 0000:01:00.0) with Cuda compute capability 2.1. The minimum required Cuda capability is 3.5.
 E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: add = Add[T=DT_COMPLEX64, _device=""/device:GPU:0""](Complex, Complex_1)]]
 Traceback (most recent call last):
 File ""test_div_gpu_prob.py"", line 12, in 
 c = sess.run(c)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 382, in run
 run_metadata_ptr)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 655, in _run
 feed_dict_string, options, run_metadata)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 723, in _do_run
 target_list, options, run_metadata)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 743, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'add': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: add = Add[T=DT_COMPLEX64, _device=""/device:GPU:0""](Complex, Complex_1)]]
 Caused by op u'add', defined at:
 File ""test_div_gpu_prob.py"", line 9, in 
 c = a + b
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py"", line 755, in binary_op_wrapper
 return func(x, y, name=name)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 70, in add
 result = _op_def_lib.apply_op(""Add"", x=x, y=y, name=name)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
 op_def=op_def)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1232, in init
 self._traceback = _extract_stack()
 <denchmark-h:h3>What have you tried?</denchmark-h>
 
 
 Implementation using builtin Tensorflow functions works, if the real and imaginary parts are separated. See the code below:
 
 <denchmark-code>import numpy as np
 import tensorflow as tf
 
 def complex_add(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr + yr, xi + yi)
 
 def complex_sub(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr - yr, xi - yi)
 
 def complex_mul(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)
 
 def complex_div(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     d = tf.square(yr) + tf.square(yi)
     return tf.complex((xr*yr+xi*yi)/d, (xi*yr-xr*yi)/d)
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
 
         with tf.Session() as sess:
 
             a_, b_, c = sess.run([a,b,complex_add(a,b)])
             assert np.allclose(c, a_ + b_)
 
             a_, b_, c = sess.run([a,b,complex_sub(a,b)])
             assert np.allclose(c, a_ - b_)
 
             a_, b_, c = sess.run([a,b,complex_mul(a,b)])
             assert np.allclose(c, a_ * b_)
 
             a_, b_, c = sess.run([a,b,complex_div(a,b)])
             assert np.allclose(c, a_ / b_)
 </denchmark-code>
 
 It would be nice to have such functions transparent with the built-in CPU implementations.
 	",1.0,sbrodeur,2016-08-03T18:17:08Z,"
 		Note: implementations using built-in Tensorflow functions as show above doesn't solve gradient issues caused by the handling of complex numbers:
 <denchmark-code>import tensorflow as tf
 
 def complex_mul(x, y):
     xr, xi = tf.real(x), tf.imag(x)
     yr, yi = tf.real(y), tf.imag(y)
     return tf.complex(xr*yr - xi*yi, xr*yi + xi*yr)
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = complex_mul(a, b)
 
         grad = tf.gradients([c], [a])
 
         with tf.Session() as sess:
             grad = sess.run(grad)
 </denchmark-code>
 
 This code will fail with the following error:
 E tensorflow/core/client/tensor_c_api.cc:485] Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: gradients/Shape = Shape<denchmark-link:Complex_2>T=DT_COMPLEX64, _device=""/device:GPU:0""</denchmark-link>
 ]]
 Traceback (most recent call last):
 File ""test_div_gpu_grad_prob.py"", line 19, in 
 grad = sess.run(grad)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 382, in run
 run_metadata_ptr)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 655, in _run
 feed_dict_string, options, run_metadata)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 723, in _do_run
 target_list, options, run_metadata)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 743, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'gradients/Shape': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.
 [[Node: gradients/Shape = Shape<denchmark-link:Complex_2>T=DT_COMPLEX64, _device=""/device:GPU:0""</denchmark-link>
 ]]
 Caused by op u'gradients/Shape', defined at:
 File ""test_div_gpu_grad_prob.py"", line 16, in 
 grad = tf.gradients([c], [a])
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 367, in gradients
 grad_ys = _DefaultGradYs(grad_ys, ys, colocate_gradients_with_ops)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py"", line 230, in _DefaultGradYs
 array_ops.shape(y),
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py"", line 131, in shape
 return gen_array_ops.shape(input, name=name)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 1922, in shape
 result = _op_def_lib.apply_op(""Shape"", input=input, name=name)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py"", line 703, in apply_op
 op_def=op_def)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 2310, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File ""/usr/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 1232, in 
 self._traceback = _extract_stack()
 		",2.0,sbrodeur,2016-08-03T22:53:42Z,"
 		It seems that support for complex64 types is piecemeal, by op-type and device-type.  Bringing in <denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  for a comment on the policy.
 		",3.0,sbrodeur,2016-08-03T22:57:51Z,"
 		Yes, this is a good feature request bordering on a bug. Please check the Op registrations of the affected ops, and you'll probably find that the templates of many of them are not specialized for complex data types. It is a relatively simple thing to fix, and I'd love PRs that do it.
 		",53af29cb8503c7ed55a23d22090dd39ce0056a7a,Olivia Nordquist,2016-09-14 16:31:25-07:00,MODIFY,0,tensorflow\core\kernels\cwise_op_add.cc,tensorflow\core\kernels\cwise_op_add.cc,0.0,"22,23","22,23",MODIFY,0.0,tensorflow\core\kernels\cwise_op_gpu_add.cu.cc,tensorflow\core\kernels\cwise_op_gpu_add.cu.cc,4.0,sbrodeur,2016-08-08T12:56:47Z,"
 		<denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>
 : Are you currently working on this?
 If not, I could go ahead and attempt a fix.
 		",5.0,sbrodeur,2016-08-08T14:02:21Z,"
 		<denchmark-link:https://github.com/ibab>@ibab</denchmark-link>
 : I did not yet attempt a fix. I've looked a at little at Eigen:
 """"
 [https://eigen.tuxfamily.org/dox-devel/TopicCustomizingEigen.html]
 Thus, for the simple calculations here, should I expect Eigen to provide compatible functors, e.g. :
 <denchmark-code>template <typename T>
 struct add : base<T, Eigen::internal::scalar_sum_op<T> > {
   static const bool use_bcast_optimization = true;
 };
 </denchmark-code>
 
 This code is in file cwise_ops.h
 Does this means the fix is similar to <denchmark-link:https://github.com/tensorflow/tensorflow/pull/2263>#2263</denchmark-link>
 , i.e. just adding the complex64 type when we register the kernels?
 		",6.0,sbrodeur,2016-08-08T14:14:47Z,"
 		Yes, you won't have to implement the operations themselves, you just need to enable them.
 For example, you can look at the supported types for the addition op here:
 
 
 
 tensorflow/tensorflow/core/kernels/cwise_op_add.cc
 
 
          Line 22
       in
       32bd3d0
 
 
 
 
 
 
  REGISTER4(BinaryOp, GPU, ""Add"", functor::add, float, Eigen::half, double, 
 
 
 
 
 
 You would need to add complex64 and complex128 to the macro (and change it into REGISTER6).
 You should make sure that the GPU tests are enabled for complex64 and complex128 for each op that has been extended, for example here: 
 
 
 tensorflow/tensorflow/python/kernel_tests/cwise_ops_test.py
 
 
          Line 362
       in
       32bd3d0
 
 
 
 
 
 
  def testComplex64Basic(self): 
 
 
 
 
 .
 		",0.0,22,22,,,,,MODIFY,2.0,tensorflow\python\kernel_tests\cwise_ops_test.py,tensorflow\python\kernel_tests\cwise_ops_test.py,1.0,743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,sbrodeur,2016-08-08T14:27:15Z,"
 		Thanks for the information <denchmark-link:https://github.com/ibab>@ibab</denchmark-link>
 ! I will attempt a fix myself and send a PR soon!
 		",testComplex128Basic,self,730,743,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,sbrodeur,2016-08-12T16:43:15Z,"
 		So far, I can make it work with some operations (add, sub) by simply adding the complex data types when registering the kernels: e.g. 
 
 
 tensorflow/tensorflow/core/kernels/cwise_op_add.cc
 
 
          Line 22
       in
       32bd3d0
 
 
 
 
 
 
  REGISTER4(BinaryOp, GPU, ""Add"", functor::add, float, Eigen::half, double, 
 
 
 
 
 
 <denchmark-code>DEFINE_BINARY6(add, Eigen::half, float, double, int64, complex64, complex128);
 </denchmark-code>
 
 Compilation errors however occur for multiplication (and division), as seen below.
 Searching the web, I found here that CUDA may not support std::complex because of STL incompatibilities:
 <denchmark-link:https://forum.kde.org/viewtopic.php?f=74&t=123919>https://forum.kde.org/viewtopic.php?f=74&t=123919</denchmark-link>
 
 It seems to solve this problem, people have been using reimplementations of the std:complex type (e.g. from thrust, cuda_complex or cusp) so that it can be used in device code:
 <denchmark-link:https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h>https://github.com/thrust/thrust/blob/2ef13096187b40a35a71451d09e49b14074b0859/thrust/complex.h</denchmark-link>
 
 <denchmark-link:https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp>https://github.com/jtravs/cuda_complex/blob/master/cuda_complex.hpp</denchmark-link>
 
 <denchmark-link:https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h>https://github.com/cusplibrary/cusplibrary/blob/master/cusp/complex.h</denchmark-link>
 
 Would the Eigen library implementing something similar to what thrust uses solve the issue in Tensorflow?
 <denchmark-h:h3>Compilation output</denchmark-h>
 
 INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_mul.cu.cc:
 nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>
 
 <denchmark-code>^
 </denchmark-code>
 
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>
 
 <denchmark-code>^
 </denchmark-code>
 
 In file included from /usr/local/cuda-7.5/include/host_config.h:161:0,
 from /usr/local/cuda-7.5/include/cuda_runtime.h:76,
 from :0:
 /usr/include/features.h:330:4: warning: #warning _FORTIFY_SOURCE requires compiling with optimization (-O) [-Wcpp]
 <denchmark-h:h1>warning _FORTIFY_SOURCE requires compiling with optimization (-O)</denchmark-h>
 
 <denchmark-code>^
 </denchmark-code>
 
 nvcc warning : option '--relaxed-constexpr' has been deprecated and replaced by option '--expt-relaxed-constexpr'.
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 12 errors detected in the compilation of ""/tmp/tmpxft_0000430f_00000000-12_cwise_op_gpu_mul.cu.compute_35.cpp2.i"".
 		",,,,,,,,,,,,,,,1.0,728,,testComplex64Basic,self,715,728,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,sbrodeur,2016-08-13T02:37:33Z,"
 		Strange, your errors seem to be caused by the fact that Eigen is trying to assign values from an int Tensor into a complex Tensor.
 I don't think that's supposed to happen usually.
 I've tried enabling complex mul and div just now, and it successfully compiled and ran when I restricted the ops to run on the GPU.
 I'm also using CUDA 7.5, not sure what else could be different between our setups.
 		",10.0,sbrodeur,2016-08-13T15:40:17Z,"
 		Here is my configuration:
 GPU: Tesla K40c
 Operating System: CentOS Linux 7 (Core)
 Kernel: Linux 3.10.0-327.22.2.el7.x86_64
 Architecture: x86-64
 C Compiler: gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)
 CUDA Compiler: Cuda compilation tools, release 7.5, V7.5.17
 I will try with a more recent gcc (e.g. 4.9.2) to see if the compilation problem disappears.
 		",11.0,sbrodeur,2016-08-13T16:30:20Z,"
 		I'm also using  and.
 Not sure what could be causing this if these two are the same.
 I've uploaded my changes to <denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae08bd449d25f80e9af8ae4830eb7ae2670>ibab@8c3baae</denchmark-link>
 , so you might want to compare these with yours.
 If that doesn't help we should try rebasing to the same TensorFlow commit.
 		",12.0,sbrodeur,2016-08-13T18:30:58Z,"
 		Sadly, I obtain the same errors if I clone and compile the fork <denchmark-link:https://github.com/ibab/tensorflow/commit/8c3baae>ibab/tensorflow@8c3baae</denchmark-link>
  without any modifications.
 <denchmark-link:https://github.com/ibab>@ibab</denchmark-link>
  - What is your Linux distribution?
 		",13.0,sbrodeur,2016-08-13T18:42:39Z,"
 		I'm running Scientific Linux 6, which should be virtually identical to Red Hat 6.
 I get my compiler toolchain from anaconda, though.
 I'll try to make sure it's not something weird on my end.
 		",14.0,sbrodeur,2016-08-13T18:54:49Z,"
 		On my side, I'll try a build on my laptop which runs the latest Debian 8 (Jessie). I don't have a Nvidia GPU but I should nevertheless be able to compile with CUDA.
 		",15.0,sbrodeur,2016-08-13T19:11:38Z,"
 		Okay, I've rebuilt tensorflow after a bazel clean --expunge to make extra sure that I'm using the right Eigen version, as I've changed it around a few times previously, but it still built successfully.
 Edit: Btw, do you also get compiler warnings about calling __host__ functions from device code as in the link you posted above?
 		",16.0,sbrodeur,2016-08-14T14:50:00Z,"
 		I do not get compiler warnings about calling host functions from device code.
 I just tried to build on my laptop (Debian 8, up-to-date) with configuration;
 gcc (Debian 4.9.2-10) 4.9.2
 Cuda compilation tools, release 7.5, V7.5.17
 I obtained the same errors, so it does not seem related to gcc or distribution. I also tried to build with the latest eigen (3782cd1de9c4) on the Centos 7 machine, and that did not help either. I will try building with CUDA 8, after which I will be clueless about those compilation issues.
 Edit:  same errors with CUDA 8.
 		",17.0,sbrodeur,2016-08-14T16:10:20Z,"
 		I've tried compiling with different compute capabilities, but it still compiled without errors.
 The fact that you reproduced it on two different systems makes me think that it's a problem with my setup, though.
 Unfortunately nvcc doesn't give us a lot of information in the error message (like which template instantiations we are dealing with) :(
 		",18.0,sbrodeur,2016-08-23T22:17:49Z,"
 		<denchmark-link:https://github.com/ibab>@ibab</denchmark-link>
  , <denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>
  , thank you so much for working on this. I think it would really speed up one of my projects. Is there any new progress? Are you planning to include this on the next release of TensorFlow? What about basic math functions such as , ?
 Thanks again!
 		",19.0,sbrodeur,2016-08-24T14:31:48Z,"
 		<denchmark-link:https://github.com/iportillo>@iportillo</denchmark-link>
  - I will give it another try today. It would also significantly accelerate my experiments, since everything could run on the GPU. I'll try to see if it would be easy to use CUDABlas directly (rather than Eigen) for the basic math functions on complex numbers.
 tf.complex_abs is easy to implement on GPU right now:
 <denchmark-code>def complex_abs(x):
     return tf.sqrt(tf.square(tf.real(x)) + tf.square(tf.imag(x)))
 </denchmark-code>
 
 By tf.exp(), do you mean converting from the Cartesian to the complex exponential form (angle and norm)? To calculate the angle, this means implementing the atan2 function (for complex x + iy):
 <denchmark-code>def atan2(y, x):
     angle = tf.select(tf.greater(x,0.0), tf.atan(y/x) + np.pi, tf.zeros_like(x))
     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.greater_equal(y,0.0)), tf.atan(y/x) + np.pi, angle)
     angle = tf.select(tf.logical_and(tf.less(x,0.0),  tf.less(y,0.0)), tf.atan(y/x) - np.pi, angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.greater(y,0.0)), 0.5*np.pi * tf.ones_like(x), angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.less(y,0.0)), -0.5*np.pi * tf.ones_like(x), angle)
     angle = tf.select(tf.logical_and(tf.equal(x,0.0), tf.equal(y,0.0)), np.nan * tf.zeros_like(x), angle)
     return angle
 
 def complex_arg(x):
     return atan2(tf.imag(x), tf.real(x))
 </denchmark-code>
 
 It's not optimized but works well on GPU.
 		",20.0,sbrodeur,2016-08-24T15:12:44Z,"
 		<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>
 : We're having some problems with implementing the product and div ops for  using the Eigen Tensor library.
 Do you see why we would get an error like the following when enabling them in TensorFlow?
 <denchmark-code>external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorAssign.h(136): error: a value of type ""int"" cannot be assigned to an entity of type ""_ZNSt7complexIfE9_ComplexTE""
 </denchmark-code>
 
 <denchmark-code>$ c++filt _ZNSt7complexIfE9_ComplexTE
 std::complex<float>::_ComplexT
 </denchmark-code>
 
 Maybe we would need to switch to something like  as <denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>
  suggested?
 		",21.0,sbrodeur,2016-08-24T21:41:53Z,"
 		I made some progress! I can make multiplication and division ops work for complex numbers if I specialized the templates in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cwise_ops.h#L432</denchmark-link>
 
 <denchmark-code>template <typename T>
 struct mul : base<T, Eigen::internal::scalar_product_op<T> > {};
 
 template <typename T>
 struct multiply_complex {
   typedef std::complex<T> result_type;
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,
                                                                std::complex<T> b) const {
     return std::complex<T>(a.real()*b.real() - a.imag()*b.imag(),
                            a.real()*b.imag() + a.imag()*b.real());
   }
 };
 
 template <>
 struct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};
 
 template <>
 struct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};
 
 </denchmark-code>
 
 It seems more like a hack, but it doesn't involve changes in Eigen for now.
 Not sure what is wrong with nvcc using scalar_product_op in Eigen for complex numbers:
 <denchmark-link:https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76>https://github.com/RLovelett/eigen/blob/master/Eigen/src/Core/functors/BinaryFunctors.h#L76</denchmark-link>
 
 However, it seems tightly related to using built-in * and / operators for std:complex types.
 For instance, this fails with the same errors as in the previous posts:
 <denchmark-code>template <typename T>
 struct mul : base<T, Eigen::internal::scalar_product_op<T> > {};
 
 template <typename T>
 struct multiply_complex {
   typedef std::complex<T> result_type;
   EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE result_type operator()(std::complex<T> a,
                                                                std::complex<T> b) const {
     return a*b;
   }
 };
 
 template <>
 struct mul<std::complex<float> > : base<std::complex<float>, multiply_complex<float> > {};
 
 template <>
 struct mul<std::complex<double> > : base<std::complex<double>, multiply_complex<double> > {};
 
 </denchmark-code>
 
 		",22.0,sbrodeur,2016-08-24T23:26:17Z,"
 		I can confirm that with the above trick, I can make work a lot of very useful functions for complex numbers on GPU (e.g. square, neg, div, mul, abs) This brings support for complex gradient computation on GPU:
 <denchmark-code>import tensorflow as tf
 
 if __name__ == '__main__':
 
     with tf.device('/gpu:0'):
         N = 100
         a = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         b = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
         c = tf.complex(tf.random_normal((N,)), tf.random_normal((N,)))
 
         d = c * tf.neg(tf.square(a + b))
 
         grad = tf.gradients([d], [a])
 
         with tf.Session() as sess:
             grad = sess.run(grad)
 </denchmark-code>
 
 Should I make a PR or should we investigate further the handling of std::complex by nvcc?
 		",23.0,sbrodeur,2016-08-29T15:23:06Z,"
 		I'm not a maintainer, but I think a PR would definitely be a good idea 👍
 Maybe you can split it into two PRs, one that requires the extra scalar_prod_op specialization, and one that doesn't?
 In the long term, it would probably be best to get the specialization into Eigen itself, or to find another fix (like switching to thrust::complex).
 		",24.0,sbrodeur,2016-09-13T17:39:21Z,"
 		I've been privately writing GPU-based complex-valued ops for TF and decided to make my repository public. I think that more general support for computation of complex numbers on the GPU will be valuable to the community. However since my repository is in the early stages and isn't well tested, I think I'd like to develop it as a separate project and then port it as a TF pull request when it's more mature.   Feel free to make contributions and/or suggestions.
 <denchmark-link:https://github.com/woodshop/complex_tf>https://github.com/woodshop/complex_tf</denchmark-link>
 
 		",25.0,sbrodeur,2016-09-14T20:16:29Z,"
 		In C++14,  std::complex methods are marked as constexpr. This will ensure that they can be used inside cuda kernels even though they're not marked as __device__ functions provided that we compile with the --relaxed-constexpr flag (which TensorFlow has been doing for some time now).
 Unfortunately nvcc doesn't yet support c++14, but we can ask nvidia to start adding partial support for it starting with complex numbers.
 		",26.0,sbrodeur,2016-09-15T19:26:45Z,"
 		<denchmark-link:https://github.com/iportillo>@iportillo</denchmark-link>
  ComplexAbs (and a few others) added here: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f21642013092b53186491064335053a9e02ce010>f216420</denchmark-link>
 
 and the corresponding Eigen change:
 <denchmark-link:https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed>https://bitbucket.org/eigen/eigen/commits/6d4cd6e5cdd9c750b10cc4c6a374e4c513b267ed</denchmark-link>
 
 		",27.0,sbrodeur,2016-09-28T00:58:56Z,"
 		After adding a workaround to Eigen:
 <denchmark-link:https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344>https://bitbucket.org/eigen/eigen/commits/27f6140fa81c9fe83167d87e7aeb23031b42f344</denchmark-link>
 
 We were able to enable addition, subtraction, division, and multiplication kernels for complex types on GPU: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/93f15d4cde2f08057819f1194e5a4771f0d391ff>93f15d4</denchmark-link>
 
 		",28.0,sbrodeur,2016-10-11T15:48:20Z,"
 		<denchmark-link:https://github.com/sbrodeur>@sbrodeur</denchmark-link>
  Does TensorFlow now support all the operations you need on complex, or are there additional improvements we need to make ?
 		",29.0,sbrodeur,2016-10-25T16:54:08Z,"
 		<denchmark-link:https://github.com/benoitsteiner>@benoitsteiner</denchmark-link>
  Tensorflow now supports everything I need for handling complex numbers.
 		",30.0,sbrodeur,2016-10-25T18:04:29Z,"
 		Thanks, closing the issue.
 		",31.0,sbrodeur,2017-07-14T13:38:14Z,"
 		Is it possible to calculate a complex number divide a float number without type cast?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36268,terhorst,2020-01-28T02:49:22Z,2020-09-30T20:02:14Z,tf.debugging.assert_shapes() does not work for SparseTensor,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.15
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.1
 Python version: 3.8
 
 Describe the current behavior
 tf.debugging.assert_shapes cannot be used with sparse tensors.
 Describe the expected behavior
 tf.debugging.assert_shapes should allow you to mix and match dense and sparse tensors when checking for dimensional consistency.
 Code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 A = tf.range(3)
 tf.debugging.assert_shapes(((A, [3]),))  # works
 # raises ""ValueError: Attempt to convert a value (...) with an unsupported type (<class 'tensorflow.python.framework.sparse_tensor.SparseTensor'>) to a Tensor.
 tf.debugging.assert_shapes(((tf.sparse.from_dense(A), [3]),))
 </denchmark-code>
 
 	",1.0,terhorst,2020-01-28T08:20:23Z,"
 		<denchmark-link:https://github.com/terhorst>@terhorst</denchmark-link>
   I believe the input type of sparse_tensor is not the typical use-case for tf.debugging.assert_shapes. Neverthless it shouldn't raise the error.
 I have made a PR to fix it: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/36277>#36277</denchmark-link>
 
 Please have a check if needed.
 		",2.0,terhorst,2020-01-28T14:47:04Z,"
 		This fixes it, but converting the sparse tensor to dense solely for the purpose of checking its shape seems quite wasteful to me.
 		",3.0,terhorst,2020-01-29T01:25:39Z,"
 		Hi <denchmark-link:https://github.com/terhorst>@terhorst</denchmark-link>
 , I think this function is used for debugging only, right? If so, the efficiency shouldn't be the major problem. And I think this fix changes the least number of code which has lower possibility to induce other issues.
 		",11fc1489d01822a2e728103c3af998976b4e7cd1,Shanqing Cai,2020-09-30 12:53:54-07:00,MODIFY,0,RELEASE.md,RELEASE.md,0.0,"292,293,294,295,1199,1209,1365,1370,1378,1384,2630,3109","1195,1205,1361,1366,1374,1380,2626,3105",MODIFY,13.0,tensorflow\python\kernel_tests\check_ops_test.py,tensorflow\python\kernel_tests\check_ops_test.py,4.0,terhorst,2020-01-29T01:56:19Z,"
 		In my case, the whole reason for using a SparseTensor in the first place is
 that it cannot fit into memory as dense. With this patch, assert_shapes()
 will produce an OOM or some other form of crash, which seems like a
 regression since that is strictly more annoying and harder to debug than
 mismatched dimensions. But, it's your call; my only point in submitting
 this issue was to bring it to your attention, and I don't have the
 bandwidth to learn how to patch it myself.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Tue, Jan 28, 2020 at 8:25 PM Leslie-Fang ***@***.***> wrote:
  Hi @terhorst <https://github.com/terhorst>, I think this function is used
  for debugging only, right? If so, the efficiency shouldn't be the major
  problem. And I think this fix changes the least number of code which has
  lower possibility to induce other issues.
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#36268?email_source=notifications&email_token=AAAEOHAT2RLP3LNW6REXZF3RADLJRA5CNFSM4KMLQMXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEKFT6RA#issuecomment-579551044>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/AAAEOHEWTU3R6XWWVPRF2E3RADLJRANCNFSM4KMLQMXA>
  .
 
 
 -- 
 Jonathan
 terhorst@gmail.com
 
 		",5.0,terhorst,2020-01-29T09:07:09Z,"
 		I have tried on colab with TF version 2.1.0-rc2, 2.2.0-dev20200128 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/76389a872dc7528d8ba29760a345c37b/untitled596.ipynb>here</denchmark-link>
 .Thanks!
 		",6.0,terhorst,2020-07-21T11:54:51Z,"
 		I am able to reproduce the isue with TF 2.3-rc1, nightly version().PLease, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/4315d3365fcc9c7170084261951e80eb/untitled162.ipynb>here</denchmark-link>
 .Thanks!
 		",1.0,"2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057",,test_assert_shapes_sparse_tensor_mixed_dense_and_sparse_success,self,2047,2057,MODIFY,4.0,tensorflow\python\ops\check_ops.py,tensorflow\python\ops\check_ops.py,1.0,1799,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044",,test_assert_shapes_sparse_tensor_multiple_assertions_fail,self,2030,2044,1.0,"1966,1967,1968,1969,1970,1971,1972,1973,1974",,test_assert_shapes_sparse_tensor_symbolic_match_success,self,1966,1974,1.0,"2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071",,test_assert_shapes_sparse_tensor_mixed_dense_and_sparse_fail,self,2060,2071,1.0,"1955,1956,1957,1958,1959,1960,1961,1962,1963",,test_assert_shapes_sparse_tensor_partially_specified_target_success,self,1955,1963,1.0,"1977,1978,1979,1980,1981,1982,1983,1984,1985,1986",,test_assert_shapes_sparse_tensor_partially_specified_target_fail,self,1977,1986,7.0,terhorst,2020-09-30T20:02:16Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36268>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36268>No</denchmark-link>
 
 		",assert_shapes.tensor_name,x,1798,1801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1587,1585,_dimension_sizes,x,1569,1597,1.0,"1792,1793,1794,1799","1790,1791,1792,1797",assert_shapes,"shapes,data,summarize,message,name",1696,1947,1.0,"1154,1155,1162","1154,1161",assert_rank,"x,rank,data,summarize,message,name",1127,1186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1943,1944,1945,1946,1947,1948,1949,1950,1951,1952",,test_assert_shapes_sparse_tensor_fully_specified_target_fail,self,1943,1952,1.0,"1909,1910,1911,1912,1913,1914,1915,1916,1917",,test_assert_shapes_sparse_tensor_scalar_target_success,self,1909,1917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999",,test_assert_shapes_sparse_tensor_wrong_rank_fail,self,1989,1999,1.0,"2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027",,test_assert_shapes_sparse_tensor_multiple_assertions_success,self,2014,2027,1.0,"2002,2003,2004,2005,2006,2007,2008,2009,2010,2011",,test_assert_shapes_sparse_tensor_wrong_symbolic_match_fail,self,2002,2011,1.0,"1932,1933,1934,1935,1936,1937,1938,1939,1940",,test_assert_shapes_sparse_tensor_fully_specified_target_success,self,1932,1940,1.0,"1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929",,test_assert_shapes_sparse_tensor_nonscalar_target_fail,self,1919,1929,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36394,prashastk,2020-02-01T01:57:51Z,2020-05-21T22:10:24Z,file_io.get_matching_files indefinitely hangs,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GCP cloud shell
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): Comes pre-installed in  GCP cloud shell
 TensorFlow version (use command below): TF 2.1
 Python version: 3.7.3
 Bazel version (if compiling from source): N/A
 GCC/Compiler version (if compiling from source): N/A
 CUDA/cuDNN version: N/A
 GPU model and memory: N/A
 
 Describe the current behavior
 file_io.get_matching_files indefinitely hangs
 Describe the expected behavior
 file_io.get_matching_files should not hang!
 Code to reproduce the issue
 Note the // in the first command
 <denchmark-code>gsutil cp README-cloudshell.txt gs://<some_bucket>/test//bug.txt
 gsutil cp README-cloudshell.txt gs://<some_bucket>/test/bug.txt
 </denchmark-code>
 
 This creates a weird / folder under the test folder
 Now open python
 <denchmark-code>from tensorflow.python.lib.io import file_io
 file_io.get_matching_files('gs://<some_bucket>/test/bug.txt')
 </denchmark-code>
 
 This will hang.
 Delete the / folder and this would work fine.
 One of our training jobs hung because TF somehow created a / folder in model output directory!
 	",1.0,prashastk,2020-05-21T22:10:25Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36394>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36394>No</denchmark-link>
 
 		",2.0,prashastk,2020-05-21T22:11:41Z,"
 		Apologies for the long delay. This should be fixed in the next nightly and the next release.
 		",,,,,7bfbd3f7be0725ee9c220047fe85032cf126d92b,Mihai Maruseac,2020-05-21 15:08:45-07:00,MODIFY,2,tensorflow\core\platform\cloud\gcs_file_system.cc,tensorflow\core\platform\cloud\gcs_file_system.cc,1.0,"1357,1358,1359,1360,1363,1364,1365,1366,1367,1368,1369",1354,MODIFY,2.0,tensorflow\core\platform\cloud\gcs_file_system_test.cc,tensorflow\core\platform\cloud\gcs_file_system_test.cc,,,,,,,,,,,,,1.0,"1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995",,tensorflow::TEST,"GcsFileSystemTest,GetMatchingPaths_SlashInObjectName",1972,1995,,,,,,,,,,,,,,,tensorflow::GcsFileSystem::GetMatchingPaths,"pattern,results",1337,1379,1.0,"168,169,170,171",166,tensorflow::AddAllSubpaths,paths,163,177,,,,,,,,,,,,,,,1.0,"1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020",,tensorflow::TEST,"GcsFileSystemTest,GetMatchingPaths_SlashInObjectNameEscaped",1997,2020,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36462,gabrieldemarmiesse,2020-02-04T13:55:11Z,2020-02-28T02:20:40Z,Autograph is incompatible with typeguard,"
 Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.1.0
 Python version: 3.6.9
 Bazel version (if compiling from source): no
 GCC/Compiler version (if compiling from source): no
 CUDA/cuDNN version: no
 GPU model and memory: no gpu
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 import tensorflow as tf
 from typeguard import typechecked
 @tf.function(autograph=True)
 @typechecked
 def add(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:
     return a + b
 
 print(add(tf.ones(2), tf.zeros(2)))
 <denchmark-code>---------------------------------------------------------------------------
 NameError                                 Traceback (most recent call last)
 <ipython-input-5-5567d1a6d381> in <module>()
       4     return a + b
       5 
 ----> 6 print(add(tf.ones(2), tf.zeros(2)))
 
 8 frames
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
     566         xla_context.Exit()
     567     else:
 --> 568       result = self._call(*args, **kwds)
     569 
     570     if tracing_count == self._get_tracing_count():
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
     613       # This is the first call of __call__, so we have to initialize.
     614       initializers = []
 --> 615       self._initialize(args, kwds, add_initializers_to=initializers)
     616     finally:
     617       # At this point we know that the initialization is complete (or less
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
     495     self._concrete_stateful_fn = (
     496         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
 --> 497             *args, **kwds))
     498 
     499     def invalid_creator_scope(*unused_args, **unused_kwds):
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
    2387       args, kwargs = None, None
    2388     with self._lock:
 -> 2389       graph_function, _, _ = self._maybe_define_function(args, kwargs)
    2390     return graph_function
    2391 
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
    2701 
    2702       self._function_cache.missed.add(call_context_key)
 -> 2703       graph_function = self._create_graph_function(args, kwargs)
    2704       self._function_cache.primary[cache_key] = graph_function
    2705       return graph_function, args, kwargs
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
    2591             arg_names=arg_names,
    2592             override_flat_arg_shapes=override_flat_arg_shapes,
 -> 2593             capture_by_value=self._capture_by_value),
    2594         self._function_attributes,
    2595         # Tell the ConcreteFunction to clean up its graph once it goes out of
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
     976                                           converted_func)
     977 
 --> 978       func_outputs = python_func(*func_args, **func_kwargs)
     979 
     980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
     437         # __wrapped__ allows AutoGraph to swap in a converted function. We give
     438         # the function a weak reference to itself to avoid a reference cycle.
 --> 439         return weak_wrapped_fn().__wrapped__(*args, **kwds)
     440     weak_wrapped_fn = weakref.ref(wrapped_fn)
     441 
 
 /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
     966           except Exception as e:  # pylint:disable=broad-except
     967             if hasattr(e, ""ag_error_metadata""):
 --> 968               raise e.ag_error_metadata.to_exception(e)
     969             else:
     970               raise
 
 NameError: in converted code:
 
     /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:3 wrapper  *
         check_argument_types(memo)
     /usr/local/lib/python3.6/dist-packages/typeguard/__init__.py:663 check_argument_types  *
         for argname, expected_type in memo.type_hints.items():
     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:339 for_stmt
         return _py_for_stmt(iter_, extra_test, body, get_state, set_state, init_vars)
     /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:348 _py_for_stmt
         if extra_test is not None and not extra_test(*state):
     /tmp/tmpnaoua4_l.py:96 extra_test
         return ag__.not_(do_return)
 
     NameError: free variable 'do_return' referenced before assignment in enclosing scope
 </denchmark-code>
 
 Describe the expected behavior
 <denchmark-code>tf.Tensor([1. 1.], shape=(2,), dtype=float32)
 </denchmark-code>
 
 Code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate the problem.
 import tensorflow as tf
 from typeguard import typechecked
 
 @tf.function(autograph=True)
 @typechecked
 def add(a: tf.Tensor, b: tf.Tensor) -> tf.Tensor:
     return a + b
 
 print(add(tf.ones(2), tf.zeros(2)))
 See the colab notebook: <denchmark-link:https://colab.research.google.com/drive/1RTg-ysyIdKME4fbJQ1D5Dxs-11YfBV5e>https://colab.research.google.com/drive/1RTg-ysyIdKME4fbJQ1D5Dxs-11YfBV5e</denchmark-link>
 
 Other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,gabrieldemarmiesse,2020-02-24T08:27:35Z,"
 		<denchmark-link:https://github.com/gabrieldemarmiesse>@gabrieldemarmiesse</denchmark-link>
 
 Can you try running the code in latest -tf-nightly ( ). The issue seemed to be fixed, kindly find the <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/3f87aa19ac8da80ccb090f842407294a/untitled5.ipynb>gist of colab</denchmark-link>
  for the same.Thanks!
 		",2.0,gabrieldemarmiesse,2020-02-24T10:38:49Z,"
 		<denchmark-link:https://github.com/ravikyram>@ravikyram</denchmark-link>
 
 This issue does not seem to be fixed with the current nightly 2.2.0-dev20200218. I get the same error as before. I tested locally and in google colab (your notebook) with tf-nightly and the error is the same.
 See my minimal notebook with tensorflow uninstalled and tf-nightly installed instead:
 <denchmark-link:https://colab.research.google.com/drive/1qkIblkiFhd-jPmC6Ki4F7xemPorRbf_->https://colab.research.google.com/drive/1qkIblkiFhd-jPmC6Ki4F7xemPorRbf_-</denchmark-link>
 
 		",3.0,gabrieldemarmiesse,2020-02-28T02:20:42Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36462>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36462>No</denchmark-link>
 
 		",a962580295172539a3a6ae5b02836aac1cabf100,Dan Moldovan,2020-02-27 18:18:06-08:00,MODIFY,1,tensorflow\python\autograph\converters\break_statements.py,tensorflow\python\autograph\converters\break_statements.py,1.0,146,146,MODIFY,1.0,tensorflow\python\autograph\converters\control_flow.py,tensorflow\python\autograph\converters\control_flow.py,4.0,gabrieldemarmiesse,2020-02-28T03:55:35Z,"
 		I'm not sure if the PR fixed it, but we can check with the next nightly.
 		",5.0,gabrieldemarmiesse,2020-02-28T19:14:00Z,"
 		I checked the repro colab with the new tf-nightly, and it appears that the fix worked.
 		",6.0,gabrieldemarmiesse,2020-02-29T10:48:13Z,"
 		I confirm! thanks it's awesome!
 		",1.0,"465,466","465,466",visit_For,"self,node",446,525,MODIFY,1.0,tensorflow\python\autograph\converters\control_flow_deprecated_py2.py,tensorflow\python\autograph\converters\control_flow_deprecated_py2.py,1.0,"490,491","490,491",MODIFY,2.0,tensorflow\python\autograph\converters\return_statements.py,tensorflow\python\autograph\converters\return_statements.py,1.0,"296,306","296,306",visit_For,"self,node",107,149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,visit_For,"self,node",474,604,visit_For,"self,node",289,309,MODIFY,0.0,tensorflow\python\autograph\pyct\anno.py,tensorflow\python\autograph\pyct\anno.py,0.0,"62,63,64,65",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\autograph\pyct\cfg.py,tensorflow\python\autograph\pyct\cfg.py,1.0,"847,848,849,850,851",,visit_For,"self,node",837,865,,,,,,,,MODIFY,1.0,tensorflow\python\autograph\pyct\static_analysis\activity.py,tensorflow\python\autograph\pyct\static_analysis\activity.py,1.0,"542,543",,visit_For,"self,node",534,549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"359,360,361","363,368",visit_FunctionDef,"self,node",333,385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\autograph\pyct\templates.py,tensorflow\python\autograph\pyct\templates.py,1.0,"121,122","123,125",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,replacements",111,127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36624,durandg12,2020-02-10T14:48:15Z,2020-02-18T22:19:59Z,LSTM return_state=True fail with tf.keras.Sequencial model,"
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
 Python version: v3.6.7:6ec5cf24b7, Oct 20 2018, 03:02:14
 
 Describe the current behavior
 The call method of a tf.keras.Sequential object fails and throws an error when one layer is an instance of the tf.keras.layers.LSTM class constructed with return_state=True. Given the error message, I believe it is because the output of the call method of such LSTM layer is a list instead of a Tensor, and the call method of Sequential does not know what to do with a list.
 Describe the expected behavior
 I think that the call method of Sequential should know that the Tensor output of LSTM is the first element of the list when return_state=True.
 Code to reproduce the issue
 Setting :
 <denchmark-code>import tensorflow as tf
 import numpy as np
 
 print('Using Tensorflow version {} (git version {})'.format(tf.version.VERSION, tf.version.GIT_VERSION))
 
 batch_size = 3
 ts = 9
 input_dim = 2
 nump = np.arange(examples*batch_size*ts*input_dim, dtype=np.float32).reshape(batch_size, ts, input_dim)
 dataset = tf.data.Dataset.from_tensor_slices(nump).batch(batch_size)
 for x in dataset:
     print(x.shape)
 return_state = True
 </denchmark-code>
 
 Output:
 <denchmark-code>Using Tensorflow version 2.1.0 (git version v2.1.0-rc2-17-ge5bf8de410)
 (3, 9, 2)
 </denchmark-code>
 
 Error with Sequential:
 <denchmark-code>model_seq = tf.keras.Sequential([tf.keras.layers.LSTM(3, return_state=return_state)])
 for x in dataset:
     print(model_seq(x))
 </denchmark-code>
 
 Output:
 <denchmark-code>---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-57-5500870ab2fc> in <module>
       1 model_seq = tf.keras.Sequential([tf.keras.layers.LSTM(3, return_state=return_state)])
       2 for x in dataset:
 ----> 3     print(model_seq(x))
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     820           with base_layer_utils.autocast_context_manager(
     821               self._compute_dtype):
 --> 822             outputs = self.call(cast_inputs, *args, **kwargs)
     823           self._handle_activity_regularization(inputs, outputs)
     824           self._set_mask_metadata(inputs, outputs, input_masks)
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/sequential.py in call(self, inputs, training, mask)
     283       # `outputs` will be the inputs to the next layer.
     284       inputs = outputs
 --> 285       mask = outputs._keras_mask
     286 
     287     return outputs
 
 AttributeError: 'list' object has no attribute '_keras_mask'
 </denchmark-code>
 
 It works when constructing the model with the Functional API:
 <denchmark-code>def lstm_model(return_state, ts, input_dim):
     inp = tf.keras.Input(shape=(ts, input_dim))
     out = tf.keras.layers.LSTM(3, return_state=return_state)(inp)
     return tf.keras.Model(inputs=inp, outputs=out)
     
 model_func = lstm_model(return_state, ts, input_dim)
 
 for x in dataset:
     print(model_func(x))
 </denchmark-code>
 
 Output:
 <denchmark-code>[<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
 array([[-8.8475537e-01,  2.9517543e-03, -9.9753261e-01],
        [-9.7553629e-01,  9.5521700e-06, -9.9959475e-01],
        [-9.9497062e-01,  3.0903845e-08, -9.9979442e-01]], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
 array([[-8.8475537e-01,  2.9517543e-03, -9.9753261e-01],
        [-9.7553629e-01,  9.5521700e-06, -9.9959475e-01],
        [-9.9497062e-01,  3.0903845e-08, -9.9979442e-01]], dtype=float32)>, <tf.Tensor: shape=(3, 3), dtype=float32, numpy=
 array([[-7.6066346e+00,  2.9581292e-03, -3.3488092e+00],
        [-8.9999275e+00,  9.5521846e-06, -4.2520967e+00],
        [-9.0000000e+00,  3.0903848e-08, -4.5915442e+00]], dtype=float32)>]
 </denchmark-code>
 
 Related question
 In my Functional API example, lstm_modelfails if I use inp = tf.keras.Input(shape=(ts, None)) instead of providing the explicit input dimension. The error message I get is:
 <denchmark-code>---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
 <ipython-input-64-9b042ffca48d> in <module>
       4     return tf.keras.Model(inputs=inp, outputs=out)
       5 
 ----> 6 model_func = lstm_model(return_state, ts, input_dim)
       7 
       8 for x in dataset:
 
 <ipython-input-64-9b042ffca48d> in lstm_model(return_state, ts, input_dim)
       1 def lstm_model(return_state, ts, input_dim):
       2     inp = tf.keras.Input(shape=(ts, None))
 ----> 3     out = tf.keras.layers.LSTM(3, return_state=return_state)(inp)
       4     return tf.keras.Model(inputs=inp, outputs=out)
       5 
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in __call__(self, inputs, initial_state, constants, **kwargs)
     642 
     643     if initial_state is None and constants is None:
 --> 644       return super(RNN, self).__call__(inputs, **kwargs)
     645 
     646     # If any of `initial_state` or `constants` are specified and are Keras
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
     746           # Build layer if applicable (if the `build` method has been
     747           # overridden).
 --> 748           self._maybe_build(inputs)
     749           cast_inputs = self._maybe_cast_inputs(inputs)
     750 
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
    2114         # operations.
    2115         with tf_utils.maybe_init_scope(self):
 -> 2116           self.build(input_shapes)
    2117       # We must set self.built since user defined build functions are not
    2118       # constrained to set self.built.
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in build(self, input_shape)
     562     if isinstance(self.cell, Layer):
     563       if not self.cell.built:
 --> 564         self.cell.build(step_input_shape)
     565 
     566     # set or validate state_spec
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in wrapper(instance, input_shape)
     304     if input_shape is not None:
     305       input_shape = convert_shapes(input_shape, to_tuples=True)
 --> 306     output_shape = fn(instance, input_shape)
     307     # Return shapes from `fn` as TensorShapes.
     308     if output_shape is not None:
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/layers/recurrent.py in build(self, input_shape)
    2299         regularizer=self.kernel_regularizer,
    2300         constraint=self.kernel_constraint,
 -> 2301         caching_device=default_caching_device)
    2302     self.recurrent_kernel = self.add_weight(
    2303         shape=(self.units, self.units * 4),
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
     444         synchronization=synchronization,
     445         aggregation=aggregation,
 --> 446         caching_device=caching_device)
     447     backend.track_variable(variable)
     448 
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
     742         dtype=dtype,
     743         initializer=initializer,
 --> 744         **kwargs_for_getter)
     745 
     746     # If we set an initializer and the variable processed it, tracking will not
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
     140       synchronization=synchronization,
     141       aggregation=aggregation,
 --> 142       shape=variable_shape if variable_shape else None)
     143 
     144 
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
     256   def __call__(cls, *args, **kwargs):
     257     if cls is VariableV1:
 --> 258       return cls._variable_v1_call(*args, **kwargs)
     259     elif cls is Variable:
     260       return cls._variable_v2_call(*args, **kwargs)
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
     217         synchronization=synchronization,
     218         aggregation=aggregation,
 --> 219         shape=shape)
     220 
     221   def _variable_v2_call(cls,
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in <lambda>(**kwargs)
     195                         shape=None):
     196     """"""Call on Variable class. Useful to force the signature.""""""
 --> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
     198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
     199       previous_getter = _make_getter(getter, previous_getter)
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
    2594         synchronization=synchronization,
    2595         aggregation=aggregation,
 -> 2596         shape=shape)
    2597   else:
    2598     return variables.RefVariable(
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
     260       return cls._variable_v2_call(*args, **kwargs)
     261     else:
 --> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
     263 
     264 
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
    1409           aggregation=aggregation,
    1410           shape=shape,
 -> 1411           distribute_strategy=distribute_strategy)
    1412 
    1413   def _init_from_args(self,
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
    1540           with ops.name_scope(""Initializer""), device_context_manager(None):
    1541             initial_value = ops.convert_to_tensor(
 -> 1542                 initial_value() if init_from_fn else initial_value,
    1543                 name=""initial_value"", dtype=dtype)
    1544           if shape is not None:
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in <lambda>()
     120           (type(init_ops.Initializer), type(init_ops_v2.Initializer))):
     121         initializer = initializer()
 --> 122       init_val = lambda: initializer(shape, dtype=dtype)
     123       variable_dtype = dtype.base_dtype
     124   if use_resource is None:
 
 ~/path/to/python3.6/site-packages/tensorflow_core/python/ops/init_ops_v2.py in __call__(self, shape, dtype)
     413       scale /= max(1., fan_out)
     414     else:
 --> 415       scale /= max(1., (fan_in + fan_out) / 2.)
     416     if self.distribution == ""truncated_normal"":
     417       # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)
 
 TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
 </denchmark-code>
 
 Is it normal? If so, why is that?
 	",1.0,durandg12,2020-02-11T11:06:24Z,"
 		Was able to reproduce the issue. Please find the Gist <denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/895c54df8a066fec5cfd7fdf27fc431a/36624.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,durandg12,2020-02-14T22:42:59Z,"
 		So for Sequential model, we expect the layer within it only have one input and one output. The LSTM layer with return_states=True will cause it to return more than 1 output, which violate this rule.
 I think the sequential model code need to be updated to show more explicit error for this case. We already show it if your model has the input_shape (which trigger model build under the hood), but we missed it in the deferred build case (input_shape is not provided by layers, but inferred when actual input is provided).
 		",3.0,durandg12,2020-02-18T22:20:01Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36624>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36624>No</denchmark-link>
 
 		",619ca02f2d9ff61aedf7de6e6b43116e859f6913,Scott Zhu,2020-02-14 16:21:39-08:00,MODIFY,2,tensorflow\python\keras\engine\sequential.py,tensorflow\python\keras\engine\sequential.py,1.0,"203,214","198,199,200,201,212,213,214,215",MODIFY,4.0,tensorflow\python\keras\engine\sequential_test.py,tensorflow\python\keras\engine\sequential_test.py,,,,,,,,,,,,,1.0,,"224,225",test_invalid_use_cases.compute_output_shape,"self,input_shape",224,225,,,,,,,,,,,,,,,add,"self,layer",148,231,1.0,"288,289",,call,"self,inputs,training,mask",268,294,,,,,,,,,,,,,,,1.0,,"218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233",test_invalid_use_cases,self,212,233,1.0,,"221,222",test_invalid_use_cases.call,"self,inputs",221,222,1.0,"391,392,393,394,395,396,397,398,399,400",,test_multi_output_layer_not_accepted,self,380,400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36700,EmGarr,2020-02-12T16:35:26Z,2020-03-26T17:45:30Z,tf2 isn't enabled in tensorflow_core.python.keras.layers.__init__,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
 TensorFlow installed from (source or binary): pip install tensorflow==2.1
 TensorFlow version (use command below): 2.1
 Python version: 3.6.6
 CUDA/cuDNN version: Not used
 GPU model and memory: Not used
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Whenever you import a layer using the path:
 from tensorflow.python.keras.layers
 It will import the layer using the tensorflow 1.X behavior.
 Which isn't the case when we use tensorflow.keras.layers.
 The issue is that every networks from tf.keras.applications (resnet, densenet...) use those import which can lead to some severe bugs (e.g: BatchNormalization).
 Describe the expected behavior
 Importing from from tensorflow.python.keras.layers and from tensorflow.keras.layers should have exactly the same behavior (2.X).
 Code to reproduce the issue
 from tensorflow.python.keras.layers import BatchNormalization as buggy_BN
 from tensorflow.keras.layers import BatchNormalization as good_BN
 
 print(good_BN()._USE_V2_BEHAVIOR) # TRUE
 print(buggy_BN()._USE_V2_BEHAVIOR) # FALSE
 Other info / logs
 I think that the issue could be fix by changing this <denchmark-link:https://github.com/tensorflow/tensorflow/blob/9bd55fcb645500a2c859cb3390f32b3a7c48327f/tensorflow/python/tf2.py#L43>tensorflow_core.python.tf2</denchmark-link>
 .
 from
 def enabled():
   """"""Returns True iff TensorFlow 2.0 behavior should be enabled.""""""
   if _force_enable is None:
     return os.getenv(""TF2_BEHAVIOR"", ""0"") != ""0""
   else:
     return _force_enable
 to
 def enabled():
   """"""Returns True iff TensorFlow 2.0 behavior should be enabled.""""""
   if _force_enable is None:
     return os.getenv(""TF2_BEHAVIOR"", ""1"") != ""0""
   else:
     return _force_enable
 it will make TF2_BEHAVIOR enabled by default
 	",1.0,EmGarr,2020-02-13T05:25:41Z,"
 		Was able to reproduce the issue. Please find the Gist <denchmark-link:https://colab.sandbox.google.com/gist/amahendrakar/7097b0aa2d706811bc19d086206fd55a/36700.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,EmGarr,2020-02-13T23:32:35Z,"
 		<denchmark-link:https://github.com/EmGarr>@EmGarr</denchmark-link>
  Importing using  is not a suggested way for importing any module as mentioned <denchmark-link:https://github.com/tensorflow/tensorflow/issues/32957#issuecomment-543819065>here</denchmark-link>
  and <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33075#issuecomment-539070546>another resource</denchmark-link>
 . Thanks!
 		",3.0,EmGarr,2020-02-13T23:44:51Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  I do not use this layer but however all the base model of tensorflow keras do use those imports. It means that whenever you use a Resnet50 (or others) you'll use the batch normalization from tf 1.X which has a totally different behavior for trainable is False.
 		",35a382295ad81f7080d306c9b09b0edaa451fcfc,Martin Wicke,2020-03-25 10:58:45-07:00,MODIFY,0,tensorflow\api_template.__init__.py,tensorflow\api_template.__init__.py,0.0,"16,17,20,21,24,25,44,45,46,47","16,17,20,21,24,25",,,,,4.0,EmGarr,2020-02-17T20:27:57Z,"
 		
 @jvishnuvardhan I do not use this layer but however all the base model of tensorflow keras do use those imports. It means that whenever you use a Resnet50 (or others) you'll use the batch normalization from tf 1.X which has a totally different behavior for trainable is False.
 
 I encountered this problem just yesterday when attempting to use transfer learning with pretrained models from tensorflow.keras.application-package. The validation loss and accuracy aren’t progressing as one would expect (except with VGG16 and others which don’t use BN). Still after loading the model weights I manually set this flag to be true for each BN layers and after that everything seemed to work as one would expect.
 		",5.0,EmGarr,2020-03-21T03:42:45Z,"
 		<denchmark-link:https://github.com/EmGarr>@EmGarr</denchmark-link>
  we have encountered this issue and was able to solve this as mentioned <denchmark-link:https://github.com/tensorflow/tensorflow/issues/36366#issuecomment-601985968>here</denchmark-link>
 
 However this has to be fixed by tf team for good as all pretrtained keras models are referencing v1 layers and giving bad results
 		",6.0,EmGarr,2020-03-25T17:59:53Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,EmGarr,2020-03-26T17:45:29Z,"
 		this issue was fixed in this <denchmark-link:https://github.com/tensorflow/tensorflow/commit/410852dbd24899e22f0020f9fdc9757f527dda55>commit</denchmark-link>
 
 and the fix has been cherrypicked into r2.2 branch.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,EmGarr,2020-03-26T17:45:31Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/36700>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37230,GeorgeLiang3,2020-03-02T12:43:36Z,2020-03-30T23:27:38Z,tf.function second derivative error,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 
 Have I written custom code (as opposed to using a stock
 example script provided in TensorFlow): yes
 
 
 OS Platform and Distribution (e.g.,
 Linux Ubuntu 16.04): macOS Mojave 10.14.6
 
 
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
 the issue happens on mobile device: N/A
 
 
 TensorFlow installed from (source or
 binary): - TensorFlow version (use command below):binary  v2.1.0-rc2-17
 
 
 Python version:  - Bazel
 version (if compiling from source): Python 3.7.1
 
 
 GCC/Compiler version (if compiling from
 source):
 
 
 CUDA/cuDNN version: - GPU model and memory:
 
 
 Describe the current behavior
 I have a custom function need to use slicing in a for loop. The first-order derivative is working properly but the second-order derivative gives the error. The error only occurs when the function is decorated with tf.function. Below is a simplified code to reproduce the error.
 Describe the expected behavior
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>import tensorflow as tf
 x =tf.random.uniform([9],minval = -50,maxval = -30,seed = 1,dtype = tf.float32)
 
 @tf.function
 def A(x):
     return x[1]
 
 @tf.function
 def g(x):
 
     Z_sum = tf.constant([0.],dtype = tf.float32)
 
     for i in tf.range(x.shape[0]):
         Z_sum = tf.add(Z_sum, A(x))
 
     return Z_sum
 
 with tf.GradientTape() as t:
     t.watch(x)
     with tf.GradientTape() as tt:
         tt.watch(x)
         loss = g(x)
     jac = tt.gradient(loss,x)
 hess = t.gradient(jac,x)
 </denchmark-code>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 <denchmark-h:hr></denchmark-h>
 
 InvalidArgumentError                      Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
 2325       with c_api_util.tf_buffer() as buf:
 -> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
 2327         data = c_api.TF_GetBuffer(buf)
 InvalidArgumentError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.
 During handling of the above exception, another exception occurred:
 ValueError                                Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
 330     try:
 --> 331       xla_compile = op.get_attr(""_XlaCompile"")
 332       xla_separate_compiled_gradients = op.get_attr(
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
 2329       # Convert to ValueError for backwards compatibility.
 -> 2330       raise ValueError(str(e))
 2331     x = attr_value_pb2.AttrValue()
 ValueError: Operation 'gradients/while_grad/while_grad' has no attr named '_XlaCompile'.
 During handling of the above exception, another exception occurred:
 InvalidArgumentError                      Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
 2325       with c_api_util.tf_buffer() as buf:
 -> 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)
 2327         data = c_api.TF_GetBuffer(buf)
 InvalidArgumentError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.
 During handling of the above exception, another exception occurred:
 ValueError                                Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
 330     try:
 --> 331       xla_compile = op.get_attr(""_XlaCompile"")
 332       xla_separate_compiled_gradients = op.get_attr(
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)
 2329       # Convert to ValueError for backwards compatibility.
 -> 2330       raise ValueError(str(e))
 2331     x = attr_value_pb2.AttrValue()
 ValueError: Operation 'gradients/TensorListPushBack_grad/TensorListPopBack' has no attr named '_XlaCompile'.
 During handling of the above exception, another exception occurred:
 ValueError                                Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
 467               as_ref=input_arg.is_ref,
 --> 468               preferred_dtype=default_dtype)
 469         except TypeError as err:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
 1313     if ret is None:
 -> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
 1315
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
 316   _ = as_ref
 --> 317   return constant(v, dtype=dtype, name=name)
 318
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
 257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
 --> 258                         allow_broadcast=True)
 259
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
 295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
 --> 296           allow_broadcast=allow_broadcast))
 297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
 438     if values is None:
 --> 439       raise ValueError(""None values not supported."")
 440     # if dtype is provided, forces numpy array to be the type
 ValueError: None values not supported.
 During handling of the above exception, another exception occurred:
 ValueError                                Traceback (most recent call last)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
 481             observed = ops.convert_to_tensor(
 --> 482                 values, as_ref=input_arg.is_ref).dtype.name
 483           except ValueError as err:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
 1313     if ret is None:
 -> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
 1315
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
 316   _ = as_ref
 --> 317   return constant(v, dtype=dtype, name=name)
 318
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
 257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
 --> 258                         allow_broadcast=True)
 259
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
 295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
 --> 296           allow_broadcast=allow_broadcast))
 297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
 438     if values is None:
 --> 439       raise ValueError(""None values not supported."")
 440     # if dtype is provided, forces numpy array to be the type
 ValueError: None values not supported.
 During handling of the above exception, another exception occurred:
 ValueError                                Traceback (most recent call last)
  in 
 21         tt.watch(x)
 22         loss = g(x)
 ---> 23     jac = tt.gradient(loss,x)
 24 hess = t.gradient(jac,x)
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
 1027         output_gradients=output_gradients,
 1028         sources_raw=flat_sources_raw,
 -> 1029         unconnected_gradients=unconnected_gradients)
 1030
 1031     if not self._persistent:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
 75       output_gradients,
 76       sources_raw,
 ---> 77       compat.as_str(unconnected_gradients.value))
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backward_function_wrapper(*args)
 1254           break
 1255       return backward._call_flat(  # pylint: disable=protected-access
 -> 1256           processed_args, remapped_captures)
 1257
 1258     return _backward_function_wrapper, recorded_outputs
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
 1695         possible_gradient_type,
 1696         executing_eagerly)
 -> 1697     forward_function, args_with_tangents = forward_backward.forward()
 1698     if executing_eagerly:
 1699       flat_outputs = forward_function.call(
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self)
 1421     """"""Builds or retrieves a forward function for this call.""""""
 1422     forward_function = self._functions.forward(
 -> 1423         self._inference_args, self._input_tangents)
 1424     return forward_function, self._inference_args + self._input_tangents
 1425
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self, inference_args, input_tangents)
 1183       (self._forward, self._forward_graph, self._backward,
 1184        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (
 -> 1185            self._forward_and_backward_functions(inference_args, input_tangents))
 1186     return self._forward
 1187
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)
 1329     outputs = self._func_graph.outputs[:self._num_inference_outputs]
 1330     return self._build_functions_for_outputs(
 -> 1331         outputs, inference_args, input_tangents)
 1332
 1333
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)
 888             self._func_graph.inputs,
 889             grad_ys=gradients_wrt_outputs,
 --> 890             src_graph=self._func_graph)
 891
 892       captures_from_forward = [
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
 667                 # functions.
 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --> 669                                          lambda: grad_fn(op, *out_grads))
 670               else:
 671                 # For function call ops, we add a 'SymbolicGradient'
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
 334       xla_scope = op.get_attr(""_XlaScope"").decode()
 335     except ValueError:
 --> 336       return grad_fn()  # Exit early
 337
 338   if not xla_compile:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()
 667                 # functions.
 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --> 669                                          lambda: grad_fn(op, *out_grads))
 670               else:
 671                 # For function call ops, we add a 'SymbolicGradient'
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _WhileGrad(op, *grads)
 357   body_grad_graph, args = _create_grad_func(
 358       ys, xs, non_none_grads, cond_graph, body_graph,
 --> 359       util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
 360
 361   if body_grad_graph.while_op_needs_rewrite:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _create_grad_func(ys, xs, grads, cond_graph, body_graph, name, while_op, maximum_iterations)
 599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,
 600                                          maximum_iterations, while_op,
 --> 601                                          body_graph_inputs, body_graph_outputs))
 602
 603   # Update the list of outputs with tensors corresponding to the captured
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
 976                                           converted_func)
 977
 --> 978       func_outputs = python_func(*func_args, **func_kwargs)
 979
 980       # invariant: func_outputs contains only Tensors, CompositeTensors,
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in (*args)
 595   grad_func_graph = func_graph_module.func_graph_from_py_func(
 596       name,
 --> 597       lambda *args: _grad_fn(ys, xs, args, body_graph),
 598       args, {},
 599       func_graph=_WhileBodyGradFuncGraph(name, cond_graph, body_graph,
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/while_v2.py in _grad_fn(ys, xs, args, func_graph)
 655   grad_outs = gradients_util._GradientsHelper(
 656       ys, xs, grad_ys=grad_ys, src_graph=func_graph,
 --> 657       unconnected_gradients=""zero"")
 658
 659   # TODO(b/118712257): Handle the case when grad_outs has None's e.g. when there
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)
 667                 # functions.
 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --> 669                                          lambda: grad_fn(op, *out_grads))
 670               else:
 671                 # For function call ops, we add a 'SymbolicGradient'
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)
 334       xla_scope = op.get_attr(""_XlaScope"").decode()
 335     except ValueError:
 --> 336       return grad_fn()  # Exit early
 337
 338   if not xla_compile:
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in ()
 667                 # functions.
 668                 in_grads = _MaybeCompile(grad_scope, op, func_call,
 --> 669                                          lambda: grad_fn(op, *out_grads))
 670               else:
 671                 # For function call ops, we add a 'SymbolicGradient'
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/list_ops.py in _PopBackGrad(op, dlist, delement)
 187         element_shape=gen_list_ops.tensor_list_element_shape(
 188             op.outputs[0], shape_type=dtypes.int32))
 --> 189   return gen_list_ops.tensor_list_push_back(dlist, delement), None
 190
 191
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_list_ops.py in tensor_list_push_back(input_handle, tensor, name)
 761   _, _, _op, _outputs = _op_def_library._apply_op_helper(
 762         ""TensorListPushBack"", input_handle=input_handle, tensor=tensor,
 --> 763                               name=name)
 764   _result = _outputs[:]
 765   if _execute.must_record_gradient():
 ~/anaconda3/envs/bys/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
 484             raise ValueError(
 485                 ""Tried to convert '%s' to a tensor and failed. Error: %s"" %
 --> 486                 (input_name, err))
 487           prefix = (""Input '%s' of '%s' Op has type %s that does not match"" %
 488                     (input_name, op_type_name, observed))
 ValueError: Tried to convert 'tensor' to a tensor and failed. Error: None values not supported.
 	",1.0,GeorgeLiang3,2020-03-04T09:14:20Z,"
 		I think this is related to issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/15219>#15219</denchmark-link>
 , is there any solution for it now?
 		",2.0,GeorgeLiang3,2020-03-04T10:20:01Z,"
 		Could able to reproduce the issue with Tf 2.1.
 Please find the gist <denchmark-link:https://colab.sandbox.google.com/gist/gadagashwini/28674efbe2941bb129d5622c13892277/untitled416.ipynb>here</denchmark-link>
 . Thanks!
 		",3.0,GeorgeLiang3,2020-03-05T03:50:45Z,"
 		<denchmark-link:https://github.com/GeorgeLiang3>@GeorgeLiang3</denchmark-link>
  I don't run into any error when I am using tf-nightly. Please find my gist <denchmark-link:https://colab.sandbox.google.com/gist/gowthamkpr/f7bcc6f630e4864f0c269256f9d3df96/untitled37.ipynb>here</denchmark-link>
 . Thanks!
 		",cf09044d9e7b232080f95f0f910a6803904df1de,Saurabh Saxena,2020-03-30 16:22:09-07:00,MODIFY,1,tensorflow\python\eager\pywrap_gradient_exclusions.cc,tensorflow\python\eager\pywrap_gradient_exclusions.cc,1.0,413,"413,836",MODIFY,2.0,tensorflow\python\kernel_tests\list_ops_test.py,tensorflow\python\kernel_tests\list_ops_test.py,4.0,GeorgeLiang3,2020-03-05T10:41:35Z,"
 		<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
   Thanks for your reply! But if I add one more 'if' or 'for' statement in the function, the both give the same error. Please find my gist <denchmark-link:https://colab.research.google.com/drive/1p0bb2ePkc6ect0Q26RVB_BByOUFtppeq>here</denchmark-link>
 
 		",5.0,GeorgeLiang3,2020-03-10T12:27:54Z,"
 		<denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
  Any solution to this? Thanks in advance
 		",6.0,GeorgeLiang3,2020-03-30T23:27:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37230>No</denchmark-link>
 
 		",1.0,"1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686",,testPopBackGrad,self,1668,1686,MODIFY,1.0,tensorflow\python\ops\list_ops.py,tensorflow\python\ops\list_ops.py,1.0,"189,190",,,,,,,,,OpGradientUnusedOutputIndices,op_name,411,881,,,,,,,,,,,,,,,,,,,,,,1.0,"1672,1673,1674,1675,1676",,testPopBackGrad.g,x,1672,1676,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_PopBackGrad,"op,dlist,delement",183,191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37480,ayushmankumar7,2020-03-10T14:31:49Z,2020-03-24T08:01:21Z,'third_party/tensorflow/compiler/aot:codegen_test' No such directory found,"
 Thank you for submitting a TensorFlow documentation issue. Per our GitHub
 policy, we only address code/doc bugs, performance issues, feature requests, and
 build/installation issues on GitHub.
 The TensorFlow docs are open source! To get involved, read the documentation
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/codegen_test.cc>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/aot/codegen_test.cc</denchmark-link>
 
 Please provide a link to the documentation entry, for example:
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 // To update the golden file, flip update_golden to true and run the
 // following:
 // bazel test --test_strategy=local 
 //   third_party/tensorflow/compiler/aot:codegen_test
 <denchmark-h:h3>Clear description</denchmark-h>
 
 Line 154
 I wanted to update the golden file, but the given directory is not present in the third_party library.
 For example, why should someone use this method? How is it useful?
 <denchmark-h:h3>Correct links</denchmark-h>
 
 Is the link to the source code correct?
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Are all parameters defined and formatted correctly?
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Are return values defined?
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Are the errors defined? For example,
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises</denchmark-link>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Is there a usage example?
 See the API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
 
 on how to write testable usage examples.
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 Are there currently visuals? If not, will it clarify the content?
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Are you planning to also submit a pull request to fix the issue? See the docs
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 ,
 docs API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
  and the
 docs style guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_style>https://www.tensorflow.org/community/contribute/docs_style</denchmark-link>
 
 	",1.0,ayushmankumar7,2020-03-12T22:10:44Z,"
 		<denchmark-link:https://github.com/ayushmankumar7>@ayushmankumar7</denchmark-link>
  Can you please be clear on what you are trying to explain here. Thanks!
 		",2.0,ayushmankumar7,2020-03-13T12:28:21Z,"
 		In line 154, it says // To update the golden file, flip update_golden to true and run the
 // following:
 // bazel test --test_strategy=local 
 //   third_party/tensorflow/compiler/aot:codegen_test
 but /tensorflow/compiler/aot:codegen_test not present in third_party folder
 		",,,,,282828af67de29d13dd2c69d96413c030b02543c,Mark Daoust,2020-03-24 01:00:08-07:00,MODIFY,1,tensorflow\compiler\aot\codegen_test.cc,tensorflow\compiler\aot\codegen_test.cc,1.0,157,157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::tfcompile::CompareWithGoldenFile,"tensorflow_relative_golden_file_name,expected_contents,ignore_cr",142,176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37777,shiningrain,2020-03-21T10:12:31Z,2020-03-26T18:26:53Z,It seems that Tensorflow needs a check for the unreasonable parameter `input_dim=0` in the layer `Embedding`.,"
 <denchmark-h:h2>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using example directory):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 & Linux Ubuntu 18.04
 Tensorflow version：2.1.0-cpu (using 'pip install TensorFlow' to download directly)
 Python version: 3.6.9
 CUDA/cuDNN version: -
 GPU model and memory: -
 
 <denchmark-h:h2>Describe the current behavior</denchmark-h>
 
 When I build the model with an illogical parameter input_dim = 0in the layer Embedding, Tensorflow uses this unreasonably parameter to build and even save the model. The detailed performance in building the model is shown in the following picture。
 <denchmark-link:https://user-images.githubusercontent.com/46860123/77224293-27355480-6b9f-11ea-9d96-df3e852ca4b3.png></denchmark-link>
 
 <denchmark-h:h2>Key insights</denchmark-h>
 
 To sum up, input_dim or output_dim = 0 are unreasonable corner cases. ** Tensorflow seems to lack in checking this corner case**. This may lead Tensorflow users to create and even save a wrong model, which will bring potential risks in the subsequent usage.
 <denchmark-h:h2>Code to reproduce the issue</denchmark-h>
 
 import numpy as np
 import tensorflow.keras.layers as L 
 from tensorflow.keras import Model, Input
 import tensorflow
 import os
 
 print(tensorflow.__version__) 
 
 kwargs = {
     'input_dim': 0, # you can also set input_dim to 0 to test
 	'output_dim': 18,
 	'mask_zero': True
 }
 input = (10 * np.random.random((32,10)))
 layer = L.Embedding(**kwargs)
 x = Input(batch_shape=input.shape)
 y = layer(x)
 bk_model = Model(x, y)
 model_path = os.path.join('./', 'mode.h5')
 bk_model.save(model_path, bk_model)
 print('finish')
 	",1.0,shiningrain,2020-03-22T20:04:17Z,"
 		The code you have written is not correct you are not passing any input to the model.TensorFlow creates a static graph and the checks for the correct dimension during execution
 Write you code then at the end writebk_model(input) .You will see some error this is because the dimension of your tensor is (32,10) now change 'input_dim': 0 to 'input_dim': 10 and run again,it will work.
 		",2.0,shiningrain,2020-03-23T04:25:20Z,"
 		<denchmark-link:https://github.com/shiningrain>@shiningrain</denchmark-link>
 
 Could you please update on the above comment
 		",3.0,shiningrain,2020-03-24T02:42:51Z,"
 		
 The code you have written is not correct you are not passing any input to the model.TensorFlow creates a static graph and the checks for the correct dimension during execution
 Write you code then at the end writebk_model(input) .You will see some error this is because the dimension of your tensor is (32,10) now change 'input_dim': 0 to 'input_dim': 10 and run again,it will work.
 
 Hello! Thank you for your reply!
 I have tested the code with the change you said. And when I set the  ,it did make a normal prediction.  But as I described above, 
 I saw on the TensorFlow document <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding>here</denchmark-link>
 . On this page, it mentions that the inputdim should be greater than zero, but it seems that we can still build the model with .
 I want to emphasize that , thus the corner case like can be used to build a model by TensorFlow and save a corresponding model----even if this model is just a static graph. This may confuse users.
 Additionally, adding a check for the illogical parameter is not a difficult job. As shown <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe>here</denchmark-link>
 , Tensorflow has previously fixed a similar problem that people can build a model when using 
 		",f61175812426009a4c96e51befb2951612990903,A. Unique TensorFlower,2020-03-26 11:25:22-07:00,MODIFY,0,tensorflow\python\keras\layers\embeddings.py,tensorflow\python\keras\layers\embeddings.py,0.0,"115,116,117",,,,,,4.0,shiningrain,2020-03-26T08:22:35Z,"
 		Closing this since it has been fixed <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe>here</denchmark-link>
 .
 		",5.0,shiningrain,2020-03-26T08:22:37Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37777>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37777>No</denchmark-link>
 
 		",6.0,shiningrain,2020-03-26T09:03:37Z,"
 		
 Closing this since it has been fixed here.
 
 Thank you for your reply!
 I wonder if you are using a wrong <denchmark-link:https://github.com/tensorflow/tensorflow/commit/1e102f63964365d82d7f22402b7ba21e0e0e64fe>link</denchmark-link>
 in your reply, because this link is the same as the one in my comments and it is related to the conv layer, not the Embedding.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,shiningrain,2020-03-26T09:12:31Z,"
 		
 
 Closing this since it has been fixed here.
 
 Thank you for your reply!
 I wonder if you are using a wrong linkin your reply, because this link is the same as the one in my comments and it is related to the conv layer, not the Embedding.
 
 Oh sorry, I meant to close another issue
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,shiningrain,2020-03-26T18:26:55Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37777>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37777>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37983,hannesdm,2020-03-27T18:02:30Z,2020-04-11T23:32:20Z,Calling next with a default value on an exhausted Dataset iterator raises an OutOfRangeError in graph mode,"
 System information
 
 Have I written custom code: Yes
 OS Platform and Distribution:  Windows 10
 TensorFlow installed from binary: 2.1.0
 
 Describe the current behavior
 next(iterator, default) is supposed to give the next element in the iterator or the value given as default if the iterator is at the end.
 However, when using the above construction in a function with @tf.function, the default value is not returned and an error (tensorflow.python.framework.errors_impl.OutOfRangeError) is produced when trying to call next on an iterator that is at the end.
 When running this code in eager mode, the default value is returned as expected.
 Describe the expected behavior
 In graph mode the default value should be returned when at the end of an iterator.
 Standalone code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 
 x = tf.convert_to_tensor([[1], [2], [3]])
 ds = tf.data.Dataset.from_tensor_slices(x)
 dsi = iter(ds)
 
 
 @tf.function # remove this to get the expected behaviour
 def func():
     for _ in range(4):
         tf.print(next(dsi, -1))
 
 
 func()
 </denchmark-code>
 
 Output (see below for a full stacktrace):
 <denchmark-code>[1]
 [2]
 [3]
 2020-03-27 18:56:09.523946: W tensorflow/core/common_runtime/base_collective_executor.cc:217] BaseCollectiveExecutor::StartAbort Out of range: End of sequence
 	 [[{{node IteratorGetNext_3}}]]
 </denchmark-code>
 
 Expected output:
 <denchmark-code>[1]
 [2]
 [3]
 -1
 
 </denchmark-code>
 
 
  <denchmark-link:https://colab.research.google.com/drive/1PBxoXiE48aC-bo-aY-Bau1Igt4Aj6OFy>https://colab.research.google.com/drive/1PBxoXiE48aC-bo-aY-Bau1Igt4Aj6OFy</denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4395116/stacktrace.txt>stacktrace.txt</denchmark-link>
 
 	",1.0,hannesdm,2020-03-28T12:05:58Z,"
 		<denchmark-link:https://github.com/hannesdm>@hannesdm</denchmark-link>
 , I have tried after removing  decorator and got expected output.
 		",2.0,hannesdm,2020-03-28T13:28:33Z,"
 		
 @hannesdm, I have tried after removing @tf.function decorator and got expected output.
 
 Yes, everything works as it should without @tf.function, the bug only occurs in graph mode i.e. with @tf.function.
 		",3.0,hannesdm,2020-04-11T23:32:21Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37983>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/37983>No</denchmark-link>
 
 		",95ea3404528afcb1a74dd5f0946ea8d17beda28b,Dan Moldovan,2020-04-11 16:30:42-07:00,MODIFY,5,tensorflow\python\autograph\operators\py_builtins.py,tensorflow\python\autograph\operators\py_builtins.py,1.0,"404,405,406,407",,MODIFY,7.0,tensorflow\python\autograph\operators\py_builtins_test.py,tensorflow\python\autograph\operators\py_builtins_test.py,,,,,,,,,,,,,1.0,"307,308,309,310,311,312",,test_next_tf_iterator_error_checking_structures.test_fn,default_val,307,312,,,,,,,,,,,,,,,next_,"iterator,default",404,407,1.0,"447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473",,_verify_structure_compatible,"input_name,spec_name,input_,spec",447,473,1.0,"411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444",,_verify_spec_compatible,"input_name,spec_name,input_,spec",411,444,1.0,"488,489,490,491",,next_py,"iterator,default",488,491,1.0,"264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281",,test_next_tf_iterator.test_fn,"go_out_of_range,with_default",264,281,1.0,"292,293,294,295,296,297,298,299,300,301,302",,test_next_tf_iterator_error_checking,self,292,302,1.0,"261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290",,test_next_tf_iterator,self,261,290,1.0,"295,296,297,298",,test_next_tf_iterator_error_checking.test_fn,,295,298,1.0,"304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330",,test_next_tf_iterator_error_checking_structures,self,304,330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"476,477,478,479,480,481,482,483,484,485",,next_tf_iterator,"iterator,default",476,485,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"252,253,254,255,256,257,258,259",,test_next_normal,self,252,259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3824,ylongqi,2016-08-15T18:34:24Z,2017-05-01T16:22:27Z,Unclear documentation and behavior for sampler in Tensorflow,"
 For the samplers implemented in tensorflow, e.g. tf.nn.fixed_unigram_candidate_sampler. The behavior is not well-defined in the document. For instance, I would expect the labels specified in true_classes will be excluded from the sampling pool, and the sampling will be conducted for each batch. But according to my experiments, neither of above is true.
 Consider the following code:
 `import tensorflow as tf
 labels_matrix = tf.reshape(tf.constant([1, 2, 3, 4], dtype=tf.int64), [-1, 1])
 sampled_ids, _, _ = tf.nn.fixed_unigram_candidate_sampler(
 true_classes = labels_matrix,
 num_true = 1,
 num_sampled = 1,
 unique = True,
 range_max = 5,
 distortion = 0.0,
 unigrams = range(5)
 )
 init = tf.initialize_all_variables()
 with tf.Session() as sess:
 sess.run(init)
 print sess.run([sampled_ids])
 `
 The output can be 3, which actually belongs to the set of true classes. - Also, the output has the dimension [1], which basically means that the sampling is only conducted once, not for each batch.
 Can someone help to clarify this?
 	",1.0,ylongqi,2017-01-24T22:00:13Z,"
 		Is this still current?
 		",2.0,ylongqi,2017-01-25T17:30:29Z,"
 		Yes, this is probably still a problem in the docs, and it will be useful for the TF writer to keep the issue for reference.
 		",3.0,ylongqi,2017-08-07T13:17:51Z,"
 		any solutions for this issue? I also find this problem in version1.2
 		",71319c58a3436fbd7081e49c52878dd1ba1772b5,A. Unique TensorFlower,2017-04-28 15:33:58-07:00,MODIFY,0,tensorflow\core\ops\candidate_sampling_ops.cc,tensorflow\core\ops\candidate_sampling_ops.cc,0.0,"84,134,183,232,292,363","84,134,183,232,292,363",MODIFY,0.0,tensorflow\python\ops\candidate_sampling_ops.py,tensorflow\python\ops\candidate_sampling_ops.py,,,,,,,,,,,,,0.0,"56,57,58,66,67,68,69,118,173,237","56,64,65,114,169,233",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38349,0x0badc0de,2020-04-08T09:30:26Z,2020-10-02T16:47:50Z,`nan` gradient when `tf.where` is used,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock
 example script provided in TensorFlow):  Yes
 OS Platform and Distribution (e.g.,
 Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster)
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
 the issue happens on mobile device:
 TensorFlow installed from (source or
 binary): binary
 TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 / v1.12.1-29016-g38797a1c8b 2.2.0-dev20200407
 Python version: 3.7.7
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: - GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 Well-defined function with tf.where has nan gradients at points where tf.where inactive branch is undefined.
 Describe the expected behavior
 Inactive branch should be ignored in gradients calculations.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>import tensorflow as tf
 
 for ex in range(-3, 3):
     x = tf.convert_to_tensor(10.**ex)
     with tf.GradientTape() as g:
         g.watch(x)
         y = tf.where(x >= -1., x, tf.math.log1p(-x))
 #         y = tf.where(x >= -1., x, tf.math.log(1.-x))
 #         y = tf.where(x >= -1., x, 1./(1.-x))
     dy_dx = g.gradient(y, x)
     print(f'y({x})={y}, dy/dx({x})={dy_dx}')
 </denchmark-code>
 
 All 3 functions above are well defined for positive values used for testing. Still they show no gradient at point 1.. while it has to be equal to 1.
 <denchmark-code>y(0.0010000000474974513)=0.0010000000474974513, dy/dx(0.0010000000474974513)=1.0
 y(0.009999999776482582)=0.009999999776482582, dy/dx(0.009999999776482582)=1.0
 y(0.10000000149011612)=0.10000000149011612, dy/dx(0.10000000149011612)=1.0
 y(1.0)=1.0, dy/dx(1.0)=nan
 y(10.0)=10.0, dy/dx(10.0)=1.0
 y(100.0)=100.0, dy/dx(100.0)=1.0
 </denchmark-code>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,0x0badc0de,2020-04-08T16:16:55Z,"
 		I have tried on colab with TF version 2.1.0 , 2.2.0-rc2 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/806f63f2cf04070a4601289d7003cf0a/untitled24.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,0x0badc0de,2020-04-08T21:21:52Z,"
 		This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.
 You can find more detail here, along with a recipe for how to avoid it: <denchmark-link:https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444>https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444</denchmark-link>
 
 In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.
 		",3.0,0x0badc0de,2020-04-08T21:21:54Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>No</denchmark-link>
 
 		",d6c0858665de6036de24991b29d74b182cfcf5ae,codeadmin_peritiae,2020-07-25 09:28:01+02:00,MODIFY,1,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,1.0,"4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504",,,,,,4.0,0x0badc0de,2020-04-08T21:27:19Z,"
 		Shouldn't this be documented with big warning in tf.where docs in this case?
 		",5.0,0x0badc0de,2020-04-08T22:25:09Z,"
 		Indeed it should.
 		",6.0,0x0badc0de,2020-04-09T02:42:48Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  Hello, this is my first time contributing to TensofFlow lib. From the thread I gather you would require the  be updated. If it is so can I work on this?
 		",,,,,,,,,,,,,,,,,,,,,,where_v2,"condition,x,y,name",4423,4531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,0x0badc0de,2020-04-11T18:53:42Z,"
 		Hello <denchmark-link:https://github.com/0x0badc0de>@0x0badc0de</denchmark-link>
  , <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
 
 Should the updated doc contain a something like a warning? or will a small note at the end, about the input not being Nan will do? Also should the workaround for avoiding it also be added to the doc?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,0x0badc0de,2020-04-11T19:16:08Z,"
 		<denchmark-link:https://github.com/joemaren>@joemaren</denchmark-link>
  <denchmark-link:https://github.com/anorak-k>@anorak-k</denchmark-link>
 
 Sorry for the delay. Feel free to send a PR - it's only a matter of adding a paragraph to the docstring.
 The text should be more in the lines of a warning. Something like: Important: if any of the inputs contain NaN values, etc.. And yes, it should include the workaround as well, which is something in the lines of: instead of tf.where(x, ops_that_can_nan(z), ...), write tf.where(x, ops_that_can_nan(tf.where(x, z, safe_value)), ...).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,0x0badc0de,2020-04-13T10:02:31Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  I have added the change and raised a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>
 
 		",10.0,0x0badc0de,2020-04-18T22:25:55Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  Thanks for your reply. However, I would like to mention that this behavior also happens when the generated value in the inactive branch  (i.e.  or ). Here is a minimal reproducible example:
 import tensorflow as tf
 
 a = tf.Variable(10.)
 with tf.GradientTape() as tape:
   out = tf.where(a < 15., a, tf.math.pow(10.0, tf.math.exp(a)))
   grads = tape.gradient(out, a)
 
 print(grads)
 # tf.Tensor(nan, shape=(), dtype=float32)
 And also if we reverse the condition such that the branch with infinite value is selected, the gradient would be infinite (which is a bit surprising that it does not generate nan instead, like above):
 with tf.GradientTape() as tape:
   out = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(a)))
   grads = tape.gradient(out, a)
 
 print(grads)
 # tf.Tensor(inf, shape=(), dtype=float32)
 So this behavior happens for both nan and infinite values in inactive branch. I wish it wasn't like this, because it's a bit unreasonable and makes it impossible to use user-defined ops/functions which generate extremely large values for some input values; hence, that inner tf.where workaround may not be practical always (unfortunately, even gradient clipping does not help with this, because clipping a nan value produces nan in TF).
 CC: <denchmark-link:https://github.com/anorak-k>@anorak-k</denchmark-link>
  for potential consideration in your PR after <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  confirms this.
 		",11.0,0x0badc0de,2020-04-19T02:27:57Z,"
 		<denchmark-link:https://github.com/mkaze>@mkaze</denchmark-link>
  that's true - nan, inf and any other special FP value will disrupt the gradient calculation.
 What happens internally is that the gradients are aggregated in this fashion: 1 * <grad of branch taken> + 0 * <grad of branch not taken>. In the former case, you have 0 * inf = nan. In the latter case, you have 1 * inf = inf. I agree it's very confusing, unfortunately a naive fix would add significant overhead to gradient calculations.
 Moreover, the forward calculation doesn't need to result in a nan or inf. You can also get weird results if the gradient alone is nan or inf. For example, the cube root function is defined and well-behaved everywhere, but its derivative at zero is infinite. So this will give you a nan gradient too:
 <denchmark-code>a = tf.Variable(0.0)
 with tf.GradientTape() as tape:
   out = tf.where(a < 1, a, tf.pow(a, 1.0/3.0))
   grads = tape.gradient(out, a)
 print(grads)
 </denchmark-code>
 
 I think the tf.where workaround is useful with infinite values as well, so long as the branch not taken is forced to take a gradient that can be safely multiplied by 0. For your example, it would be something like this:
 <denchmark-code>dummy_safe_value = 0
 safe_a = tf.where(a > 15., dummy_safe_value, a)
 out = tf.where(a > 15., a, tf.math.pow(10.0, tf.math.exp(safe_a)))
 </denchmark-code>
 
 I agree that it sometimes can be impractical to do, but in principle it should always be possible as long as you control the inputs to the sensitive functions - all they have to do is force finite values in all the elements that are dropped.
 		",12.0,0x0badc0de,2020-05-07T14:42:20Z,"
 		I want to fix the issue <denchmark-link:https://github.com/tensorflow/tensorflow/issues/38349>#38349</denchmark-link>
 
 		",13.0,0x0badc0de,2020-05-14T15:47:54Z,"
 		
 This is due to a limitation limitation in how gradients are calculated. Unfortunately, it is unlikely to be fixed in the foreseable future.
 You can find more detail here, along with a recipe for how to avoid it: https://stackoverflow.com/questions/33712178/tensorflow-nan-bug/42497444#42497444
 In short, if the input to a tf.where contains NaNs, the gradient will always be NaN, regardless whether the input is actually used or not, and the workaround is to prevent the inputs from ever containing NaNs.
 
 You can simply have it raise a value error if its getting Nan inputs. Or does it not work like that?
 		",14.0,0x0badc0de,2020-05-29T11:17:54Z,"
 		Can I work on this issue if someone isn't now?
 		",15.0,0x0badc0de,2020-05-29T14:36:50Z,"
 		<denchmark-link:https://github.com/tushar-dalal>@tushar-dalal</denchmark-link>
  The challenge is that verifying for such NaN inputs can be taking on performance. When debugging,  can indeed help with that.
 <denchmark-link:https://github.com/unicorn-io>@unicorn-io</denchmark-link>
  Feel free to tackle it, but note that it's extremely challenging to solve. That said, there was a PR (<denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>
 ) to add a warning message to the docs of tf.where, it would be useful to revive it.
 		",16.0,0x0badc0de,2020-05-29T16:19:20Z,"
 		I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue.
 		",17.0,0x0badc0de,2020-06-02T01:38:13Z,"
 		
 I am motivated to do this can you give me some tips to start with I will try my best to understand and resolve this issue. @mdanatg
 
 		",18.0,0x0badc0de,2020-06-02T12:11:03Z,"
 		<denchmark-link:https://github.com/unicorn-io>@unicorn-io</denchmark-link>
  You can start by looking at the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h#L149>gradient code</denchmark-link>
  and understanding how it works. Then you can reproduce when happens in the case of a where with bad gradients.
 		",19.0,0x0badc0de,2020-06-02T15:26:53Z,"
 		Cool I'll get to it
 		",20.0,0x0badc0de,2020-06-08T09:39:52Z,"
 		Hey i would like to work on it. can also help please
 		",21.0,0x0badc0de,2020-06-17T11:34:28Z,"
 		
 Cool I'll get to it
 
 This bug cannot be fixed as of now it seems.
 		",22.0,0x0badc0de,2020-06-17T11:51:13Z,"
 		It's indeed very challenging to fix. However, the documentation of affected ops, like tf.where can still be updated to alert the users about it.
 		",23.0,0x0badc0de,2020-06-18T05:00:16Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  isn't <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38497>#38497</denchmark-link>
  addressing this and is closed?
 		",24.0,0x0badc0de,2020-06-18T11:40:47Z,"
 		You mean <denchmark-link:https://github.com/tensorflow/tensorflow/pull/38467>#38467</denchmark-link>
 ? It's closed due to staleness, and it would be useful to revive. By the looks of it it's safe to assume noone else is working on it.
 		",25.0,0x0badc0de,2020-07-01T16:44:28Z,"
 		Seems like its a long time since the last activity. Is this issue still open to be worked on?
 		",26.0,0x0badc0de,2020-07-01T16:47:47Z,"
 		I think so. There are two parts to it: (1) updating the docs of tf.where, which is fairly straightforward, and (2) actually trying to address the issue, which is a significant undertaking because it involves a rather fundamental issue.
 		",27.0,0x0badc0de,2020-07-08T11:41:29Z,"
 		Is this issue still addressable ?
 		",28.0,0x0badc0de,2020-07-25T07:42:20Z,"
 		Nice to be part of the group.
 Please, have a look to my pull request for the workaround: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721>#41721</denchmark-link>
 
 I'm going to work on the main issue too.
 I'll be happy to cooperate with anybody else interested.
 		",29.0,0x0badc0de,2020-07-27T13:18:53Z,"
 		<denchmark-link:https://github.com/codeadmin-peritiae>@codeadmin-peritiae</denchmark-link>
  The PR appears to be empty. Perhaps there's an issue with the git client?
 		",30.0,0x0badc0de,2020-08-14T14:30:40Z,"
 		Just to follow up on the events. It looks like codeadmin-peritiae had an issue with his original PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41721>#41721</denchmark-link>
  where he had trouble with his SSH certificate. He then opened up another PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775>#41775</denchmark-link>
  which is currently blocked since some of the checks haven't completed. By the looks of it, the documentation update part of this problem is almost completed.
 		",31.0,0x0badc0de,2020-09-28T18:49:30Z,"
 		Is it issue solvable for a beginner ? if yes can I work on it? <denchmark-link:https://github.com/Harsh188>@Harsh188</denchmark-link>
  <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
 
 		",32.0,0x0badc0de,2020-09-29T02:25:02Z,"
 		I think this issue should be closed, <denchmark-link:https://github.com/tensorflow/tensorflow/pull/41775>#41775</denchmark-link>
  got merged and fixed the issue with the documentation.
 		",33.0,0x0badc0de,2020-10-02T16:47:51Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38349>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38403,keithchugg,2020-04-09T19:52:33Z,2020-06-25T03:18:51Z,model.reset_states() does not work for bidirectional-RNNs in tf.keras,"
 System information
 
 
 Have I written custom code (as opposed to using a stock
 example script provided in TensorFlow): YES
 
 
 OS Platform and Distribution (e.g.,
 Linux Ubuntu 16.04): Ubuntu 18.04 LTS
 
 
 TensorFlow installed from (source or
 binary): binary
 
 
 TensorFlow version (use command below):
 TF 2.1
 and
 tf-nightly==2.2.0.dev20200407
 (both have bug around this issue, but different issues)
 
 
 Python version:  3.7.4
 
 
 CUDA/cuDNN version: 10.1, 7.6.5
 
 
 GPU model and memory: 2080ti, 11GB.  Bug is on both CPU/GPU.
 
 
 Describe the current behavior
 model.reset_states() does not work for bidirectional, stateful recurrent layers (bidi-RNNs).
 TF 2.1: model.reset_states() does nothing for stateful bidi-RNNs.
 tf-nightly: calling model.reset_states() for stateful bidi-RNNs causes a crash
 
 This was reported as a bug in TF 2.0 -- <denchmark-link:https://github.com/tensorflow/tensorflow/issues/34055>model.reset_states() does nothing for bidi-RNNs</denchmark-link>
 .  I thought this was fixed in tf-nightly at that time, but has returned in TF 2.1.
 model.reset_states() for standard RNNs changed in TF 2.1. has the following behavior:
 
 if model is stateful with NO initial state input: resets state to zero
 if model is stateful with initial state input: resets state to state input
 otherwise the state is carried over form the last call.
 
 Thus the expected behavior for stateful bidi-RNNs is:
 
 if model is stateful with NO initial state input: resets fwd and bwd state to zero
 if model is stateful with initial state input: resets fwd state to fwd state input and  resets bwd state to bwd state input
 otherwise the fwd state and bwd state are carried over form the last call (as is done in stateful bidi-RNNs).
 
 Standalone code to reproduce the issue
 Code to show this behavior with no state-inputs:
 import os
 os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
 os.environ['CUDA_VISIBLE_DEVICES']=''
 
 import numpy as np
 from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
 from tensorflow.keras.models import Model
 
 REC = LSTM
 
 sequence_length = 3
 feature_dim = 1
 features_in = Input(batch_shape=(1, sequence_length, feature_dim)) 
 
 rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in)
 stateless_model = Model(inputs=[features_in], outputs=[rnn_out])
 
 stateful_rnn_out = Bidirectional( REC(1, activation=None, use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in)
 stateful_model = Model(inputs=features_in, outputs=stateful_rnn_out)
 
 stateful_model.set_weights( stateless_model.get_weights() )
 
 x_in = np.random.normal(0,10,sequence_length)
 x_in = x_in.reshape( (1, sequence_length, feature_dim) )
 
 def print_bidi_out(non_stateful_out, stateful_out):
 	fb = ['FWD::', 'BWD::']
 
 	for i in range(2):
 		print(fb[i])
 		print(f'non_stateful: {non_stateful_out.T[i]}')
 		print(f'stateful: {stateful_out.T[i]}')
 		print(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')
 
 
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 print('\n** RESETING STATES in STATEFUL MODEL **\n')
 stateful_model.reset_states()
 non_stateful_out = stateless_model.predict(x_in).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(x_in).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 Code to demo with initial-state inputs:
 import os
 os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'
 os.environ['CUDA_VISIBLE_DEVICES']=''
 
 import numpy as np
 from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, LSTM, Bidirectional
 from tensorflow.keras.models import Model
 
 REC = LSTM
 
 sequence_length = 3
 feature_dim = 1
 features_in = Input(batch_shape=(1, sequence_length, feature_dim)) 
 state_h_fwd_in = Input(batch_shape=(1, 1))
 state_h_bwd_in = Input(batch_shape=(1, 1))
 state_c_fwd_in = Input(batch_shape=(1, 1))
 state_c_bwd_in = Input(batch_shape=(1, 1))
 
 four_state_shape = [state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]
 two_state_shape = [state_h_fwd_in, state_h_bwd_in]
 
 if REC == LSTM:
     rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=four_state_shape)
     stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=four_state_shape)
     rnn_inputs = [features_in, state_h_fwd_in, state_c_fwd_in, state_h_bwd_in, state_c_bwd_in]
 else:
     if REC == SimpleRNN:
         rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)
         stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)
     else:
         rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=False))(features_in, initial_state=two_state_shape)
         stateful_rnn_out = Bidirectional( REC(1, activation='linear', use_bias=False, return_sequences=True, return_state=False, stateful=True))(features_in, initial_state=two_state_shape)
     rnn_inputs = [features_in, state_h_fwd_in, state_h_bwd_in]
 
 stateless_model = Model(inputs=rnn_inputs, outputs=rnn_out)
 stateful_model = Model(inputs=rnn_inputs, outputs=stateful_rnn_out)
 
 
 # toy_weights = [np.asarray([[ 1.0]], dtype=np.float32), np.asarray([[0.5 ]], dtype=np.float32), np.asarray([[ -1.0 ]], dtype=np.float32), np.asarray([[ -0.5 ]], dtype=np.float32)]
 # stateless_model.set_weights(toy_weights)
 # stateful_model.set_weights(toy_weights)
 
 stateful_model.set_weights( stateless_model.get_weights() )
 
 stateful_model.save('temp_stateful.h5')
 stateless_model.save('temp_stateless.h5')
 
 x_in = np.random.normal(0,10,sequence_length)
 x_in = np.asarray([1,0,0])
 x_in = x_in.reshape( (1, sequence_length, feature_dim) )
 
 fwd_initial_h = np.asarray(2.75).reshape(1,1)
 fwd_initial_c = np.asarray(1.3).reshape(1,1)
 bwd_initial_h = np.asarray(-2.0).reshape(1,1)
 bwd_initial_c = np.asarray(-1.2).reshape(1,1)
 
 # fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
 # fwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
 # bwd_initial_h = np.asarray(np.random.normal(0,10)).reshape(1,1)
 # fwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)
 # bwd_initial_c = np.asarray(np.random.normal(0,10)).reshape(1,1)
 
 if REC == LSTM:
     rnn_input = [x_in, fwd_initial_h, fwd_initial_c, bwd_initial_h, bwd_initial_c]
 else:
     rnn_input = [x_in, fwd_initial_h, bwd_initial_h] 
     
 
 def print_bidi_out(non_stateful_out, stateful_out):
 	fb = ['FWD::', 'BWD::']
 
 	for i in range(2):
 		print(fb[i])
 		print(f'non_stateful: {non_stateful_out.T[i]}')
 		print(f'stateful: {stateful_out.T[i]}')
 		print(f'delta: {stateful_out.T[i]-non_stateful_out.T[i]}')
 
 non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 
 print('\n** RESETING STATES in STATEFUL MODEL **\n')
 stateful_model.reset_states()
 non_stateful_out = stateless_model.predict(rnn_input).reshape((sequence_length,2))
 stateful_out = stateful_model.predict(rnn_input).reshape((sequence_length,2))
 print_bidi_out(non_stateful_out, stateful_out)
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 sample output for a SimpleRNN with input states -- using TF 2.1:
 <denchmark-code>FWD::
 non_stateful: [7.375   3.6875  1.84375]
 stateful: [7.375   3.6875  1.84375]
 delta: [0. 0. 0.]
 BWD::
 non_stateful: [ 11.5 -25.   50. ]
 stateful: [ 11.5 -25.   50. ]
 delta: [0. 0. 0.]
 FWD::
 non_stateful: [7.375   3.6875  1.84375]
 stateful: [1.921875   0.9609375  0.48046875]
 delta: [-5.453125  -2.7265625 -1.3632812]
 BWD::
 non_stateful: [ 11.5 -25.   50. ]
 stateful: [-2.4375  2.875  -5.75  ]
 delta: [-13.9375  27.875  -55.75  ]
 
 ** RESETING STATES in STATEFUL MODEL **
 
 FWD::
 non_stateful: [7.375   3.6875  1.84375]
 stateful: [1.2402344 0.6201172 0.3100586]
 delta: [-6.1347656 -3.0673828 -1.5336914]
 BWD::
 non_stateful: [ 11.5 -25.   50. ]
 stateful: [-0.6953125 -0.609375   1.21875  ]
 delta: [-12.1953125  24.390625  -48.78125  ]
 </denchmark-code>
 
 Crash when using 4/7 tf-nightly:
 <denchmark-code>Traceback (most recent call last):
   File ""temp_bidi_state_in.py"", line 89, in <module>
     stateful_model.reset_states()
   File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 473, in reset_states
     layer.reset_states()
   File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/wrappers.py"", line 676, in reset_states
     self.forward_layer.reset_states()
   File ""/home/keith/.pyenv/versions/tfn/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 903, in reset_states
     spec_shape = nest.flatten(self.input_spec[0])[0].shape
 AttributeError: 'NoneType' object has no attribute 'shape'
 </denchmark-code>
 
 	",1.0,keithchugg,2020-04-13T08:00:59Z,"
 		Works without issues with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/4dcad65983174e0f482251cf99f534d0/38403.ipynb>TF v2.1</denchmark-link>
 . Was able to reproduce the issue with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/230e360bfceb18658b1c800e4a7fd6f6/38403-2-2.ipynb>TF v2.2.0-rc2</denchmark-link>
  and <denchmark-link:https://colab.research.google.com/gist/amahendrakar/65a2ce796d53b8b649918ee5bbf36cb3/38403-tf-nightly.ipynb>TF-nightly</denchmark-link>
 . Please find the attached gist. Thanks!
 		",2.0,keithchugg,2020-04-13T16:05:51Z,"
 		
 Works without issues with TF v2.1. Was able to reproduce the issue with TF v2.2.0-rc2 and TF-nightly. Please find the attached gist. Thanks!
 
 For TF 2.1, it is not crashing, but it is not resetting the states in the stateful bidi models.  The results after the state-reset should be the same for the stateful and nonstateful models.  The only case that is working correctly is the when there is no initial-state-input and TF 2.2rc or tf-nightly is used.  This is consistent with my original post.  Sorry, it is a bit detailed, but here is teh summary:
 
 TF 2.1: state reset for stateful models does nothing.  Should resets the state to zero if no initial state is input or back to the input initial state if it is provided (same as uni-directional RNNs).
 TF 2.2rc and tf-nightly: behavior is correct when no initial state is provided, but reset_state crashes when initial-state is input to stateful models.
 
 Fixing the 2.2/nightly behavior would be a great fix as 2.2 is on the way..  Thanks!!
 		",3.0,keithchugg,2020-04-13T17:39:46Z,"
 		Thanks for reporting the issue. Will take a closer look.
 		",60b167181081c14ff88c77ae62049cab8a5ba4c7,Scott Zhu,2020-04-14 10:15:01-07:00,MODIFY,1,tensorflow\python\keras\layers\recurrent.py,tensorflow\python\keras\layers\recurrent.py,1.0,"697,698,699,700,701,702,703,704","697,698,699,700",MODIFY,1.0,tensorflow\python\keras\layers\recurrent_test.py,tensorflow\python\keras\layers\recurrent_test.py,4.0,keithchugg,2020-06-25T00:02:13Z,"
 		<denchmark-link:https://github.com/keithchugg>@keithchugg</denchmark-link>
  I think this was resolved in recent  by <denchmark-link:https://github.com/qlzh727>@qlzh727</denchmark-link>
  . I checked it with  and cannot reproduce the issue. <denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/be5bbce7844f13ffc44fda50d392e2fb/38403-tf-nightly.ipynb>Here</denchmark-link>
  is the gist for your reference.
 Please verify once and close the issue, if this was resolved for you. Thanks!
 		",5.0,keithchugg,2020-06-25T03:18:50Z,"
 		Oh, thanks for the notice, I think this should be fixed already. I just forgot to close the github issue.
 		",6.0,keithchugg,2020-06-25T03:18:53Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38403>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38403>No</denchmark-link>
 
 		",1.0,"1378,1379,1380,1381,1383,1384,1385,1386",,test_full_input_spec,self,1367,1386,MODIFY,1.0,tensorflow\python\keras\layers\wrappers_test.py,tensorflow\python\keras\layers\wrappers_test.py,1.0,"1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259",,,,,,,,,__call__,"self,inputs,initial_state,constants,kwargs",654,718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_full_input_spec,self,1240,1259,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38459,siavash-khodadadeh,2020-04-11T16:59:53Z,2020-06-08T18:45:06Z,Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0,"
 System information
 
 Have I written custom code (as opposed to using a stock
 example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g.,
 Linux Ubuntu 16.04): Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if
 the issue happens on mobile device:  No
 TensorFlow installed from (source or
 binary): - TensorFlow version (use command below): 2.2.0-dev20200411
 Python version: 3.6.3
 Bazel
 version (if compiling from source):
 GCC/Compiler version (if compiling from
 source):
 CUDA/cuDNN version: 10.1- GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with: 1. TF 1.0:  2. TF 2.0: 
 Describe the current behavior
 When instantiating a batch norm layer like this:
 tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')
 I get the error:
 Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0
 Describe the expected behavior
 It is not always the expected behavior. Consider meta-learning for example. We are going to see just one batch of training data and we want to adapt all means and variances to this batch, this means the momentum should be zero.
 Then after applying a few training iterations, we evaluate on the same batch norm layer with training=False and that also should work fine.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>import tensorflow as tf
 import numpy as np
 
 inp = tf.keras.layers.Input(shape=(84, 84, 3))
 dense = tf.keras.layers.Conv2D(10, 3, activation=None)(inp)
 bn = tf.keras.layers.BatchNormalization(momentum=0.0, center=True, scale=False, name='bn1')(dense)
 rel = tf.keras.layers.ReLU()(bn)
 flat = tf.keras.layers.Flatten()(rel)
 out = tf.keras.layers.Dense(1, )(flat)
 model = tf.keras.models.Model(inputs=inp, outputs=out)
 
 model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())
 model.fit(x=np.random.uniform(size=(4, 84, 84, 3)), y=np.random.uniform(size=(4, 1)), epochs=1)
 model.evaluate(x=np.random.uniform(size=(3, 84, 84, 3)), y=np.random.uniform(size=(3, 1)))
 model.predict(x=np.random.uniform(size=(1, 84, 84, 3)))
 </denchmark-code>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,siavash-khodadadeh,2020-04-12T12:15:20Z,"
 		<denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
 , I have tried to reproduce issue. But I have faced no issue in this code. For you reference link of gist is <denchmark-link:https://gist.github.com/khimraj/25f451b7abaade36dd5390fdabb5a935>here</denchmark-link>
 .
 		",2.0,siavash-khodadadeh,2020-04-12T16:21:07Z,"
 		<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>
 , I guess you have to update to 2.2.0-dev20200412 (tf-nightly) and then this happens.
 In previous versions, this does not cause an error, and that is why I think it might be a bug.
 		",3.0,siavash-khodadadeh,2020-04-12T16:34:01Z,"
 		<denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
 , I have used tensorflow version 2.2.0-rc2.
 		",3cfba9571bcc4be237bfdfa3498c66073ae59280,Scott Zhu,2020-06-08 11:36:54-07:00,MODIFY,0,tensorflow\python\ops\nn_impl.py,tensorflow\python\ops\nn_impl.py,0.0,"1618,1619,1620,1621,1622","1618,1619,1620,1621,1622,1623,1624,1625,1626,1627",,,,,4.0,siavash-khodadadeh,2020-04-12T16:49:44Z,"
 		<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>
  Sorry, I am not very familiar with versioning. Is rc2 newer than nightly build?
 I mean if this is not resolved, will this be the behavior in TF 2.2.1 or TF 2.3.x?
 		",5.0,siavash-khodadadeh,2020-04-13T12:34:11Z,"
 		Was able to run the code without any issues on <denchmark-link:https://colab.research.google.com/gist/amahendrakar/2ad5bccc80083753db0f6657c83ef1ab/38459.ipynb>TF v2.2.0-rc2</denchmark-link>
 .
 Facing an error stating , while running on <denchmark-link:https://colab.research.google.com/gist/amahendrakar/e85b2e37339e4c478b666b53edab264f/38459-tf-nightly.ipynb#scrollTo=aDTL7bbKf82y>TF-nightly</denchmark-link>
 . Please find the attached gist. Thanks!
 		",6.0,siavash-khodadadeh,2020-04-13T15:00:19Z,"
 		Hi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
 , <denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>
   , I think  tf-nightly-2.2.0-dev20200412 is the latest build(12/04/2020) than tf-2.2.0-rc2(28/03/2020) . Generally it should not throw an error since if is_training is true  that implies whitening is done on the same input batch rather than over the moving average statistic of the set.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,siavash-khodadadeh,2020-04-13T15:14:22Z,"
 		Hello <denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>
 , Sorry, I did not get what you mean by whitening. I checked and when training is False, there is no issue. The only problem is when is_training is True and if I understand you correctly, this is a bug and should be fixed.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,siavash-khodadadeh,2020-04-13T15:33:22Z,"
 		Hi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
  ,by whitening I mean using the mean as 0 and sd/variance as 1 before passing an input vector xi {1,2...m} through an activation unit .I think according to the implementation there is mention of a batch normalised vector value X = x-E[x]/sqrt(var(x) + epsilon) where E[x] is the expectation and epsilon is just added to prevent division by 0. There are other parameters gamma and beta which govern the gradient step (scale and shift rule).During training as false there is a moving average over the input vector of previous layers which is ""whitened"" by applying the above metric and then passed into the activation units  - relu sigmoid etc of the current layer. Yes you are correct this should not be occurring if is_training is true because in that case there is no need of such moving statistic over previous layers,input  batch of current state is whitened and used.If mean and variance are none in this case(training=true) then it is not possible to determine the normalised value which is to be passed into the activation unit.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,siavash-khodadadeh,2020-04-13T15:45:12Z,"
 		<denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>
  Thank you for your explanation.
 		",10.0,siavash-khodadadeh,2020-04-13T15:57:12Z,"
 		Sure <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
  , glad I could help.
 		",11.0,siavash-khodadadeh,2020-04-16T13:49:46Z,"
 		Hi <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
  , <denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>
  ,<denchmark-link:https://github.com/khimraj>@khimraj</denchmark-link>
  ,<denchmark-link:https://github.com/amahendrakar>@amahendrakar</denchmark-link>
  in the latest nightly release- tf-nightly 2.2.0.dev20200416 the issue is present. I found that according to this commit :<denchmark-link:https://github.com/tensorflow/tensorflow/commit/84f2ec1d60b5bb14a59ccef8f8fa7eb5a1096e8f#diff-ef8609a43751227afcaacc838670a96f>84f2ec1#diff-ef8609a43751227afcaacc838670a96f</denchmark-link>
     on  <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py</denchmark-link>
   ,there is mention that if exponential_avg_factor==1.0and is _training ==true ,then the mean and variance should be none. By comparing commits , I found that since is_training  considers current batch inputs,by setting the exponential average to 1 would imply effective mean =0 and variance=0 (mean=(1-exponential_average_factor)exponential_average_factor) and same for variance). However, I think this requires further analysis since  momentum can be 0 on a current input batch and this should not raise exception on mean and variance values since it only belongs to the current batch.
 		",12.0,siavash-khodadadeh,2020-04-16T14:12:32Z,"
 		<denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>
 
 I wanted to ask a question. When we set the momentum to zero during training=True, we expect the moving mean and moving variance to be updated based on just the latest batch the model sees. Is that correct?
 		",13.0,siavash-khodadadeh,2020-04-16T14:53:55Z,"
 		Yes <denchmark-link:https://github.com/siavash-khodadadeh>@siavash-khodadadeh</denchmark-link>
  , that is correct .
 		",14.0,siavash-khodadadeh,2020-04-16T15:12:43Z,"
 		Thank you for your response, <denchmark-link:https://github.com/abhilash1910>@abhilash1910</denchmark-link>
 . Just wanted to make sure that I understood it accurately.
 		",15.0,siavash-khodadadeh,2020-06-08T18:45:07Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38459>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38516,phemmer,2020-04-14T02:12:53Z,2020-04-15T20:47:00Z,Cannot use set_visible_devices with mixed_precision,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): kind of. Combination of 2 example scripts
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux - Fedora 31
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or
 binary): source
 TensorFlow version (use command below): v2.2.0-rc3-0-gaad398b5e9 2.2.0-rc3
 Python version: 3.7.6
 Bazel version (if compiling from source): 2.0.0
 GCC/Compiler version (if compiling from source): gcc (GCC) 9.2.1 20190827 (Red Hat 9.2.1-1)
 CUDA/cuDNN version: CUDA 10.2, cuDNN 7.6.5.33
 GPU model and memory: 2x GeForce RTX 2080 Ti 12gb
 
 Describe the current behavior
 When attempting to use tf.config.set_visible_devices() in conjunction with tf.python.keras.mixed_precision.experimental.policy.set_policy(), the Tensorflow errors with:
 <denchmark-code>RuntimeError: TensorFlow device (GPU:0) is being mapped to multiple CUDA devices (0 now, and 1 previously), which is not supported. This may be the result of providing different GPU configurations (ConfigProto.gpu_options, for example different visible_device_list) when creating multiple Sessions in the same process. This is not  currently supported, see https://github.com/tensorflow/tensorflow/issues/19083
 </denchmark-code>
 
 Describe the expected behavior
 No error
 Standalone code to reproduce the issue
 <denchmark-code>import tensorflow as tf
 devices = tf.config.list_physical_devices('GPU')
 tf.config.set_visible_devices(devices[1:], 'GPU')
 
 from tensorflow.python.keras.mixed_precision.experimental import policy as mixed_precision
 mixed_precision.set_policy(mixed_precision.Policy('mixed_float16'))
 </denchmark-code>
 
 	",1.0,phemmer,2020-04-14T04:43:34Z,"
 		<denchmark-link:https://github.com/phemmer>@phemmer</denchmark-link>
 , I have run this code on google colab GPU but i don't found any issue. For your reference link of gist is <denchmark-link:https://gist.github.com/khimraj/262dbd66a71c266c81f17249f7ba67fe>here</denchmark-link>
 .
 		",2.0,phemmer,2020-04-14T04:49:59Z,"
 		And google collab gives you multiple GPUs? I don't know what this will do if you only have 1 GPU
 		",3.0,phemmer,2020-04-14T22:28:09Z,"
 		Ugh I thought I fixed this in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/10e5748ddfcf0c60e5ef0a90bb72a34bc55190ec>10e5748</denchmark-link>
  but didn't. I still call  even though I intended not to call it.
 I will fix, but I'm unsure if this can be cherrypicked into TF 2.2 this late in the process.
 As a hacky workaround, you can fix by adding the following lines of code anywhere before setting the policy.
 <denchmark-code>if tf.__version__.startswith('2.2.'):
   from tensorflow.python.keras.mixed_precision.experimental import device_compatibility_check
   device_compatibility_check.log_device_compatibility_check = lambda policy_name, skip_local: None
 </denchmark-code>
 
 This replaces the problematic function with one that does nothing. The function normally just logs some info. Due to this bug, the function fails if list_physical_devices is called.
 		",2730e4b0bcba80799ddc10f52081927848540f30,Reed Wanderman-Milne,2020-04-15 13:44:29-07:00,MODIFY,1,tensorflow\python\keras\mixed_precision\experimental\device_compatibility_check.py,tensorflow\python\keras\mixed_precision\experimental\device_compatibility_check.py,1.0,158,157,,,,,4.0,phemmer,2020-04-15T20:47:01Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38516>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38516>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log_device_compatibility_check,"policy_name,skip_local",137,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38640,shiningrain,2020-04-17T12:59:04Z,2020-04-27T20:07:51Z,"K.cast_to_floatx() will convert ""None"" to ""Nan"" and lead the ReLU to Nan output.","
 System information
 
 Have I written custom code (as opposed to using example directory):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Linux Ubuntu 18.04
 TensorFlow backend (yes / no):  yes
 TensorFlow version:  1.14.0-cpu
 Python version:  3.6.9
 CUDA/cuDNN version:  -
 GPU model and memory:  -
 
 Describe the current behavior
 I found that if I used ReLU(threshold = None) in tensorflow.keras, without any errors or warnings, Tensorflow will return a matrix with Nan.  (Detailed configuration and codes for reproduction can be found in the following part) .
 For this reason, I did some investigations and found that when the parameters in ReLU are passed to /tensorflow/python/keras/layers/advanced_activations.py line near line 311, K.cast_to_floatx()  will incorrectly convert the ""None"" parameter to ""Nan"" and pass it to the backend for calculation (refer to Figure 1 and Figure 2).
 ""Nan"" and ""None"" should have different meanings, but K.cast_to_floatx did not distinguish between ""Nan"" and ""None"" during the calculation, which led to the usage of a ""Nan"" parameter in the tensorflow calculation. This further affects the final output result and makes the output with  ""Nan"". This operation may confuse the users.
 Is there a difference in meaning between None and Nan in the implementation of K.cast_to_floatx?  Judging from the current results, their meanings are different.   This issue not only affect ReLU, but also affect ThresholdReLU, LeakyReLU and other operations using K.cast_to_floatx() to convert the parameters.
 <denchmark-h:h2>Code to reproduce the issue</denchmark-h>
 
 <denchmark-code>import os
 import numpy as np
 import tensorflow as tf
 import tensorflow.keras.layers as L
 from tensorflow.keras.models import load_model
 
 
 root_path = ""./Your Path""
 layer_name=""ReLU""
 kwargs={'max_value': 0.5761369157060329, 'negative_slope': 0.7845179761191806, 'threshold': None}
 input= (10 * np.random.randn(1,32,32,16)).astype(np.float32)
 from tensorflow.keras import Model, Input
 layer_cls = getattr(L, layer_name)
 layer = layer_cls(**kwargs)
 x = Input(batch_shape=input.shape)
 y = layer(x)
 bk_model =Model(x, y)
 model_path = os.path.join(root_path, 'model.h5')
 bk_model.save(model_path, bk_model)
 model = load_model(model_path)     
 output = model.predict(input)
 nanresult=np.isnan(output).any()
 print(nanresult)
 </denchmark-code>
 
 	",1.0,shiningrain,2020-04-17T15:15:59Z,"
 		<denchmark-link:https://github.com/shiningrain>@shiningrain</denchmark-link>
 
 i ran the code shared and face <denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/673d2c21fcf02091f8243c5ac51f72a9/38640.ipynb>this error</denchmark-link>
 
 		",2.0,shiningrain,2020-04-17T15:32:52Z,"
 		
 @shiningrain
 i ran the code shared and face this error
 
 Hello! Thanks for your reply!
 The ""root path"" in my code is ""./ Your Path"", this means that you can set any path you want to save the following model... You can also delete it and just save the model in the current dir. You can also use this code:
 <denchmark-code>import os
 import numpy as np
 import tensorflow as tf
 import tensorflow.keras.layers as L
 from tensorflow.keras.models import load_model
 
 
 layer_name=""ReLU""
 kwargs={'max_value': 0.5761369157060329, 'negative_slope': 0.7845179761191806, 'threshold': None}
 input= (10 * np.random.randn(1,32,32,16)).astype(np.float32)
 from tensorflow.keras import Model, Input
 layer_cls = getattr(L, layer_name)
 layer = layer_cls(**kwargs)
 x = Input(batch_shape=input.shape)
 y = layer(x)
 bk_model =Model(x, y)
 bk_model.save(""model.h5"", bk_model)
 model = load_model(""model.h5"")     
 output = model.predict(input)
 nanresult=np.isnan(output).any()
 print(nanresult)
 </denchmark-code>
 
 As the following picture shown, the nanresult is True, which means there is nan in output
 <denchmark-link:https://user-images.githubusercontent.com/46860123/79586408-a452db80-8103-11ea-85f7-9b1e0310ef3f.png></denchmark-link>
 
 		",3.0,shiningrain,2020-04-18T07:53:10Z,"
 		Hello, <denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
   I can also reproduce this issue in Tensorflow 2.1.0-cpu version, shown in the following figure. You can use the code in my reply to reproduce it.
 <denchmark-link:https://user-images.githubusercontent.com/46860123/79631518-6ea60500-818c-11ea-9493-221ccd343132.png></denchmark-link>
 
 		",3db8df8ffafe5bcd83a12b92bc4c8287cd80237f,A. Unique TensorFlower,2020-04-27 13:01:35-07:00,MODIFY,1,tensorflow\python\keras\layers\advanced_activations.py,tensorflow\python\keras\layers\advanced_activations.py,1.0,"352,353,354",,,,,,4.0,shiningrain,2020-04-20T15:31:18Z,"
 		i am able to replicate this issue, please find the <denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/bd0e794e3005cee01640dca7926e946f/untitled141.ipynb>gist here</denchmark-link>
 
 		",5.0,shiningrain,2020-04-20T15:51:34Z,"
 		
 i am able to replicate this issue, please find the gist here
 
 Thanks for your reply! That is exactly what I want to show.
 K.cast_to_floatx() will convert None to nan, which are not the same things. And then it leads to nan output.
 		",6.0,shiningrain,2020-04-26T22:51:28Z,"
 		The none to nan conversion is due to numpy behavior.
 
 
 
 tensorflow/tensorflow/python/keras/backend.py
 
 
          Line 183
       in
       e5bf8de
 
 
 
 
 
 
  return np.asarray(x, dtype=floatx()) 
 
 
 
 
 
 import numpy as np
 np.asarray(None, dtype=float)
 output:
 array(nan)
 However the documentation says  . Perhaps we can add check to pass a float value to threshold argument.
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#arguments_2>https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#arguments_2</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,__init__,"self,max_value,negative_slope,threshold,kwargs",344,361,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,shiningrain,2020-04-27T20:07:52Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38640>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38640>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,shiningrain,2020-04-28T04:06:33Z,"
 		
 The none to nan conversion is due to numpy behavior.
 
 
 
 tensorflow/tensorflow/python/keras/backend.py
 
 
          Line 183
       in
       e5bf8de
 
 
 
 
 
 
  return np.asarray(x, dtype=floatx()) 
 
 
 
 
 
 import numpy as np
 np.asarray(None, dtype=float)
 output:
 array(nan)
 However the documentation says threshold: Float . Perhaps we can add check to pass a float value to threshold argument.
 https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU#arguments_2
 
 That's a good idea! Thanks for your fix.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38906,jakublipinski,2020-04-26T08:42:01Z,2020-06-05T15:53:21Z,MemoryOptimizer produces broken graph with AlreadyExistsError exception while running GRU layer on Tensorflow 2.2.0rc_3,"
 System information
 
 Custom model built using keras
 MacBook Pro, 8-Core Intel Core i9, macOS Catalina 10.15.4
 TensorFlow installed from pip in virtual environment
 TensorFlow v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3
 Python 3.7.5
 Running on CPU
 
 Describe the current behavior
 The code snippet listed below outputs multiple tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource warnings and finally exists with tensorflow.python.framework.errors_impl.AlreadyExistsError exception.
 Note: the code works correctly if the GRU layer size is decreased from 320 to 80. It also works if TensorFlow is downgraded to version 2.0.1.
 The issue is related to <denchmark-link:https://github.com/tensorflow/tensorflow/issues/23780>#23780</denchmark-link>
  issue reported in 2018. This issue offers code to reproduce it and occurs on the latest version of TensorFlow.
 Describe the expected behavior
 The code should work without exception.
 Standalone code to reproduce the issue
 import numpy as np
 
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense, Flatten, Bidirectional, GRU
 from tensorflow.keras.layers import Conv1D, MaxPooling1D
 
 x = np.random.rand(1000, 401, 17)
 y = np.random.choice([0, 1], size=(1000, 301))
 
 model = Sequential()
 model.add(Conv1D(filters=320, kernel_size=26, activation='relu', input_shape=(401, x.shape[2])))
 model.add(MaxPooling1D(pool_size=13, strides=13))
 model.add(Bidirectional(GRU(320, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))
 model.add(Flatten())
 model.add(Dense(2000, activation=""relu""))
 model.add(Dense(301, activation=""sigmoid""))
 model.compile(loss=""binary_crossentropy"", optimizer=""rmsprop"", metrics=[""accuracy""])
 
 model.summary()
 
 model.fit(x=x, y=y, epochs=1, verbose=1)
 The Google Colab notebook is available <denchmark-link:https://colab.research.google.com/drive/1YohTA6Hxi3H6aOQgFj34C0tKErW2V_rL>here</denchmark-link>
 . The error is reproducible.
 Other info / logs
 The code above generates following output:
 <denchmark-code>Model: ""sequential""
 _________________________________________________________________
 Layer (type)                 Output Shape              Param #   
 =================================================================
 conv1d (Conv1D)              (None, 376, 320)          141760    
 _________________________________________________________________
 max_pooling1d (MaxPooling1D) (None, 28, 320)           0         
 _________________________________________________________________
 bidirectional (Bidirectional (None, 28, 640)           1232640   
 _________________________________________________________________
 flatten (Flatten)            (None, 17920)             0         
 _________________________________________________________________
 dense (Dense)                (None, 2000)              35842000  
 _________________________________________________________________
 dense_1 (Dense)              (None, 301)               602301    
 =================================================================
 Total params: 37,818,701
 Trainable params: 37,818,701
 Non-trainable params: 0
 _________________________________________________________________
 2020-04-26 10:19:57.349570: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
 2020-04-26 10:19:57.363399: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
 2020-04-26 10:19:57.377361: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
 ... (repeated multiple times) ...
 2020-04-26 10:19:57.677304: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_step_0/gradient_tape/sequential/bidirectional/forward_gru/while/sequential/bidirectional/forward_gru/while_grad/body/_577/gradients/AddN_7/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
 Traceback (most recent call last):
   File ""alreadyexists_err.py"", line 21, in <module>
     model.fit(x=x, y=y, epochs=1, verbose=1)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
     return method(self, *args, **kwargs)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 851, in fit
     tmp_logs = train_function(iterator)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
     result = self._call(*args, **kwds)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 644, in _call
     return self._stateless_fn(*args, **kwds)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2420, in __call__
     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1665, in _filtered_call
     self.captured_inputs)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1746, in _call_flat
     ctx, args, cancellation_manager=cancellation_manager))
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 598, in call
     ctx=ctx)
   File ""venv/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
     inputs, attrs, num_outputs)
 tensorflow.python.framework.errors_impl.AlreadyExistsError:  Resource __per_step_0/gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
 	 [[{{node gradient_tape/sequential/bidirectional/backward_gru/while/sequential/bidirectional/backward_gru/while_grad/body/_877/gradients/AddN_8/tmp_var}}]] [Op:__inference_train_function_7551]
 
 Function call stack:
 train_function
 </denchmark-code>
 
 	",1.0,jakublipinski,2020-04-27T09:46:15Z,"
 		I have tried on colab with TF version 2.2.0-rc3 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/8652e1dae44721b792b47b6e9e45052c/untitled828.ipynb>here</denchmark-link>
 .Thanks!
 		",2.0,jakublipinski,2020-04-27T16:11:55Z,"
 		Adding <denchmark-link:https://github.com/saxenasaurabh>@saxenasaurabh</denchmark-link>
  who works on core ops to provide more information about while loop.
 		",3.0,jakublipinski,2020-04-27T16:26:33Z,"
 		<denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  mentioned <denchmark-link:https://github.com/tensorflow/tensorflow/issues/23780#issuecomment-498334346>here</denchmark-link>
  that this may be due to a bug in grappler generating non-unique names. <denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  <denchmark-link:https://github.com/ezhulenev>@ezhulenev</denchmark-link>
  do you know if this was fixed?
 		",80a93674eafc224a45cbe96c65e993e9735634a3,Eugene Zhulenev,2020-06-04 09:42:10-07:00,MODIFY,3,tensorflow\core\kernels\variable_ops.cc,tensorflow\core\kernels\variable_ops.cc,1.0,"32,33,34,35,36,37,38,39,40",,,,,,4.0,jakublipinski,2020-04-28T11:05:30Z,"
 		I dug a bit further into this issue. This is what I found:
 It's indeed an optimizer issue, namely the ScopedAllocatorOptimizer optimizer.
 Interestingly, you cannot turn this optimizer off using tf.config.optimizer.set_experimental_options(). The scoped_allocator_optimization key is treated as Bool at:
 
 
 
 tensorflow/tensorflow/python/eager/context.py
 
 
          Line 958
       in
       109fd38
 
 
 
 
 
 
  rewriter_toggle(""scoped_allocator_optimization"") 
 
 
 
 
 
 while it's interpreted as int at:
 
 
 
 tensorflow/tensorflow/core/grappler/optimizers/meta_optimizer.cc
 
 
          Line 292
       in
       8e0eecc
 
 
 
 
 
 
  if (cfg_.scoped_allocator_optimization()) { 
 
 
 
 
 
 Should I file another issue for that?
 		",5.0,jakublipinski,2020-04-29T16:08:15Z,"
 		Let me rename this bug to pointing to ScopedAllocatorOptimizer.
 		",6.0,jakublipinski,2020-06-05T15:53:21Z,"
 		Verified this is fixed with latest tf-nightly.
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::TemporaryVariableName,"var_name,control_frame",32,40,1.0,"110,114,119",,tensorflow::TemporaryVariableOp::Compute,context,106,125,1.0,"163,166",,tensorflow::DestroyTemporaryVariableOp::Compute,context,154,171,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,jakublipinski,2020-06-05T15:53:23Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38906>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38932,galah92,2020-04-27T07:09:13Z,2020-05-21T22:21:53Z,New TFLiteConverter not working with tf.complex64,"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab CPU
 TensorFlow version (use command below): 2.2.0-rc3
 
 Describe the current behavior
 New TFLiteConverter not working with tf.complex64
 Disabling the new converter (converter.experimental_new_converter = False) works.
 Standalone code to reproduce the issue
 @tf.function
 def foo(x, y):
     return x @ y
 
 
 x = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.complex64)
 y = tf.constant([[2, 3], [4, 5], [6, 7]], dtype=tf.complex64)
 
 foo_concrete = foo.get_concrete_function(x, y)
 converter = tf.lite.TFLiteConverter.from_concrete_functions([foo_concrete])
 foo_tflite = converter.convert()
 <denchmark-link:https://colab.research.google.com/drive/1snYjxYYJaOYPQOeQ1ec2o0xPHQB0ILY2>colab example</denchmark-link>
 
 Thanks
 	",1.0,galah92,2020-04-28T01:13:34Z,"
 		<denchmark-link:https://github.com/karimnosseir>@karimnosseir</denchmark-link>
  ,could you take a look? thanks!
 		",2.0,galah92,2020-05-16T10:51:56Z,"
 		Any update on this?
 		",3.0,galah92,2020-05-21T22:18:35Z,"
 		Sorry for the delay.
 I have a fix that should get merged soon.
 Note: that FullyConnected op in TFLite doesn't support complex, so you will need to enable TF_SELECT_OPS to run it.
 		",85c637969a25228065a276044691dab020984361,Karim Nosir,2020-05-21 15:20:53-07:00,MODIFY,1,tensorflow\compiler\mlir\lite\python\tf_tfl_flatbuffer_helpers.cc,tensorflow\compiler\mlir\lite\python\tf_tfl_flatbuffer_helpers.cc,1.0,"124,125",,,,,,4.0,galah92,2020-05-21T22:21:54Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38932>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/38932>No</denchmark-link>
 
 		",5.0,galah92,2020-06-03T14:39:07Z,"
 		<denchmark-link:https://github.com/karimnosseir>@karimnosseir</denchmark-link>
  Thanks! Are there plans to support complex for FullyConnected op in TFLite?
 		",6.0,galah92,2020-06-08T21:42:19Z,"
 		<denchmark-link:https://github.com/galah92>@galah92</denchmark-link>
  Sorry for the delay.
 No immediate plans.
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::internal::ConvertIODataTypeToDataType,dtype,104,129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39075,hartikainen,2020-05-01T09:54:27Z,2020-05-05T19:06:12Z,ForwardAccumulator fails with `experimental_run_functions_eagerly(True)`,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below):  2.2.0rc4
 Python version: 3.7.5
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 
 
 Running the examples in <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator>tf.ForwardAccumulator docs</denchmark-link>
  fail with  when running with .
 
 Running the examples in <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator>tf.ForwardAccumulator docs</denchmark-link>
  with  work the same way as when running with .
 
 This is the standard example from <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator>https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator</denchmark-link>
 , with just the  call added.
 import tensorflow as tf
 
 tf.config.experimental_run_functions_eagerly(True)
 
 
 v = tf.Variable([1., 2.])
 with tf.autodiff.ForwardAccumulator(
     v,
     # The ""vector"" in Hessian-vector product.
     tf.constant([1., 0.])) as acc:
   with tf.GradientTape() as tape:
     y = tf.reduce_sum(v ** 3.)
   backward = tape.gradient(y, v)
 backward  # gradient from backprop
 
 acc.jvp(backward)  # forward-over-backward Hessian-vector product
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 <denchmark-code>...
     self._push_tape()
   File ""/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 849, in _push_tape
     watch_accessed_variables=self._watch_accessed_variables)
   File ""/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/tape.py"", line 48, in push_new_tape
     return Tape(tape)
 RecursionError: maximum recursion depth exceeded
 </denchmark-code>
 
 	",1.0,hartikainen,2020-05-04T06:08:10Z,"
 		<denchmark-link:https://github.com/hartikainen>@hartikainen</denchmark-link>
 
 I have tried in colab with TF 2.1.0, 2.2-rc4 and i am able to reproduce the issue.With  i am able to reproduce the issue.However with i am not seeing any issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/6a9e8b37f8f59badd116f25c9de947f9/untitled847.ipynb>here</denchmark-link>
 .Thanks!
 		",2.0,hartikainen,2020-05-04T09:07:32Z,"
 		Yep, that's what I see too: fails with tf.config.experimental_run_functions_eagerly(True) and works with tf.config.experimental_run_functions_eagerly(False). Sorry if that was not clear from the title and description.
 		",3.0,hartikainen,2020-05-05T18:44:55Z,"
 		Thank you for the report. I'll opt that forwardprop utility function out of run_functions_eagerly. The change should land in a few hours.
 		",3e6697b916c9e775dc61375b913d21ba9d22126f,Allen Lavoie,2020-05-05 12:05:03-07:00,MODIFY,0,tensorflow\python\eager\forwardprop.py,tensorflow\python\eager\forwardprop.py,0.0,"28,148,149,150,151,152,153,154,156","26,148,150",MODIFY,1.0,tensorflow\python\eager\forwardprop_test.py,tensorflow\python\eager\forwardprop_test.py,4.0,hartikainen,2020-05-05T19:06:13Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39075>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39075>No</denchmark-link>
 
 		",,,,,,,,,1.0,"238,239,240,241,242,243,244,245,246,247",,testRunFunctionsEagerly,self,238,247,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39186,grofte,2020-05-05T13:38:20Z,2020-05-08T00:05:00Z,tf.data.experimental.make_csv_dataset modifies mutable variables passed to it,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 TensorFlow installed from (source or binary): docker
 TensorFlow version (use command below): TF 2.1.0
 
 Describe the current behavior
 tf.data.experimental.make_csv_dataset modifies passed variables in place. So if you call
 
 the variable  is changed. It's in line 463 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/data/experimental/ops/readers.py#L463>https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/data/experimental/ops/readers.py#L463</denchmark-link>
  but it may happen to other variables passed. Specifically the list sent to  is replaced by a list of the indices of those columns in the file to read.
 Describe the expected behavior
 A function should never modify the mutable objects passed to it. This is only ever appropriate for methods of a class.
 	",1.0,grofte,2020-05-06T05:00:41Z,"
 		<denchmark-link:https://github.com/grofte>@grofte</denchmark-link>
 
 Please provide with simple stand alone code for us to replicate the issue faced.
 		",2.0,grofte,2020-05-06T11:15:15Z,"
 		import tensorflow as tf
 import pandas as pd
 
 df = pd.DataFrame({'a': [2, 3, 5], 'b': [3, 4, 6], 'c': [4, 5, 7]})
 df.to_csv('tf.csv', index=False)
 columns_to_use = ['b', 'c']
 print(columns_to_use)
 ds = tf.data.experimental.make_csv_dataset('tf.csv', 
                                             batch_size=1, 
                                             select_columns=columns_to_use)
 print(columns_to_use)
 ['b', 'c']
 [1, 2]
 		",3.0,grofte,2020-05-06T11:38:51Z,"
 		I am able to replicate this issue please find the <denchmark-link:https://colab.sandbox.google.com/gist/Saduf2019/e52ece5e44c25385b36a7e6288063ece/untitled166.ipynb>gist here</denchmark-link>
 
 		",79acb0824b8bbb1fb887d2ff625f2f170d80fe1f,Rachel Lim,2020-05-07 16:54:35-07:00,MODIFY,2,tensorflow\python\data\experimental\kernel_tests\csv_dataset_test.py,tensorflow\python\data\experimental\kernel_tests\csv_dataset_test.py,1.0,"44,46","44,46",MODIFY,1.0,tensorflow\python\data\experimental\ops\readers.py,tensorflow\python\data\experimental\ops\readers.py,4.0,grofte,2020-05-06T19:29:30Z,"
 		<denchmark-link:https://github.com/rachellim>@rachellim</denchmark-link>
  could you please take a look? thanks
 		",5.0,grofte,2020-05-07T22:39:25Z,"
 		thanks for catching this <denchmark-link:https://github.com/grofte>@grofte</denchmark-link>
  ! fix incoming.
 		",6.0,grofte,2020-05-08T00:05:01Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39186>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39186>No</denchmark-link>
 
 		",1.0,"186,187,188,189,195,196,197,198,202,203,206,207,209","186,192,193,197,200,201,203",_get_sorted_col_indices,"select_columns,column_names",182,209,,,,,,,,,,,,,,,_setup_files,"self,inputs,linebreak,compression_type",42,60,1.0,"583,584,585,586,587,588,589",,testCsvDataset_immutableParams,self,583,589,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39222,naturomics,2020-05-06T10:42:16Z,2020-05-07T16:18:58Z,Typos in source code docs,"
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/fedc6d951faa73936a1154d6507d54240614d416/tensorflow/python/eager/backprop.py#L532>Here</denchmark-link>
  a minor typo in the source code:  ==> 
 	",1.0,naturomics,2020-05-06T23:45:15Z,"
 		Thanks for the issue. I have sent a fix for this.
 		",2.0,naturomics,2020-05-07T16:18:58Z,"
 		Closing as fixed by <denchmark-link:https://github.com/tensorflow/tensorflow/commit/fe972004ab02ff454749bea5780e70d4a4633c3a>fe97200</denchmark-link>
 
 		",,,,,fe972004ab02ff454749bea5780e70d4a4633c3a,A. Unique TensorFlower,2020-05-06 16:51:31-07:00,MODIFY,1,tensorflow\python\eager\backprop.py,tensorflow\python\eager\backprop.py,1.0,532,532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,make_vjp,"f,params,persistent",502,571,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39462,ben-arnao,2020-05-12T15:20:32Z,2020-11-13T04:47:47Z,ReduceLROnPlateau keeps executing lr reduction block of code after min_lr has been reached,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
 TensorFlow installed from (source or binary): through pip
 TensorFlow version (use command below): 2.2
 Python version: 3.7
 Bazel version (if compiling from source): n/a
 GCC/Compiler version (if compiling from source): n/a
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 ReduceLR will execute the part of the code that reduces LR even if lr ""equals"" min_lr. While this is not a problem if we are just dealing with LR since the value will technically never go below min_lr, it is an issue if you are trying to do any other execution in this block of code (ie. weight decay).
 Given that there is a min check for this block of code, i assume it was not intended for us to enter this block of code if min_lr has already been achieved. Either get rid of the min check if we don't care about re-executing this code for no reason, or make sure this block of code is not executed if lr is at min-lr.
 This issue clearly seems to be a round/precision related issue where
 we set our lr K.set_value(self.model.optimizer.lr, new_lr)
 But then next iteration we fetch the lr
 old_lr = float(K.get_value(self.model.optimizer.lr))
 and get a slightly different value so that
 if old_lr > self.min_lr:
 does not work.
 So yes in a way this does not affect default code, and it only matter if we do custom code, but i still think this is a bug that really shouldn't happen.
 Describe the expected behavior
 Code block that is only supposed to execute if lr does not equal min_lr does not execute if lr equals min_lr
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>x = np.random.normal(size=10)
 y = np.random.normal(size=10)
 
 model = Sequential()
 model.add(Dense(1))
 
 reduce_lr = ReduceLROnPlateau(monitor='loss',
                               min_delta=0.01,
                               patience=3,
                               min_lr=0.001,
                               verbose=2)
 
 model.compile(loss='mse', optimizer=SGD(0.01))
 
 history = model.fit(x,
                     y,
                     epochs=25,
                     verbose=2,
                     shuffle=True,
                     batch_size=1,
                     callbacks=[reduce_lr])
 </denchmark-code>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,ben-arnao,2020-05-13T07:48:15Z,"
 		<denchmark-link:https://github.com/ben-arnao>@ben-arnao</denchmark-link>
 
 I have tried in colab with TF version 2.2 and i am seeing the error message ().Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/0b2846d4aec8d371afb67e3da83caf17/untitled885.ipynb>here.</denchmark-link>
 .Thanks!
 		",2.0,ben-arnao,2020-05-18T14:34:31Z,"
 		<denchmark-link:https://github.com/ravikyram>@ravikyram</denchmark-link>
 
 I'm unsure of the way the imports were done in your colab but the code below works and does reproduce this behavior for me...
 <denchmark-code>import numpy as np
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Dense
 from tensorflow.keras.callbacks import ReduceLROnPlateau
 from tensorflow.keras.optimizers import SGD
 
 x = np.random.normal(size=10)
 y = np.random.normal(size=10)
 
 model = Sequential([Dense(1)])
 
 
 reduce_lr = ReduceLROnPlateau(monitor='loss',
                               min_delta=0.01,
                               patience=3,
                               min_lr=0.001,
                               verbose=2)
 
 model.compile(loss='mse', optimizer=SGD(0.01))
 
 history = model.fit(x,
                     y,
                     epochs=50,
                     verbose=2,
                     shuffle=True,
                     batch_size=1,
                     callbacks=[reduce_lr])
 </denchmark-code>
 
 Please see <denchmark-link:https://colab.research.google.com/gist/ben-arnao/a241db4603230a60c4425c1bb9852d95/untitled885.ipynb>here</denchmark-link>
 
 		",3.0,ben-arnao,2020-05-19T07:33:45Z,"
 		I have tried in colab with TF version 2.2.0 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/3201a4bdecc82391871b4344c79baed2/untitled906.ipynb>here</denchmark-link>
 . Thanks!
 		",b3461c38fd09abc6f31f69ade9bc632fabbdb73a,Ben Arnao,2020-05-20 14:04:54-04:00,MODIFY,1,tensorflow\python\keras\callbacks.py,tensorflow\python\keras\callbacks.py,1.0,"2264,2265","2264,2265",,,,,4.0,ben-arnao,2020-11-13T04:47:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39462>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39462>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_epoch_end,"self,epoch,logs",2244,2273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39649,leeyeetonn,2020-05-18T14:07:49Z,2020-06-19T07:14:11Z,tf.math.reduce_mean takes too long and produces wrong result when input_tensor is uint32/64 and axis is array,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04, macOS 10.14.6
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below):v2.1.0-rc2-17-ge5bf8de410 2.1.0 & v2.2.0-rc4-8-g2b96f3662b 2.2.0
 Python version:3.7.6
 Bazel version (if compiling from source):NA
 GCC/Compiler version (if compiling from source):NA
 CUDA/cuDNN version:NA
 GPU model and memory:NA
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 tf.math.reduce_mean hangs(?) or takes forever to compute for certain input (with dtype uint64 and uint32, and axis an array). Around when the slow down occurs, the function produces incorrect result.
 This function affects other functions' performance: tf.math.reduce_std which calls tf.math.reduce_variance which calls tf.math.reduce_mean
 Describe the expected behavior
 It should not take forever to compute nor produce an incorrect result.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 An example that showed incorrect result:
 import tensorflow as tf
 import numpy as np
 import time
 
 input_tensor = np.arange(4).astype('uint64')  # also occurs for 'uint32'
 
 for i in range(20):
   axis = [0] * i
   print('axis = ', axis)
   start = time.time()
   res = tf.math.reduce_mean(input_tensor, axis=axis)
   end = time.time()
   time_diff = end - start
   print('result = ', res.numpy())
   print('-- took %d sec --' % time_diff)
 An example that both became extremely slow and produced an incorrect result:
 import tensorflow as tf
 import numpy as np
 import time
 
 # 15 is a magic number chosen for illustration
 # a smaller number may not cause the slow-down but could still show the wrong result
 input_tensor = np.arange(15).astype('uint64')  # also occurs for 'uint32'
 
 for i in range(20):
   # produces incorrect result when i == 8.
   # slow down occurs around when i == 10
   axis = [0] * i
   print('axis = ', axis)
   start = time.time()
   res = tf.math.reduce_mean(input_tensor, axis=axis)
   end = time.time()
   time_diff = end - start
   print('result = ', res.numpy())
   print('-- took %d sec --' % time_diff)
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,leeyeetonn,2020-05-19T07:09:28Z,"
 		I have tried in colab with TF version 2.1.0 ,2.2.0 and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.sandbox.google.com/gist/ravikyram/5261bff929134c88bd17f6fd57e05c39/untitled904.ipynb>here.</denchmark-link>
 .Thanks!
 		",2.0,leeyeetonn,2020-06-19T07:14:13Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39649>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39649>No</denchmark-link>
 
 		",,,,,e972c5572634efd188696038e9241b75cdcd69bc,Gaurav Jain,2020-06-19 00:12:21-07:00,MODIFY,0,tensorflow\core\framework\register_types.h,tensorflow\core\framework\register_types.h,0.0,"156,157,158,170,171,172,173","156,157,158,159,160,161,162,163,164,165,177,178,179,180",MODIFY,1.0,tensorflow\core\framework\types.cc,tensorflow\core\framework\types.cc,,,,,,,,,,,,,1.0,,"241,242,243,244,245",tensorflow::DataTypeSize,dt,228,250,MODIFY,0.0,tensorflow\core\kernels\BUILD,tensorflow\core\kernels\BUILD,0.0,"4903,4905",,MODIFY,0.0,tensorflow\core\kernels\concat_lib_cpu.cc,tensorflow\core\kernels\concat_lib_cpu.cc,0.0,,"119,120",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\concat_op.cc,tensorflow\core\kernels\concat_op.cc,0.0,,"211,212",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\constant_op.cc,tensorflow\core\kernels\constant_op.cc,0.0,,214,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\control_flow_ops.cc,tensorflow\core\kernels\control_flow_ops.cc,0.0,,"104,110,112,113,314",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\core\kernels\data\dataset_test_base.cc,tensorflow\core\kernels\data\dataset_test_base.cc,1.0,,"223,224",MODIFY,0.0,tensorflow\core\kernels\dense_update_ops.cc,tensorflow\core\kernels\dense_update_ops.cc,0.0,,101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\dynamic_partition_op.cc,tensorflow\core\kernels\dynamic_partition_op.cc,MODIFY,0.0,tensorflow\core\kernels\fill_functor.cc,tensorflow\core\kernels\fill_functor.cc,MODIFY,0.0,tensorflow\core\kernels\gather_op.cc,tensorflow\core\kernels\gather_op.cc,MODIFY,0.0,tensorflow\core\kernels\identity_op.cc,tensorflow\core\kernels\identity_op.cc,MODIFY,0.0,tensorflow\core\kernels\ragged_gather_op.cc,tensorflow\core\kernels\ragged_gather_op.cc,MODIFY,0.0,tensorflow\core\kernels\ragged_tensor_from_variant_op.cc,tensorflow\core\kernels\ragged_tensor_from_variant_op.cc,0.0,,"311,312",MODIFY,0.0,tensorflow\core\kernels\ragged_tensor_to_tensor_op.cc,tensorflow\core\kernels\ragged_tensor_to_tensor_op.cc,0.0,,"564,565",,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\ragged_tensor_to_variant_op.cc,tensorflow\core\kernels\ragged_tensor_to_variant_op.cc,0.0,,"216,217",,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\resource_variable_ops.cc,tensorflow\core\kernels\resource_variable_ops.cc,0.0,,515,,,,,MODIFY,0.0,tensorflow\core\kernels\split_lib_cpu.cc,tensorflow\core\kernels\split_lib_cpu.cc,0.0,,46,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\core\kernels\split_op.cc,tensorflow\core\kernels\split_op.cc,0.0,,407,MODIFY,0.0,tensorflow\core\kernels\strided_slice_op.cc,tensorflow\core\kernels\strided_slice_op.cc,0.0,,"443,444",MODIFY,0.0,tensorflow\core\kernels\strided_slice_op_impl.h,tensorflow\core\kernels\strided_slice_op_impl.h,0.0,,"290,291",MODIFY,0.0,tensorflow\core\kernels\topk_op.cc,tensorflow\core\kernels\topk_op.cc,0.0,,"261,279",ADD,0.0,None,tensorflow\core\kernels\topk_op_gpu_uint32.cu.cc,,,,ADD,0.0,None,tensorflow\core\kernels\topk_op_gpu_uint64.cu.cc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::data::DatasetOpsTestBase::ExpectEqual,"a,b",215,231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,"167,168",,,,,,,,,,,,MODIFY,4.0,tensorflow\core\util\batch_util.cc,tensorflow\core\util\batch_util.cc,MODIFY,0.0,tensorflow\core\util\saved_tensor_slice_util.h,tensorflow\core\util\saved_tensor_slice_util.h,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,"48,49,101,102",140,0.0,,"214,215",0.0,,125,0.0,,"299,300",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"311,312",tensorflow::batch_util::MaybeMoveSliceToElement,"parent,element,index",296,318,1.0,,"210,211",tensorflow::batch_util::CopySliceToElement,"parent,element,index",195,217,1.0,,"185,186",tensorflow::batch_util::CopyElementToSlice,"element,parent,index",172,192,1.0,,"283,284",tensorflow::batch_util::CopyContiguousSlices,"src,src_offset,dst_offset,num_slices,dst",219,290,0.0,"119,121",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39718,wwwind,2020-05-20T14:15:44Z,2020-06-05T19:20:39Z,"TF Lite nightly: Model with Fully Connected layer can't be converted, fully quantization, int8","
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
 TensorFlow installed from (source or binary): tf-nightly
 TensorFlow version (or github SHA if from source): tf-nightly
 
 Command used to run the converter or code if you’re using the Python API
 If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-link:https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing>https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing</denchmark-link>
 
 <denchmark-code>import numpy as np
 import tensorflow as tf
 
 mnist = tf.keras.datasets.mnist
 train_data, test_data = mnist.load_data()
 
 pre_process = lambda x: x / 255.0
 num_calib = 1000
 calib_data = pre_process(
             train_data[0][0 : num_calib].astype(np.float32)
         )
 
 model = tf.keras.Sequential(
             [
                 tf.keras.layers.InputLayer(input_shape=(28, 28)),
                 tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
                 tf.keras.layers.Conv2D(
                     filters=12, kernel_size=(3, 3), activation=tf.nn.relu
                 ),
                 tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                 tf.keras.layers.Flatten(),
                 tf.keras.layers.Dense(10, activation=tf.nn.softmax),
             ]
         )
 model.summary()
 
 train_images = pre_process(train_data[0])
 train_labels = train_data[1]
 test_images = pre_process(test_data[0])
 test_labels = test_data[1]
 # Train the digit classification model
 model.compile(
   optimizer=""adam"",
   loss=""sparse_categorical_crossentropy"",
   metrics=[""accuracy""],
 )
 model.fit(
   train_images,
   train_labels,
   epochs=1,
   validation_data=(test_images, test_labels),
 )
 
 def _get_calib_data_func():
   def representative_data_gen():
     for input_value in calib_data:
       input_value = np.expand_dims(input_value, axis=0).astype(np.float32)
       yield [input_value]
 
   return representative_data_gen
 
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 converter.representative_dataset = _get_calib_data_func()
 
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 tflite_model_INT8 = converter.convert()
 </denchmark-code>
 
 <denchmark-code>**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor sequential_2/reshape_2/Shape
 Empty min/max for tensor sequential_2/reshape_2/Shape**
 </denchmark-code>
 
 Also, please include a link to the saved model or GraphDef
 <denchmark-code>https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing
 </denchmark-code>
 
 Failure details
 If the conversion is successful, but the generated model is wrong,
 state what is wrong:
 
 Producing wrong results and/or decrease in accuracy
 Producing correct results, but the model is slower than expected (model generated from old converter)
 
 RNN conversion support
 If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.
 Any other info / logs
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
 	",1.0,wwwind,2020-05-20T15:20:46Z,"
 		<denchmark-link:https://github.com/wwwind>@wwwind</denchmark-link>
 ,
 I was able to reproduce the issue with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/4a4ea0178b34597d6035794ba3eb3e36/39718-nightly.ipynb>TF-nightly</denchmark-link>
 . However, the code works with the stable version <denchmark-link:https://colab.research.google.com/gist/amahendrakar/54d9272a3c32841d1532a994cc5820bb/39718-2-0.ipynb>TF v2.2</denchmark-link>
 . Please check the linked gist.
 Could you please try running the code with TF v2.2 and let us know if it works. Thanks!
 		",2.0,wwwind,2020-05-20T15:35:07Z,"
 		<denchmark-link:https://github.com/amahendrakar>@amahendrakar</denchmark-link>
  Yes, I confirm - it works in tensorflow 2.2.0
 		",3.0,wwwind,2020-05-20T17:41:14Z,"
 		<denchmark-link:https://github.com/wwwind>@wwwind</denchmark-link>
 ,
 Thank you for the update. Please feel free to close the issue if resolved. Thanks!
 		",268a0ea1502532e0e127e71d0ad42cc4e8ad81c6,Suharsh Sivakumar,2020-06-05 12:13:43-07:00,MODIFY,16,tensorflow\lite\tools\optimize\quantize_model.cc,tensorflow\lite\tools\optimize\quantize_model.cc,1.0,"1112,1119,1120,1121,1122,1123,1124","1174,1175,1176,1177,1181,1183,1184",,,,,4.0,wwwind,2020-05-25T15:52:25Z,"
 		I get the same error when dealing with tf-nightly. However, I can't go back to 2.2 because a new error suddenly shows up: Tensor 'input_1' has invalid shape '[None, None, None, 3]'. I've seen in other thread it's recommended to use tf-nightly to fix the last error.
 So I can see here kind of deadlock between versions.
 		",5.0,wwwind,2020-05-27T07:35:05Z,"
 		
 @amahendrakar Yes, I confirm - it works in tensorflow 2.2.0
 
 Closing this issue as resolved.
 		",6.0,wwwind,2020-05-27T07:36:13Z,"
 		<denchmark-link:https://github.com/sramirez>@sramirez</denchmark-link>
 ,
 Could you please submit a new issue from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/new/choose>this link</denchmark-link>
  and fill in the template along with the complete code, so that we can track the issue there. Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,tflite::optimize::EnsureBiasScaleCompatibility,"model,operator_names,real_value_op_set,error_reporter",1110,1250,1.0,"886,895,896,897,898,899,900",,tflite::optimize::QuantizeWeightsInputOutput,"model,allow_float,operator_names,real_value_op_set,error_reporter",883,925,1.0,"1003,1010,1011,1012,1013,1014,1015","1036,1037,1038",tflite::optimize::FillQuantizationParams,"model,operator_names,real_value_op_set,error_reporter",1001,1107,1.0,364,"294,295,296,303,304,305,306",tflite::optimize::ApplyConstraints,"model,operator_names,error_reporter",294,364,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,wwwind,2020-05-27T09:27:07Z,"
 		<denchmark-link:https://github.com/amahendrakar>@amahendrakar</denchmark-link>
  Could this bug be fixed in tf-nightly ?
 I am working with tf-nightly and I really appreciate if it will be fixed in the current code.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,wwwind,2020-06-03T16:38:39Z,"
 		Thanks, I will take a look into this.
 		",1.0,"637,638,639,640,641","565,566,567,568,569",tflite::optimize::QuantizeOpInput,"model,subgraph_idx,op_idx,property,input,error_reporter",481,661,1.0,"80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132",,tflite::optimize::PopulateRealValueOpSet,"model,operator_names",80,132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,wwwind,2020-06-03T18:39:43Z,"
 		<denchmark-link:https://github.com/wwwind>@wwwind</denchmark-link>
  I have updated your code and <denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/05d9d1bcfe32caead45728ac250e8dbf/39718-nightly.ipynb>here</denchmark-link>
  is the working version of your code with . Thanks!
 		",10.0,wwwind,2020-06-03T18:41:23Z,"
 		At a high level there are two ways to fix this issue (one of which <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  pointed out).
 
 Try setting it in keras and keeping conversion the same way:
 
 <denchmark-code>input = tf.keras.layers.Input(shape=(225, 225, 3), batch_size=1)
 </denchmark-code>
 
 
 Use set_shape on the model input and fix the batch size to 1 (example here: https://www.tensorflow.org/lite/convert/python_api#examples_).
 
 		",11.0,wwwind,2020-06-05T19:20:41Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39718>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"886,895,896,897,898,899,900","862,863,864,865",tflite::optimize::QuantizeBiases,"model,operator_names,error_reporter",852,903,,,,,,,,1.0,"930,939,940,941,942,943,1003,1010,1011,1012,1013,1014,1015","931,932,933",tflite::optimize::FillQuantizationParams,"model,operator_names,error_reporter",923,1025,1.0,"1112,1119,1120,1121,1122,1123,1124","1036,1037,1038",tflite::optimize::EnsureBiasScaleCompatibility,"model,operator_names,error_reporter",1028,1164,1.0,"930,939,940,941,942,943","931,932,933",tflite::optimize::QuantizeBiases,"model,operator_names,real_value_op_set,error_reporter",928,981,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1260,1261,1262,1263,1264,1268,1269,1270,1272",,tflite::optimize::QuantizeModel,"builder,model,input_type,output_type,allow_float,operator_names,error_reporter",1255,1282,1.0,"364,365,366,367,374,375,376,377,378",,tflite::optimize::ApplyConstraints,"model,operator_names,real_value_op_set,error_reporter",364,436,1.0,"73,74,75,76",,tflite::optimize::IsRealValueOp,"real_value_op_set,operator_name",73,76,1.0,"58,59",,tflite::optimize::GetOperatorProperty,"operator_names,model,subgraph_index,op_idx,operator_name",53,71,1.0,,"822,823,824",tflite::optimize::QuantizeWeightsInputOutput,"model,allow_float,operator_names,error_reporter",811,849,1.0,"41,42,43,44,45,46,47,48",,tflite::optimize::IsFloatTensor,"subgraph,tensor_idx",41,48
39756,jeremyholleman,2020-05-21T13:49:56Z,2020-05-29T00:41:37Z,problem running visualize.py at import flatbuffersn,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Mac OS 10.15.3
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
 TensorFlow installed from (source or binary): from source
 TensorFlow version (use command below): v1.12.1-32248-g8670c85844 2.2.0
 Python version: 3.7.4
 Bazel version (if compiling from source): 3.0.0
 GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.59) (from gcc --version)
 CUDA/cuDNN version: n/a
 GPU model and memory: n/a
 
 Describe the current behavior
 visualize.py script fails with this error:
 ImportError: cannot import name 'flatbuffersn' from 'flatbuffers.python' (/private/var/tmp/_bazel_jeremy/95159cfd4782ce915016562181875cd6/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/lite/tools/visualize.runfiles/flatbuffers/python/__init__.py)
 I am running it with the command
 
 from the top of my tensorflow source directory.  The build phase seems to work fine, then the import error seems to happen on running visualize.py.  Full output in attached file.
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4662612/vis_dump.txt>vis_dump.txt</denchmark-link>
 
 The target tflite file can be downloaded <denchmark-link:http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128_quant.tgz>here</denchmark-link>
 , but I don't think the tflite file ever gets loaded, so I doubt that the specific file is relevant.
 The directory where it is trying to import from is listed here.  There is no flatbuffersn file or directory and the init.py file is empty.
 <denchmark-code>ls -l /private/var/tmp/_bazel_jeremy/95159cfd4782ce915016562181875cd6/execroot/org_tensorflow/bazel-out/darwin-opt/bin/tensorflow/lite/tools/visualize.runfiles/flatbuffers/python/
 total 0
 -r-xr-xr-x   1 jeremy  wheel    0 May 20 16:15 __init__.py*
 drwxr-xr-x   3 jeremy  wheel   96 May 21 09:39 __pycache__/
 drwxr-xr-x  10 jeremy  wheel  320 May 20 16:15 flatbuffers/
 </denchmark-code>
 
 Describe the expected behavior
 I would expect it to dump an html file to ~/tmp/mobnet.html containing a visualization of the mobilenet model in the target tflite file.
 Standalone code to reproduce the issue
 The command line above.
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,jeremyholleman,2020-05-28T13:34:19Z,"
 		Yeah, I can actually reproduce this, and I think the import name should be flatbuffers but the following \n is somehow not correctly escaped and ended up being interpreted as character n.
 This must be a bug somewhere in the flatbuffer build rule, or the flatbuffer tool. Let me dig a bit more about it.
 		",2.0,jeremyholleman,2020-05-28T14:53:12Z,"
 		Found the issue, which was caused by the different  tool behavior between linux and mac, as described here: <denchmark-link:https://stackoverflow.com/a/22203933/922135>https://stackoverflow.com/a/22203933/922135</denchmark-link>
 
 I'll send a fix soon, but in the meantime you should be able to try the fix locally.
 In the tensorflow/third_party/flatbuffers/build_defs.bzl file at line365, change:
 <denchmark-code>""import flatbuffers\\n/' > %s""
 </denchmark-code>
 
 to
 <denchmark-code>""import flatbuffers\\'$'\\n/' > %s""
 </denchmark-code>
 
 and try running the visualize tool again.
 		",3.0,jeremyholleman,2020-05-29T00:41:39Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39756>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39756>No</denchmark-link>
 
 		",f9fb66cdb7419d2eadf7faea995f1aacb032104b,YoungSeok Yoon,2020-05-28 17:34:10-07:00,MODIFY,0,third_party\flatbuffers\build_defs.bzl,third_party\flatbuffers\build_defs.bzl,0.0,365,365,,,,,4.0,jeremyholleman,2020-05-29T10:08:31Z,"
 		That fixed it for me.  Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
39976,gnovack,2020-05-29T02:42:14Z,2020-06-06T01:38:34Z,RandomContrast Layer - confusing __init__ error message,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
 TensorFlow installed from (source or binary): Colab
 TensorFlow version (use command below): 2.2.0
 Python version: 3.6.9
 
 Describe the current behavior
 When instantiating a RandomContrast layer object with a value > 1. (e.g. 2) for the Factor parameter, a ValueError is raised with the following error message which is somewhat confusing in this scenario: 'Factor cannot have negative values, got 2'
 Describe the expected behavior
 A ValueError should be raised with a more appropriate error message, something like: 'Factor cannot be greater than 1, got 2'
 Standalone code to reproduce the issue
 random_contrast_layer = tf.keras.layers.experimental.preprocessing.RandomContrast(2)
 	",1.0,gnovack,2020-05-29T15:12:16Z,"
 		Was able to reproduce the issue with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/46ccdd512567b0dd3cf99a9548aead9c/39976.ipynb>TF v2.2</denchmark-link>
  and <denchmark-link:https://colab.research.google.com/gist/amahendrakar/54056391942fbc1344d936558161b88f/39976-tf-nightly.ipynb>TF-nightly</denchmark-link>
 . Please find the attached gist. Thanks!
 		",2.0,gnovack,2020-06-06T01:07:04Z,"
 		<denchmark-link:https://github.com/gnovack>@gnovack</denchmark-link>
  Thanks for reporting this issue. The RandomContrast layer looks for two element factor (lower_bound, upper_bound) or single element where the factor should be between 0 and 1.
 The error message is improved. Please take a look at the <denchmark-link:https://colab.research.google.com/gist/jvishnuvardhan/6df7f036a25ff296903e9a74febc7e4e/39976-tf-nightly.ipynb>gist here</denchmark-link>
 . Thanks!
 Please verify once and close the issue if this was resolved for you. Thanks!
 		",3.0,gnovack,2020-06-06T01:38:33Z,"
 		Looks great. Thanks <denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
 
 		",25213f58c433d3712931b4071d2498bb67f8c2ca,A. Unique TensorFlower,2020-06-01 12:21:48-07:00,MODIFY,1,tensorflow\python\keras\layers\preprocessing\image_preprocessing.py,tensorflow\python\keras\layers\preprocessing\image_preprocessing.py,1.0,"1069,1070","1069,1070",,,,,4.0,gnovack,2020-06-06T01:38:35Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39976>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/39976>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,factor,seed,name,kwargs",1061,1073,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40050,DNXie,2020-06-01T14:37:30Z,2020-06-03T21:43:11Z,Unclear shape dependency of `value` in  `tf.keras.backend.moving_average_update` documentation,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/backend/moving_average_update>https://www.tensorflow.org/api_docs/python/tf/keras/backend/moving_average_update</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 Unclear shape dependency of input value. According to the document, value should have the same shape as variable, but it is unclear what is variable.
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Yes
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Yes
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 No
 	",1.0,DNXie,2020-06-01T14:55:26Z,"
 		<denchmark-link:https://github.com/DNXie>@DNXie</denchmark-link>
 
 Please provide details about what platform you are using (operating system, architecture). Also include your TensorFlow version. Also, did you compile from source or install a binary?
 Make sure you also include the exact command if possible to produce the output included in your test case. If you are unclear what to include see the issue template displayed in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/new>the Github new issue template</denchmark-link>
 .
 We ask for this in the issue submission template, because it is really difficult to help without that information. Thanks!
 		",2.0,DNXie,2020-06-02T01:55:18Z,"
 		<denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
 
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.2.0-rc3
 Python version: 3.8.2
 
 		",,,,,cd3ed4fe8cfa83418ff5937cb52560013667cba4,Mark Daoust,2020-06-03 14:41:09-07:00,MODIFY,1,tensorflow\python\keras\backend.py,tensorflow\python\keras\backend.py,1.0,"1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1675,1676,1677,1681","1647,1650,1651,1655",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,moving_average_update,"x,value,momentum",1646,1685,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40103,DNXie,2020-06-03T01:55:34Z,2020-06-11T22:36:41Z,Unclear type/dimension dependency of `filters` in  `conv1d/3d_transpose` documentation,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose>https://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose</denchmark-link>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose>https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 Unclear type and dimension dependency of input filters. According to the document, filters should have the same type as value and the in_channel dimension must match that of value, but it is unclear what value is.
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Yes
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Yes
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose>https://www.tensorflow.org/api_docs/python/tf/nn/conv1d_transpose</denchmark-link>
 : Yes
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose>https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose</denchmark-link>
 :  No, the ""Raises"" list is not provided or defined
 <denchmark-h:h2>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.2.0-rc3
 Python version: 3.8.2
 
 	",1.0,DNXie,2020-06-03T19:02:24Z,"
 		input is an alias of value here. I will try and update the docs. Thanks!
 		",2.0,DNXie,2020-06-10T19:09:05Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you.
 		",,,,,cef0c8cf713a718c01dd3da582c94d4e09a54753,A. Unique TensorFlower,2020-06-07 22:39:54-07:00,MODIFY,0,tensorflow\python\ops\nn_ops.py,tensorflow\python\ops\nn_ops.py,0.0,"1965,1967,1982,3121,3123,3145","1965,1967,1982,3121,3123,3145",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40328,etsygankov,2020-06-09T18:21:05Z,2020-06-18T17:51:34Z,Subclassed model with ConvLSTM2D layer can't be saved as a SavedModel in TF2.2,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code, extended an example from TF guides
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04/Mac OS 10.15
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.1 and 2.2
 Python version: 3.6
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.1
 GPU model and memory: 1080Ti 11Gb
 
 Describe the current behavior
 As a header states the model build with Subclassing API with ConvLSTM2D layer inside can't be saved as a SavedModel. Given that keras (.h5) model format doesn't support saving subclassed models I am left with no option to save the model architecture to file.
 The issues appears in TF2.2 while there seems to be no bug in earlier version 2.1
 Describe the expected behavior
 The code is to work without issues in both TF2.2 and TF2.1
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-link:https://colab.research.google.com/drive/1zfhnbz_dHfPloT9mzk0ei5F4aFaZGgnt?usp=sharing>https://colab.research.google.com/drive/1zfhnbz_dHfPloT9mzk0ei5F4aFaZGgnt?usp=sharing</denchmark-link>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 import numpy as np
 import tensorflow as tf
 print(tf.__version__)
 from tensorflow import keras
 from tensorflow.keras.layers import ConvLSTM2D, Bidirectional, LSTM
 
 class CustomModel(keras.Model):
   def __init__(self, hidden_units):
     super(CustomModel, self).__init__()
     self.lstm = Bidirectional(ConvLSTM2D(filters=16, kernel_size=(1, 1), return_sequences=True, return_state=True))
     self.dense_layers = [keras.layers.Dense(u) for u in hidden_units]
 
   def call(self, inputs, training=None, mask=None):
     x = inputs
     x, _, _, _, _ = self.lstm(x)
     for layer in self.dense_layers:
       x = layer(x)
     return x
 
 model = CustomModel([16, 16, 10])
 # Build the model by calling it
 input_arr = tf.random.uniform((1, 10, 10, 10, 5))
 outputs=model.predict(input_arr)
 model.save('my_model')
 
 # Delete the custom-defined model class to ensure that the loader does not have
 # access to it.
 del CustomModel
 
 loaded = keras.models.load_model('my_model')
 Similar issue discussed on stackoverflow
 <denchmark-link:https://stackoverflow.com/questions/61362953/keras-convlstm2d-valueerror-when-saving-model>https://stackoverflow.com/questions/61362953/keras-convlstm2d-valueerror-when-saving-model</denchmark-link>
 
 	",1.0,etsygankov,2020-06-10T14:33:40Z,"
 		Was able to reproduce the issue with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/2da837292cbfa01854fa77f2d2791d15/40328.ipynb>TF v2.2</denchmark-link>
  and <denchmark-link:https://colab.research.google.com/gist/amahendrakar/6dbb8421ea8815a3c1a41110969c7d6a/40328-tf-nightly.ipynb#scrollTo=S8MN8xgvnjdw>TF-nightly</denchmark-link>
 . Works without any issues with <denchmark-link:https://colab.research.google.com/gist/amahendrakar/cb19b976d100df61f01d41e17d2180d0/40328-2-1.ipynb>TF v2.1</denchmark-link>
 . Please find the attached gist. Thanks!
 		",2.0,etsygankov,2020-06-18T17:51:36Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40328>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40328>No</denchmark-link>
 
 		",3.0,etsygankov,2020-07-01T16:04:46Z,"
 		Saving works in TF 2.3.0-rc0, thanks!
 		",47582983cb1064b5bb81233db4f0adeeaa10b74d,Scott Zhu,2020-06-18 10:40:41-07:00,MODIFY,1,tensorflow\python\keras\saving\saved_model\layer_serialization.py,tensorflow\python\keras\saving\saved_model\layer_serialization.py,1.0,"162,163,164,165,166,167,168,169","162,163",MODIFY,1.0,tensorflow\python\keras\saving\saved_model\saved_model_test.py,tensorflow\python\keras\saving\saved_model\saved_model_test.py,,,,,,,,,,,,,1.0,"776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794",,testSaveStatelessConvLSTM2D,self,776,794,,,,,,,,,,,,,,,_get_serialized_attributes_internal,"self,serialization_cache",158,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40839,mchanchee,2020-06-26T16:24:28Z,2020-07-08T19:49:10Z,"tf.io.decode_image(img, channels=3) outputs 4 channels when reading 4-channel BMP","
  attached a sample BMP file
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4845825/rgb32.zip>rgb32.zip</denchmark-link>
 
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution: Windows 10
 TensorFlow installed from (source or binary): binary (pip)
 TensorFlow version: 2.2.0
 Python version: 3.7.7
 
 Describe the current behavior
 When reading in a 4-channel BMP:
 
 tf.io.decode_image(img, channels=3) gives shape (..., ..., 4) instead of (..., ..., 3)
 tf.io.decode_bmp(img, channels=3) gives the following error
 
 <denchmark-code>Traceback (most recent call last):
   File ""channels.py"", line 44, in <module>
     loop()
   File ""channels.py"", line 14, in loop
     img = tf.io.decode_bmp(img, channels=3)
   File ""C:\Users\mattchee\Miniconda3\lib\site-packages\tensorflow\python\ops\gen_image_ops.py"", line 899, in decode_bmp
     _ops.raise_from_not_ok_status(e, name)
   File ""C:\Users\mattchee\Miniconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 6653, in raise_from_not_ok_status     
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""<string>"", line 3, in raise_from
 tensorflow.python.framework.errors_impl.InvalidArgumentError: channels attribute 3 does not match bits per pixel from file 4 [Op:DecodeBmp]
 </denchmark-code>
 
 I'm following <denchmark-link:https://www.tensorflow.org/tutorials/load_data/images#load_using_tfdata>this guide</denchmark-link>
  to load images efficiently so  is not an option.
 Describe the expected behavior
 This is inconsistent with tf.io.decode_image(img, channels=3) and tf.io.decode_png(img, channels=3) which give shape (..., ..., 3) when reading a 4-channel PNG.
 Both tf.io.decode_image(img, channels=3) and tf.io.decode_bmp(img, channels=3) would be expected to give shape (..., ..., 3) when reading in a 4-channel BMP.
 Standalone code to reproduce the issue
 img = tf.io.read_file(img_path)
 img = tf.io.decode_image(img, channels=3)
 print(img.shape) # This prints (64, 127, 4)
 Or
 img = tf.io.read_file(img_path)
 img = tf.io.decode_bmp(img, channels=3) # Error
 print(img.shape)
 	",1.0,mchanchee,2020-06-27T16:54:25Z,"
 		<denchmark-link:https://github.com/mchanchee>@mchanchee</denchmark-link>
  Can you share a sample bmp file to reproduce the issue?
 		",2.0,mchanchee,2020-06-28T16:44:18Z,"
 		Can you also test with tf-nightly. There has been a refactoring of tf.io.decode_image recently which I think covers this issue too.
 		",3.0,mchanchee,2020-06-29T03:36:32Z,"
 		Thank you for reporting the issue. It appears to be an inherent issue with decode_bmp op. I will look into it and loop back with updates. In the meantime, it would be helpful if you could please share the bmp file for reproducing the issue.
 		",0859ec0386ffa55739cbe831f38942c53027c12f,Hye Soo Yang,2020-07-06 10:50:42-07:00,MODIFY,0,tensorflow\core\BUILD,tensorflow\core\BUILD,0.0,"3005,3006,3007,3008,3009,3010,3011",,MODIFY,3.0,tensorflow\core\kernels\decode_image_op.cc,tensorflow\core\kernels\decode_image_op.cc,4.0,mchanchee,2020-06-29T13:40:33Z,"
 		Thank you for looking into it
 <denchmark-link:https://github.com/yongtang>@yongtang</denchmark-link>
  <denchmark-link:https://github.com/hyeygit>@hyeygit</denchmark-link>
   I've added a sample BMP file to the issue description.
 <denchmark-link:https://github.com/mihaimaruseac>@mihaimaruseac</denchmark-link>
  I've tried with  and got the same problem as described above.
 		",5.0,mchanchee,2020-07-08T19:49:10Z,"
 		I've tried it with tf-nightly 2.4.0-dev20200708 and it's all good.
 Thank you very much for fixing the issue!
 		",6.0,mchanchee,2020-07-08T19:49:12Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40839>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40839>No</denchmark-link>
 
 		",1.0,"906,907,914,916,919,921,924,925,926,927,928,929,930,931,932,933,934","898,899,906,908,911,913,928,931",tensorflow::DecodeImageV2Op::DecodeBMP,"input,row_size,output,width,height,channels,top_down",896,936,ADD,0.0,tensorflow\core\lib\bmp\testdata\grayscale_small.bmp,tensorflow\core\lib\bmp\testdata\grayscale_small.bmp,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\grayscale_small_3channels.bmp,tensorflow\core\lib\bmp\testdata\grayscale_small_3channels.bmp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"906,907,914,916,919,921,924,925,926,927,928,929,930,931,932,933,934,941,942,943,944,945,952,953,954,955,956,959","906,908,911,913,928,931",tensorflow::DecodeImageV2Op::DecodeBMP,"input,row_size,output,width,height,output_channels,input_channels,top_down",904,964,1.0,"790,791,792,793,794,795,796,797,798,819,820,821,822,823,824,825,826,827,828,829,833,836,851,852,853,859,861,862,864,866","789,790,791,792,793,794,795,796,797,798,799,800,801,802,823,827,830,845,846,852,854,856,858,876,877,879",tensorflow::DecodeImageV2Op::DecodeBmpV2,"context,input",764,881,,,,,,,,,,,,,,,,,,,,,,7.0,mchanchee,2020-07-09T02:55:37Z,"
 		Again, thank you for reporting, also for confirming that the issue is fixed!
 		",,,,,,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\grayscale_small_4channels.bmp,tensorflow\core\lib\bmp\testdata\grayscale_small_4channels.bmp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\rgb_small.bmp,tensorflow\core\lib\bmp\testdata\rgb_small.bmp,,,,,,,,,,,,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\rgb_small_255.bmp,tensorflow\core\lib\bmp\testdata\rgb_small_255.bmp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\rgba_small.bmp,tensorflow\core\lib\bmp\testdata\rgba_small.bmp,,,,ADD,0.0,tensorflow\core\lib\bmp\testdata\rgba_small_255.bmp,tensorflow\core\lib\bmp\testdata\rgba_small_255.bmp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\ops\image_ops_impl.py,tensorflow\python\ops\image_ops_impl.py,MODIFY,1.0,tensorflow\python\ops\image_ops_test.py,tensorflow\python\ops\image_ops_test.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"2667,2668,2671,2672","2667,2670",_bmp,,2658,2672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testBmpChannels,self,5182,5234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4084,LaurentMazare,2016-08-28T20:58:19Z,2016-09-06T18:19:04Z,Process hanging when using TF_SessionRun with multiple times the same input,"
 It seems that if the same input appears multiple times in the inputs argument of TF_SessionRun (from c_api.h) then the TF_SessionRun call never returns.
 This issue can be reproduced by modifying c_api_test.cc and replacing the line:
 csession.SetInputs({{feed, Int32Tensor(3)}});
 With:
 csession.SetInputs({{feed, Int32Tensor(3)}, {feed, Int32Tensor(3)}});
 According to gdb, the process is waiting for a mutex in the RunState destructor from DirectSession.
 <denchmark-h:h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>
 
 none
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: Linux 4.4
 Installed version of CUDA and cuDNN: none
 If installed from source, provide
 
 The commit hash (git rev-parse HEAD)
 008bcae
 The output of bazel version
 Build label: 0.3.1
 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Fri Jul 29 09:09:52 2016 (1469783392)
 Build timestamp: 1469783392
 Build timestamp as int: 1469783392
 
 <denchmark-h:h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</denchmark-h>
 
 See above.
 <denchmark-h:h3>What other attempted solutions have you tried?</denchmark-h>
 
 <denchmark-h:h3>Logs or other output that would be helpful</denchmark-h>
 
 (If logs are large, please upload as attachment or provide link).
 	",1.0,LaurentMazare,2016-08-30T22:12:43Z,"
 		Thanks for the report.
 		",2.0,LaurentMazare,2016-08-31T18:31:43Z,"
 		In general the C API is unforgiving when it comes to usage, but this should be an easy and cheap piece of validation code to add.
 		",3.0,LaurentMazare,2016-08-31T22:35:54Z,"
 		Running tests now... it should appear after the next push.
 		",c35e69c56941d79163dc9f054f57c199b1a4cc44,Derek Murray,2016-08-31 17:31:15-07:00,MODIFY,1,tensorflow\core\common_runtime\direct_session_test.cc,tensorflow\core\common_runtime\direct_session_test.cc,1.0,"400,401,402,403,404,405,406,407",,MODIFY,1.0,tensorflow\core\graph\subgraph.cc,tensorflow\core\graph\subgraph.cc,,,,,,,,,,,,,1.0,"238,239,240,241,242,243,244,245,246",238,tensorflow::subgraph::RewriteGraphForExecution,"g,fed_outputs,fetch_outputs,target_node_names,device_info",228,282,,,,,,,,,,,,,,,tensorflow::TEST,"DirectSessionTest,MultipleFeedTest",337,408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
40895,aroig,2020-06-28T17:24:03Z,2020-10-31T00:18:38Z,nested gradients for convolution layer fail under tf.function,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
 TensorFlow installed from (source or binary): pip tf_nightly
 TensorFlow version (use command below): v1.12.1-35353-gcbb94efa58 2.5.0-dev20200628
 Python version:3.6.9
 Bazel version (if compiling from source): -
 GCC/Compiler version (if compiling from source): -
 CUDA/cuDNN version: 10.1
 GPU model and memory: GeForce GTX 1050 Ti with Max-Q 4 Gb
 
 Describe the current behavior
 The code below works in eager mode, but fails with the following error if using tf.function
 <denchmark-code>Traceback (most recent call last):
   File ""bugreport.py"", line 55, in <module>
     value = func(x)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
     result = self._call(*args, **kwds)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 823, in _call
     self._initialize(args, kwds, add_initializers_to=initializers)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 697, in _initialize
     *args, **kwds))
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2870, in _get_concrete_function_internal_garbage_collected
     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3227, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 3089, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 600, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File ""bugreport.py"", line 48, in func
     grads = tape.gradient(loss, variables)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 1073, in gradient
     unconnected_gradients=unconnected_gradients)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py"", line 77, in imperative_grad
     compat.as_str(unconnected_gradients.value))
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 162, in _gradient_function
     return grad_fn(mock_op, *out_grads)
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 50, in _Conv2DBackpropInputGrad
     strides=op.get_attr(""strides""),
   File ""/home/abdo/tmp/venv/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py"", line 121, in get_attr
     raise KeyError(attr)
 KeyError: 'strides'
 </denchmark-code>
 
 Describe the expected behavior
 I would expect this code to work the same under eager mode or wrapped with tf.function.
 Standalone code to reproduce the issue
 import tensorflow as tf
 
 shape = (2, 3, 3, 5, 7)
 init = tf.random.normal(shape=shape, dtype=tf.float32)
 weights = tf.Variable(initial_value=init, trainable=True, shape=shape, dtype=tf.float32)
 
 def conv(x):
     kernel = tf.reshape(tf.eye(3 * 3, dtype=x.dtype), shape=(3, 3, 1, 3 * 3))
 
     x = tf.reshape(x, shape=(2 * 5, 1, 4, 4))
     x = tf.nn.conv2d(x, kernel, strides=(1,1), padding='SAME', data_format='NCHW')
     x = tf.reshape(x, shape=(2, 5 * 3 * 3, 4 * 4))
 
     W = tf.reshape(weights, shape=(2, 3 * 3, 5, 7))
     W = tf.transpose(W, perm=[0, 2, 1, 3])
     W = tf.reshape(W, shape=(2, 5 * 3 * 3, 7))
     
     y = tf.linalg.matmul(W, x, transpose_a=True)
     y = tf.reshape(y, shape=(2, 7, 4, 4))
     y = tf.square(y)
 
     return y
 
 def func_flat(x_flat):
     x_unflat = tf.reshape(x_flat, shape=(2, 5, 4, 4))
 
     with tf.GradientTape() as tape:
         tape.watch(x_unflat)
         y = conv(x_unflat)
         u = tf.reduce_sum(y, axis=[-3, -2, -1])
         u = tf.reshape(u, shape=(2, 1))
 
     jac = tape.batch_jacobian(u, x_unflat)
     return jac
 
 variables = [weights]
 
 @tf.function(autograph=False)
 def func(x):
     with tf.GradientTape() as tape:
         tape.watch(weights)
 
         y_flat = func_flat(x)
         loss = tf.reduce_sum(tf.square(y_flat))
 
     grads = tape.gradient(loss, variables)
 
     return tf.reduce_sum(grads)
 
 
 x = tf.random.normal(shape=(2, 5 * 4 * 4), dtype=tf.float32)
 
 value = func(x)
 print(value)
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,aroig,2020-06-29T17:09:17Z,"
 		<denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
   yes, I forgot to mention it in the report, there is some amount of non-determinism. sometimes it reports KeyError on strides, sometimes on the explicit_paddings, but the cause of the error seems to be the same I reported.
 		",2.0,aroig,2020-06-29T17:48:23Z,"
 		Also, let me clarify that this requires a notebook with GPU. If we remove the @tf.function(autograph=False) line, it works as expected on gpu, but would complain about an unrelated thing on cpu: no Conv2D kernel in the NCHW order.
 		",3.0,aroig,2020-06-30T15:47:46Z,"
 		I ran the code on different versions of tf and the error is different as reported, please find the gist here <denchmark-link:https://colab.research.google.com/gist/Saduf2019/3ba4a2f8f9f8f220fcee89398adfc6cf/untitled246.ipynb>for nightly</denchmark-link>
 , <denchmark-link:https://colab.research.google.com/gist/Saduf2019/135221a048088eedc9ab3d638e2f934e/untitled246.ipynb>1.12</denchmark-link>
 ,
 		",3d1f1b062dafc5cea4561d0a48538c60aef5aa5e,Allen Lavoie,2020-10-30 17:17:09-07:00,MODIFY,3,tensorflow\python\eager\backprop_test.py,tensorflow\python\eager\backprop_test.py,1.0,"1790,1791,1792,1793,1794,1795",,MODIFY,1.0,tensorflow\python\ops\parallel_for\pfor.py,tensorflow\python\ops\parallel_for\pfor.py,4.0,aroig,2020-06-30T16:05:45Z,"
 		<denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
  As I mentioned before, the error on the nightly gist above is almost the same. If you repeat the execution several times it changed back and forth between KeyError on strides, explicit_paddings or dilations.
 The stack trace is identical. See an other execution on this <denchmark-link:https://colab.research.google.com/drive/1v9ge0nrS8hpe5EChn251TNUeOXRcEKvw#scrollTo=SRWRfyFm8Zy9>gist</denchmark-link>
 
 		",5.0,aroig,2020-10-31T00:18:39Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40895>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/40895>No</denchmark-link>
 
 		",6.0,aroig,2020-11-23T23:53:01Z,"
 		Hi,
 Any update on this?
 <denchmark-link:https://github.com/aroig>@aroig</denchmark-link>
  did you find a solution?
 		",1.0,"1004,1005,1006,1007,1008",1004,_create_op,"op_type,inputs,op_dtypes,attrs",1000,1010,,,,,,,,,,,,,,,test_grad_jacobian_conv._outer,,1790,1795,1.0,"1776,1777,1778,1779,1780,1781,1782,1783",,test_grad_jacobian_conv._inner,x,1776,1783,1.0,"1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798",,test_grad_jacobian_conv,self,1775,1798,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,aroig,2020-12-01T22:07:08Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/issues/45063>#45063</denchmark-link>
  might be a duplicate, <denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
  <denchmark-link:https://github.com/gowthamkpr>@gowthamkpr</denchmark-link>
  you could consider that reproduction too.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41270,y-ich,2020-07-10T08:43:59Z,2020-07-14T04:28:49Z,"GPU delegate library is libtensorflowlite_gpu_delegate.so, not libtensorflowlite_gpu_gl.so.","
 Thank you for submitting a TensorFlow documentation issue. Per our GitHub
 policy, we only address code/doc bugs, performance issues, feature requests, and
 build/installation issues on GitHub.
 The TensorFlow docs are open source! To get involved, read the documentation
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 Please provide a link to the documentation entry, for example:
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod</denchmark-link>
 
 <denchmark-link:https://www.tensorflow.org/lite/performance/gpu_advanced>https://www.tensorflow.org/lite/performance/gpu_advanced</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-code>bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:gl_delegate                  # for static library
 bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_gl.so  # for dynamic library
 </denchmark-code>
 
 should be changed to,
 <denchmark-code>bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                  # for static library
 bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library
 </denchmark-code>
 
 because gl_delegate is not GPU delegate runtime library, it is for OpenGL delegate, right?
 <denchmark-h:h3>Clear description</denchmark-h>
 
 For example, why should someone use this method? How is it useful?
 <denchmark-h:h3>Correct links</denchmark-h>
 
 Is the link to the source code correct?
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Are all parameters defined and formatted correctly?
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Are return values defined?
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Are the errors defined? For example,
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises</denchmark-link>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Is there a usage example?
 See the API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
 
 on how to write testable usage examples.
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 Are there currently visuals? If not, will it clarify the content?
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Are you planning to also submit a pull request to fix the issue? See the docs
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 ,
 docs API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
  and the
 docs style guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_style>https://www.tensorflow.org/community/contribute/docs_style</denchmark-link>
 
 	",1.0,y-ich,2020-07-13T01:23:06Z,"
 		Thanks for letting us know. I'll update the doc.
 BTW, you can also update the doc directory <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/gpu_advanced.md</denchmark-link>
 
 		",,,,,,,,,052d45f39a62f4684801ca29ad6b6a593ce4a8fa,Terry Heo,2020-07-13 21:23:46-07:00,MODIFY,0,tensorflow\lite\g3doc\performance\gpu_advanced.md,tensorflow\lite\g3doc\performance\gpu_advanced.md,0.0,"125,126","125,126",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4131,pronobis,2016-08-31T17:26:25Z,2016-09-08T16:51:24Z,reduce_max and maximum give different results for negative infinity,"
 Using tf.maximum with negative inf inputs as follows:
 <denchmark-code>tf.maximum(-math.inf, -math.inf).eval()
 </denchmark-code>
 
 gives the expected result -inf
 However, tf.reduce_max, on the same inputs:
 <denchmark-code>tf.reduce_max([-math.inf, -math.inf]).eval()
 </denchmark-code>
 
 gives: -3.40282e+38 which is the min float32.
 For positive infinity inputs, both functions result in inf.
 <denchmark-h:h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>
 
 I posted this as an SO question first:
 <denchmark-link:http://stackoverflow.com/questions/39211546/bug-in-tensorflow-reduce-max-for-negative-infinity>http://stackoverflow.com/questions/39211546/bug-in-tensorflow-reduce-max-for-negative-infinity</denchmark-link>
 
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: Ubuntu 16.04
 Installed version of CUDA and cuDNN:
 <denchmark-code>-rw-r--r-- 1 root root   560184 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudadevrt.a
 lrwxrwxrwx 1 root root       16 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0
 lrwxrwxrwx 1 root root       19 May 25 17:52 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27
 -rw-r--r-- 1 root root   394472 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27
 -rw-r--r-- 1 root root   737516 May 25 17:48 /usr/local/cuda-8.0/lib64/libcudart_static.a
 lrwxrwxrwx 1 root root       13 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so -> libcudnn.so.5
 lrwxrwxrwx 1 root root       17 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5 -> libcudnn.so.5.0.5
 -rwxr-xr-x 1 root root 78065952 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn.so.5.0.5
 -rw-r--r-- 1 root root 68709594 Jul  4 16:22 /usr/local/cuda-8.0/lib64/libcudnn_static.a
 
 </denchmark-code>
 
 Installed from source.
 Commit hash: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/3cb39956e622b322e43547cf2b6e337020643f21>3cb3995</denchmark-link>
 
 Bazel:
 <denchmark-code>Build label: 0.3.0
 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Fri Jun 10 11:38:23 2016 (1465558703)
 Build timestamp: 1465558703
 Build timestamp as int: 1465558703
 
 </denchmark-code>
 
 	",1.0,pronobis,2016-08-31T23:51:56Z,"
 		<denchmark-link:https://github.com/pronobis>@pronobis</denchmark-link>
  Thanks for the bug report. I have fixed the bug internally at Google and in the open source Eigen repository (<denchmark-link:https://bitbucket.org/eigen/eigen/commits/741b932c41cebff49c4419b7e0ef9910ef1b2907>https://bitbucket.org/eigen/eigen/commits/741b932c41cebff49c4419b7e0ef9910ef1b2907</denchmark-link>
 ). We will have to wait until a few unrelated issues are resolved in Eigen before pushing the fix to open source TensorFlow. Stay tuned! :-)
 		",2.0,pronobis,2016-09-03T01:33:03Z,"
 		<denchmark-link:https://github.com/pronobis>@pronobis</denchmark-link>
  Update: The issues in Eigen have been resolved and the fix for this issue will be pushed out on Monday.
 		",3.0,pronobis,2016-09-05T00:50:24Z,"
 		Great! Thanks!
 		",fadc1f3c869c15b1890221bb6dfb0f7bd0f7c23d,Benoit Steiner,2016-09-02 16:17:43-07:00,MODIFY,4,tensorflow\core\kernels\constant_op_gpu.cu.cc,tensorflow\core\kernels\constant_op_gpu.cu.cc,1.0,,48,MODIFY,0.0,tensorflow\workspace.bzl,tensorflow\workspace.bzl,,,,,,,,,,,,,0.0,"15,16","15,16",,,,,,,,,,,,,,,,,,,Eigen::internal::scalar_const_op::packetOp,"Index,Index",48,50,1.0,45,"46,47",Eigen::internal::scalar_const_op::packetOp,,45,47,1.0,40,"40,41,42",Eigen::internal::scalar_const_op::operator ( ),,40,42,1.0,44,"41,42",Eigen::internal::scalar_const_op::operator ( ),"Index,Index",41,44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41674,dansitu,2020-07-23T20:04:14Z,2020-07-25T00:19:47Z,TensorFlow Lite for Microcontrollers sigaborts with a MobileNetV2 alpha=0.1 model,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.5, Linux
 GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.62) c++1
 
 Describe the current behavior
 I am using TensorFlow Lite for Microcontrollers at commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/4f69f62c61ecf3cd23286324af62d00643186ec2>4f69f62</denchmark-link>
 .
 I've trained two MobileNetV2 models in Keras with 48x48 input size and a single input channel, then converted to int8 quantized.
 I am attempting to run both models using TensorFlow Lite for Microcontrollers on x86, built with clang on MacOS and with gcc on Ubuntu.
 The first model has a MobileNetV2 filter scaling factor (a) of 0.35. This model runs perfectly.
 The second model has a scaling factor of 0.1. This model sigaborts during the Invoke() call.
 Strangely, both models run perfectly when executed from the OpenMV H7+ (Arm Cortex-M7), and the smaller model runs perfectly on the H7. It might be worth noting that on the OpenMV devices the model is stored in dynamic memory. That said, I've tried declaring the model without static on x86 and it has no impact.
 I've attached zips containing both model files, plus an example program that exhibits the sigabort.
 To build and run the example with an empty input:
 <denchmark-code>make -f Makefile.tflite
 ./build/edge-impulse-standalone """"
 </denchmark-code>
 
 Within the example code, the call to Invoke() is on line 260 of edge-impulse-sdk/classifier/ei_run_classifier.h.
 To switch to the 0.35 model, which doesn't sigabort, replace tflite-model, model-parameters, and edge-impulse-sdk directories with the versions contained within 0.35 grayscale.zip.
 Describe the expected behavior
 The a=0.1 model should run successfully on x86, the same as the 0.35 does.
 Standalone code to reproduce the issue
 Example code here:
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4968485/example-standalone-inferencing.zip>example-standalone-inferencing.zip</denchmark-link>
 
 The  model files are located here:
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4968496/models.zip>models.zip</denchmark-link>
 
 	",1.0,dansitu,2020-07-24T09:40:04Z,"
 		The actual crash is here: tensorflow/lite/kernels/internal/reference/integer_ops/add.h in the Add function.
 <denchmark-code>TFLITE_DCHECK_LE(params.input1_offset, int8_max_value);
 </denchmark-code>
 
 Here params.input1_offset is 128, which is invalid (max is 127). I assume this is a parser or quantization bug.
 Changing this parameter to 127 solves the issue, but I'm not sure where it comes from.
 <denchmark-code>  ArithmeticParams *p = (ArithmeticParams*)&params;
 
   if (p->input1_offset > 127) {
       p->input1_offset = 127;
   }
 </denchmark-code>
 
 		",2.0,dansitu,2020-07-24T17:55:26Z,"
 		Thank you Jan! Since this seems to perhaps be a converter issue, I'm attaching the SavedModel files for both models.
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4973569/0.1-grayscale-savedmodel.zip>0.1-grayscale-savedmodel.zip</denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/4973570/0.35-grayscale-savedmodel.zip>0.35-grayscale-savedmodel.zip</denchmark-link>
 
 		",3.0,dansitu,2020-07-25T00:19:49Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/41674>No</denchmark-link>
 
 		",dd918be82cb9702cc9ca022179629fbd8c6d3ed9,Nat Jeffries,2020-07-24 17:14:38-07:00,MODIFY,3,tensorflow\lite\kernels\internal\reference\integer_ops\add.h,tensorflow\lite\kernels\internal\reference\integer_ops\add.h,1.0,"71,72","69,70,71,72,73",,,,,4.0,dansitu,2020-08-09T21:28:02Z,"
 		<denchmark-link:https://github.com/dansitu>@dansitu</denchmark-link>
 
 <denchmark-link:https://github.com/janjongboom>@janjongboom</denchmark-link>
 
 Hi, i have run into problems running similar NN to yours.
 `base_model = tf.keras.applications.MobileNetV2(input_shape=(48, 48, 1), alpha=0.35, weights=None, include_top=False)
 x = base_model.output
 x = tf.keras.layers.Flatten()(x)
 x = Dense(2)(x) #final layer with softmax activation for N classes
 preds = tf.keras.layers.Softmax()(x)
 model=Model(inputs=base_model.input,outputs=preds) #specify the inputs and outputs
 converter = tf.lite.TFLiteConverter.from_keras_model(model)
 def representative_dataset():
 for i in range(500):
 yield([np.random.rand(1,48,48,1).astype(np.float32)])
 converter.optimizations = [tf.lite.Optimize.DEFAULT]
 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
 converter.representative_dataset = representative_dataset
 tflite_model = converter.convert()
 open(""mobilenet_test.tflite"", ""wb"").write(tflite_model)
 `
 the model crush on the microcontroller in assert (tflite::PreprocessSoftmaxScaling)
 thanks
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tflite::reference_integer_ops::Add,"params,input1_shape,input1_data,input2_shape,input2_data,output_shape,output_data",67,77,1.0,42,"64,65",tflite::reference_integer_ops::AddElementwise,"size,params,input1_data,input2_data,output_data",39,65,1.0,"26,27,28,29,30,31,32,33,34,35","31,32,33,34,35",tflite::reference_integer_ops::CheckArithmeticParams,params,26,35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
41712,EnderWiggin14,2020-07-24T21:08:42Z,2020-07-28T22:08:19Z,"Conv1DTranspose Dilation support - Might be a bug, IDK.","
 Thank you for submitting a TensorFlow documentation issue. Per our GitHub
 policy, we only address code/doc bugs, performance issues, feature requests, and
 build/installation issues on GitHub.
 The TensorFlow docs are open source! To get involved, read the documentation
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 Please provide a link to the documentation entry, for example:
 <denchmark-link:>https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1DTranspose</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 Conv1DTranspose - Dilation - Does not inform uses that dilation doesn't work for any value  of Dilation>1 because it isn't implemented yet.
 <denchmark-h:h3>Clear description</denchmark-h>
 
 Currently documentation says:
 ""an integer, specifying the dilation rate to use for dilated convolution. Currently, specifying a dilation_rate value != 1 is incompatible with specifying a stride value != 1.""
 This may not be implemented yet in the newest of nightly build, but with my tf-nightly==2.5.0dev20200629 build this didn't work. I fear updating to new nightly builds in case in breaks my research code which relies on nightly builds until Conv1DTranspose is released in a supported build.
 <denchmark-code>InvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.
 	 [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\Users\[username]\Desktop\libs_python\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]
 
 Function call stack:
 train_function
 </denchmark-code>
 
 This is with stride == 1.
 <denchmark-h:h3>Correct links</denchmark-h>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional.py</denchmark-link>
 
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Yes, setting my dilation to 1 gets rid of the issue.
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Not necessary. (I'm not sure if you are asking if I have define returns in my code or if my code returns a defined value, or if the documentation claims to return something)
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 <denchmark-code>InvalidArgumentError:  Current libxsmm and customized CPU implementations do not yet support dilation rates larger than 1.
 	 [[node test1_AE/decoder/conv1d_transpose/conv1d_transpose (defined at D:\Users\[username]\Desktop\libs_python\nn4n_autoencoder4.py:120) ]] [Op:__inference_train_function_2185]
 
 Function call stack:
 train_function
 </denchmark-code>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Nightly build, so no.
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 No.
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 I will not be doing so.
 <denchmark-h:h3>Test Code</denchmark-h>
 
 Note 1: This is built with tf-nightly==2.5.0dev20200626 which was removed from the PyPi archive for unknown reasons.
 Note 2: model.fit must be called for the error to occur. Simpy constructing and compiling the network was not enough to reproduce the error.
 <denchmark-code>import tensorflow as tf
 import tensorflow.keras as krs
 import numpy as np
 
 train_data = np.random.uniform(-1,1,(20,20))
 
 inputs = krs.Input((20,1))
 
 x = inputs
 
 x = krs.layers.Conv1D(1,3,strides = 1,padding='same',dilation_rate=2,activation='relu')(x)
 x = krs.layers.Flatten()(x)
 x = krs.layers.Dense(10,activation='relu')(x)
 x = krs.layers.Dense(2,activation='relu')(x)
 x = krs.layers.Dense(10,activation='relu')(x)
 x = krs.layers.Dense(20,activation='relu')(x)
 x = krs.layers.Reshape(target_shape=(20,1))(x)
 x = krs.layers.Conv1DTranspose(1,3,strides=1,dilation_rate=2,padding='same',activation='relu',output_padding=0)(x)
 output = krs.layers.Flatten()(x)
 
 model = krs.Model(inputs,output,name='test')
 
 model.compile(optimizer='adam',loss='MSE')
 
 model.summary()
 
 model.fit(train_data,train_data)
 </denchmark-code>
 
 	",1.0,EnderWiggin14,2020-07-27T04:48:39Z,"
 		<denchmark-link:https://github.com/EnderWiggin14>@EnderWiggin14</denchmark-link>
 
 Could you please update the issue template, we do not find any details.
 Please provide with simple indented stand alone code such that we can replicate the issue faced or if possible please provide a colab gist with error faced.
 		",2.0,EnderWiggin14,2020-07-27T12:49:30Z,"
 		Well, this was originally labeled as a Documentation issue so that ""Details"" section wasn't part of the standard template. I will try to get a short example added to the issue.
 Update: I have added code that reproduces the error on my system.
 		",3.0,EnderWiggin14,2020-07-27T14:05:38Z,"
 		I ran the code on nightly, please find the <denchmark-link:https://colab.research.google.com/gist/Saduf2019/c7953f7c83b7c403adadec092931076f/untitled284.ipynb>gist here</denchmark-link>
  and on <denchmark-link:https://colab.research.google.com/gist/Saduf2019/79a85b1424a878f63d7fa31b369c5f68/untitled296.ipynb>2.3.0rc0 the dilation support error is seen as reported</denchmark-link>
 .
 		",75801da4cd321aabbf79e78da1e5de1a10ba4c2a,A. Unique TensorFlower,2020-07-27 11:48:35-07:00,MODIFY,0,tensorflow\python\keras\layers\convolutional.py,tensorflow\python\keras\layers\convolutional.py,0.0,865,,,,,,4.0,EnderWiggin14,2020-07-27T17:01:13Z,"
 		It is very odd that the TF nightly build used in the GIST (tf-nightly==2.4.0dev20200724) doesn't have Conv1DTranspose() support. I have another system that's running a slightly older tf-nightly build (tf-nightly==2.3.0dev20200622) which was still available 2-3 weeks ago but appears to now be deleted. This is one of the two builds that I actively use as well, and it supports the Conv1DTranspose() method, but also lacks the dilation support.
 		",5.0,EnderWiggin14,2020-07-27T18:21:08Z,"
 		I can repro this with latest nightly build. See <denchmark-link:https://colab.research.google.com/gist/ymodak/a230402addcdda55e1cb87f7c1447ab7/untitled284.ipynb>gist</denchmark-link>
 
 When  and  in the  layer the code executes successfully.
 However for dilation_rate > 1 it fails.
 Perhaps the documentation can be updated for the  parameter. Thanks!
 		",6.0,EnderWiggin14,2020-07-28T22:08:19Z,"
 		Docs are updated now.  See commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/75801da4cd321aabbf79e78da1e5de1a10ba4c2a#diff-aa6c341a4b212afc57b49be73e689dc2>75801da</denchmark-link>
 
 Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,EnderWiggin14,2020-07-29T16:57:38Z,"
 		Thanks! Is there any news as to when the dilation_rate > 1 will be supported?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42217,kalaluthien,2020-08-11T04:00:22Z,2020-08-12T19:31:06Z,Typo in TFLite CoreML framework build command example,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary):
 TensorFlow version (use command below):
 Python version:
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 Describe the current behavior
 There is a single typo in build command example:
 
 
 
 tensorflow/tensorflow/lite/experimental/ios/BUILD.apple
 
 
         Lines 82 to 94
       in
       4b901e2
 
 
 
 
 
 
  # bazel build -c opt --config=ios_fat //tensorflow/lite/experimental/ios:TensorFlowLiteCCoreMl_framework 
 
 
 
  tflite_ios_static_framework( 
 
 
 
      name = ""TensorFlowLiteCCoreML_framework"", 
 
 
 
      hdrs = [ 
 
 
 
          "":coreml_delegate.h"", 
 
 
 
      ], 
 
 
 
      allowlist_symbols_file = "":allowlist_TensorFlowLiteCCoreML.txt"", 
 
 
 
      bundle_name = ""TensorFlowLiteCCoreML"", 
 
 
 
      minimum_os_version = TFL_MINIMUM_OS_VERSION, 
 
 
 
      deps = [ 
 
 
 
          ""//tensorflow/lite/experimental/delegates/coreml:coreml_delegate"", 
 
 
 
      ], 
 
 
 
  ) 
 
 
 
 
 
 This introduce bazel build fails to newbies:
 <denchmark-code>ERROR: Skipping '//tensorflow/lite/experimental/ios:TensorFlowLiteCCoreMl_framework': no such target '//tensorflow/lite/experimental/ios:TensorFlowLiteCCoreMl_framework': target 'TensorFlowLiteCCoreMl_framework' not declared in package 'tensorflow/lite/experimental/ios' (did you mean 'TensorFlowLiteCCoreML_framework'?) defined by /Users/mumu/hpcnt/tensorflow/tensorflow/lite/experimental/ios/BUILD
 WARNING: Target pattern parsing failed.
 ERROR: no such target '//tensorflow/lite/experimental/ios:TensorFlowLiteCCoreMl_framework': target 'TensorFlowLiteCCoreMl_framework' not declared in package 'tensorflow/lite/experimental/ios' (did you mean 'TensorFlowLiteCCoreML_framework'?) defined by /Users/mumu/hpcnt/tensorflow/tensorflow/lite/experimental/ios/BUILD
 INFO: Elapsed time: 21.011s
 INFO: 0 processes.
 FAILED: Build did NOT complete successfully (1 packages loaded)
 </denchmark-code>
 
 Describe the expected behavior
 At Line 82, TensorFlowLiteCCoreMl_framework should be fixed to TensorFlowLiteCCoreML_framework
 Standalone code to reproduce the issue
 Other info / logs Include any logs or source code that would be helpful to
 	",1.0,kalaluthien,2020-08-12T19:31:06Z,"
 		<denchmark-link:https://github.com/kalaluthien>@kalaluthien</denchmark-link>
  Thanks for finding the typo. we have updated the doc and it is already reflecting in the  <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/ios/BUILD.apple#L82>branch</denchmark-link>
 .
 As this is resolved, I am closing this issue. Thanks!
 		",2.0,kalaluthien,2020-08-12T19:31:08Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42217>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42217>No</denchmark-link>
 
 		",,,,,916bd91c2bc1ded18ca460520e922a71c3033418,A. Unique TensorFlower,2020-08-11 21:24:14-07:00,MODIFY,0,tensorflow\lite\experimental\ios\BUILD.apple,tensorflow\lite\experimental\ios\BUILD.apple,0.0,82,82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42281,DNXie,2020-08-12T20:06:58Z,2020-10-27T00:19:52Z,tf.nn.ctc_beam_search_decoder crashes(bad_alloc) when top_paths is large,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below):2.1.0
 Python version:3.7.6
 Bazel version (if compiling from source):N/A
 GCC/Compiler version (if compiling from source):N/A
 CUDA/cuDNN version:N/A
 GPU model and memory:N/A
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 tf.nn.ctc_beam_search_decoder crashes(bad_alloc) when top_paths is extremely large
 Describe the expected behavior
 Expect no crashes
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 import tensorflow as tf
 tf.nn.ctc_beam_search_decoder(inputs=tf.ones((1,1,1)), sequence_length=[1], top_paths=1000000000000)
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
   what():  std::bad_alloc
 Aborted (core dumped)
 	",1.0,DNXie,2020-08-13T06:08:12Z,"
 		I have tried in colab with TF version 2.1,2.3,nightly versions() and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/2969447f67d86720918d264620763d03/untitled247.ipynb>here</denchmark-link>
 .Thanks!
 		",2.0,DNXie,2020-08-18T16:56:59Z,"
 		Depending on the version of TF i use, I get different errors.
 Internally; I get the error
 ValueError: Attr top_paths has value 1000000000000 out of range for an int32
 using python3.6; which I think is the real expected error.
 In OSS ipython3 I get the error:
 MemoryError: std::bad_alloc
 at the line
 <denchmark-code>--> 118   _result = _execute.execute(b""CTCBeamSearchDecoder"", top_paths + top_paths +
     119                              top_paths + 1, inputs=_inputs_flat, attrs=_attrs,
     120                              ctx=ctx, name=name)
 </denchmark-code>
 
 (eager execution)
 		",3.0,DNXie,2020-08-18T16:58:03Z,"
 		<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  who should take this over?  looks like an error in attr size validation?  or perhaps we're allowing an int64 through when it should be an int32; and then differences in tcmalloc vs. some other malloc externally?
 		",7bd42cf6ba061ba7c06c072c9d962abe331461eb,Rohan Jain,2020-10-26 17:18:34-07:00,MODIFY,1,tensorflow\core\framework\node_def_util.cc,tensorflow\core\framework\node_def_util.cc,1.0,"450,453,454,455,456",450,MODIFY,1.0,tensorflow\core\framework\node_def_util_test.cc,tensorflow\core\framework\node_def_util_test.cc,4.0,DNXie,2020-08-18T16:58:53Z,"
 		The internal error is correctly identified here, I think:
 <denchmark-code>.../tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)
    2003         op_def = self._graph._get_op_def(node_def.op)
    2004       self._c_op = _create_c_op(self._graph, node_def, inputs,
 -> 2005                                 control_input_ops, op_def)
    2006       name = compat.as_str(node_def.name)
    2007     # pylint: enable=protected-access
 
 .../tensorflow/python/framework/ops.py in _create_c_op(graph, node_def, inputs, control_inputs, op_def)
    1843   except errors.InvalidArgumentError as e:
    1844     # Convert to ValueError for backwards compatibility.
 -> 1845     raise ValueError(str(e))
    1846 
    1847   return c_op
 
 ValueError: Attr top_paths has value 1000000000000 out of range for an int32
 </denchmark-code>
 
 		",5.0,DNXie,2020-08-18T16:59:32Z,"
 		<denchmark-link:https://github.com/rohan100jain>@rohan100jain</denchmark-link>
  can you help triage this? The error seems to be in the TF C API layer.
 		",6.0,DNXie,2020-10-27T00:19:54Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42281>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42281>No</denchmark-link>
 
 		",1.0,"451,452,453,454,455,456,457,458,459,460,461,462,463",,tensorflow::TEST,"OutputTypesForNode,LargeOutput",451,463,MODIFY,0.0,tensorflow\python\eager\pywrap_tfe_src.cc,tensorflow\python\eager\pywrap_tfe_src.cc,0.0,"3820,3823,3834,3837,3838,3839,3840,3841,3842,3843,3844,3845","3820,3823,3834",MODIFY,2.0,tensorflow\python\eager\pywrap_tfe_test.py,tensorflow\python\eager\pywrap_tfe_test.py,1.0,"241,242,243,244,245,246,247,248,249,250,251,252,253,254,255",,tensorflow::AddArgToSig,,445,507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testSlowPathExecute_VeryLargeOutputs,self,241,255,MODIFY,1.0,tensorflow\python\tfe_wrapper.cc,tensorflow\python\tfe_wrapper.cc,1.0,"207,208,209,210,211,212,213",,tensorflow::InputTFE_OutputTensorHandles,num_outputs,189,358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"229,230,231,232,233,234,235,236,237",,testFastPathExecute_VeryLargeOutputs,self,229,237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42331,DNXie,2020-08-13T19:11:01Z,2020-08-21T19:07:53Z,tf.nest.flatten crashes (abort) when expand_composites's constraints are violated,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below):2.1.0
 Python version:3.7.6
 Bazel version (if compiling from source):N/A
 GCC/Compiler version (if compiling from source):N/A
 CUDA/cuDNN version:N/A
 GPU model and memory:N/A
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 tf.nest.flatten crashes (abort) when expand_composites is 0D boolean is violated.
 Describe the expected behavior
 Expect no crash
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 import tensorflow as tf
 import numpy as np
 tf.nest.flatten(structure = np.zeros((1)), expand_composites=tf.ones((2)))
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 terminate called after throwing an instance of 'pybind11::error_already_set'
   what():  SystemError: <class 'type'> returned a result with an error set
 
 At:
   /root/miniconda3/lib/python3.7/site-packages/numpy/core/arrayprint.py(1388): _array_repr_implementation
   /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/util/nest.py(338): flatten
   <stdin>(1): <module>
 
 Aborted (core dumped)
 	",1.0,DNXie,2020-08-14T14:26:36Z,"
 		Was able to reproduce the issue with TF v2.1, TF v2.3 and TF-nightly. Please find the gist of it <denchmark-link:https://colab.research.google.com/gist/amahendrakar/b0a49b8ac355427b554aca7a08e12c75/42331.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,DNXie,2020-08-14T18:05:37Z,"
 		<denchmark-link:https://github.com/DNXie>@DNXie</denchmark-link>
  How is this different from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/42329>#42329</denchmark-link>
 ? If this is duplicate, then close it. We will follow the <denchmark-link:https://github.com/tensorflow/tensorflow/issues/42329>#42329</denchmark-link>
 . Thanks!
 		",3.0,DNXie,2020-08-15T23:54:26Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
   Is the fix for <denchmark-link:https://github.com/tensorflow/tensorflow/issues/42329>#42329</denchmark-link>
  also fix the bug for this API? Thanks!
 		",35c2a97ddca6da7d5a21d5ee3e2869eec68299f9,A. Unique TensorFlower,2020-08-21 12:00:12-07:00,MODIFY,1,tensorflow\python\util\nest.py,tensorflow\python\util\nest.py,1.0,340,,,,,,4.0,DNXie,2020-08-17T16:49:05Z,"
 		<denchmark-link:https://github.com/DNXie>@DNXie</denchmark-link>
  Sorry. I didn't notice the difference between those two issues. Now I understood the difference. Thanks!
 		",5.0,DNXie,2020-08-21T00:04:16Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
  Could you please also fix this one? Thanks!
 		",6.0,DNXie,2020-08-21T19:07:55Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42331>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42331>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,flatten,"structure,expand_composites",275,341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42364,dakl,2020-08-14T12:52:54Z,2020-09-01T21:55:44Z,`tf.data.experimental.snapshot()` hangs when using GCS paths,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9.12 stretch
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): 2.3
 Python version: 3.5.7
 Bazel version (if compiling from source): NA
 GCC/Compiler version (if compiling from source): NA
 CUDA/cuDNN version: CUDA 10.1
 GPU model and memory: Nvidia Tesla T4
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 tf.data.experimental.snapshot() hangs when using a Google Storage path.
 Describe the expected behavior
 tf.data.experimental.snapshot() works.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>import tensorflow as tf
 for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/my-path')): break
 </denchmark-code>
 
 hangs. Using a local path
 <denchmark-code>for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break
 </denchmark-code>
 
 works as expected.
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,dakl,2020-08-14T12:53:28Z,"
 		ping <denchmark-link:https://github.com/piwell>@piwell</denchmark-link>
  <denchmark-link:https://github.com/carlthome>@carlthome</denchmark-link>
 
 		",2.0,dakl,2020-08-18T01:34:05Z,"
 		Confirming that I was able to reproduce this in Colab and with a GCP deep learning vm instance. Although it was hanging, the metadata files did get written to the bucket. Did you see the same?
 		",3.0,dakl,2020-08-21T12:28:35Z,"
 		Yes, <denchmark-link:https://github.com/nikitamaia>@nikitamaia</denchmark-link>
 , we saw the same behavior with files popping up on the bucket.
 		",532966cad34471bded2b0483737e8e8d23bc4720,Frank Chen,2020-09-01 14:51:39-07:00,MODIFY,1,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc,1.0,"677,678,679,680,681,682,690,691,692,693,709,710","676,677,678,686,702",MODIFY,1.0,tensorflow\core\platform\cloud\gcs_file_system.cc,tensorflow\core\platform\cloud\gcs_file_system.cc,4.0,dakl,2020-08-21T12:29:55Z,"
 		
 Using a local path
 for _ in tf.data.Dataset.range(10).apply(tf.data.experimental.snapshot('gs://my-bucket/deleteme')): break
 
 
 Ping <denchmark-link:https://github.com/dakl>@dakl</denchmark-link>
 , the MCVE is not using a local path. Edit the OP?
 		",5.0,dakl,2020-08-29T05:39:27Z,"
 		Thanks for reporting this issue! I've investigated and the underlying cause is gcs_file_system.cc not handling calls to NewAppendableFile() correctly when the file does not yet exist.
 The deadlock happens due to the following sequence of events:
 The error is propagated all the way back to AsyncWriter::WriterThread, which then returns with the error code.
 However, by this time, the entire writing process has finished and the main iterator thread calls SignalEOF from GetNextInternal. SignalEOF acquires the WriterThread's mu_ lock and then tries to clear() writers_ vector, containing all the AsyncWriters.
 To clear the vector, C++ needs to call AsyncWriter's default destructor, which then blocks on the thread_ within AsyncWriter finishing.
 Now, the AsyncWriter thread is still trying to call the done() function that was passed in, which tries to acquire the same mu_ lock that SignalEOF was already holding.
 This results in a deadlock where the main thread calling writers_.clear() cannot proceed because the AsyncWriter thread has not terminated, but the AsyncWriter thread is blocked trying to acquire the mu_ lock held by the main thread.
 Will fix the underlying problem and the deadlock in an upcoming commit.
 		",6.0,dakl,2020-09-01T21:55:45Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42364>No</denchmark-link>
 
 		",1.0,"1297,1298,1299",,tensorflow::GcsFileSystem::NewAppendableFile,"fname,token,result",1277,1340,MODIFY,1.0,tensorflow\core\platform\cloud\gcs_file_system_test.cc,tensorflow\core\platform\cloud\gcs_file_system_test.cc,1.0,"1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474",,,,,,,,,tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::Iterator::Writer::GetNextInternal,"ctx,out_tensors,end_of_sequence",651,721,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::TEST,"GcsFileSystemTest,NewAppendableFile_ObjectDoesNotExist",1445,1474,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42386,aingo03304,2020-08-15T06:55:39Z,2021-01-03T19:21:30Z,GradientTape.gradient needs to check target type.,"
 
 
 
 tensorflow/tensorflow/python/eager/backprop.py
 
 
          Line 991
       in
       b36436b
 
 
 
 
 
 
        target: a list or nested structure of Tensors or Variables to be 
 
 
 
 
 
 Recently, I wrote some code below which is very simple,
 tape.gradient(loss, model.trainable_variables)
 but it raises TypeError: Cannot convert value None to a Tensorflow DType.
 It turns out my custom loss function is returning None. yes, I know I was dumb.
 I had to check all of the related tensorflow code lines to find this dumb problem.
 So, I suggest the type checking block inside of this code.
 I think this suggestion can be helpful for fools like me.
 Thanks in advance :)
 	",1.0,aingo03304,2020-08-17T10:07:54Z,"
 		<denchmark-link:https://github.com/aingo03304>@aingo03304</denchmark-link>
 ,
 In order to expedite the trouble-shooting process, could you please provide the TensorFlow version, the complete code to reproduce the issue and the dataset you are using. Thanks!
 		",2.0,aingo03304,2020-08-18T06:26:06Z,"
 		<denchmark-link:https://github.com/amahendrakar>@amahendrakar</denchmark-link>
  Thank you for the kind and quick response!
 You can reproduce with this code sample which is very simple.
 import tensorflow as tf
 def get_loss(y_true, y_pred):
     pass
 
 with tf.GradientTape() as tape:
     tape.gradient(get_loss(1, 2), None)
 It raises TypeError: Cannot convert value None to a TensorFlow DType.on tensorflow 2.3.0.
 It is foolish code, but the error is raised in tensorflow/python/framework/dtypes.py and the message doesn't tell you that the loss function is wrong.
 Because the error message should help people to find out where the code has a fault, I suggested you add some type check features in gradient function which is visible to users.
 And the docstring on gradient function says,
 <denchmark-code> Args:
       target: a list or nested structure of Tensors or Variables to be
         differentiated.
       sources: a list or nested structure of Tensors or Variables. `target`
         will be differentiated against elements in `sources`.
       output_gradients: a list of gradients, one for each element of
         target. Defaults to None.
       unconnected_gradients: a value which can either hold 'none' or 'zero' and
         alters the value which will be returned if the target and sources are
         unconnected. The possible values and effects are detailed in
         'UnconnectedGradients' and it defaults to 'none'.
 </denchmark-code>
 
 I would add the message like TypeError: target should be a list or nested structure of Tensors or Variables to be differentiated, but (TYPE) given.
 		",3.0,aingo03304,2020-08-21T05:47:56Z,"
 		<denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>
  Hi, can you tell me what the bug related to this issue? Just curious about it.
 		",f25125e50bab365642335413356466883bf7f361,A. Unique TensorFlower,2021-01-03 11:12:43-08:00,MODIFY,0,tensorflow\python\eager\backprop.py,tensorflow\python\eager\backprop.py,0.0,"1010,1032,1033,1034,1035,1036",,,,,,4.0,aingo03304,2021-01-03T19:21:30Z,"
 		Closing this issue since the <denchmark-link:https://github.com/tensorflow/tensorflow/commit/f25125e50bab365642335413356466883bf7f361>3edb086</denchmark-link>
  commit fixes it. Thanks!
 		",5.0,aingo03304,2021-01-03T19:21:31Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42386>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/42386>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
42458,danielplatt,2020-08-18T12:08:10Z,2020-08-19T17:34:25Z,CosineSimilarity documentation range incorrect,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity>https://www.tensorflow.org/api_docs/python/tf/keras/losses/CosineSimilarity</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 ""Note that it is a negative quantity between -1 and 0"" should be changed to ""Note that it is a negative quantity between -1 and 1""
 <denchmark-h:h3>Correct links</denchmark-h>
 
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Don't plan to submit pull request
 	",1.0,danielplatt,2020-08-18T16:10:08Z,"
 		There is no negative quantity in (0,1).
 		",2.0,danielplatt,2020-08-19T06:22:01Z,"
 		This is fixed now with commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/7e7641d95c6c9b7e46b129c10ec7a965fb2f848d>7e7641d</denchmark-link>
  . Thanks!
 		",,,,,7e7641d95c6c9b7e46b129c10ec7a965fb2f848d,A. Unique TensorFlower,2020-08-18 22:20:48-07:00,MODIFY,0,tensorflow\python\keras\losses.py,tensorflow\python\keras\losses.py,0.0,"1731,1732,1733,1734,1735,1736,1737","1731,1732,1733,1734,1735,1736",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43449,woodyx218,2020-09-22T04:59:17Z,2020-09-23T01:26:57Z,tf.autodiff.ForwardAccumulator fails for Embedding layer,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): pip
 TensorFlow version (use command below): 2.4.0-dev20200813
 Python version: 3.8.3
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 10.1.168/7.6.5
 GPU model and memory:  GeForce GTX 1050/4GB
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 Calculating the Jacobian-vector product of an embedding layer produces
 AttributeError: 'IndexedSlices' object has no attribute '_as_tf_output'
 Describe the expected behavior
 No error, just like Dense, LSTM, convolutional layers. See a notebook link below that shows this error is ONLY related to Embedding layer.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 <denchmark-code>import tensorflow as tf
 
 class RNN_Model(tf.keras.Model):
     def __init__(self):
         super(RNN_Model, self).__init__()
         self.embed=tf.keras.layers.Embedding(5,1)
         self.d2 = tf.keras.layers.Dense(2)
         self(tf.constant([4,3,2])) # initialize
     
     @tf.function
     def call(self, x):
         x = self.embed(x)
         return self.d2(x)
      
 model=RNN_Model()
 
 v=[tf.ones(w.shape) for w in model.trainable_variables]
 with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
     loss = tf.reduce_sum(tf.constant([1,0])-model(tf.constant([[2,2,2], [1,1,1]]), training=True))
 acc.jvp(loss)
 </denchmark-code>
 
 See a complete example in <denchmark-link:https://drive.google.com/file/d/10Cb1nxcovmBSNE5zJOvRhyu-lHYhux15/view?usp=sharing>this notebook</denchmark-link>
 
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,woodyx218,2020-09-22T11:17:46Z,"
 		Have you tried to build the model  (model.build) after model creation model=RNN_Model()?
 		",2.0,woodyx218,2020-09-22T15:00:29Z,"
 		Yes I did. Still got the same error. It has nothing to do with building but with embedding layer. Please see my newly added example notebook.
 		",3.0,woodyx218,2020-09-22T15:20:32Z,"
 		I don't think it is only the embedding layer. Can you try to remove  @tf.function from the call in the embedding case?
 		",79fce11a6cc8c3d9fd85e9a04b596fd4ea4d7b79,Allen Lavoie,2020-09-22 18:26:01-07:00,MODIFY,3,tensorflow\python\eager\forwardprop_test.py,tensorflow\python\eager\forwardprop_test.py,1.0,"893,894,895,896,897,898",,MODIFY,0.0,tensorflow\python\eager\function.py,tensorflow\python\eager\function.py,4.0,woodyx218,2020-09-22T18:38:33Z,"
 		If I remove @tf.function from the call, there is some other error. Please take a look at the notebook hyper-link. It is only the embedding layer. Keeping @tf.function works fine for other layers.
 		",5.0,woodyx218,2020-09-22T18:54:01Z,"
 		If I remove @tf.function from the call is working:
 <denchmark-code>import tensorflow as tf
 from tensorflow.keras.layers import Dense, Embedding,Bidirectional,LSTM
 
 class RNN_Model(tf.keras.Model):
     def __init__(self, dataset):
         super(RNN_Model, self).__init__()
         self.embed=Embedding(maxfeature,64)
         self.blstm = Bidirectional(LSTM(64))
         self.d1 = Dense(64, activation='relu')
         self.d2 = Dense(2)
                 
     # Doing this to initialize model.trainable_variables
         for text, labels in dataset:
             self(text)
             break
 
     def call(self, x):
         x = self.embed(x)
         x = self.blstm(x)
         x = self.d1(x)
         x = self.d2(x)
         return tf.nn.log_softmax(x)
         
 ## Training Settings
 batch_size = 64
 maxfeature=100
 SEQ_LEN=32
 
 ## Load Problems
 (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=maxfeature)
 x_train=tf.keras.preprocessing.sequence.pad_sequences(x_train,maxlen=SEQ_LEN)
 train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(5000).batch(batch_size)
 
 model=RNN_Model(train_ds)
 
 loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
 
 # Compute JVP
 card = train_ds.cardinality()
 step=0
 for images, labels in train_ds:
     step+=1
     print(""\r%d/%d"" % (step , card), end="""")
     v=[tf.ones(w.shape) for w in model.trainable_variables]
     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
         loss = loss_obj(labels, model(images, training=True))
     acc.jvp(loss)
 </denchmark-code>
 
 		",6.0,woodyx218,2020-09-22T18:57:58Z,"
 		/cc <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  if could be interested in this tracing case.
 		",0.0,"912,913,914,915,916,917,918",,,,,,MODIFY,3.0,tensorflow\python\keras\integration_test\forwardprop_test.py,tensorflow\python\keras\integration_test\forwardprop_test.py,1.0,"275,276",,,,,,,,,testIndexSlicesGrad,self,893,898,1.0,"900,901,902,903,904,905,906,907,908,909",,testIndexSlicesGradInFunction,self,900,909,1.0,"902,903",,testIndexSlicesGradInFunction.f,a,902,903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,woodyx218,2020-09-22T19:01:58Z,"
 		I am afraid on my end, it shows the following error with your code:
 
 
 StagingError                              Traceback (most recent call last)
  in 
 44     v=[tf.ones(w.shape) for w in model.trainable_variables]
 45     with tf.autodiff.ForwardAccumulator(primals = model.trainable_variables, tangents = v) as acc:
 ---> 46         loss = loss_obj(labels, model(images, training=True))
 47     acc.jvp(loss)
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
 988
 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
 --> 990           outputs = call_fn(inputs, *args, **kwargs)
 991
 992         if self._activity_regularizer:
  in call(self, x)
 17     def call(self, x):
 18         x = self.embed(x)
 ---> 19         x = self.blstm(x)
 20         x = self.d1(x)
 21         x = self.d2(x)
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\wrappers.py in call(self, inputs, initial_state, constants, **kwargs)
 528
 529     if initial_state is None and constants is None:
 --> 530       return super(Bidirectional, self).call(inputs, **kwargs)
 531
 532     # Applies the same workaround as in RNN.__call__
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
 988
 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
 --> 990           outputs = call_fn(inputs, *args, **kwargs)
 991
 992         if self._activity_regularizer:
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\wrappers.py in call(self, inputs, training, mask, initial_state, constants)
 641         forward_state, backward_state = None, None
 642
 --> 643       y = self.forward_layer(forward_inputs,
 644                              initial_state=forward_state, **kwargs)
 645       y_rev = self.backward_layer(backward_inputs,
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent.py in call(self, inputs, initial_state, constants, **kwargs)
 660
 661     if initial_state is None and constants is None:
 --> 662       return super(RNN, self).call(inputs, **kwargs)
 663
 664     # If any of initial_state or constants are specified and are Keras
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\base_layer.py in call(self, *args, **kwargs)
 988
 989         with ops.enable_auto_cast_variables(self._compute_dtype_object):
 --> 990           outputs = call_fn(inputs, *args, **kwargs)
 991
 992         if self._activity_regularizer:
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent_v2.py in call(self, inputs, mask, training, initial_state)
 1269           # GPU implementation when GPU is available.
 1270           if can_use_gpu:
 -> 1271             last_output, outputs, new_h, new_c, runtime = gpu_lstm(
 1272                 **gpu_lstm_kwargs)
 1273           else:
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\layers\recurrent_v2.py in gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths)
 1514       # Reverse axis 0 since the input is already convert to time major.
 1515       inputs = array_ops.reverse(inputs, axis=[0])
 -> 1516     outputs, h, c, _ = gen_cudnn_rnn_ops.cudnn_rnn(
 1517         inputs, input_h=init_h, input_c=init_c, params=params, is_training=True,
 1518         rnn_mode='lstm')
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\ops\gen_cudnn_rnn_ops.py in cudnn_rnn(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name)
 97       pass
 98     try:
 ---> 99       return cudnn_rnn_eager_fallback(
 100           input, input_h, input_c, params, rnn_mode=rnn_mode,
 101           input_mode=input_mode, direction=direction, dropout=dropout,
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\ops\gen_cudnn_rnn_ops.py in cudnn_rnn_eager_fallback(input, input_h, input_c, params, rnn_mode, input_mode, direction, dropout, seed, seed2, is_training, name, ctx)
 180                              attrs=_attrs, ctx=ctx, name=name)
 181   if _execute.must_record_gradient():
 --> 182     _execute.record_gradient(
 183         ""CudnnRNN"", _inputs_flat, _attrs, _result)
 184   _result = _CudnnRNNOutput._make(_result)
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py in _record_gradient(op_name, inputs, attrs, results)
 173
 174 def _record_gradient(op_name, inputs, attrs, results):
 --> 175   return pywrap_tfe.TFE_Py_RecordGradient(op_name, inputs, attrs, results,
 176                                           ops.get_name_scope())
 177
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py in _jvp_dispatch(op_name, attr_tuple, inputs, outputs, tangents, use_batch)
 211   # means we may trace a few more exact shapes before moving on to relaxation.
 212   if _TRACE_COUNT.get(op_name, 0) < _TRACE_COUNT_LIMIT:
 --> 213     return _jvp_exact_shapes(op_name, attr_tuple, inputs, outputs, tangents,
 214                              use_batch)
 215   return _jvp_relaxed_shapes(op_name, attr_tuple, inputs, outputs, tangents,
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in call(self, *args, **kwargs)
 2937     with self._lock:
 2938       graph_function, flat_args, flat_kwargs = 
 -> 2939           self._maybe_define_function(args, kwargs)
 2940     return graph_function._filtered_call(flat_args, flat_kwargs)  # pylint: disable=protected-access
 2941
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in _maybe_define_function(self, args, kwargs)
 3342
 3343       self._function_cache.missed.add(call_context_key)
 -> 3344       graph_function = self._create_graph_function(args, kwargs)
 3345       self._function_cache.primary[cache_key] = graph_function
 3346
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
 3187     arg_names = base_arg_names + missing_arg_names
 3188     graph_function = ConcreteFunction(
 -> 3189         func_graph_module.func_graph_from_py_func(
 3190             self._name,
 3191             self._python_function,
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
 985         _, original_func = tf_decorator.unwrap(python_func)
 986
 --> 987       func_outputs = python_func(*func_args, **func_kwargs)
 988
 989       # invariant: func_outputs contains only Tensors, CompositeTensors,
 ~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\func_graph.py in wrapper(*args, **kwargs)
 972           except Exception as e:  # pylint:disable=broad-except
 973             if hasattr(e, ""ag_error_metadata""):
 --> 974               raise e.ag_error_metadata.to_exception(e)
 975             else:
 976               raise
 StagingError: in user code:
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py:179 _jvp_helper_wrapper  *
     return _jvp_helper(op_name, attr_tuple, inputs, outputs, tangents)
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\forwardprop.py:138 _jvp_helper  **
     nontrivial_output_tangents = transpose_tape.gradient(
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py:1080 gradient
     flat_grad = imperative_grad.imperative_grad(
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\imperative_grad.py:71 imperative_grad
     return pywrap_tfe.TFE_Py_TapeGradient(
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\backprop.py:151 _gradient_function
     grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access
 C:\Users\Fight4Life\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\framework\registry.py:98 lookup
     raise LookupError(
 
 LookupError: gradient registry has no entry for: CudnnRNNBackprop
 
 
 		",testEmbeddingLayerInFunction.call,"self,x",275,276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,woodyx218,2020-09-22T19:09:02Z,"
 		I've just run your notebook on colab and it is running fine on CPU but we have the same error on the GPU runtime.
 		",,,,,,,,,,,,,,,1.0,"269,270,271,272",,testEmbeddingLayerInFunction.__init__,self,269,272,1.0,"265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286",,testEmbeddingLayerInFunction,self,265,286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,woodyx218,2020-09-22T19:11:18Z,"
 		The GPU one I think that it is another issue like <denchmark-link:https://github.com/tensorflow/tensorflow/issues/37091#issuecomment-612205749>#37091 (comment)</denchmark-link>
  /cc <denchmark-link:https://github.com/kaixih>@kaixih</denchmark-link>
 
 		",10.0,woodyx218,2020-09-23T01:26:58Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43449>No</denchmark-link>
 
 		",11.0,woodyx218,2020-09-23T03:08:15Z,"
 		Although it works on CPU now, it is still desirable to work on GPU.
 		",12.0,woodyx218,2020-09-23T10:11:28Z,"
 		<denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  Are you still interested for the tracing part of the issue?
 <denchmark-code>@tf.function
 def call(self, x):
 </denchmark-code>
 
 		",13.0,woodyx218,2020-09-23T15:59:06Z,"
 		Looks like the GPU issue has another bug (which is closed, but Scott would have more background there). It's just a missing RegisterGradient call for the gradient op.
 <denchmark-link:https://github.com/bhack>@bhack</denchmark-link>
  what tracing issue? The fix has a test with/without tf.function.
 		",14.0,woodyx218,2020-09-23T16:13:03Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  Yes I've retested the code example now and it is working also on GPU (at least on Colab).
 It was just a problem about timezones with tf-nightly builds to catch that commit.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43529,shlomi-amitai,2020-09-24T08:14:32Z,2020-09-25T01:31:12Z,'tf.TensorScatterUpdate' Conversion to tflite,"
 System information
 
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 windows 10
 TensorFlow installed from (source or binary):
 python binary
 TensorFlow version (or github SHA if from source):
 2.3
 
 Provide the text output from tflite_convert
 error: 'tf.TensorScatterUpdate' op is neither a custom op nor a flex op
 (im already using TF ops)
 <denchmark-code># Copy and paste here
 </denchmark-code>
 
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 Also, please include a link to a GraphDef or the model if possible.
 Any other info / logs
 Include any logs or source code that would be helpful to diagnose the problem.
 If including tracebacks, please include the full traceback. Large logs and files
 should be attached.
 	",1.0,shlomi-amitai,2020-09-24T09:02:40Z,"
 		
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 
 		",2.0,shlomi-amitai,2020-09-24T09:03:38Z,"
 		<denchmark-link:https://github.com/shlomi-amitai>@shlomi-amitai</denchmark-link>
 
 Please share simple stand alone code to replicate the issue along with error log for us to analyse.
 		",3.0,shlomi-amitai,2020-09-25T00:20:47Z,"
 		I will add tf.TensorScatterUpdate op and related ops to the Select TF ops option. Please stay tuned.
 		",8dbab4f830b2f77199c22e7c1763bea8e523079b,Jaesung Chung,2020-09-24 18:29:16-07:00,MODIFY,1,tensorflow\lite\delegates\flex\allowlisted_flex_ops.cc,tensorflow\lite\delegates\flex\allowlisted_flex_ops.cc,1.0,"523,524,525,526,527",,,,,,4.0,shlomi-amitai,2020-09-25T01:30:12Z,"
 		I added tf.TensorScatterUpdate op support in the Select TF ops option. Please try the conversion again with the tomorrow's tf-nightly version.
 		",5.0,shlomi-amitai,2020-09-29T12:32:56Z,"
 		Hi <denchmark-link:https://github.com/abattery>@abattery</denchmark-link>
  ,
 Thank you so much, I tried, and don't see any error regarding the tf.TensorScatterUpdate op,
 though I get now a new error related to tf.concat op.
 Maybe something is broken there?
 (problem in concat_415)
 		",6.0,shlomi-amitai,2020-09-29T12:42:33Z,"
 		:0: error: loc(""Concat_415""): operand type 'tensor<1x32x112x20xf32>' is not compatible with preceding operands; expected dimension at index 3: 160
 :0: note: loc(""Concat_415""): see current operation: %1202 = ""tf.ConcatV2""(%525, %652, %1201, %1197, %1189, %1185, %24) {device = """"} : (tensor<1x64x112x160xf32>, tensor<1x128x112x160xf32>, tensor<1x32x112x20xf32>, tensor<1x32x112x10xf32>, tensor<1x32x112x5xf32>, tensor<1x32x112x2xf32>, tensor) -> tensor<1x?x112x160xf32>
 		",,,,,,,,,,,,,,,,,,,,,,tflite::flex::GetFlexAllowlist,,25,570,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4358,danijar,2016-09-13T17:59:22Z,2016-09-19T23:37:37Z,Wrong example script in the docs for preprocessing data,"
 I was reading the <denchmark-link:https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#preprocessing>docs for preprocessing</denchmark-link>
 , which is a small paragraph linking to the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10.py>CIFAR-10 network</denchmark-link>
  as an example. However, that script does not perform any preprocessing. Do we have a better example illustrating preprocessing steps like data normalization, distorting images, etc?
 	",1.0,danijar,2016-09-13T21:23:27Z,"
 		This is a question better suited for StackOverflow. Please ask it there and tag it with the tensorflow tag.""
 		",2.0,danijar,2016-09-13T21:44:44Z,"
 		<denchmark-link:https://github.com/andydavis1>@andydavis1</denchmark-link>
  Are you sure? I'm mainly pointing at a ""bug"" in the docs. I'm not asking for anything, my question was just to suggest a way of improving the docs.
 		",3.0,danijar,2016-09-13T21:58:14Z,"
 		Ah ok. I took that last sentence you wrote "" Do we have a better example illustrating preprocessing steps like data normalization, distorting images, etc?"". As a general question.
 		",345132fb42c05807206e892a3cb497c0bcd58af1,Danijar Hafner,2016-09-19 16:37:37-07:00,MODIFY,0,tensorflow\g3doc\how_tos\reading_data\index.md,tensorflow\g3doc\how_tos\reading_data\index.md,0.0,"175,256","175,256",,,,,4.0,danijar,2016-09-15T17:06:39Z,"
 		The folder holding the file referenced in the docs has, e.g. <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_input.py>https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/models/image/cifar10/cifar10_input.py</denchmark-link>
 , which does contain a  routine, is this what you're looking for? If so, then the easiest fix is just to change the link in the docs to point to the cifar10_input.py file instead of the cifar10.py file that imports it (line 47)
 		",5.0,danijar,2016-09-15T20:37:31Z,"
 		Thanks. Yes, I think that would be suitable. Do you want to create a pull request? Otherwise, I can do that.
 		",6.0,danijar,2016-09-16T19:29:16Z,"
 		You can go ahead.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4361,bsautermeister,2016-09-13T18:43:34Z,2017-02-13T17:42:10Z,Update tf.contrib.layers.batch_norm() docs,"
 Tensorflow version that I use : 0.10 (pip package)
 <denchmark-h:hr></denchmark-h>
 
 I took heavy use of tf.contrib.layers.batch_norm() the last weeks.
 After facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:
 
 #1122
 http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow
 
 I would suggest to do following improvements to make it more clear:
 1) Update example in doc-string:
 The example tells in case we use update_collections on its defaults, we have to include this:
 <denchmark-code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
 if update_ops:
     updates = tf.group(update_ops)
     total_loss = control_flow_ops.with_dependencies([updates], total_loss)
 </denchmark-code>
 
 But this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:
 <denchmark-code>from tensorflow.python import control_flow_ops
 
 update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
 if update_ops:
     updates = tf.tuple(update_ops)
     total_loss = control_flow_ops.with_dependencies(updates, total_loss)
 </denchmark-code>
 
 As a side question, why do we apply it to the total_loss, and not to the train_op directly, as described in the doc-string text. Added a dependency to total_loss works, but grouping it with the train_op would make the example more clear in my opinion, because we do batch-statistic updates only during training.
 2) UPDATE_OPS in combination with reuse varscope:
 This is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to UPDATE_OPS nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?
 Or is it required to filter the update-ops after collecting them with update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS), so that each one is executed just once?
 To sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:
 <denchmark-code>if not reuse:
     # Collect the updates to be computed later.
     ops.add_to_collections(updates_collections, update_moving_mean)
     ops.add_to_collections(updates_collections, update_moving_variance)
 </denchmark-code>
 
 In my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling tf.get_collection(tf.GraphKeys.UPDATE_OPS). As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.
 3) Handling of is_training parameter:
 I have seen a lot of examples people doing something like this in their code to handle the is_training parameter:
 <denchmark-code>def batch_norm_layer(x,train_phase,scope_bn):
     bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
     updates_collections=None,
     is_training=True)
     bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
     updates_collections=None,
     is_training=False)
     bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
     return bn
 </denchmark-code>
 
 As far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.
 4) Usage on Multi-GPU configuration
 a) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).
 b) When I use tf.contrib.batch_norm() within a multi-GPU system, I get an error like this:
 <denchmark-code>InvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': 
 Could not satisfy explicit device specification '/device:GPU:1' because no supported kernel 
 for GPU devices is available.
 ...
 </denchmark-code>
 
 Hence, to we have to wrap evey batch_norm() call with tf.device(""/cpu:0"")? I guess this might have bad impacts on performance, right?
 Thanks!
 PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know...
 	",1.0,bsautermeister,2016-09-15T00:10:46Z,"
 		Agree, I believe there is bug in batch_norm.
 		",2.0,bsautermeister,2016-09-15T21:38:57Z,"
 		With bug in batch_norm, which point's of my list do you actually mean? And could you propose any workaround?
 		",3.0,bsautermeister,2016-09-17T00:16:01Z,"
 		Dont know why, I cannot do multi-gpu training when batch_norm moving_avg is applied, but when I update my tf to master version and update my cuda,cudnn, the problem go away.
 		",a6e6d0aa2cad5e1d50b4f5cbed427a5df9267098,tvn,2016-09-16 16:16:34-07:00,MODIFY,0,tensorflow\contrib\layers\python\layers\layers.py,tensorflow\contrib\layers\python\layers\layers.py,0.0,138,138,,,,,4.0,bsautermeister,2016-09-23T04:41:45Z,"
 		<denchmark-link:https://github.com/shlens>@shlens</denchmark-link>
  Could you take a look at this? Thanks.
 		",5.0,bsautermeister,2016-09-23T15:38:18Z,"
 		<denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>
  would you have a suggested edit on the docstring that would make the layer more clear?
 <denchmark-link:https://github.com/argman>@argman</denchmark-link>
 @, it sounds like your error is fixed, correct?
 		",6.0,bsautermeister,2016-09-24T01:00:38Z,"
 		<denchmark-link:https://github.com/shlens>@shlens</denchmark-link>
  , yes, I just update tf to the newest
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,bsautermeister,2016-10-20T02:44:32Z,"
 		Is reuse=True working? Whenever I'm trying 'reuse=True' I get errors like - ""Variable norm0/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?"" I'm following the docstring and providing the 'scope' too. As far as I understand, when a variable is to be created using tf.get_variable() and reused, first, it has to be created and then its reuse is to be enabled by using - tf.get_variable_scope().reuse_variables().
 Without ""reuse=True"" in 'tf.contrib.layers.batch_norm()', I think, the right moving mean and variances will not be restored.
 I'm using twnsorflow version 0.11
 Please inform me if this is not the right place to raise this issue. I got to it from <denchmark-link:https://github.com/tensorflow/tensorflow/issues/1122>#1122</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,bsautermeister,2016-10-24T21:29:52Z,"
 		I have the same issue as <denchmark-link:https://github.com/dasabir>@dasabir</denchmark-link>
  when trying to reuse a batch_norm layer within a variable scope.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,bsautermeister,2016-11-01T05:03:25Z,"
 		For (2), I agree with <denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>
  because as I believe adding dependences on  looks sound. For some reasons, one may compute loss value (i.e. forward-prop) for validation datapoints; but with dependences on  batch-normalization statistics are also taken from validation set.
 For (3), do we need to share the BN parameters for bn_train and bn_inference? (in the original code different BN variables like beta, gamma are present for those two)
  def batch_norm_layer(x, train_phase, scope_bn):
    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
 -  updates_collections=None, is_training=True)
 +  updates_collections=None, is_training=True, scope=scope_bn)
    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
 -  updates_collections=None, is_training=False)
 +  updates_collections=None, is_training=False, scope=scope_bn, reuse=True)
    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
    return bn
 NOTE: I simply ignored the invalid moving average/variance update in the code for simplicity.
 		",10.0,bsautermeister,2016-11-02T07:14:19Z,"
 		<denchmark-link:https://github.com/dasabir>@dasabir</denchmark-link>
  and <denchmark-link:https://github.com/jfsantos>@jfsantos</denchmark-link>
  I had same issue. But by speficying the scope_name for batch_norm, the issue was fixed. Under a scope with reusable=True,  will always create new norm_variables and make them reusable which gives you the error. One thing you can do it is to specify the norm_scope name like this . When you reuse this norm layer, just do  or use  under a reusable scope. Hope this is helpful.
 		",11.0,bsautermeister,2016-12-27T09:51:03Z,"
 		I noticed that the docs haven't been updated yet. Would it be useful if the docs instead said:
 update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
 with tf.control_dependencies(update_ops):
     train_step = tf.train.GradientDescentOptimizer(0.01).minimize(total_loss)
 As for proper reuse across multiple data streams, it looks like a shareable version is still <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/normalization.py#L200>in the works</denchmark-link>
 .
 As an aside, to the best of my understanding, the notion of a shareable BN layer should be treated with some care. Depending on the use-case, I think there should be an option to distinguish sharing of the moving averages from the sharing of the beta/gamma parameters <denchmark-link:https://arxiv.org/pdf/1603.09025v4.pdf>as noted here</denchmark-link>
 .
 		",12.0,bsautermeister,2017-01-27T20:06:49Z,"
 		Is this still a problem with tf.nn.batch_norm?
 		",13.0,bsautermeister,2017-02-13T17:42:10Z,"
 		Closing due to lack of recent activity. Please update the issue if it persists and we will reopen.
 		",14.0,bsautermeister,2018-01-19T11:08:46Z,"
 		When you use batch normalization  across multi gpus, how to update variance?
 		",15.0,bsautermeister,2018-04-06T11:57:12Z,"
 		I solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):
 <denchmark-code>scope = tf.get_variable_scope()
 bn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)
 bn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)
 </denchmark-code>
 
 You have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:
 Variable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
 		",16.0,bsautermeister,2018-06-07T08:41:11Z,"
 		I obey <denchmark-link:https://github.com/wjiangcmu>@wjiangcmu</denchmark-link>
  's advice, it works.
 the code:
 33         self.is_training = tf.placeholder(tf.bool, name='MODE')
 // first use:
 94         self.img_bn1 = tf.cond(self.is_training,
 95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),
 96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))
 // add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)
 126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
 127         print('update_ops')
 128         for key in update_ops:
 129             print(key)
 131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]
 // second use:
 132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)
 // weight update and dependent extra_ops(moving mean and variance)
 242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )
 243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)
 244
 245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops
 246         self.i2t_updates = tf.group(*i2t_train_ops)
 in addition,  in order to update each batch_norm only once, according to <denchmark-link:https://github.com/bsautermeister>@bsautermeister</denchmark-link>
  's ""UPDATE_OPS in combination with reuse varscope"", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.
 Hope this will be helpful for others.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43781,yashi21,2020-10-05T07:35:40Z,2020-10-05T23:47:38Z,[MLIR] TFlite contains unused file,"
 tensorflow/compiler/mlir/lite/transforms/load_quantization_recipe.cc is called via
 tf-opt -allow-unregistered-dialect -tfl-load-recipe %s | FileCheck %s
 However, according to <denchmark-link:https://github.com/tensorflow/tensorflow/commit/36167cb04c890f55f8abf1df1d79bf4bf6361d08>this commit</denchmark-link>
 , ""-allow-unregistered-dialect"" is disabled (the only way to call the file is deprecated).
 Hence, in  can be removed as it is not involved in any mlir::tfl passes.
 	",1.0,yashi21,2020-10-05T21:27:42Z,"
 		Thanks for reporting this! Will remove the unused dependency.
 		",2.0,yashi21,2020-10-05T23:47:39Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43781>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43781>No</denchmark-link>
 
 		",,,,,1831af3c012643a5d61012bf71b653e075dcfd22,Jaesung Chung,2020-10-05 16:46:38-07:00,MODIFY,0,tensorflow\compiler\mlir\lite\BUILD,tensorflow\compiler\mlir\lite\BUILD,0.0,,480,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43816,nicolas-harraudeau-sonarsource,2020-10-06T12:19:08Z,2020-11-30T18:43:38Z,"TypeError in ""Bidirectional.compute_output_shape""","
 Hello,
 While analyzing Tensorflow on SonarCloud I saw what looks like an error in <denchmark-link:https://github.com/tensorflow/tensorflow/blob/d6426459bca2971b611ca9773858e55f6cc1a9af/tensorflow/python/keras/layers/wrappers.py#L522>tensorflow/python/keras/layers/wrappers.py</denchmark-link>
 :
 <denchmark-link:https://user-images.githubusercontent.com/40498978/95199955-eb31ac00-07dd-11eb-8bee-2a0c006ed40f.png></denchmark-link>
 
 You can see the issue in SonarCloud <denchmark-link:https://sonarcloud.io/project/issues?id=nicolas-harraudeau-sonarsource_tensorflow&issues=AXT60Og4BMD9OHnI8p_E&open=AXT60Og4BMD9OHnI8p_E>here</denchmark-link>
 .
 After reviewing the code myself I can indeed see that state_shape will have the type tuple, and using the operator + on a list and a tuple will fail with the exception:
 
 TypeError: can only concatenate list (not ""tuple"") to list
 
 Here is the flow which leads to this issue:
 <denchmark-code>output_shape = tuple(output_shape.as_list())  # output_shape is a tuple
 ...
 if self.return_state:
       state_shape = output_shape[1:]  # state_shape is a tuple
 ...
 return [output_shape] + state_shape + copy.copy(state_shape)  # the expression ""[output_shape] + state_shape"" will fail
 </denchmark-code>
 
 In case you have any question, suggestion or if you see a False Positive on SonarCloud you can reach out on <denchmark-link:https://community.sonarsource.com/>SonarSource community forum</denchmark-link>
 .
 A few notes in case you want to use SonarCloud:
 
 I am currently testing the python analyzer so the project on SonarCloud will only show python issues, but SonarCloud can also analyze C/C++ code and other languages.
 SonarCloud can also import pylint issues in case you want to use a rule SonarCloud does not already provide. Note however that pylint rules and SonarCloud rules are implemented differently. You might see new issues with SonarCloud, or less issues in some cases (we try to avoid False Positives as much as possible).
 It is free for open-source projects.
 
 	",1.0,nicolas-harraudeau-sonarsource,2020-10-07T11:00:15Z,"
 		<denchmark-link:https://github.com/nicholasbutlin>@nicholasbutlin</denchmark-link>
 
 Please share your tf version as you have not filled in the issue template.
 		",2.0,nicolas-harraudeau-sonarsource,2020-10-07T11:20:51Z,"
 		
 @nicholasbutlin
 Please share your tf version as you have not filled in the issue template.
 
 Hi - Not me this time! Think this is for someone else...
 		",3.0,nicolas-harraudeau-sonarsource,2020-10-07T11:47:44Z,"
 		/cc <denchmark-link:https://github.com/mihaimaruseac>@mihaimaruseac</denchmark-link>
 
 		",5a8b0d3e80cef5ffba362d747a0f449c90862b5d,Scott Zhu,2020-11-30 10:42:09-08:00,MODIFY,1,tensorflow\python\keras\layers\wrappers.py,tensorflow\python\keras\layers\wrappers.py,1.0,"559,560,561,562,565,567","558,559,560,562,563,566,568",MODIFY,1.0,tensorflow\python\keras\layers\wrappers_test.py,tensorflow\python\keras\layers\wrappers_test.py,4.0,nicolas-harraudeau-sonarsource,2020-10-07T12:48:35Z,"
 		Hi <denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
 . The bug I am describing here is directly in the master branch of tensorflow. I didn't check if it was released in a specific version of tensorflow.
 I read the template but it doesn't seem to apply in this case. I am not describing a runtime issue but rather a bug visible in tensorflow's code. I apologize if there is a specific template for this case and I missed it.
 		",5.0,nicolas-harraudeau-sonarsource,2020-10-07T16:51:55Z,"
 		I think this might be a leftover from Py2. Let's try to convert both sides of + to list?
 		",6.0,nicolas-harraudeau-sonarsource,2020-10-07T18:00:01Z,"
 		A list of tuples?
 		",1.0,"969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991",,test_Bidirectional_output_shape,"self,rnn",969,991,,,,,,,,,,,,,,,compute_output_shape,"self,input_shape",556,575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,nicolas-harraudeau-sonarsource,2020-10-20T05:09:52Z,"
 		<denchmark-link:https://github.com/nicolas-harraudeau-sonarsource>@nicolas-harraudeau-sonarsource</denchmark-link>
 
 Is this still an issue.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,nicolas-harraudeau-sonarsource,2020-10-20T06:47:09Z,"
 		Hi <denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
 ,
 As far as I can see the code remained the same, so <denchmark-link:https://github.com/tensorflow/tensorflow/blob/574463afb8581ca65988aee0ce1c956b4614e061/tensorflow/python/keras/layers/wrappers.py#L525>the bug is still there</denchmark-link>
 . The same pattern I described earlier is visible:
 <denchmark-code>output_shape = tuple(output_shape.as_list())  # output_shape is a tuple
 ...
 if self.return_state:
       state_shape = output_shape[1:]  # state_shape is a tuple
 ...
 return [output_shape] + state_shape + copy.copy(state_shape)  # the expression ""[output_shape] + state_shape"" will fail
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,nicolas-harraudeau-sonarsource,2020-11-30T18:43:40Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43816>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/43816>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
43998,vermouth1992,2020-10-14T06:43:09Z,2020-10-14T22:27:51Z,Update BatchNormalization documentation,"
 Thank you for submitting a TensorFlow documentation issue. Per our GitHub
 policy, we only address code/doc bugs, performance issues, feature requests, and
 build/installation issues on GitHub.
 The TensorFlow docs are open source! To get involved, read the documentation
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization>https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 The document should mention that the axis can take in a list of integers, not just an integer. I tested it and it is already implemented in the TensorFlow. I was not aware of it and used reshape and transpose, which is inefficient.
 <denchmark-h:h3>Correct links</denchmark-h>
 
 Is the link to the source code correct?
 <denchmark-h:h3>Parameters defined</denchmark-h>
 
 Are all parameters defined and formatted correctly?
 The axis can be a list of integers.
 <denchmark-h:h3>Returns defined</denchmark-h>
 
 Are return values defined?
 <denchmark-h:h3>Raises listed and defined</denchmark-h>
 
 Are the errors defined? For example,
 <denchmark-link:https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises>https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises</denchmark-link>
 
 <denchmark-h:h3>Usage example</denchmark-h>
 
 Is there a usage example?
 See the API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
 
 on how to write testable usage examples.
 <denchmark-h:h3>Request visuals, if applicable</denchmark-h>
 
 Are there currently visuals? If not, will it clarify the content?
 <denchmark-h:h3>Submit a pull request?</denchmark-h>
 
 Are you planning to also submit a pull request to fix the issue? See the docs
 contributor guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs>https://www.tensorflow.org/community/contribute/docs</denchmark-link>
 ,
 docs API guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_ref>https://www.tensorflow.org/community/contribute/docs_ref</denchmark-link>
  and the
 docs style guide: <denchmark-link:https://www.tensorflow.org/community/contribute/docs_style>https://www.tensorflow.org/community/contribute/docs_style</denchmark-link>
 
 	",1.0,vermouth1992,2020-10-14T10:39:30Z,"
 		<denchmark-link:https://github.com/vermouth1992>@vermouth1992</denchmark-link>
 
 Could you please explain the issue faced.
 		",2.0,vermouth1992,2020-10-14T18:26:34Z,"
 		There is no practical issue I am facing. However, the documentation is misleading. For example, if we want the batch normalization layer to normalize over multiple dimensions, the following code is working
 import tensorflow as tf
 layer = tf.keras.layers.BatchNormalization(axis=[0, 2])
 a = layer(tf.random.normal([3, 100, 10]))
 However, the documentation says that the axis argument must be an integer, which confuses me at the beginning. So I have to use a reshape + transpose to implement the same functionality, which is ineffective.
 		",3.0,vermouth1992,2020-10-14T22:27:51Z,"
 		Thanks for your issue. The docs are now updated.
 
 
 
 tensorflow/tensorflow/python/keras/layers/normalization.py
 
 
          Line 83
       in
       9318f78
 
 
 
 
 
 
      axis: Integer or a list of integers, the axis that should be normalized 
 
 
 
 
 
 		",9318f787e6d33adabc5c1b18c6663d1432f646f9,A. Unique TensorFlower,2020-10-14 15:19:40-07:00,MODIFY,0,tensorflow\python\keras\layers\normalization.py,tensorflow\python\keras\layers\normalization.py,0.0,"83,84","83,84",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44011,dhruvin2910,2020-10-14T12:46:16Z,2020-12-16T01:00:18Z,ops defined inside tf.while_loop's cond/body or tf.cond's true_fn/false_fn functions ignore their enclosed tf.device if the tf.while_loop/tf.cond itself is inside a tf.device,"
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): binary
 TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
 Python version: Python 3.7.8
 Bazel version (if compiling from source):
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version:
 GPU model and memory:
 
 You can collect some of this information using our environment capture
 <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh>script</denchmark-link>
 
 You can also obtain the TensorFlow version with:
 
 TF 1.0: python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
 TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
 
 Describe the current behavior
 tf.print doesn't print on specified device (using tf.device) if it is inside a for loop in graph mode.
 Initially I though it's an autograph issue. But if you trace the graph and inspect it in tensorboard (color by device), the graph does have that PrintV2 placed where it's supposed to.
 Now, I'm not sure why it's happening.
 I'm yet to verify if this happens across all the ops, or just the tf.print.
 Waiting for the team to let me know what to check next.
 Describe the expected behavior
 tf.print should print on specified device (using tf.device). Everything works fine if I'm using tf.while_loop instead.
 Can be seen from sample code.
 Standalone code to reproduce the issue
 Provide a reproducible test case that is the bare minimum necessary to generate
 the problem. If possible, please share a link to Colab/Jupyter/any notebook.
 THIS SNIPPET IS NOT CORRECT ANYMORE, PLEASE READ A COUPLE OF COMMENTS BELOW.
 import tensorflow as tf
 
 # assume there's a tf.distribute.Server running on rpi.local:2222
 tf.config.experimental_connect_to_cluster(
     tf.train.ClusterSpec({'worker': ['dhruvins-macbook-air.local:2222', 'rpi.local:2222']}),
     job_name='worker',
     task_index=0
 )
 
 laptop = '/job:worker/task:0'
 rpi = '/job:worker/task:1'
 
 
 @tf.function
 def test_for():
     with tf.device(rpi):
         for i in tf.range(3):
             tf.print('rpi', 'for', i)
             with tf.device(laptop):
                 tf.print('laptop', 'for', i)
 
 
 @tf.function
 def test_while():
     def cond(i):
         return i < 3
 
     def body(i):
         with tf.device(rpi):
             tf.print('rpi', 'while', i)
             with tf.device(laptop):
                 tf.print('laptop', 'while', i)
         return [i + 1]
 
     tf.while_loop(cond, body, [0], parallel_iterations=1)
 
 
 test_for()
 test_while()
 Other info / logs Include any logs or source code that would be helpful to
 diagnose the problem. If including tracebacks, please include the full
 traceback. Large logs and files should be attached.
 	",1.0,dhruvin2910,2020-10-19T15:26:21Z,"
 		<denchmark-link:https://github.com/jvishnuvardhan>@jvishnuvardhan</denchmark-link>
 
 I ran this code on nightly, the code keep running and does not produce any output, please find the <denchmark-link:https://colab.research.google.com/gist/Saduf2019/cdad7be80aca26f332c5e366cfbced34/untitled445.ipynb>gist here</denchmark-link>
 .
 		",2.0,dhruvin2910,2020-10-19T17:52:37Z,"
 		<denchmark-link:https://github.com/Saduf2019>@Saduf2019</denchmark-link>
  One will need to modify their colab code a bit to point to existing (running) (s).
 The reason why it's stuck is because it's unable to connect to rpi.local:2222 while running inside colab's container.
 The sample code I provided requires at least two tensorflow servers. On same machine, or on different ones (i.e. my case).
  Here's the <denchmark-link:https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb>reproducible version</denchmark-link>
 . Let me know if this suffices.
 		",3.0,dhruvin2910,2020-10-20T12:41:03Z,"
 		<denchmark-link:https://github.com/dhruvin2910>@dhruvin2910</denchmark-link>
  there is a notable difference between the two snippets of code: In the autograph case, the  is places outside the loop. In the tf.while_loop case, the  is placed inside the loop. So to make them equivalent, you'd need to either write:
 <denchmark-code>for i in tf.range(3):
     with tf.device(rpi):
 </denchmark-code>
 
 or
 <denchmark-code>with tf.device(rpi):
     def cond(i):
         ...
     def body(i):
         ...
 
     tf.while_loop(...)
 </denchmark-code>
 
 And I believe that highlights the bug: if we place a tf.device outside a tf.while_loop, its body does not respect it. Things work as expected when the tf.device is placed inside the loop body. Could you verify that in your setup?
 At any rate, this looks like a placement bug in TensorFlow.
 		",1ce28a3f8a7670eda543176c7fe3a78b5db11a1e,Allen Lavoie,2020-12-15 16:59:01-08:00,MODIFY,2,tensorflow\core\common_runtime\function_test.cc,tensorflow\core\common_runtime\function_test.cc,1.0,1292,1292,MODIFY,1.0,tensorflow\core\common_runtime\inline_function_utils.cc,tensorflow\core\common_runtime\inline_function_utils.cc,4.0,dhruvin2910,2020-10-20T12:46:39Z,"
 		<denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  You sure this is intended for me? Did you mean to tag dhruvin2910 and accidentally selected me in dropdown somehow? Because although I have contributed to tensorflow in past, I don't think I have touched this part of the code.
 Just realized I am logged into this from my work account (JayNakrani) and my personal account (dhananjay92) was tagged
 		",5.0,dhruvin2910,2020-10-20T12:49:55Z,"
 		<denchmark-link:https://github.com/JayNakrani>@JayNakrani</denchmark-link>
  sorry about that. I meant to tag <denchmark-link:https://github.com/dhruvin2910>@dhruvin2910</denchmark-link>
 . Not sure how the autocomplete filled your tag.
 		",6.0,dhruvin2910,2020-10-20T14:00:18Z,"
 		
 there is a notable difference between the two snippets of code: In the autograph case, the tf.device is places outside the loop. In the tf.while_loop case, the tf.device is placed inside the loop. So to make them equivalent, you'd need to either write: ...
 
 <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  Yes, I intended to provide what you just suggested. The problem originated from a bit involving code. Somehow, I missed wrapping the  with  instead, while I was simplifying it to reproduce this issue.
 
 And I believe that highlights the bug: if we place a tf.device outside a tf.while_loop, its body does not respect it. Things work as expected when the tf.device is placed inside the loop body. Could you verify that in your setup?
 
 Yes. <denchmark-link:https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb#scrollTo=vFcc6P_S_ebz>Updated Colab</denchmark-link>
 
 		",1.0,"234,235,236,241,246","234,239,244",tensorflow::MultiDeviceFunctionBodyPlacer::BodyNodeDevice,ndef,221,251,MODIFY,22.0,tensorflow\python\kernel_tests\cond_v2_test.py,tensorflow\python\kernel_tests\cond_v2_test.py,1.0,"1579,1580,1581","1580,1581",,,,,,,,tensorflow::tensorflow::TEST_F.TEST_F,"FunctionLibraryRuntimeTest,ExpandInlineFunctionsAndPlaceInlinedNodes",1190,1306,1.0,1292,1292,tensorflow::TEST_F,"FunctionTest,WXPlusB",136,2390,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,dhruvin2910,2020-10-21T03:23:47Z,"
 		As I mentioned earlier, the graph seems to have PrintV2 placed on expected device.
 I'm unable to upload the logs properly to tensorboard.dev, so here's a screenshot (blue ones are on rpi and green ones are on my macbook):
 <denchmark-link:https://user-images.githubusercontent.com/9679326/96669104-27d2db00-137a-11eb-8dc9-63eeaae26a48.png></denchmark-link>
 
 
 At any rate, this looks like a placement bug in TensorFlow.
 
 Yes, I think the problem lies in the execution of the graph.
 		",testDeviceBeforeCond._cond_wrapper2,,1579,1581,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,dhruvin2910,2020-10-21T06:49:10Z,"
 		I tested if all ops are misplaced or not with this code.
 I don't really know a proper way to find it out. But here's an experiment (and assumptions) to test it:
 
 A very long while loop may be substituted for a long running task (with parallel iterations set to 1)
 
 
 With proper control_dependencies set, one can measure computation time of a set of ops.
 If two devices have different compute power, one can guess the device placement by the amount of time taken, faster task would mean it was placed on the device with better compute.
 
 
 
 
 Since both long running tasks are assigned to different devices, they should run concurrently.
 One can guess the device placement is wrong if they are run in sequence. Can be guessed from total time, or from print statements.
 
 
 
 def task(n):
     with tf.name_scope('task'):
         return tf.while_loop(lambda i: i < n, lambda i: i + 1, [0], parallel_iterations=1)[0]
 
 
 def timed(f, *args):
     with tf.name_scope('timed'):
         start = tf.timestamp(name='start')
         with tf.control_dependencies([start]):
             r = f(*args)
         with tf.control_dependencies([r]):
             end = tf.timestamp(name='end')
         return tf.subtract(end, start, name='time'), r
 
 
 @tf.function
 def test_while():
     with tf.device(rpi):
         def cond(i):
             return i < 1
 
         def body(i):
             t, n = timed(task, 100_000)
             tf.print('rpi', 'while', t, n)
             with tf.device(laptop):
                 t, n = timed(task, 100_000)
                 tf.print('laptop', 'while', t, n)
             return [i + 1]
 
         tf.while_loop(cond, body, [0], parallel_iterations=1)
 Observations:
 
 The tasks are running in sequence, so there's something more to this story.
 I have a rasppberry pi running at 700MHz and a MacBook Air running at 1.8GHz. The tasks take roughly the same time (6s), and the total time was around 12s. Again there's something missing.
 
 <denchmark-code>rpi while 6.2778100967407227 100000
 laptop while 6.16434907913208 100000
 </denchmark-code>
 
 Note:
 Since I may have made mistakes in my assumptions or the code, I don't think the experiment is conclusive.
 I think it may still help further investigation.
 		",,,,,,,,,,,,,,,1.0,"956,957","956,957",testGradientTapeOfCondWithResourceVariableInFunction.testGradientTapeOfCondWithResourceVariableInFunction.fn_with_cond.true_fn,,956,957,1.0,"301,302",302,testDefunInCond.testDefunInCond.true_fn.fn,,301,302,1.0,"1628,1629,1630",,testDeviceBeforeRemote._wrapper2,,1628,1630,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,dhruvin2910,2020-10-21T07:11:30Z,"
 		Just discovered tf.debugging.set_log_device_placement(True), and I can confirm that the device placement is indeed incorrect.
 <denchmark-code>...
 while/body/_1/while/PrintV2: (PrintV2): /job:worker/replica:0/task:1/device:CPU:0
 ...
 while/body/_1/while/PrintV2_1: (PrintV2): /job:worker/replica:0/task:1/device:CPU:0
 ...
 </denchmark-code>
 
 Both of these are placed on task:1 (aka rpi).
 Also, not just the tf.print, but all the ops in the loop (edit: both cond and body) are are placed incorrectly. Tested with tf.random.uniform.
 		",10.0,dhruvin2910,2020-10-21T08:22:38Z,"
 		tf.cond and if ...: else: ... (with autograph) both have same problem.
 @tf.function
 def test_cond():
     with tf.device(rpi):
         def true_fn():
             tf.print('rpi')
             with tf.device(laptop):
                 tf.print('laptop')
 
         def false_fn():
             tf.print('rpi')
             with tf.device(laptop):
                 tf.print('laptop')
 
         tf.cond(tf.greater_equal(tf.random.uniform(()), 0.5), true_fn, false_fn)
 
 
 @tf.function
 def test_if():
     with tf.device(rpi):
         if tf.greater_equal(tf.random.uniform(()), 0.5):
             tf.print('rpi')
             with tf.device(laptop):
                 tf.print('laptop')
         else:
             tf.print('rpi')
             with tf.device(laptop):
                 tf.print('laptop')
 No tf.print (or any other op placed via tf.device) will be placed on laptop
 		",11.0,dhruvin2910,2020-10-21T09:02:29Z,"
 		After a lot of experiments, here's a concise example of what is working and what is not.
 import tensorflow as tf
 
 # assume there's a tf.distribute.Server running on rpi.local:2222
 tf.config.experimental_connect_to_cluster(
     tf.train.ClusterSpec({'worker': ['dhruvins-macbook-air.local:2222', 'rpi.local:2222']}),
     job_name='worker',
     task_index=0
 )
 
 laptop = '/job:worker/task:0'
 rpi = '/job:worker/task:1'
 
 
 @tf.function
 def works():
     def true_fn():
         # this tf.device will be respected
         with tf.device(laptop):
             tf.print('expected on', 'laptop')
 
     def false_fn():
         # this tf.device will be respected
         with tf.device(rpi):
             tf.print('expected on', 'rpi')
 
     tf.cond(tf.constant(True), true_fn, false_fn)
 
 
 @tf.function
 def does_not_work():
     # we enclose everything inside a tf.device
     with tf.device(rpi):
         def true_fn():
             # this tf.device will be ignored
             with tf.device(laptop):
                 tf.print('expected on', 'laptop')
 
         def false_fn():
             # this tf.device will be ignored
             with tf.device(rpi):
                 tf.print('expected on', 'rpi')
 
         # tf.cond is in tf.device now
         tf.cond(tf.constant(True), true_fn, false_fn)
 
 
 works()
 does_not_work()
 Both calls must print 'expected on laptop' on laptop.
 But the second call (aka does_not_work) prints on rpi instead.
 <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  can you verify this?
 		",12.0,dhruvin2910,2020-10-21T12:21:01Z,"
 		Thanks for the detailed investigation. We're having a closer look at the cause.
 Just to double check, in the last snippet you meant tf.device(laptop), right (since the outer device is already rpi)?
 		",13.0,dhruvin2910,2020-10-21T14:59:19Z,"
 		
 Just to double check, in the last snippet you meant tf.device(laptop), right (since the outer device is already rpi)?
 
 works used to flip a coin and printed on a device at random, but to make the code deterministic, I set the condition as True. So in the example false_fn is never evaluated and any of rpi or laptop would work.
 I tried to demonstrate that if you take a correctly behaving tf.cond code (works in our example), and wrap it with a tf.device, the ops inside true_fn and false_fn don't get distributed the way they were originally.
 All the ops are placed where tf.cond is placed.
 		",14.0,dhruvin2910,2020-10-22T00:56:12Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/commit/d3698cdfd94c858092ebbb9af7ab0815e9bc78c1>d3698cd</denchmark-link>
  should fix the  scope inheritance issue. It also has a test that ops don't always follow cond placement (even if there's a device scope around the cond).
 Please give it a try and re-open if something's still broken.
 		",15.0,dhruvin2910,2020-10-22T00:56:14Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011>No</denchmark-link>
 
 		",16.0,dhruvin2910,2020-10-22T04:33:08Z,"
 		Thank you so much <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
 . I'll see if the issue is fixed in the nightly (when the commit gets in nightly) and let you know.
 For my future reference, <denchmark-link:https://colab.research.google.com/gist/dhruvin2910/d8b0765a28b8182303105b29dd57262c/untitled445.ipynb#scrollTo=vFcc6P_S_ebz>colab</denchmark-link>
 
 		",17.0,dhruvin2910,2020-10-25T03:36:56Z,"
 		<denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  I think the commit you added must have arrived in nightly.
 I have been trying to see if the issue is still there or not for 3 days, and the problem persists.
 See the colab above.
 PS: I'm unable to reopen this issue. <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  can you still reproduce the issue?
 		",18.0,dhruvin2910,2020-10-27T15:00:09Z,"
 		<denchmark-link:https://github.com/dhruvin2910>@dhruvin2910</denchmark-link>
  I'm not sure if the issue can be accurately reproduced in the colab setup. IIUC, the colab doesn't see the output from any of the two cluster workers. At the same time, I suspect w1_proc will capture outputs from both workers.
 Looking at the op placement: print(test_while.get_concrete_function().graph.as_graph_def()), if we search for op: ""PrintV2"", I can see that the two Print ops are placed one on worker/task:0 and worker/task:1, as expected.
 I don't know if it's possible to configure a cluster that includes the current colab process. <denchmark-link:https://github.com/guptapriya>@guptapriya</denchmark-link>
  ?
 		",19.0,dhruvin2910,2020-10-27T17:34:45Z,"
 		
 I'm not sure if the issue can be accurately reproduced in the colab setup. IIUC, the colab doesn't see the output from any of the two cluster workers. At the same time, I suspect w1_proc will capture outputs from both workers.
 
 <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  I tried a simple  code (example below) and the workers (in the same colab) had correct outputs.
 @tf.function
 def test():
     with tf.device(w0):
       tf.print(""This is a test print. Should be done on w0"")
     with tf.device(w1):
       tf.print(""This is a test print. Should be done on w1"")
 
 Looking at the op placement: print(test_while.get_concrete_function().graph.as_graph_def()), if we search for op: ""PrintV2"", I can see that the two Print ops are placed one on worker/task:0 and worker/task:1, as expected.
 
 I mentioned exactly this in my previous <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44011#issuecomment-713271763>#44011 (comment)</denchmark-link>
 . Ops in the graph have proper device assigned, but the ops are placed incorrectly at runtime. See this <denchmark-link:https://github.com/tensorflow/tensorflow/issues/44011#issuecomment-713360321>#44011 (comment)</denchmark-link>
 .
 		",20.0,dhruvin2910,2020-10-27T17:50:30Z,"
 		Thanks, and sorry I missed those bits. I concur, there is a bug still present in the placement logic. I suggest updating the test colab to make that clearer:
 <denchmark-code>@tf.function
 def test_while():
     with tf.device(w0):
       tf.print('w0', 'control test')
     with tf.device(w1):
       ...
 </denchmark-code>
 
 		",21.0,dhruvin2910,2020-12-16T00:58:06Z,"
 		The second issue looks like it was related to function inlining (when inlining, the function's placement partially overrode the body's placement; conds were inlined as two functions). I have a change that should go through soon for that.
 Thank you for the report.
 		",22.0,dhruvin2910,2020-12-16T01:00:19Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44011>No</denchmark-link>
 
 		",23.0,dhruvin2910,2020-12-19T11:01:08Z,"
 		The issue seems to be fixed in 2.4 . Thanks <denchmark-link:https://github.com/mdanatg>@mdanatg</denchmark-link>
  and <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
 .
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"949,951,952,953,954,956,957,959,960,962,963,965","948,949,950,951,953,954,956,957,959,960,962",testGradientTapeOfCondWithResourceVariableInFunction,self,948,965,1.0,"1563,1564,1565","1563,1565",testDeviceBeforeCond._cond_wrapper,,1563,1565,1.0,"306,307","306,307",testDefunInCond.false_fn,,306,307,1.0,"952,953,954,956,957,959,960,962,963","953,954,956,957,959,960,962",testGradientTapeOfCondWithResourceVariableInFunction.fn_with_cond,,952,963,1.0,"1609,1610,1611",,testDeviceBeforeRemote._wrapper,,1609,1611,1.0,"298,300,301,302,304","298,300,302,303",testDefunInCond.true_fn,,298,304,1.0,"1599,1600,1601,1602,1603,1604,1605,1606,1608,1609,1610,1611,1613,1614,1615,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634","1599,1600,1601,1602,1603,1604",testDeviceBeforeRemote,"self,functional_op_to_test",1599,1634,1.0,"1602,1603,1604,1605,1606","1602,1603,1604",testDeviceBeforeRemote._fn,,1602,1606,1.0,"1554,1555,1556,1557,1558,1559,1560,1562,1563,1564,1565,1567,1568,1569,1570,1571,1573,1574,1575,1576,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588","1552,1553,1554,1556,1557,1558,1559,1561,1562,1563,1565,1566,1567,1569,1570,1571,1572,1574,1575,1576,1577,1578,1580,1581,1583,1584,1585,1586,1587",testDeviceBeforeCond,self,1552,1588,1.0,"1467,1475,1476",,setUp,self,1466,1476,1.0,"1573,1574,1575,1576","1574,1575,1576",testDeviceBeforeCond.fn2,,1573,1576,1.0,"1621,1622,1623,1624,1625",,testDeviceBeforeRemote._fn2,,1621,1625,1.0,"294,295,296,298,300,301,302,304,306,307,309,310,311","294,296,297,298,300,302,303,305,306,307",testDefunInCond,self,293,311,1.0,"1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659",,testColocationBeforeCond,self,1636,1659,1.0,"1638,1639,1640,1641,1642",,testColocationBeforeCond._fn,,1638,1642,1.0,"959,960","959,960",testGradientTapeOfCondWithResourceVariableInFunction.testGradientTapeOfCondWithResourceVariableInFunction.fn_with_cond.false_fn,,959,960,1.0,"1554,1555,1556,1557,1558,1559,1560","1554,1556,1557,1558,1559",testDeviceBeforeCond.fn,,1554,1560,1.0,"1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655",,testColocationBeforeCond._cond_wrapper,,1645,1655,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44278,jackd,2020-10-24T03:59:35Z,2020-12-11T18:07:59Z,Unexpected `snapshot` behaviour with `flat_map` in tf-nightly,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
 TensorFlow installed from (source or binary): binary (pip)
 TensorFlow version (use command below): '2.4.0-dev20201023'
 Python version: 3.7.7
 
 Describe the current behavior
 A dataset formed by flat_mapping multiple snapshotted datasets which have each been iterated over individually (thus producing files on disk) results in a dataset which seemingly does not use those files on disk. This is different to the behaviour of cache in tf-2.3 and tf-nightly and snapshot in tf-2.3
 Describe the expected behavior
 snapshot to work equivalently in 2.3 as tf-nightly, and similarly to cache.
 
 Colab <denchmark-link:https://colab.research.google.com/drive/13mXDdNKNg04MEDvFTwmQq0tAw8L0MY02?usp=sharing>here</denchmark-link>
 .
 import os
 from tempfile import TemporaryDirectory
 
 import numpy as np
 import tensorflow as tf
 
 
 def as_numpy(ds: tf.data.Dataset):
     return np.array([x.numpy() for x in ds])
 
 
 def get_data(
     num_repeats=2,
     snap=False,
     preprocess_early=False,
     preprocess_late=False,
     del_rng=False,
 ):
     """"""
     Get numpy results from a data pipeline.
 
     The pipeline looks like:
         1. range
         2. add stateful random noise
         3. create `num_repeats` `cache`d or `snapshot`ted versions
         4. `flat_map` if num_repeats > 1
 
     Args:
         num_repeats: number of duplicates created in step 3 above.
         snap: use `snapshot` (otherwise use `cache`)
         preprocess_early: if True, we iterate over individually cached / snapshotted
             datasets prior to flat-mapping.
         preprocess_late: if True, we iterate over the `flat_map`ped dataset
         del_rng: if True, we delete the rng responsible for generating random noise in
             step 2. This will cause an error if this map function is called again,
             rather than using cached / snapshotted files on disk.
 
     Returns:
         Two iterations of the repeated dataset.
     """"""
     rng = tf.random.Generator.from_seed(0)
     dataset = tf.data.Dataset.range(10).map(
         lambda x: tf.cast(x, tf.float32) + rng.uniform(())
     )
     with TemporaryDirectory() as tmp_dir:
         paths = [os.path.join(tmp_dir, f""repeat-{i}"") for i in range(num_repeats)]
         if snap:
             datasets = [
                 dataset.apply(tf.data.experimental.snapshot(path)) for path in paths
             ]
         else:
             datasets = [dataset.cache(path) for path in paths]
         if preprocess_early:
             # iterate over datasets individually to force saving to file
             for ds in datasets:
                 as_numpy(ds)
         if num_repeats == 1:
             (dataset,) = datasets
         else:
             dataset = tf.data.Dataset.from_tensor_slices(datasets).flat_map(lambda x: x)
         if preprocess_late:
             # iterate over concatenated dataset to force saving to file
             as_numpy(dataset)
         if del_rng:
             # this will cause an error is the original mapped dataset is called
             del rng
         return as_numpy(dataset), as_numpy(dataset)
 
 
 class SnapshotTest(tf.test.TestCase):
     def test_consistent(self):
         base0, base1 = get_data()
         np.testing.assert_equal(base0, base1)
 
     def test_reproducible(self):
         base0, _ = get_data()
         s0, s1 = get_data()
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_snapshot(self):
         base0, _ = get_data()
         s0, s1 = get_data(snap=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_late(self):
         base0, _ = get_data()
         s0, s1 = get_data(snap=True, preprocess_late=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_late_del_rng(self):
         base0, _ = get_data()
         s0, s1 = get_data(snap=True, preprocess_late=True, del_rng=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_early(self):
         base0, _ = get_data()
         s0, s1 = get_data(snap=True, preprocess_early=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_early_del_rng(self):
         base0, _ = get_data()
         s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_no_repeats(self):
         # preprocess_early is equivalent to preprocess_late here
         base0, _ = get_data(num_repeats=1)
         s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
     def test_preprocess_del_rng_no_repeats(self):
         # preprocess_early is equivalent to preprocess_late here
         base0, _ = get_data(num_repeats=1)
         s0, s1 = get_data(snap=True, preprocess_early=True, num_repeats=1, del_rng=True)
         np.testing.assert_equal(s0, s1)
         np.testing.assert_equal(s0, base0)
 
 
 if __name__ == ""__main__"":
     tf.test.main()
 Other info / logs
 Failed test output:
 <denchmark-code>======================================================================
 ERROR: test_preprocess_early_del_rng (__main__.SnapshotTest)
 SnapshotTest.test_preprocess_early_del_rng
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2113, in execution_mode
     yield
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 733, in _next_internal
     output_shapes=self._flat_output_shapes)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2579, in iterator_get_next
     _ops.raise_from_not_ok_status(e, name)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6862, in raise_from_not_ok_status
     six.raise_from(core._status_to_exception(e.code, message), None)
   File ""<string>"", line 3, in raise_from
 tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.
 	 [[{{node stateful_uniform/StatefulUniform}}]] [Op:IteratorGetNext]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""foob.py"", line 107, in test_preprocess_early_del_rng
     s0, s1 = get_data(snap=True, preprocess_early=True, del_rng=True)
   File ""foob.py"", line 67, in get_data
     return as_numpy(dataset), as_numpy(dataset)
   File ""foob.py"", line 9, in as_numpy
     return np.array([x.numpy() for x in ds])
   File ""foob.py"", line 9, in <listcomp>
     return np.array([x.numpy() for x in ds])
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 747, in __next__
     return self._next_internal()
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py"", line 739, in _next_internal
     return structure.from_compatible_tensor_list(self._element_spec, ret)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/contextlib.py"", line 130, in __exit__
     self.gen.throw(type, value, traceback)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/context.py"", line 2116, in execution_mode
     executor_new.wait()
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/executor.py"", line 69, in wait
     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
 tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/_AnonymousVar6/N10tensorflow3VarE does not exist.
 	 [[{{node stateful_uniform/StatefulUniform}}]]
 
 ======================================================================
 FAIL: test_preprocess_early (__main__.SnapshotTest)
 SnapshotTest.test_preprocess_early
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File ""foob.py"", line 103, in test_preprocess_early
     np.testing.assert_equal(s0, base0)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 342, in assert_equal
     return assert_array_equal(actual, desired, err_msg, verbose)
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 931, in assert_array_equal
     verbose=verbose, header='Arrays are not equal')
   File ""/home/jackd/anaconda3/envs/tf-nightly/lib/python3.7/site-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
     raise AssertionError(msg)
 AssertionError: 
 Arrays are not equal
 
 Mismatched elements: 20 / 20 (100%)
 Max absolute difference: 0.90819454
 Max relative difference: 1.9366292
  x: array([0.91562 , 1.45509 , 2.253555, 3.829305, 4.681193, 5.65526 ,
        6.401854, 7.514806, 8.184864, 9.174181, 0.130606, 1.063369,
        2.513922, 3.190604, 4.433053, 5.044663, 6.653943, 7.007094,
        8.878403, 9.046815], dtype=float32)
  y: array([0.311793, 1.18098 , 2.761353, 3.138052, 4.027518, 5.460741,
        6.235661, 7.175892, 8.786037, 9.549028, 0.860469, 1.631952,
        2.669349, 3.255722, 4.884421, 5.066545, 6.267429, 7.34992 ,
        8.16538 , 9.955009], dtype=float32)
 
 ----------------------------------------------------------------------
 Ran 10 tests in 0.849s
 
 FAILED (failures=1, errors=1, skipped=1)
 </denchmark-code>
 
 	",1.0,jackd,2020-10-27T12:24:41Z,"
 		<denchmark-link:https://github.com/ymodak>@ymodak</denchmark-link>
 
 I am able to replicate the issue reported,please find the <denchmark-link:https://colab.research.google.com/gist/Saduf2019/919b0216f01d1b8d41444ee0f24b47f3/untitled453.ipynb>gist here</denchmark-link>
 .
 		",2.0,jackd,2020-10-30T20:14:21Z,"
 		Hi <denchmark-link:https://github.com/jackd>@jackd</denchmark-link>
 , thank you for the thorough testing and reproduction details! I dug into the issue and found a couple behaviors that combined to cause this issue:
 
 Datasets passed through flat_map functions are not optimized before iterating over them.
 This commit changed SnapshotDataset so that when the dataset is optimized, the dataset hash changes. This happens because the compression type gets changed/resolved from AUTO to SNAPPY, and compression type is included in the snapshot hash.
 
 I'm preparing a fix that will address both issues by always computing snapshot hashes based on the structure of the dataset before optimization.
 To work around the issue in the short term, you could change the way you implement preprocess_early to use flat_map, like so:
 if preprocess_early:
     # iterate over datasets individually to force saving to file
     for ds in datasets:
         as_numpy(tf.data.Dataset.from_tensors(ds).flat_map(lambda x: x))
 		",3.0,jackd,2020-12-11T18:07:58Z,"
 		This should be fixed now, right?
 Please reopen if that is not the case.
 		",cbc7f31b7d6aac81158be084201d3b3e8e346907,Andrew Audibert,2020-11-03 11:59:47-08:00,MODIFY,4,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc,1.0,"377,378,379,380,381,382,408,409",,MODIFY,0.0,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.h,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.h,4.0,jackd,2020-12-11T18:08:01Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44278>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44278>No</denchmark-link>
 
 		",,,,,,,,,0.0,"49,50,80,81",,,,,,MODIFY,0.0,tensorflow\core\ops\experimental_dataset_ops.cc,tensorflow\core\ops\experimental_dataset_ops.cc,0.0,"971,972",,MODIFY,2.0,tensorflow\python\data\experimental\kernel_tests\snapshot_test.py,tensorflow\python\data\experimental\kernel_tests\snapshot_test.py,1.0,"360,361,362,363,364,365,366,367,368,369,370",,tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::AsGraphDefInternal,"ctx,b,output",350,415,1.0,"838,839,840,841","843,844,845,846,847",tensorflow::data::experimental::SnapshotDatasetV2Op::SnapshotDatasetV2Op,ctx,822,847,1.0,307,"307,308",tensorflow::data::experimental::SnapshotDatasetV2Op::Dataset::Dataset,"ctx,input,hash,path,compression,reader_prefix,writer_prefix,reader_func,shard_func",297,313,1.0,"854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,885,886","849,850,851,852,853,854,855,856,868,869",tensorflow::data::experimental::SnapshotDatasetV2Op::MakeDataset,"ctx,input,output",849,887,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testReadUsingFlatMap,self,360,370,MODIFY,0.0,tensorflow\tools\api\golden\v1\tensorflow.raw_ops.pbtxt,tensorflow\tools\api\golden\v1\tensorflow.raw_ops.pbtxt,0.0,4125,4125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\tools\api\golden\v2\tensorflow.raw_ops.pbtxt,tensorflow\tools\api\golden\v2\tensorflow.raw_ops.pbtxt,0.0,4125,4125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"373,374,375,376,377,378,379,380,381,382,383,384,385,386",,testReadOptimizableUsingFlatMap,self,373,386,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44646,jplu,2020-11-06T12:26:32Z,2020-11-20T00:22:43Z,Bug when a custom tf.keras.models.Model has multiple class inheritance,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux Ubuntu 20.04
 TensorFlow installed from (source or binary): pip
 TensorFlow version (use command below): 2.3
 Python version: conda env with Python 3.7.9
 CUDA/cuDNN version: 10.1
 GPU model and memory: GeForce RTX 2080 Super with Max-Q Design 8GB
 
 Describe the current behavior
 Creating a custom model that inherit of at least one other class than tf.keras.models.Model, the following exception is raised:
 <denchmark-code>File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 457, in _method_wrapper
     result = method(self, *args, **kwargs)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 255, in __init__
     inject_functional_model_class(self.__class__)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in <genexpr>
     cls.__bases__ = tuple(inject_functional_model_class(base)
   File ""C:\Users\snake\miniconda3\envs\transformers\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 144, in inject_functional_model_class
     cls.__bases__ = tuple(inject_functional_model_class(base)
 TypeError: can't set attributes of built-in/extension type 'object'
 </denchmark-code>
 
 Describe the expected behavior
 Being able to create a custom model with different mixins.
 Standalone code to reproduce the issue
 Here a simple piece of code to reproduce the issue:
 <denchmark-code>import tensorflow as tf
 
 class PrintMixin:
     def custom_print(self):
         print(""Hello world"")
 
 class CustomModel(tf.keras.models.Model, PrintMixin):
     def __init__(self, *args, **kwargs):
         my_input = tf.keras.layers.Input(shape=(16,))
         dense = tf.keras.layers.Dense(32, activation='relu')
         output = dense(my_input)
         outputs = {""output"": output}
 
         super().__init__(inputs=[my_input], outputs=outputs, *args, **kwargs)
 
 
 my_model = CustomModel()
 </denchmark-code>
 
 Other info / logs
 Apparently when giving the inputs and outputs parameters, TensorFlow tries to inject an attribute to all the classes and super classes until reaching tf.keras.models.Model. Here the piece of code from the file training.py line 136:
 <denchmark-code>def inject_functional_model_class(cls):
   """"""Inject `Functional` into the hierarchy of this class if needed.""""""
   from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
   from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top
   if cls == Model or cls == training_v1.Model:
     return functional.Functional
 
   cls.__bases__ = tuple(inject_functional_model_class(base)
                         for base in cls.__bases__)
   # Trigger any `__new__` class swapping that needed to happen on `Functional`
   # but did not because functional was not in the class hierarchy.
   cls.__new__(cls)
 
   return cls
 </denchmark-code>
 
 But when it tries to check the superclass of my mixin class, which is object an error is raised saying that we cannot add an attribute to the object type. For me the following update of the method fix the issue:
 <denchmark-code>def inject_functional_model_class(cls):
   """"""Inject `Functional` into the hierarchy of this class if needed.""""""
   from tensorflow.python.keras.engine import functional  # pylint: disable=g-import-not-at-top
   from tensorflow.python.keras.engine import training_v1  # pylint: disable=g-import-not-at-top
   if cls == Model or cls == training_v1.Model:
     return functional.Functional
   if cls == 'object':
     return cls
 
   cls.__bases__ = tuple(inject_functional_model_class(base)
                         for base in cls.__bases__)
   # Trigger any `__new__` class swapping that needed to happen on `Functional`
   # but did not because functional was not in the class hierarchy.
   cls.__new__(cls)
 
   return cls
 </denchmark-code>
 
 Here we return the object class as it is. But I don't know if it is a proper fix that won't bring another error elsewhere.
 First, I wanted to know if it is really a bug?
 If not, how I could do a proper custom model with mixin classes and my inputs/outputs.
 If, yes, is the fix I proposed ok and if needed I can open a PR with it.
 Thanks!
 	",1.0,jplu,2020-11-08T18:05:49Z,"
 		Was able to reproduce the issue with TF v2.3 and TF-nightly. Please find the gist of it <denchmark-link:https://colab.research.google.com/gist/amahendrakar/3b58866c9c9e0ac935296d26b903de22/44646-2-3.ipynb>here</denchmark-link>
 . Thanks!
 		",2.0,jplu,2020-11-18T17:27:48Z,"
 		Thanks for reporting the issue. Let me take a close look.
 		",3.0,jplu,2020-11-20T00:22:45Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44646>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44646>No</denchmark-link>
 
 		",8f68aad1107df679843da96a990773e9fc30201c,Scott Zhu,2020-11-19 16:21:10-08:00,MODIFY,9,tensorflow\python\keras\engine\functional_test.py,tensorflow\python\keras\engine\functional_test.py,1.0,"2488,2489,2490",,MODIFY,2.0,tensorflow\python\keras\engine\training.py,tensorflow\python\keras\engine\training.py,,,,,,,,,,,,,1.0,"118,119,120,121",,inject_functional_model_class,cls,112,129,,,,,,,,,,,,,,,__init__,"self,foo,kwargs",2488,2490,1.0,"2508,2509,2510,2511,2512",,testFunctionalSubclass,self,2508,2512,1.0,"2502,2503",,get_bar,self,2502,2503,1.0,"2498,2499,2500",,__init__,"self,bar,kwargs",2498,2500,1.0,"237,238,239,240,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263",234,__init__,"self,args,kwargs",227,320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"2536,2537,2538,2539,2540,2541,2542,2543",,testSubclassModelPreMixin,self,2536,2543,1.0,"2478,2479,2480,2481,2482,2483",,__init__,"self,args,kwargs",2478,2483,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"2492,2493",,get_foo,self,2492,2493,,,,,,,,1.0,"2514,2515,2516,2517,2518,2519,2520,2521,2522",,testFunctionalSubclassPreMixin,self,2514,2522,1.0,"2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534",,testFunctionalSubclassPostMixin,self,2524,2534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
44983,Engineero,2020-11-18T15:52:14Z,2020-12-25T03:56:48Z,tf.image.per_image_standardization unexpected behavior with unsigned integer input,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4
 TensorFlow installed from (source or binary): pip
 TensorFlow version (use command below): v2.2.0-57-g25fba035f3 2.2.1
 Python version: 3.7.7
 CUDA/cuDNN version: 10.1.243
 GPU model and memory: Nvidia Tesla V100-DGXS-32GB,
 
 Describe the current behavior
 tf.image.per_image_standardization has no special handling for unsigned integers but still converts back to unsigned before returning. Thus uint inputs get saturated at zero (fully half of the data!) and at their maximum value. Currently no explanation of uint behavior is given in the docstring, and no warning or other indication that behavior might be unexpected is shown.
 Describe the expected behavior
 In previous versions of TensorFlow (I believe changed after 1.14), per_image_standardization just returned a float rather than converting back to the original datatype, so it didn't have this problem. I would expect something safe like that, leaving it up to the developer to decide how they want this conversion handled, or at least a warning printed in the event a uint input is given. Some documentation specifying that uints are not handled differently and will be saturated might be nice too.
 
 <denchmark-link:https://colab.research.google.com/drive/1PqjTWtgLPE7hb5Dza2fZdFNbhlyW3mKl?usp=sharing>Colab notebook link</denchmark-link>
 .
 Other info / logs
 No tracebacks or logs because it doesn't throw an error. I would call this a failure mode, but it fails silently. I suspect it's an edge case that isn't tested.
 	",1.0,Engineero,2020-11-19T02:47:39Z,"
 		<denchmark-link:https://github.com/Engineero>@Engineero</denchmark-link>
 
 Please, grant me access to colab link. Thanks!
 		",2.0,Engineero,2020-11-19T02:49:34Z,"
 		<denchmark-link:https://github.com/ravikyram>@ravikyram</denchmark-link>
 
 Sorry, haven't used colab much. It should be fixed now.
 		",3.0,Engineero,2020-11-19T03:26:06Z,"
 		I have tried in colab with TF version 2.3 and nightly version() and was able to reproduce the issue. Please, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/88120ddb0fd04644a3d2d4075ca97f7f/untitled524.ipynb>here</denchmark-link>
 . Thanks!
 		",b6be9714e878a7dd0d1405bd7a83e021ba4b561a,Hye Soo Yang,2020-12-20 13:15:39-08:00,MODIFY,1,tensorflow\python\ops\image_ops_impl.py,tensorflow\python\ops\image_ops_impl.py,1.0,"1840,1849,1855,1860","1840,1849,1850,1851,1852,1853,1859",MODIFY,3.0,tensorflow\python\ops\image_ops_test.py,tensorflow\python\ops\image_ops_test.py,4.0,Engineero,2020-11-19T03:28:19Z,"
 		For the record, I think the behavior changed between 1.14 and 1.15, which is why I suspect an untested edge case. I had a use case where this affected training, and while it's certainly not backwards-incompatible (doesn't error out anyway), it is a surprise.
 		",5.0,Engineero,2020-11-19T06:17:58Z,"
 		<denchmark-link:https://github.com/Engineero>@Engineero</denchmark-link>
 , related issue:
 <denchmark-link:https://github.com/tensorflow/tensorflow/issues/33892>#33892</denchmark-link>
 
 		",6.0,Engineero,2020-11-19T06:21:55Z,"
 		Here is the offending commit:
 <denchmark-link:https://github.com/tensorflow/tensorflow/commit/906e0e3bc0dfe12db19afa261e4d793b73cb64ec>906e0e3</denchmark-link>
 
 		",1.0,"1660,1661,1662,1663,1664,1665,1666,1668","1659,1661,1665",testBasic,self,1659,1668,,,,,,,,,,,,,,,per_image_standardization,image,1823,1860,,,,,,,,,,,,,,,,,,,,,,1.0,"1666,1668,1672",,testBasic,"self,data_type",1666,1675,1.0,,"1688,1689,1690,1691,1692,1693,1694,1695,1696,1697",testPreservesDtype,self,1688,1697,,,,,,,,,,,,,,,,,,,,,,7.0,Engineero,2020-11-19T06:24:46Z,"
 		Oh, hey <denchmark-link:https://github.com/mixxen>@mixxen</denchmark-link>
 ! Good find, thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,Engineero,2020-12-04T15:33:21Z,"
 		It looks like simply casting (not with convert_image_dtype, just tf.cast) to float prior to per_image_normalization prevents information loss. I'd create a pull request, but I think the bigger question is what is the intended behavior when given unsigned integer input and how should that be communicated? Also covering this edge case with tests.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,Engineero,2020-12-20T21:05:50Z,"
 		Thank you for reporting the issue <denchmark-link:https://github.com/Engineero>@Engineero</denchmark-link>
  and identifying the offending commit <denchmark-link:https://github.com/mixxen>@mixxen</denchmark-link>
 .
 The changes in <denchmark-link:https://github.com/tensorflow/tensorflow/commit/906e0e3bc0dfe12db19afa261e4d793b73cb64ec>906e0e3</denchmark-link>
  appears to have changed the behavior of  unintentionally and should be reverted back in my opinion. It especially does not make sense for unsigned integer data types because images that have zero-mean will consist of negative and positive values (unless all values are 0). The API function has been working with  dtypes (until the breaking change) and should continue to do so. I'll get a fix in shortly.
 		",10.0,Engineero,2020-12-25T03:56:48Z,"
 		Here is a <denchmark-link:https://colab.research.google.com/drive/1nqzU22tZ0RVAaDm6drWPkfGV8cc7A_3x?usp=sharing>colab</denchmark-link>
  verifying the fix with . Closing the issue; please feel free to reopen if you encounter additional issues.
 		",11.0,Engineero,2020-12-25T03:56:50Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44983>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/44983>No</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45054,edend10,2020-11-21T02:00:20Z,2021-01-07T20:18:36Z,"Concatenating sparse keras input layers results in None shape, preventing use in Dense layer","
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
 TensorFlow installed from (source or binary): Colab
 TensorFlow version (use command below): 2.3
 Python version: 3.6.9
 Bazel version (if compiling from source): None
 GCC/Compiler version (if compiling from source): None
 CUDA/cuDNN version: None
 GPU model and memory: None
 
 Describe the current behavior
 Using the Keras functional API, when concatenating together multiple instances of tensorflow.keras.layers.Input with sparse=True, the resulting SparseTensor has a None shape (even when batch_size is specified). Feeding the merged layer with shape None into a dense layer results in an error:
 <denchmark-code>ValueError: The last dimension of the inputs to `Dense` should be defined. Found `None`.
 </denchmark-code>
 
 Describe the expected behavior
 Concatenating non-sparse instances of tensorflow.keras.layers.Input (i.e. sparse=False) results in a tensor which has a defined shape (if batch_size is specified) and can be fed into a dense layer successfully. Additionally a single sparse input layer has a shape and can be fed into a dense layer. I would expect concatenated sparse input layers to act similarly.
 The dense shape of the merged sparse tensor should already be known as we set the shapes of the individual input layers.
 Standalone code to reproduce the issue
 <denchmark-code>sp_a = tf.keras.layers.Input(shape=(1,), batch_size=2, sparse=True)
 sp_b = tf.keras.layers.Input(shape=(2,), batch_size=2, sparse=True)
 
 sp_merged = tf.keras.layers.concatenate(inputs=[sp_a, sp_b], axis=1)
 
 print(sp_merged.shape) # prints ""(None, None)""
 
 sp_result = tf.keras.layers.Dense(4, activation='relu', name='dense1')(sp_merged) # fails with above ValueError
 </denchmark-code>
 
 Also tried concatenating with tf.keras.layers.Concatenate and tf.sparse.concat instead. Same result.
 Reproduced success (non-sparse input) and failure (sparse input) cases in Colab:
 <denchmark-link:https://colab.research.google.com/drive/1g9a9lzpw28NqIPI1Z3-fYLJke1fa10Af?usp=sharing>https://colab.research.google.com/drive/1g9a9lzpw28NqIPI1Z3-fYLJke1fa10Af?usp=sharing</denchmark-link>
 
 	",1.0,edend10,2020-11-23T07:41:48Z,"
 		I have tried in colab with TF version 2.3, nightly version() and was able to reproduce the issue.Please, find the gist <denchmark-link:https://colab.research.google.com/gist/ravikyram/d735c0ed788454efba238a30af594edb/untitled532.ipynb>here.</denchmark-link>
  Thanks!
 		",2.0,edend10,2020-12-03T18:27:24Z,"
 		Thanks for the report; looks like a bug.
 		",3.0,edend10,2021-01-07T20:18:37Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45054>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45054>No</denchmark-link>
 
 		",8438d59abbd7a2c3d1c48bfd91b118aae53bbb14,Scott Zhu,2021-01-07 12:17:29-08:00,MODIFY,0,tensorflow\python\keras\layers\BUILD,tensorflow\python\keras\layers\BUILD,0.0,687,,MODIFY,1.0,tensorflow\python\keras\layers\merge_test.py,tensorflow\python\keras\layers\merge_test.py,,,,,,,,,,,,,1.0,"376,377,378,379,380,381,382,383,384,385,386,387,388",,test_merge_concatenate_sparse_shape,self,376,388,MODIFY,1.0,tensorflow\python\ops\sparse_ops.py,tensorflow\python\ops\sparse_ops.py,1.0,"422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447","422,423,424,425,426,427,428",MODIFY,1.0,tensorflow\python\ops\sparse_ops_test.py,tensorflow\python\ops\sparse_ops_test.py,1.0,"290,291,292,293,294,295,296,297",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,sparse_concat_v2,"axis,sp_inputs,expand_nonconcat_dims,name",397,447,testSparseConcatStaticShape,self,290,297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45113,alexdmiller,2020-11-23T18:35:07Z,2020-11-25T13:02:41Z,Documentation for using a private CocoaPod spec for TensorFlow Lite is not correct,"
 <denchmark-h:h2>URL(s) with the issue:</denchmark-h>
 
 <denchmark-link:https://www.tensorflow.org/lite/guide/build_ios#using_local_tensorflow_lite_core>https://www.tensorflow.org/lite/guide/build_ios#using_local_tensorflow_lite_core</denchmark-link>
 
 <denchmark-h:h2>Description of issue (what needs changing):</denchmark-h>
 
 <denchmark-h:h3>Clear description</denchmark-h>
 
 The documentation linked above says to change the following line in the TensorFlowLiteC.podspec file:
 <denchmark-code>  s.source       = { :http => ""file://<path_to_TensorFlowLiteC_framework.zip>"" }
 </denchmark-code>
 
 It then asks you to follow the instructions on <denchmark-link:https://guides.cocoapods.org/making/private-cocoapods.html>creating a private CocoaPod repo</denchmark-link>
  to use this pod in your project.
 That guide asks you to run the following:
 <denchmark-code>pod repo push tfliteswift TensorFlowLiteC.podspec
 </denchmark-code>
 
 But when you do this, you get an error message about the podspec line:
 <denchmark-code>[!] Error installing TensorFlowLiteC
  -> TensorFlowLiteC (2.3.0)
     - ERROR | [iOS] unknown: Encountered an unknown error ([!] /usr/local/anaconda3/bin/curl -f -L -o /var/folders/5j/wwtlv8lx0m5fmhc3hy5w5_000000gp/T/d20201123-17126-1ko7ol4/file.zip file://<redacted>/tensorflow/bazel-bin/tensorflow/lite/experimental/ios/TensorFlowLiteC_framework.zip --create-dirs --netrc-optional --retry 2 -A 'CocoaPods/1.10.0 cocoapods-downloader/1.4.0'
 
 curl: (3) URL using bad/illegal format or missing URL
 </denchmark-code>
 
 I don't believe the :http => ""file://<path_to_TensorFlowLiteC_framework.zip>"" line makes sense, because you're specifying a local file path for the http key. It's clearly not an http URL. Therefore I believe this documentation is incorrect / incomplete.
 I have yet to get my app compiling with a custom build of TensorFlowLite, so I don't know the solution. Any help appreciated.
 	",1.0,alexdmiller,2020-11-24T19:30:35Z,"
 		Everyone here says this works: <denchmark-link:https://github.com/CocoaPods/cocoapods-packager/issues/216#issuecomment-513514364>CocoaPods/cocoapods-packager#216 (comment)</denchmark-link>
 
 You just need a triple / like file:///path. file:// is the protocol,  /path is the path
 <denchmark-code>$ curl file://path
 
 curl: (3) URL using bad/illegal format or missing URL
 </denchmark-code>
 
 		",2.0,alexdmiller,2020-11-24T19:36:12Z,"
 		I'm sending a little fix to make this clearer.
 		",,,,,cfe685d8127222d01125893909be092b4dd7ae89,Mark Daoust,2020-11-25 05:01:10-08:00,MODIFY,0,tensorflow\lite\g3doc\guide\build_ios.md,tensorflow\lite\g3doc\guide\build_ios.md,0.0,"128,129",128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45195,psunn,2020-11-26T07:49:51Z,2020-12-03T23:44:17Z,//tensorflow/lite/python:lite_v2_test failed,"
 With current master branch (commit <denchmark-link:https://github.com/tensorflow/tensorflow/commit/38d042b105bf06c89cfc2a5c0412983215b4faf3>38d042b</denchmark-link>
 ), TEST //tensorflow/lite/python:lite_v2_test failed.
 
 bazelisk test //tensorflow/lite/python:lite_v2_test
 
 
 ERROR: /mirror-tensorflow/tensorflow/lite/python/BUILD:205:1: in deps attribute of py_test rule //tensorflow/lite/python:lite_v2_test: '//tenso
 rflow/lite/kernels/hashtable:hashtable_op_kernels' does not have mandatory providers: 'py' or 'PyInfo'
 ERROR: Analysis of target '//tensorflow/lite/python:lite_v2_test' failed; build aborted: Analysis of target '//tensorflow/lite/python:lite_v2_test' failed
 
 	",1.0,psunn,2020-12-03T23:44:18Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45195>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45195>No</denchmark-link>
 
 		",,,,,,,,,077fe29d9d1f2149dd8c74bcd2f99de7b5fd1506,Jaesung Chung,2020-12-03 15:43:07-08:00,MODIFY,0,tensorflow\lite\kernels\hashtable\BUILD,tensorflow\lite\kernels\hashtable\BUILD,0.0,"1,2,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71",,ADD,0.0,None,tensorflow\lite\kernels\hashtable\hashtable_ops_wrapper.cc,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\python\BUILD,tensorflow\lite\python\BUILD,0.0,219,219,MODIFY,1.0,tensorflow\lite\python\lite_v2_test.py,tensorflow\lite\python\lite_v2_test.py,1.0,"803,804",802,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testModelWithHashTableInitializer,self,792,826,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45243,SalvaPedraza,2020-11-28T09:54:36Z,2020-12-08T20:13:55Z,tf.keras.applications.mobilenet_v3.preprocess_input documentation not according source code,"
 In <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input>https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input</denchmark-link>
 
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet_v3/preprocess_input#returns>Said </denchmark-link>
  that:
 Preprocessed numpy.array or a tf.Tensor with type float32.
 The inputs pixel values are scaled between -1 and 1, sample-wise.
 However in source code in GitHub you find this:
 @keras_export('keras.applications.mobilenet_v3.preprocess_input')
 def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument
 return x
 And if you check it in a notebook you see that as source code state it is doing nothing. Do not scale and do not change dtype to float32 of the input.
 	",1.0,SalvaPedraza,2020-12-08T18:52:50Z,"
 		Thanks for reporting the issue.
 To give more context, the preprocess_input() was a standard method for all models to normalize the input data. However, it is quite common that user forget to call this method before they feed the data to model, which result into poor result. When we realized this problem, we move the preprocessing logic from this method to the model itself for some new models (like mobilenet_v3). In order to keep the alignment between old and new model, we still keep this method for new model, but as an empty shell, so that user can easily switching between different version of models without changing much code.
 I will update the docstring for this method to make it align with the implementation detail.
 		",,,,,,,,,52cdef300393aa1e2a4220030f77839edbae8bd3,Scott Zhu,2020-12-08 12:12:21-08:00,MODIFY,1,tensorflow\python\keras\applications\mobilenet_v3.py,tensorflow\python\keras\applications\mobilenet_v3.py,1.0,"557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574","566,567,568,569",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,preprocess_input,"x,data_format",556,575,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45662,Flamefire,2020-12-14T17:00:46Z,2021-01-05T22:24:28Z,Wrong device returned for GPUCompatibleFIFOQueueTests.testEnqueueDequeue test,"
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): 2.4.0rc4
 Python version: 3.7.4
 Bazel version (if compiling from source): 3.4.1
 GCC/Compiler version (if compiling from source): GCC 8.3.0
 CUDA/cuDNN version: 10.1
 GPU model and memory: V100
 
 Describe the current behavior
 In the //tensorflow/python/kernel_tests:fifo_queue_test target the GPUCompatibleFIFOQueueTests.testEnqueueDequeue returns the wrong device:  /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0
 See below log
 Standalone code to reproduce the issue
 Run bazel test
 Other info / logs
 <denchmark-code>FAIL: testEnqueueDequeue (__main__.GPUCompatibleFIFOQueueTests)
 GPUCompatibleFIFOQueueTests.testEnqueueDequeue
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1221, in decorated
     run_eagerly(self, **kwargs)
   File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1205, in run_eagerly
     f(self, *args, **kwargs)
   File ""/tmp/bazel-tf/20db8ac50b74c328e6dea9b20829b459/execroot/org_tensorflow/bazel-out/ppc-opt/bin/tensorflow/python/kernel_tests/fifo_queue_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/fifo_queue_test.py"", line 425, in testEnqueueDequeue
     self.assertEqual(elems[0].device, dequeued_tensor.device)
 AssertionError: 
 - /job:localhost/replica:0/task:0/device:CPU:0
 ?                                        ^
 + /job:localhost/replica:0/task:0/device:GPU:0
 ?                                        ^
 
 </denchmark-code>
 
 	",1.0,Flamefire,2021-01-05T22:24:29Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45662>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45662>No</denchmark-link>
 
 		",,,,,,,,,671c78343c3427858bd5674baab80c3c8c429815,Allen Lavoie,2021-01-05 14:23:18-08:00,MODIFY,1,tensorflow\python\kernel_tests\fifo_queue_test.py,tensorflow\python\kernel_tests\fifo_queue_test.py,1.0,"418,419,420",418,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testEnqueueDequeue,self,414,429,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45894,jackd,2020-12-21T00:47:02Z,2020-12-28T21:11:04Z,tf.data.experimental.assert_cardinality incompatible with INFINITE_CARDINALITY,"
 System information
 
 Have I written custom code: yes
 OS Platform and Distribution: Ubuntu 18.04
 TensorFlow installed from: binary
 TensorFlow version: v1.12.1-47912-gec43aacb56f 2.5.0-dev20201219
 Python version: 3.7.7
 
 Describe the current behavior: tf.data.experimental.assert_cardinality  can be used to fix Dataset.cardinality in instances where it cannot be inferred. In situations where the cardinality is infinite, this raises an error where it shouldn't.
 Describe the expected behavior if ds = ds_base.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY), then an error should be raised if ds_base stops producing elements, as opposed to when the first element is produced.
 
 <denchmark-link:https://colab.research.google.com/drive/13s9geZzR4xUtYJ6ImINl0kuisL-n8H_6?usp=sharing>Notebook</denchmark-link>
 
 Code (same as notebook):
 import tensorflow as tf
 
 ds = tf.data.Dataset.range(5).repeat().flat_map(lambda i: tf.data.Dataset.range(i))
 print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # False
 ds = ds.apply(tf.data.experimental.assert_cardinality(tf.data.INFINITE_CARDINALITY))
 print(ds.cardinality() == tf.data.INFINITE_CARDINALITY)  # True
 
 for example in ds.take(1):
     pass
 # tensorflow.python.framework.errors_impl.FailedPreconditionError:
 # Input dataset was expected to contain -1 elements but contained at least 1 element.
 	",1.0,jackd,2020-12-21T11:17:52Z,"
 		<denchmark-link:https://github.com/jackd>@jackd</denchmark-link>
 ,
 As per <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/data#other-members>the documentation</denchmark-link>
 , this behavior seems to be expected. What do you think?
 		",2.0,jackd,2020-12-21T11:50:53Z,"
 		I think tf.data.INFINITE_CARDINALITY should consistently be used to indicate infinite cardinality, regardless of how it is encoded. I can understand why this error occurs, but I would consider it a bugged edge-case.
 At the very least I would expect an error when creating  tf.data.experimental.assert_cardinality with a negative cardinality value, though that would just make me change this from a bug report to a feature request.
 		",3.0,jackd,2020-12-28T21:11:05Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45894>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/45894>No</denchmark-link>
 
 		",59f5abfbc8dc5559c361f80f4fa4a006db825e40,Andrew Audibert,2020-12-28 13:09:55-08:00,MODIFY,2,tensorflow\core\kernels\data\experimental\assert_cardinality_dataset_op.cc,tensorflow\core\kernels\data\experimental\assert_cardinality_dataset_op.cc,1.0,"152,153,154",,MODIFY,0.0,tensorflow\python\data\experimental\kernel_tests\assert_cardinality_test.py,tensorflow\python\data\experimental\kernel_tests\assert_cardinality_test.py,4.0,jackd,2020-12-28T21:23:48Z,"
 		Thanks for reporting this <denchmark-link:https://github.com/jackd>@jackd</denchmark-link>
 ! I agree with your assessment that it's a bugged edge-case, and submitted a fix so that asserting infinite cardinality will work as you described.
 		",,,,,,,,,0.0,"56,57,58,59,60,61,62,63,64,65",,,,,,,,,,,,,,,,,,,,tensorflow::data::experimental::AssertCardinalityDatasetOp::Dataset::Iterator::ElementString,n,151,156,1.0,"117,118",116,tensorflow::data::experimental::AssertCardinalityDatasetOp::Dataset::Iterator::GetNextInternal,"ctx,out_tensors,end_of_sequence",103,125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46020,advaitjain,2020-12-28T21:50:28Z,2021-01-08T00:41:55Z,Common location for portable bash helper functions / aliases,"
 <denchmark-link:https://github.com/orgs/tensorflow/teams/micro>@tensorflow/micro</denchmark-link>
 
 PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/46011>#46011</denchmark-link>
  fixes the use of  to be compatible with a Mac. We are already using  in additional places too.
 While we should get <denchmark-link:https://github.com/tensorflow/tensorflow/pull/46011>#46011</denchmark-link>
  merged, it would be better to have a common location for these helper functions / aliases.
 I can imagine collecting these into a common helper script (for example micro/tools/bash_helpers.sh) and have something like:
 UNAME_S=`uname -s`
 
 if [ UNAME_S == Linux]; then
   alias tflm_md5sum='md5sum'
 else if [ UNAME_S == Darwin ]; then
   alias tflm_md5sum='md5 -r'
 fi
 
 We would then need to change the different download scripts to determine the directory in which the script lives, something like: 
 
 
 tensorflow/tensorflow/lite/micro/tools/ci_build/test_bluepill.sh
 
 
         Lines 21 to 23
       in
       59f5abf
 
 
 
 
 
 
  SCRIPT_DIR=""$(cd ""$(dirname ""${BASH_SOURCE[0]}"")"" && pwd)"" 
 
 
 
  ROOT_DIR=${SCRIPT_DIR}/../../../../.. 
 
 
 
  cd ""${ROOT_DIR}"" 
 
 
 
 
 
 and then have
 source tensorflow/lite/micro/tools/bash_helper.sh
 	",1.0,advaitjain,2021-01-08T00:41:57Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46020>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46020>No</denchmark-link>
 
 		",,,,,,,,,c5ce162da8efb465e2ba8a8825049614813892a9,Advait Jain,2021-01-05 23:33:27-08:00,ADD,0,None,tensorflow\lite\micro\tools\make\bash_helpers.sh,,,,ADD,0.0,None,tensorflow\lite\micro\tools\make\ext_libs\cmsis_download.sh,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\micro\tools\make\ext_libs\cmsis_nn.inc,tensorflow\lite\micro\tools\make\ext_libs\cmsis_nn.inc,0.0,"8,9,13,14,15,16","8,9,10,11,15",MODIFY,0.0,tensorflow\lite\micro\tools\make\flatbuffers_download.sh,tensorflow\lite\micro\tools\make\flatbuffers_download.sh,0.0,"34,35,36,37,38,39,97","91,92,93,94,95,96,97,98,99,100,101,102,103,110",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\micro\tools\make\renode_download.sh,tensorflow\lite\micro\tools\make\renode_download.sh,0.0,"34,35,36,37,38,39,82","76,77,78,79,80,81",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\micro\tools\make\targets\apollo3evb_makefile.inc,tensorflow\lite\micro\tools\make\targets\apollo3evb_makefile.inc,0.0,"15,16,17,18,19",13,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\micro\tools\make\targets\bluepill_makefile.inc,tensorflow\lite\micro\tools\make\targets\bluepill_makefile.inc,0.0,"13,14,15,16,17",6,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tensorflow\lite\micro\tools\make\targets\stm32f4_makefile.inc,tensorflow\lite\micro\tools\make\targets\stm32f4_makefile.inc,0.0,"16,17,18,19,20",9,MODIFY,0.0,tensorflow\lite\micro\tools\make\third_party_downloads.inc,tensorflow\lite\micro\tools\make\third_party_downloads.inc,0.0,,"28,29,30",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46128,summa-code,2021-01-04T05:04:06Z,2021-01-18T18:16:11Z,"This throws ERROR: features = tf.io.parse_example(..., features=make_parse_example_spec(columns))","
 Please make sure that this is a bug. As per our
 GitHub Policy,
 we only address code/doc bugs, performance issues, feature requests and
 build/installation issues on GitHub. tag:bug_template
 System information
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
 Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
 TensorFlow installed from (source or binary): Source
 TensorFlow version (use command below): Latest from this week
 Python version: 3.8.x
 Bazel version (if compiling from source): 3.1.0
 GCC/Compiler version (if compiling from source):
 CUDA/cuDNN version: 11.2
 GPU model and memory:
 
 Describe the current behavior
 I was trying this code, but it throws exception
 https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures
 <denchmark-code>ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.```
 
 
 Here is the full code from that page,
 
 
 ```# Behavior of some cells or feature columns may depend on whether we are in
 # training or inference mode, e.g. applying dropout.
 training = True
 rating = sequence_numeric_column('rating')
 watches = sequence_categorical_column_with_identity(
     'watches', num_buckets=1000)
 watches_embedding = embedding_column(watches, dimension=10)
 columns = [rating, watches_embedding]
 
 sequence_input_layer = SequenceFeatures(columns)
 features = tf.io.parse_example(...,
                                features=make_parse_example_spec(columns))
 sequence_input, sequence_length = sequence_input_layer(
    features, training=training)
 sequence_length_mask = tf.sequence_mask(sequence_length)
 
 rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size, training=training)
 rnn_layer = tf.keras.layers.RNN(rnn_cell, training=training)
 outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
 </denchmark-code>
 
 	",1.0,summa-code,2021-01-04T10:07:19Z,"
 		<denchmark-link:https://github.com/summa-code>@summa-code</denchmark-link>
 
 Please share colab link or simple standalone code with supporting files to reproduce the issue. It helps us in localizing the issue faster.I am seeing the error message(NameError: name 'sequence_numeric_column' is not defined) while trying to reproduce the issue. Thanks!
 		",2.0,summa-code,2021-01-04T15:53:02Z,"
 		That is what i have given in the description, and also is in this link !!!!!!!
 And Did you include the tensor packages ?
 <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures>https://www.tensorflow.org/api_docs/python/tf/keras/experimental/SequenceFeatures</denchmark-link>
 
 <denchmark-code>import tensorflow as tf
 from tensorflow.keras import layers
 from tensorflow import feature_column
 
 training = True
 rating = feature_column.sequence_numeric_column('rating')
 watches = feature_column.sequence_categorical_column_with_identity(
     'watches', num_buckets=1000)
 watches_embedding = feature_column.embedding_column(watches, dimension=10)
 columns = [rating, watches_embedding]
 
 sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
 features = tf.io.parse_example(...,
                                features=feature_column.make_parse_example_spec(columns))
 sequence_input, sequence_length = sequence_input_layer(
    features, training=training)
 sequence_length_mask = tf.sequence_mask(sequence_length)
 
 rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size, training=training)
 rnn_layer = tf.keras.layers.RNN(rnn_cell, training=training)
 outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask) 
 
 </denchmark-code>
 
 		",3.0,summa-code,2021-01-05T05:08:46Z,"
 		I have tried in colab with TF version <denchmark-link:https://colab.research.google.com/gist/ravikyram/445470fd54502d56b948ebc76743b505/untitled594.ipynb>2.4 gist</denchmark-link>
  and Nightly version() <denchmark-link:https://colab.research.google.com/gist/ravikyram/9b2cc481b9c9390d2abdd3e516f43a84/untitled593.ipynb>gist</denchmark-link>
  and was able to reproduce the issue. Thanks!
 		",2cc955f533a9ba70512cf4a07024aaf65708e103,A. Unique TensorFlower,2021-01-18 10:14:54-08:00,MODIFY,0,tensorflow\python\keras\feature_column\sequence_feature_column.py,tensorflow\python\keras\feature_column\sequence_feature_column.py,0.0,"54,55,56,60,61,63,64,67,68,69,70,71,72,73,76,79,80,81,82","57,58,60,63,64,65,70,71",,,,,4.0,summa-code,2021-01-08T22:52:57Z,"
 		
 ValueError: Attempt to convert a value (Ellipsis) with an unsupported type (<class 'ellipsis'>) to a Tensor.
 
 Yeah, often people put ... in examples to say ""fill this in with the necessary code"". it's too bad that ... is also valid python.
 Anyway, this whole line: tf.io.parse_example(..., features=feature_column.make_parse_example_spec(columns)) is a distraction from the function being documented here. This thing doesn't need Examples or parser specifications. It needs a dictionary of SparseTensors.
 If you actually do want to parse some tensors using that construct, ... is where you's pass some SerializeToString serialized tf.Examples.
 Now, we encourage people to use doctest format >>> which we do test. But this doc is likely low enough traffic that we may never come back to fix this.
 		",5.0,summa-code,2021-01-10T17:53:38Z,"
 		I am little bit confused. So the example given looks like self contained. We are trying to extract the inputs. In the example given above what do you think goes in ... ? There is one numeric and and one categorical feature.
 		",6.0,summa-code,2021-01-11T14:42:57Z,"
 		This line:
 <denchmark-code>features = tf.io.parse_example(...,
                                features=feature_column.make_parse_example_spec(columns))
 </denchmark-code>
 
 Should be something like:
 <denchmark-code>features = {
  'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],[2.0,2.1,2.2, 2.3, 2.5]])
  'watches': tf.sparse.from_dense([[2, 85, 61, 0, 0, 0],[33,92, 2, 73, 1]])
 }
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,summa-code,2021-01-11T18:21:52Z,"
 		So this looks like feeding the actual sequence for building the model. And should there be a "","" between ratings and watches ? There was a dimension issue in the watches array, Now it is throwing some other error.
 features = {
 'rating': tf.sparse.from_dense([[1.0, 1.1, 0, 0, 0], [2.0, 2.1, 2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 61, 0, 0, 0], [33, 92, 2, 73, 1, 3]])
 }
 sequence_input, sequence_length = sequence_input_layer(features, training=training)
 raise errors.InvalidArgumentError(
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Condition x == y did not hold.
 Indices of first 2 different values:
 [[0]
 [1]]
 Corresponding x values:
 [2 5]
 Corresponding y values:
 [3 6]
 First 2 elements of x:
 [2 5]
 First 2 elements of y:
 [3 6]
 python-BaseException
 Process finished with exit code 1
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,summa-code,2021-01-11T18:26:44Z,"
 		I am just looking for a simple usage example for SequenceFeatures with couple of different sequence columns.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,summa-code,2021-01-11T18:53:16Z,"
 		All sets of sequences need the same shapes. In my earlier post they didn't match.
 This works:
 <denchmark-code>training = True
 rating = feature_column.sequence_numeric_column('rating')
 watches = feature_column.sequence_categorical_column_with_identity(
     'watches', num_buckets=1000)
 watches_embedding = feature_column.embedding_column(watches, dimension=10)
 columns = [rating, watches_embedding]
 
 features = {
  'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],[2.0,2.1,2.2, 2.3, 2.5]]),
  'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
 }
 
 sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
 sequence_input, sequence_length = sequence_input_layer(
    features, training=training)
    
 sequence_length_mask = tf.sequence_mask(sequence_length)
 </denchmark-code>
 
 		",10.0,summa-code,2021-01-11T22:31:23Z,"
 		Oops i missed that one. Ok, can we create ""features"" variable without using the real inputs ? something like this ?
 features['rating'] = tf.keras.Input(shape=(?,), name='rating', dtype=tf.float32)
 features['watches'] = tf.keras.Input(shape=(?,), name='watches', dtype=tf.float32)
 Is the shape here just one dimensional for the feature or should it have the windowed shape ?
 The timeseries example given does not do justice without using these ""Sequence"" functions. Since LSTM takes in 3 dimensional data, without knowing a working example, it is a bit hard.
 <denchmark-link:https://www.tensorflow.org/tutorials/structured_data/time_series>https://www.tensorflow.org/tutorials/structured_data/time_series</denchmark-link>
 
 		",11.0,summa-code,2021-01-13T16:23:19Z,"
 		I came across this one,
 <denchmark-link:https://github.com/tensorflow/tensorflow/issues/31240>#31240</denchmark-link>
 
 It is remarkable how durandg12 came back after almost a year after first he used it.
 		",12.0,summa-code,2021-01-15T00:02:06Z,"
 		And how do i create an input to LSTMCell, because i want to use CuDNN with GPU,
 I am using ""tf.keras.preprocessing.timeseries_dataset_from_array"" to create dataset that has the shape(batch, time sequence, features) format ? I am having sliding window dataset. It is crazy that with the lack of documentation, there is much we could do with SequenceFeature API.
 What i wanted is a slidingwindow dataset with a categorical column that is fed into LSTM network. The examples is shown everything except this crucial part. Pytorch has nice way of doing this. Wish Tensorflow has something simple to implement categorical feature for timeseries.
 		",13.0,summa-code,2021-01-17T17:54:15Z,"
 		And then i came across this one,
 <denchmark-link:https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md>https://github.com/tensorflow/community/blob/master/rfcs/20191212-keras-categorical-inputs.md</denchmark-link>
 
 This all leads to One hot encoding, even the hashing Trick does not do the trick. I am just lost here.
 		",14.0,summa-code,2021-01-20T20:09:46Z,"
 		???? Why was it closed ?
 		",15.0,summa-code,2021-01-20T20:12:11Z,"
 		The linked commit updates that example:
 <denchmark-link:https://github.com/tensorflow/tensorflow/commit/2cc955f533a9ba70512cf4a07024aaf65708e103>2cc955f</denchmark-link>
 
 <denchmark-code> ```python
 import tensorflow as tf
 # Behavior of some cells or feature columns may depend on whether we are in
 # training or inference mode, e.g. applying dropout.
 training = True
 rating = tf.feature_column.sequence_numeric_column('rating')
 watches = tf.feature_column.sequence_categorical_column_with_identity(
     'watches', num_buckets=1000)
 watches_embedding = tf.feature_column.embedding_column(watches,
                                             dimension=10)
 columns = [rating, watches_embedding]
 features = {
  'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                              [2.0,2.1,2.2, 2.3, 2.5]]),
  'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
 }
 sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
 sequence_input, sequence_length = sequence_input_layer(
    features, training=training)
 sequence_length_mask = tf.sequence_mask(sequence_length)
 hidden_size = 32
 rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
 rnn_layer = tf.keras.layers.RNN(rnn_cell)
 outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
 ```
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
46423,WeiChungChang,2021-01-14T08:38:05Z,2021-01-15T02:49:29Z,writer_test of serialization failed for squeeznet,"
 System information
 
 OS Platform and Distribution  Linux Ubuntu 16.04
 TensorFlow installed from source
 TensorFlow version (use command below):
 Python version: Python 3.5.2
 Bazel version (if compiling from source): Build label: 3.7.2
 GCC/Compiler version (if compiling from source) gcc (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
 
 Describe the current behavior
 
 download squeezenet (squeezenet.tflit)
 build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test
 bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite
 
 <denchmark-code>ERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)
 ERROR: Node number 38 (RESHAPE) failed to prepare.
 
 AllocateTensors failed on the round-tripped model.
 </denchmark-code>
 
 Describe the expected behavior
 pass write test
 Standalone code to reproduce the issue
 
 download squeezenet (squeezenet.tflit)
 build write test of serialization : bazel build -c opt //tensorflow/lite/tools/serialization:writer_test
 bazel-bin/tensorflow/lite/tools/serialization/writer_test [model folder]/squeezenet.tflite
 
 Other info / logs Include any logs or source code that would be helpful to
 <denchmark-code>ERROR: tensorflow/lite/kernels/reshape.cc:69 num_input_elements != num_output_elements (1001 != 1)
 ERROR: Node number 38 (RESHAPE) failed to prepare.
 
 AllocateTensors failed on the round-tripped model.
 </denchmark-code>
 
 A proposal of fix is at: <denchmark-link:https://github.com/tensorflow/tensorflow/pull/46422>#46422</denchmark-link>
 
 	",1.0,WeiChungChang,2021-01-14T09:12:17Z,"
 		<denchmark-link:https://github.com/thaink>@thaink</denchmark-link>
  could you take a look at this issue and the proposal fix <denchmark-link:https://github.com/tensorflow/tensorflow/pull/46422>#46422</denchmark-link>
  as well?
 		",2.0,WeiChungChang,2021-01-14T11:04:12Z,"
 		How did you create the model? Could you share your conversion code to reproduce and some valid sample inputs?
 		",3.0,WeiChungChang,2021-01-14T11:10:21Z,"
 		Dear <denchmark-link:https://github.com/abattery>@abattery</denchmark-link>
 
 The model directly comes from tflite hosted_models zoo.
 URL:
 <denchmark-link:https://www.tensorflow.org/lite/guide/hosted_models>https://www.tensorflow.org/lite/guide/hosted_models</denchmark-link>
 
 file:
 <denchmark-link:https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz>https://storage.googleapis.com/download.tensorflow.org/models/tflite/model_zoo/upload_20180427/squeezenet_2018_04_27.tgz</denchmark-link>
 
 		",f12082d7af509e9549d8e8fb2b514ccd0db0e84e,Jaesung Chung,2021-01-14 18:47:51-08:00,MODIFY,1,tensorflow\lite\tools\serialization\option_writer_generator.cc,tensorflow\lite\tools\serialization\option_writer_generator.cc,1.0,"285,286,287",284,MODIFY,1.0,tensorflow\lite\tools\serialization\writer_lib_test.cc,tensorflow\lite\tools\serialization\writer_lib_test.cc,4.0,WeiChungChang,2021-01-14T11:34:10Z,"
 		Hey <denchmark-link:https://github.com/WeiChungChang>@WeiChungChang</denchmark-link>
 
 I could reproduce your problem. The root cause is that, the deprecated new_shape attribute is not being copied into the result by the writer library. Instead of your fix at <denchmark-link:https://github.com/tensorflow/tensorflow/pull/46422>#46422</denchmark-link>
  , we will create a simple fix, instead in the writer library.
 Thank you for finding out this issue and reporting to us!
 		",5.0,WeiChungChang,2021-01-14T11:45:43Z,"
 		<denchmark-link:https://github.com/abattery>@abattery</denchmark-link>
 
 It sounds great.
 Could you put the link of the patch here when close please?
 I would like to know the detail and how to fix it from the other way.
 Thank you!
 		",6.0,WeiChungChang,2021-01-15T02:49:31Z,"
 		Are you satisfied with the resolution of your issue?
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=Yes&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46423>Yes</denchmark-link>
 
 <denchmark-link:https://docs.google.com/forms/d/e/1FAIpQLSfaP12TRhd9xSxjXZjcZFNXPGk4kc1-qMdv3gc6bEP90vY1ew/viewform?entry.85265664=No&entry.2137816233=https://github.com/tensorflow/tensorflow/issues/46423>No</denchmark-link>
 
 		",1.0,"330,331,332,333,334,335,336,337,338,339,340,341,342","329,330,331,332,376,378,380",tflite::TEST_P,"ReshapeLayerTest,ReshapeLayerTest",319,381,,,,,,,,,,,,,,,tflite::GenerateImportForReshapeOp,fp,279,298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4861,claytantor,2016-10-09T18:36:02Z,2016-10-14T02:06:13Z,Example mnist_rnn Not Working with Docker Image,"
 <denchmark-h:h1>Issue: Example mnist_rnn does run on docker image.</denchmark-h>
 
 <denchmark-code>---------------------------------------------------------------------------
 ImportError                               Traceback (most recent call last)
 <ipython-input-6-3bb5b939d552> in <module>()
       3 from __future__ import print_function
       4 
 ----> 5 from sklearn import metrics, preprocessing
       6 
       7 import tensorflow as tf
 
 ImportError: No module named sklearn
 
 </denchmark-code>
 
 <denchmark-h:h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>
 
 An example in the code base does not work with the docker image. It is the opinion of the filer that all examples should run without any need for configuration on the docker image because the project has control over what is installed on the docker image.
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/skflow/mnist_rnn.py</denchmark-link>
 
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System:
 docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow
 
 Installed version of CUDA and cuDNN:
 
 NONE, CPU based container
 If installed from binary pip package, provide:
 
 The output from python -c ""import tensorflow; print(tensorflow.__version__)"".
 
 <denchmark-code># python -c ""import tensorflow; print(tensorflow.__version__)""
 0.11.0rc0
 </denchmark-code>
 
 
 If installed from source, provide
 
 Not installed from source
 
 If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)
 
 Example given at beginning of ticket.
 
 What other attempted solutions have you tried?
 
 Removed all references to sklearn. Application works.
 <denchmark-code># It's useful to scale to ensure Stochastic Gradient Descent will do the right thing
 #scaler = preprocessing.StandardScaler()
 #X_train = scaler.fit_transform(X_train)
 #X_test = scaler.fit_transform(X_test)
 </denchmark-code>
 
 
 Logs or other output that would be helpful
 
 No logs produced.
 	",1.0,claytantor,2016-10-13T21:33:57Z,"
 		Try running sudo apt-get install python-sklearn inside the Docker container.
 Thank you for reporting this issue. I'm writing up a change to the Dockerfile now so others don't run into this same problem. It'll be exported to GitHub in a matter of days. This bug will be closed when that happens.
 		",,,,,,,,,7c79d528f43c69b6719da0c7846cd3aa56df57ef,Justine Tunney,2016-10-13 15:06:33-07:00,MODIFY,0,tensorflow\tools\docker\Dockerfile,tensorflow\tools\docker\Dockerfile,0.0,32,,MODIFY,0.0,tensorflow\tools\docker\Dockerfile.devel,tensorflow\tools\docker\Dockerfile.devel,,,,,,,,,,,,,0.0,"33,34",,,,,,MODIFY,0.0,tensorflow\tools\docker\Dockerfile.gpu,tensorflow\tools\docker\Dockerfile.gpu,0.0,32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5115,cgel,2016-10-21T13:48:48Z,2016-11-10T03:07:40Z,timeout breaks FIFOQueue,"
 Ubuntu 14.04.5 LTS
 0.10.0rc0
 Using timeout with notebooks is very useful in case you dequeue an empty queue or enqueue a full one. The problem is that after a timeout occurs a enqueue or dequeueop  throws an error
 <denchmark-code>import tensorflow as tf
 with tf.device(""/cpu:0""):
     ph = tf.placeholder(tf.float32)
     q = tf.FIFOQueue(2, tf.float32)
     enq = q.enqueue(ph)
     deq = q.dequeue()
     timeout_option = tf.RunOptions(timeout_in_ms=1000)
 sess = tf.Session()
 sess.run(deq, options=timeout_option)
 </denchmark-code>
 
 here i get the usual timeout error. The problem is that when I then run
 sess.run(enq, feed_dict={ph:2}, options=timeout_option)
 i get the error:
 <denchmark-code>---------------------------------------------------------------------------
 CancelledError                            Traceback (most recent call last)
 <ipython-input-24-9067a9d62797> in <module>()
 ----> 1 sess.run(Q, options=timeout_option)
 
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
     715     try:
     716       result = self._run(None, fetches, feed_dict, options_ptr,
 --> 717                          run_metadata_ptr)
     718       if run_metadata:
     719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
 
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
     913     if final_fetches or final_targets:
     914       results = self._do_run(handle, final_targets, final_fetches,
 --> 915                              feed_dict_string, options, run_metadata)
     916     else:
     917       results = []
 
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
     963     if handle is None:
     964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
 --> 965                            target_list, options, run_metadata)
     966     else:
     967       return self._do_call(_prun_fn, self._session, handle, feed_dict,
 
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
     983         except KeyError:
     984           pass
 --> 985       raise type(e)(node_def, op, message)
     986 
     987   def _extend_graph(self):
 
 CancelledError: Dequeue operation was cancelled
      [[Node: fifo_queue_Dequeue = QueueDequeue[_class=[""loc:@fifo_queue""], component_types=[DT_FLOAT, DT_INT64, DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""](fifo_queue)]]
      [[Node: PlaceholderWithDefault/_25 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_14_PlaceholderWithDefault"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 Caused by op u'fifo_queue_Dequeue', defined at:
   File ""<string>"", line 1, in <module>
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py"", line 469, in main
     app.start()
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py"", line 459, in start
     ioloop.IOLoop.instance().start()
   File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py"", line 160, in start
     super(ZMQIOLoop, self).start()
   File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/ioloop.py"", line 883, in start
     handler_func(fd_obj, events)
   File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
     return fn(*args, **kwargs)
   File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
     self._handle_recv()
   File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
     self._run_callback(callback, msg)
   File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
     callback(*args, **kwargs)
   File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
     return fn(*args, **kwargs)
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 281, in dispatcher
     return self.dispatch_shell(stream, msg)
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 245, in dispatch_shell
     handler(stream, idents, msg)
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 389, in execute_request
     shell.run_cell(code, store_history=store_history, silent=silent)
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2741, in run_cell
     interactivity=interactivity, compiler=compiler)
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2827, in run_ast_nodes
     if self.run_code(code):
   File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2883, in run_code
     exec(code_obj, self.user_global_ns, self.user_ns)
   File ""<ipython-input-1-73bf93787995>"", line 76, in <module>
     input_state, action, Y = q.dequeue()
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py"", line 418, in dequeue
     self._queue_ref, self._dtypes, name=name)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 863, in _queue_dequeue
     timeout_ms=timeout_ms, name=name)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 747, in apply_op
     op_def=op_def)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2372, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in __init__
     self._traceback = _extract_stack()
 </denchmark-code>
 
 	",1.0,cgel,2016-10-24T22:36:38Z,"
 		I tried from a recent version and it doesn't happen there. Do you want to upgrade?
 		",2.0,cgel,2016-10-25T18:13:06Z,"
 		I can also reproduce this on 0.11rc0
 If dequeue on empty queue is cancelled because of DeadlineExceededError,
 the queue becomes unusable -- it's no longer possible to enqueue anything
 onto the queue.
 On Fri, Oct 21, 2016 at 6:48 AM, cgel <denchmark-link:mailto:notifications@github.com>notifications@github.com</denchmark-link>
  wrote:
 
 Ubuntu 14.04.5 LTS
 0.10.0rc0
 Using timeout with notebooks is very useful in case you dequeue an empty
 queue or enqueue a full one. The problem is that after a timeout occurs a
 enqueue or dequeueop throws an error
 import tensorflow as tf
 with tf.device(""/cpu:0""):
 ph = tf.placeholder(tf.float32)
 q = tf.FIFOQueue(2, tf.float32)
 enq = q.enqueue(ph)
 deq = q.dequeue()
 timeout_option = tf.RunOptions(timeout_in_ms=1000)
 sess = tf.Session()
 sess.run(deq, options=timeout_option)
 here i get the usual timeout error. The problem is that when I then run
 sess.run(enq, feed_dict={ph:2}, options=timeout_option)
 i get the error:
 
 CancelledError                            Traceback (most recent call last)
  in ()
 ----> 1 sess.run(Q, options=timeout_option)
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in run(self, fetches, feed_dict, options, run_metadata)
 715     try:
 716       result = self._run(None, fetches, feed_dict, options_ptr,
 --> 717                          run_metadata_ptr)
 718       if run_metadata:
 719         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _run(self, handle, fetches, feed_dict, options, run_metadata)
 913     if final_fetches or final_targets:
 914       results = self._do_run(handle, final_targets, final_fetches,
 --> 915                              feed_dict_string, options, run_metadata)
 916     else:
 917       results = []
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
 963     if handle is None:
 964       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
 --> 965                            target_list, options, run_metadata)
 966     else:
 967       return self._do_call(_prun_fn, self._session, handle, feed_dict,
 /usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc in _do_call(self, fn, *args)
 983         except KeyError:
 984           pass
 --> 985       raise type(e)(node_def, op, message)
 986
 987   def _extend_graph(self):
 CancelledError: Dequeue operation was cancelled
 [[Node: fifo_queue_Dequeue = QueueDequeue_class=[""loc:@fifo_queue""], component_types=[DT_FLOAT, DT_INT64, DT_FLOAT], timeout_ms=-1, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
 [[Node: PlaceholderWithDefault/_25 = _Recvclient_terminated=false, recv_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_14_PlaceholderWithDefault"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""]]
 Caused by op u'fifo_queue_Dequeue', defined at:
 File """", line 1, in 
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py"", line 469, in main
 app.start()
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/kernelapp.py"", line 459, in start
 ioloop.IOLoop.instance().start()
 File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/ioloop.py"", line 160, in start
 super(ZMQIOLoop, self).start()
 File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/ioloop.py"", line 883, in start
 handler_func(fd_obj, events)
 File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
 return fn(_args, *_kwargs)
 File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 433, in _handle_events
 self._handle_recv()
 File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 465, in _handle_recv
 self._run_callback(callback, msg)
 File ""/usr/lib/python2.7/dist-packages/zmq/eventloop/zmqstream.py"", line 407, in _run_callback
 callback(_args, *_kwargs)
 File ""/home/cgel/.local/lib/python2.7/site-packages/tornado/stack_context.py"", line 275, in null_wrapper
 return fn(_args, *_kwargs)
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 281, in dispatcher
 return self.dispatch_shell(stream, msg)
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 245, in dispatch_shell
 handler(stream, idents, msg)
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/kernel/zmq/ipkernel.py"", line 389, in execute_request
 shell.run_cell(code, store_history=store_history, silent=silent)
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2741, in run_cell
 interactivity=interactivity, compiler=compiler)
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2827, in run_ast_nodes
 if self.run_code(code):
 File ""/home/cgel/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py"", line 2883, in run_code
 exec(code_obj, self.user_global_ns, self.user_ns)
 File """", line 76, in 
 input_state, action, Y = q.dequeue()
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py"", line 418, in dequeue
 self._queue_ref, self._dtypes, name=name)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 863, in _queue_dequeue
 timeout_ms=timeout_ms, name=name)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 747, in apply_op
 op_def=op_def)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2372, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1298, in init
 self._traceback = _extract_stack()
 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 #5115, or mute the thread
 https://github.com/notifications/unsubscribe-auth/AABaHEAbq2SwT4ovuH_LMYQUf622LxEqks5q2MLEgaJpZM4KdPpU
 .
 
 		",3.0,cgel,2016-10-25T18:22:13Z,"
 		<denchmark-link:https://github.com/gunan>@gunan</denchmark-link>
  there seems to be a enqueue/dequeue issue in 0.11, that does not appear in HEAD. A candidate for cherry-picking?
 		",f46fe646a26f0514fdfbfcea3882fd0120f24388,Derek Murray,2016-11-09 13:48:52-08:00,MODIFY,4,tensorflow\core\common_runtime\direct_session.cc,tensorflow\core\common_runtime\direct_session.cc,1.0,,"1171,1172,1173,1174,1175,1176",MODIFY,0.0,tensorflow\core\common_runtime\direct_session.h,tensorflow\core\common_runtime\direct_session.h,4.0,cgel,2016-10-25T18:22:32Z,"
 		Same behavior in 11rc1. To clarify, the problem happens if you call enqueue
 after previous dequeue operation timed out:
 conda create -n tf11rc1-cpu python=3.5
 source activate tf11rc1-cpu
 export TF_BINARY_URL=
 <denchmark-link:https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl>https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0rc1-py3-none-any.whl</denchmark-link>
 
 pip install --upgrade $TF_BINARY_URL
 export CUDA_VISIBLE_DEVICES=
 python
 import tensorflow as tf
 print(tf.version)
 q = tf.FIFOQueue(2, tf.float32)
 enq = q.enqueue(1.)
 deq = q.dequeue()
 timeout_option = tf.RunOptions(timeout_in_ms=1000)
 sess = tf.Session()
 print(sess.run(enq))  # works
 print(sess.run(deq))  # works
 print(sess.run(deq, options=timeout_option)) # times out
 print(sess.run(enq))  # doesn't work
 <denchmark-h:hr></denchmark-h>
 
 (tf11rc1-cpu) bash-3.2$ export CUDA_VISIBLE_DEVICES=
 (tf11rc1-cpu) bash-3.2$ python
 Python 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:52:12)
 [GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin
 Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
 
 
 
 import tensorflow as tf
 print(tf.version)
 0.11.0rc1
 q = tf.FIFOQueue(2, tf.float32)
 enq = q.enqueue(1.)
 deq = q.dequeue()
 timeout_option = tf.RunOptions(timeout_in_ms=1000)
 sess = tf.Session()
 print(sess.run(enq))
 None
 print(sess.run(deq))  # works
 1.0
 print(sess.run(deq, options=timeout_option)) # times out
 W tensorflow/core/kernels/queue_base.cc:302] _0_fifo_queue: Skipping
 cancelled dequeue attempt with queue not closed
 Traceback (most recent call last):
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 972, in _do_call
 return fn(*args)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 954, in _run_fn
 status, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/contextlib.py"",
 line 66, in exit
 next(self.gen)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"",
 line 463, in raise_exception_on_not_ok_status
 pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors.DeadlineExceededError: Timed out waiting
 for notification
 
 
 
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File """", line 1, in 
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 717, in run
 run_metadata_ptr)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 915, in _run
 feed_dict_string, options, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 965, in _do_run
 target_list, options, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 985, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.DeadlineExceededError: Timed out waiting
 for notification
 
 
 
 print(sess.run(enq))  # doesn't work
 Traceback (most recent call last):
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 972, in _do_call
 return fn(*args)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 954, in _run_fn
 status, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/contextlib.py"",
 line 66, in exit
 next(self.gen)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/errors.py"",
 line 463, in raise_exception_on_not_ok_status
 pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors.CancelledError: Enqueue operation was
 cancelled
 [[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],
 _class=[""loc:@fifo_queue""], timeout_ms=-1,
 _device=""/job:localhost/replica:0/task:0/cpu:0""](fifo_queue,
 fifo_queue_enqueue/component_0)]]
 
 
 
 During handling of the above exception, another exception occurred:
 Traceback (most recent call last):
 File """", line 1, in 
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 717, in run
 run_metadata_ptr)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 915, in _run
 feed_dict_string, options, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 965, in _do_run
 target_list, options, run_metadata)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/client/session.py"",
 line 985, in _do_call
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors.CancelledError: Enqueue operation was
 cancelled
 [[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],
 _class=[""loc:@fifo_queue""], timeout_ms=-1,
 _device=""/job:localhost/replica:0/task:0/cpu:0""](fifo_queue,
 fifo_queue_enqueue/component_0)]]
 Caused by op 'fifo_queue_enqueue', defined at:
 File """", line 1, in 
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/ops/data_flow_ops.py"",
 line 329, in enqueue
 return gen_data_flow_ops._queue_enqueue(self._queue_ref, vals,
 name=scope)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"",
 line 982, in _queue_enqueue
 name=name)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py"",
 line 756, in apply_op
 op_def=op_def)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"",
 line 2380, in create_op
 original_op=self._default_original_op, op_def=op_def)
 File
 ""/Users/yaroslav/anaconda/envs/tf11rc1-cpu/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"",
 line 1298, in init
 self._traceback = _extract_stack()
 CancelledError (see above for traceback): Enqueue operation was cancelled
 [[Node: fifo_queue_enqueue = QueueEnqueue[Tcomponents=[DT_FLOAT],
 _class=[""loc:@fifo_queue""], timeout_ms=-1,
 _device=""/job:localhost/replica:0/task:0/cpu:0""](fifo_queue,
 fifo_queue_enqueue/component_0)]]
 On Mon, Oct 24, 2016 at 3:36 PM, drpngx <denchmark-link:mailto:notifications@github.com>notifications@github.com</denchmark-link>
  wrote:
 
 I tried from a recent version and it doesn't happen there. Do you want to
 upgrade?
 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 #5115 (comment),
 or mute the thread
 https://github.com/notifications/unsubscribe-auth/AABaHIyD8MLoO3AHyWZdJooYn2zy_hphks5q3TL4gaJpZM4KdPpU
 .
 
 		",5.0,cgel,2016-10-25T18:25:49Z,"
 		sure, if we have a fix let's go ahead and cherrypick that into 0.11
 		",6.0,cgel,2016-10-25T18:27:47Z,"
 		I didn't test after timeout, let me see.
 		",0.0,"212,213,214,215",212,,,,,MODIFY,1.0,tensorflow\python\kernel_tests\fifo_queue_test.py,tensorflow\python\kernel_tests\fifo_queue_test.py,1.0,"1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530",,,,,,,,,tensorflow::DirectSession::WaitForNotification,"run_state,timeout_in_ms",1160,1181,1.0,"410,429,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,489,490,491,492,493,494","428,471,472,473",tensorflow::DirectSession::Run,"run_options,inputs,output_names,target_nodes,outputs,run_metadata",364,550,1.0,"1183,1194",,tensorflow::DirectSession::WaitForNotification,"run_state,cm,timeout_in_ms",1182,1199,1.0,"711,712",690,tensorflow::DirectSession::PRun,"handle,inputs,output_names,outputs",628,718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,cgel,2016-10-25T19:17:16Z,"
 		Yeah, also broken at HEAD.
 		",testReusableAfterTimeout,self,1515,1530,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,cgel,2016-10-26T01:33:58Z,"
 		Right. The dequeue has a closure which calls Cancel, but that is ignored, because the queue is not closed_.
 <denchmark-code>W1025 18:29:29.356150   47689 queue_base.cc:303] _0_fifo_queue: Skipping cancelled dequeue attempt with queue not closed
 </denchmark-code>
 
 <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  any clue?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,cgel,2016-10-27T22:04:39Z,"
 		Looks like <denchmark-link:https://github.com/tensorflow/tensorflow/blob/32d1dcc10e1fdf33dc6742337c6e0869f7b3c557/tensorflow/core/common_runtime/direct_session.cc#L1166>this</denchmark-link>
  is the code responsible:
       // TODO(sherrym): This cancels all steps in the session, even ones that
       // have not exceeded their deadline. An alternative would be to use a
       // two-level cancellation manager with a Session-global one containing
       // several step-local ones. Probably the RunState should have its own
       // CancellationManager.
       cancellation_manager_->StartCancel();
 		",10.0,cgel,2016-10-28T17:33:19Z,"
 		Do we have a fix for this? We are still accepting cherry-picks for the RC until EOD today.
 		",11.0,cgel,2016-11-07T19:26:25Z,"
 		I have some time to look at this now, so I'll attempt a quick fix.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5380,MircoT,2016-11-03T15:25:32Z,2016-11-04T18:28:13Z,'tensorflow/core/public/session.h' file not found,"
 I didn't find anything about this problem. I'm creating the python package from the source with bazel, the commands used are the same of the official guide in the section <denchmark-link:https://www.tensorflow.org/versions/r0.11/get_started/os_setup.html#create-the-pip-package-and-install>create pip package</denchmark-link>
 .
 The file .whl generated is working but doesn't have some include files that I use in a new op created in C++. The missing files are session_options.h and session.h which, however, are present in the official .whl of tensorflow that you can download (the binary package pre-compiled).
 To add those files I had to insert some requirements in tensorflow/core/BUILD:
 <denchmark-code>
 ---
  tensorflow/core/BUILD | 5 +++++
  1 file changed, 5 insertions(+)
 
 diff --git a/tensorflow/core/BUILD b/tensorflow/core/BUILD
 index 79546cc..7d9e9a2 100644
 --- a/tensorflow/core/BUILD
 +++ b/tensorflow/core/BUILD
 @@ -214,6 +214,9 @@ cc_library(
          ""platform/strong_hash.h"",
          ""platform/thread_annotations.h"",
          ""platform/types.h"",
 +        ""public/version.h"",
 +        ""public/session.h"",
 +        ""public/session_options.h"",
      ],
      visibility = [""//visibility:public""],
      deps = [
 @@ -299,6 +302,8 @@ tf_cuda_library(
          ""framework/type_traits.h"",
          ""framework/types.h"",
          ""public/version.h"",
 +        ""public/session.h"",
 +        ""public/session_options.h"",
          ""util/bcast.h"",
          ""util/cuda_kernel_helper.h"",
          ""util/device_name_utils.h"",
 -- 
 2.10.2
 </denchmark-code>
 
 Is it normal or I missed some configuration during the building? I could not fix it without this additions.
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: macOS Sierra (10.12.1)
 Installed version of CUDA and cuDNN: NO CUDA
 Source:
 
 The commit hash: ""eaa9dde98d95f843ad1d3d0f5956693991372e4a""
 Bazel version:
 
 <denchmark-code>Build label: 0.3.2-homebrew
 Build target: bazel-out/local-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Sat Oct 8 08:02:20 2016 (1475913740)
 Build timestamp: 1475913740
 Build timestamp as int: 1475913740
 </denchmark-code>
 
 <denchmark-h:h3>Example to test</denchmark-h>
 
 If you try to import a graph as you can see in the <denchmark-link:https://www.tensorflow.org/versions/r0.11/api_docs/cc/index.html>guide</denchmark-link>
  you need to require the header: .
 <denchmark-h:h3>Logs or other output that would be helpful</denchmark-h>
 
 <denchmark-code>... fatal error: 'tensorflow/core/public/session.h' file not found
 #include ""tensorflow/core/public/session.h""
          ^
 1 error generated.
 make: *** [...] Error 1
 </denchmark-code>
 
 	",1.0,MircoT,2016-11-03T17:28:04Z,"
 		Will send a fix, thanks for the report!
 		",2.0,MircoT,2017-05-12T16:20:13Z,"
 		I'm getting this error, where is the fix?
 		",,,,,b725df4aaf4fde1139f7204bae31650a25348e0f,Vijay Vasudevan,2016-11-03 12:45:19-07:00,MODIFY,0,tensorflow\tools\pip_package\BUILD,tensorflow\tools\pip_package\BUILD,0.0,24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5543,DavidNorman,2016-11-11T11:50:59Z,2017-02-15T10:55:39Z,Constant folding doesn't remove control edges,"
 I believe that when constant folding takes place, and a section of a graph is replaced by a constant, that only the data output edge of the replaced node is removed.
 I believe that I can see that a graph of nodes ending up in a Div (part of the gradient generation bit) is replaced by a Const.  The output of the Div goes to a Mul, and this is changed to the new const correctly.
 However, there is a control output from the Div going to a Const (not sure why, but it is).  This is not changed to the Div replacement.  Consequently the dead node pruning doesn't remove the original Div.
 Here is some trace:
 During the constant folding:
 Graph Before #nodes 67 #edges 109
 Graph Constant graph #nodes 32 #edges 42
 Constant foldable 32 : 67
 Replacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:ipu:0""](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant
 Replacing edge to gradients/Square_grad/mul_1:0
 During the post constant folding pruning:
 PruneForReverseReachability: gradients/Square_grad/mul_1 <- gradients/Mean_grad/truediv/_0__cf__1
 PruneForReverseReachability: gradients/Square_grad/mul/x <- gradients/Mean_grad/truediv
 After the pruning:
 Graph ConstFolding #nodes 68 #edges 110
 When the graph is passed to the device, it contains:
 Node: gradients/Square_grad/mul/x = Const<denchmark-link:%5Egradients/Mean_grad/truediv>dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 2>, _device=""/job:localhost/replica:0/task:0/device:ipu:0""</denchmark-link>
 
 	",1.0,DavidNorman,2016-11-11T13:27:17Z,"
 		Example:
 <denchmark-code>import tensorflow as tf
 
 # Creates a graph.
 a = tf.placeholder(tf.float32, [2,2])
 b = tf.placeholder(tf.float32, [2,2])
 c = tf.constant([[2.0,2.0],[2.0,2.0]])
 d = tf.constant([[2.0,2.0],[2.0,2.0]])
 e = tf.add(c, d)
 with tf.control_dependencies([e]):
   f = tf.mul(a, a)
   g = tf.mul(b, e)
   o = tf.add(f, g)
 
 sess = tf.Session()
 
 fd = {
   a: [[1.0,2.0],[3.0,4.0]],
   b: [[1.0,0.0],[0.0,1.0]],
 }
 
 sess.run(o, feed_dict=fd)
 </denchmark-code>
 
 During constant folding this leads to:
 <denchmark-code>Graph Before #nodes 11 #edges 16
 || 
 || () -> () {
 ||   n2 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_16__recv_Placeholder_0"", tensor_type=float]()
 ||   n3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_17__recv_Placeholder_1_0"", tensor_type=float]()
 ||   n4 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()
 ||   n5 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()
 ||   n6 = Add[T=float](n4, n5)
 ||   n7 = Mul[T=float](n2, n2) @ n6
 ||   n8 = Mul[T=float](n3, n6)
 ||   n9 = Add[T=float](n7, n8)
 ||   n10 = _Send[T=float, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device_incarnation=1, tensor_name=""edge_5_Add_1""](n9)
 || }
 || 
 
 
 Graph After #nodes 12 #edges 17
 || 
 || () -> () {
 ||   n11 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 4 4 4...>]()
 ||   n2 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_16__recv_Placeholder_0"", tensor_type=float]()
 ||   n3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_17__recv_Placeholder_1_0"", tensor_type=float]()
 ||   n4 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()
 ||   n5 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 2 2 2...>]()
 ||   n8 = Mul[T=float](n3, n11)
 ||   n6 = Add[T=float](n4, n5)
 ||   n7 = Mul[T=float](n2, n2) @ n6
 ||   n9 = Add[T=float](n7, n8)
 ||   n10 = _Send[T=float, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device_incarnation=1, tensor_name=""edge_5_Add_1""](n9)
 || }
 || 
 </denchmark-code>
 
 You can see that there is a constant replacing the Add, but that the Add remains, because it is referred to by the control edge.
 I think that the output should be:
 <denchmark-code>Graph After #nodes 12 #edges 17
 || 
 || () -> () {
 ||   n11 = Const[dtype=float, value=Tensor<type: float shape: [2,2] values: 4 4 4...>]()
 ||   n2 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_16__recv_Placeholder_0"", tensor_type=float]()
 ||   n3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_17__recv_Placeholder_1_0"", tensor_type=float]()
 ||   n8 = Mul[T=float](n3, n11)
 ||   n7 = Mul[T=float](n2, n2) @ n11
 ||   n9 = Add[T=float](n7, n8)
 ||   n10 = _Send[T=float, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device_incarnation=1, tensor_name=""edge_5_Add_1""](n9)
 || }
 || 
 </denchmark-code>
 
 		",2.0,DavidNorman,2016-11-11T13:41:13Z,"
 		One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.
 <denchmark-code>Graph Before #nodes 67 #edges 109
 || 
 || () -> () {
 ||   n2 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_110__recv_Placeholder_1_0"", tensor_type=float]()
 ||   n3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_109__recv_Placeholder_0"", tensor_type=float]()
 ||   n4 = Variable[container="""", dtype=float, shape=[1], shared_name=""""]()
 ||   n7 = Variable[container="""", dtype=float, shape=[1], shared_name=""""]()
 ||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()
 ||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()
 ||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()
 ||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()
 ||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()
 ||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()
 ||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()
 ||   n5 = Identity[T=float, _class=[""loc:@Variable""]](n4)
 ||   n8 = Identity[T=float, _class=[""loc:@Variable_1""]](n7)
 ||   n13 = Fill[T=float](n11, n12)
 ||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)
 ||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)
 ||   n34 = BroadcastGradientArgs[T=int32](n32, n33)
 ||   n44 = BroadcastGradientArgs[T=int32](n42, n43)
 ||   n56 = BroadcastGradientArgs[T=int32](n54, n55)
 ||   n6 = Mul[T=float](n5, n3)
 ||   n15 = Reshape[T=float, Tshape=int32](n13, n14)
 ||   n25 = Maximum[T=int32](n23, n24)
 ||   n9 = Add[T=float](n6, n8)
 ||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)
 ||   n26 = Div[T=int32](n21, n25)
 ||   n10 = Sub[T=float](n9, n2)
 ||   n27 = Cast[DstT=float, SrcT=int32](n26)
 ||   n28 = Div[T=float](n17, n27)
 ||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28
 ||   n30 = Mul[T=float](n29, n10)
 ||   n31 = Mul[T=float](n28, n30)
 ||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)
 ||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)
 ||   n36 = Reshape[T=float, Tshape=int32](n35, n32)
 ||   n38 = Neg[T=float](n37)
 ||   n39 = Reshape[T=float, Tshape=int32](n38, n33)
 ||   n40 = NoOp() @ n36, n39
 ||   n41 = Identity[T=float, _class=[""loc:@gradients/sub_grad/Reshape""]](n36) @ n40
 ||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)
 ||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)
 ||   n46 = Reshape[T=float, Tshape=int32](n45, n42)
 ||   n48 = Reshape[T=float, Tshape=int32](n47, n43)
 ||   n49 = NoOp() @ n46, n48
 ||   n50 = Identity[T=float, _class=[""loc:@gradients/add_grad/Reshape""]](n46) @ n49
 ||   n53 = Identity[T=float, _class=[""loc:@gradients/add_grad/Reshape_1""]](n48) @ n49
 ||   n51 = Mul[T=float](n50, n3)
 ||   n52 = Mul[T=float](n5, n50)
 ||   n65 = ApplyGradientDescent[T=float, _class=[""loc:@Variable_1""], use_locking=false](n7, n63, n53)
 ||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)
 ||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)
 ||   n58 = Reshape[T=float, Tshape=int32](n57, n54)
 ||   n60 = Reshape[T=float, Tshape=int32](n59, n55)
 ||   n61 = NoOp() @ n58, n60
 ||   n62 = Identity[T=float, _class=[""loc:@gradients/mul_grad/Reshape""]](n58) @ n61
 ||   n64 = ApplyGradientDescent[T=float, _class=[""loc:@Variable""], use_locking=false](n4, n63, n62)
 ||   n66 = NoOp() @ n64, n65
 || }
 || 
 
 
 Replacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:ipu:0""](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant
 
 Graph After #nodes 68 #edges 110
 || 
 || () -> () {
 ||   n67 = Const[dtype=float, value=Tensor<type: float shape: [100] values: 0.01 0.01 0.01...>]()
 ||   n2 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_110__recv_Placeholder_1_0"", tensor_type=float]()
 ||   n3 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:ipu:0"", send_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device_incarnation=1, tensor_name=""edge_109__recv_Placeholder_0"", tensor_type=float]()
 ||   n4 = Variable[container="""", dtype=float, shape=[1], shared_name=""""]()
 ||   n7 = Variable[container="""", dtype=float, shape=[1], shared_name=""""]()
 ||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()
 ||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()
 ||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()
 ||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()
 ||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()
 ||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()
 ||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()
 ||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()
 ||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()
 ||   n5 = Identity[T=float, _class=[""loc:@Variable""]](n4)
 ||   n8 = Identity[T=float, _class=[""loc:@Variable_1""]](n7)
 ||   n13 = Fill[T=float](n11, n12)
 ||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)
 ||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)
 ||   n34 = BroadcastGradientArgs[T=int32](n32, n33)
 ||   n44 = BroadcastGradientArgs[T=int32](n42, n43)
 ||   n56 = BroadcastGradientArgs[T=int32](n54, n55)
 ||   n6 = Mul[T=float](n5, n3)
 ||   n15 = Reshape[T=float, Tshape=int32](n13, n14)
 ||   n25 = Maximum[T=int32](n23, n24)
 ||   n9 = Add[T=float](n6, n8)
 ||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)
 ||   n26 = Div[T=int32](n21, n25)
 ||   n10 = Sub[T=float](n9, n2)
 ||   n27 = Cast[DstT=float, SrcT=int32](n26)
 ||   n28 = Div[T=float](n17, n27)
 ||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28
 ||   n30 = Mul[T=float](n29, n10)
 ||   n31 = Mul[T=float](n67, n30)
 ||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)
 ||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)
 ||   n36 = Reshape[T=float, Tshape=int32](n35, n32)
 ||   n38 = Neg[T=float](n37)
 ||   n39 = Reshape[T=float, Tshape=int32](n38, n33)
 ||   n40 = NoOp() @ n36, n39
 ||   n41 = Identity[T=float, _class=[""loc:@gradients/sub_grad/Reshape""]](n36) @ n40
 ||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)
 ||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)
 ||   n46 = Reshape[T=float, Tshape=int32](n45, n42)
 ||   n48 = Reshape[T=float, Tshape=int32](n47, n43)
 ||   n49 = NoOp() @ n46, n48
 ||   n50 = Identity[T=float, _class=[""loc:@gradients/add_grad/Reshape""]](n46) @ n49
 ||   n53 = Identity[T=float, _class=[""loc:@gradients/add_grad/Reshape_1""]](n48) @ n49
 ||   n51 = Mul[T=float](n50, n3)
 ||   n52 = Mul[T=float](n5, n50)
 ||   n65 = ApplyGradientDescent[T=float, _class=[""loc:@Variable_1""], use_locking=false](n7, n63, n53)
 ||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)
 ||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)
 ||   n58 = Reshape[T=float, Tshape=int32](n57, n54)
 ||   n60 = Reshape[T=float, Tshape=int32](n59, n55)
 ||   n61 = NoOp() @ n58, n60
 ||   n62 = Identity[T=float, _class=[""loc:@gradients/mul_grad/Reshape""]](n58) @ n61
 ||   n64 = ApplyGradientDescent[T=float, _class=[""loc:@Variable""], use_locking=false](n4, n63, n62)
 ||   n66 = NoOp() @ n64, n65
 || }
 || 
 
 </denchmark-code>
 
 Node n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.
 		",3.0,DavidNorman,2016-11-11T18:57:06Z,"
 		<denchmark-link:https://github.com/keveman>@keveman</denchmark-link>
  would you please comment? Thanks.
 		",73bc428901dfd6507bbea1315c8813f2048233ff,David Norman,2017-01-15 14:01:46-08:00,MODIFY,2,tensorflow\core\common_runtime\simple_placer.cc,tensorflow\core\common_runtime\simple_placer.cc,1.0,608,608,MODIFY,2.0,tensorflow\core\common_runtime\simple_placer_test.cc,tensorflow\core\common_runtime\simple_placer_test.cc,4.0,DavidNorman,2016-11-14T09:50:07Z,"
 		I should point out that if I bind the nodes to the CPU, then the results are the same.
 (these examples are for my own device back end)
 		",5.0,DavidNorman,2016-11-14T10:09:01Z,"
 		It seems that constant_folding.cc, ReplaceTensorWithConstant, assumes that the node that will be replaced will have multiple outputs, and only replaces those edges that are fed by the one replaced output.
 BroadcastGradientArgs is a good example.  It really needs to be replaced by 2 constants, and the code, as it stands, works well for that.
 Perhaps it is worth considering what it means to have a node that only has a control output assigned to it?  Is this valid?  If not, then the control output should be eliminated, then then the dead code pruning algorithm will kill off that node and it's upstream nodes.
 I've not played with the control flow (loops, conditionals), so I don't know if nodes with only control outputs are useful.   I would expect that loops and conditonals are done with Boolean edges, not control edges.  If this is the case, then I think that adding the control edge removal thing is the correct thing to do.
 		",6.0,DavidNorman,2016-11-14T11:17:48Z,"
 		I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:
 <denchmark-code>  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY
   // constraint, do not replace it.
   // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY
   // constraint, do not replace it.
 </denchmark-code>
 
 Why would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).
 In the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?
 		",1.0,"1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298",,tensorflow::TEST_F,"SimplePlacerTest,TestGeneratorNodeDoesntFollowNonColocatedConsumers",1264,1298,,,,,,,,,,,,,,,tensorflow::IsGeneratorNode,node,606,609,1.0,"733,734,735,797,798,799,800,801,802,803,804,805","733,734,735,797",tensorflow::SimplePlacer::Run,,624,814,,,,,,,,,,,,,,,1.0,"1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260",,tensorflow::TEST_F,"SimplePlacerTest,TestGeneratorNodeFollowsConsumerNode",1231,1260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,DavidNorman,2016-11-14T11:24:08Z,"
 		Here is a diff for trying out 'remove control outputs from nodes that have only a control output after constant folding' change:
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/589083/diff.txt>diff.txt</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,DavidNorman,2016-11-18T23:36:38Z,"
 		Sorry for the late response. Constant folding is conservative (perhaps overly) in what it replaces. In your case, since the original node had an outgoing control dependence, it didn't get replaced. Note that the analysis to decide whether that is safe to do is somewhat non-trivial. We have to determine whether that node is control dependent on a stateful node. We chose to let it be conservative. If you have a real example where the performance is poor because of this conservativeness, I'll be happy to take a look.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,DavidNorman,2016-11-21T10:01:45Z,"
 		I see the problem.  My case isn't really a performance issue, per se. I am trying to implement a device on TF for our new hardware.  The extra nodes of integer math are getting in the way of the initial bring-up.
 If there are stateful nodes in the graph, then why does the constant tree finder consider them acceptable to be within the constant tree?  Isn't the constant that is generated (there is one generated) is not valid after the 1st run through the graph?  In which case, the constant should not replace that section of the graph at all?
 		",10.0,DavidNorman,2016-11-24T09:54:21Z,"
 		I realize i failed to write sensible language in that last message.
 What I intended to say is that replacing a stateful node with a constant doesn't seem right.  If the stateful node changes state, then the constant is invalid. Either the stateful node should not be included in the constant sub-graph, or the constant sub-graph should be re-evaluated whenever the stateful node changes, in which case there doesn't seem to be a lot of point in having it.
 		",11.0,DavidNorman,2016-11-24T11:44:57Z,"
 		Hi,
 In fact, I'm pretty certain that the following code taken from constant_folding.cc will prevent the constant sub-graph from containing any stateful nodes.  In which case, it should be ok to apply the control edge removal.
 <denchmark-code>bool IsConstantFoldable(const FunctionLibraryDefinition* flib_def,
                         const Node* n,
                         std::function<bool(const Node*)> consider) {
   if (n->op_def().is_stateful()) {
     return false;
   }
 </denchmark-code>
 
 		",12.0,DavidNorman,2016-12-08T20:29:37Z,"
 		Started looking at this. Will have a fix soon.
 		",13.0,DavidNorman,2017-02-15T10:55:39Z,"
 		Now that my own device is an XLA device, I am no longer concerned about this.
 However this is still a problem as far as I can see.  unless the dead code removal is removing nodes that only have a control output.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5652,ptrendx,2016-11-16T23:32:51Z,2017-02-27T22:44:53Z,cifar10_multi_gpu_train.py breaks with more than 1 GPU,"
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: Ubuntu
 Installed version of CUDA and cuDNN: 8.0 and 5
 
 The commit hash (git rev-parse HEAD): 3d41cf7
 The output of bazel version:
 Build label: 0.3.2
 Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
 Build time: Fri Oct 7 17:25:10 2016 (1475861110)
 Build timestamp: 1475861110
 Build timestamp as int: 1475861110
 
 <denchmark-h:h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</denchmark-h>
 
 python cifar10_multi_gpu_train.py --num_gpus=2
 Both cifar10_train.py and cifar10_multi_gpu_train.py (without specifying num_gpus, so running on a single GPU) work.
 <denchmark-h:h3>Logs or other output that would be helpful</denchmark-h>
 
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
 Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
 Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.
 Traceback (most recent call last):
 File ""cifar10_multi_gpu_train.py"", line 280, in 
 tf.app.run()
 File ""/data/github/tensorflow/_python_build/tensorflow/python/platform/app.py"", line 43, in run
 sys.exit(main(sys.argv[:1] + flags_passthrough))
 File ""cifar10_multi_gpu_train.py"", line 276, in main
 train()
 File ""cifar10_multi_gpu_train.py"", line 180, in train
 loss = tower_loss(scope)
 File ""cifar10_multi_gpu_train.py"", line 92, in tower_loss
 loss_averages_op = loss_averages.apply(losses + [total_loss])
 File ""/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py"", line 391, in apply
 self._averages[var], var, decay, zero_debias=zero_debias))
 File ""/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py"", line 70, in assign_moving_average
 update_delta = _zero_debias(variable, value, decay)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/training/moving_averages.py"", line 177, in _zero_debias
 trainable=False)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py"", line 1024, in get_variable
 custom_getter=custom_getter)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py"", line 850, in get_variable
 custom_getter=custom_getter)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py"", line 346, in get_variable
 validate_shape=validate_shape)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py"", line 331, in _true_getter
 caching_device=caching_device, validate_shape=validate_shape)
 File ""/data/github/tensorflow/_python_build/tensorflow/python/ops/variable_scope.py"", line 650, in _get_single_variable
 ""VarScope?"" % name)
 ValueError: Variable tower_1/tower_1/conv1/weight_loss/avg/biased does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?
 	",1.0,ptrendx,2016-11-17T19:01:09Z,"
 		<denchmark-link:https://github.com/shlens>@shlens</denchmark-link>
  Could you take a look?  Scopes have been fiddled with recently, and maybe this file didn't get updated?
 		",2.0,ptrendx,2017-02-27T22:44:52Z,"
 		This was fixed in November.
 		",,,,,1b531b9d88361a3b8506399d5edae155125c5371,A. Unique TensorFlower,2016-11-18 13:07:30-08:00,MODIFY,1,tensorflow\models\image\cifar10\cifar10_multi_gpu_train.py,tensorflow\models\image\cifar10\cifar10_multi_gpu_train.py,1.0,96,"90,91,92,93",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tower_loss,scope,65,98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6602,albertz,2017-01-02T16:04:51Z,2018-01-05T22:02:48Z,fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory,"
 I try to write my own op and I have installed TensorFlow 0.12.0 with GPU support on Linux.
 This code fails:
 <denchmark-code>#include ""tensorflow/core/platform/stream_executor.h""
 </denchmark-code>
 
 With error:
 <denchmark-code>fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory
 </denchmark-code>
 
 That files does not exists.
 Some more include files seem to be missing. When grepping for DeviceMemory in the include path, the only file it finds is include/tensorflow/core/util/stream_executor_util.h.
 	",1.0,albertz,2017-01-04T03:21:09Z,"
 		AFAIK when adding new ops, you need to install from sources:
 <denchmark-link:https://www.tensorflow.org/how_tos/adding_an_op/>https://www.tensorflow.org/how_tos/adding_an_op/</denchmark-link>
 
 From your description, I think you maybe installed the pip package?
 When I look at our repo, both master and r0.12 branch has the file:
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/core/platform/stream_executor.h>https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/core/platform/stream_executor.h</denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/stream_executor.h>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/stream_executor.h</denchmark-link>
 
 So I am not sure what you installed, or what the reason might be.
 Please try installing from sources and if the problem persists, include more debugging information, detailing each step you took to reproduce the problem.
 		",2.0,albertz,2017-01-04T03:43:39Z,"
 		It's possible our stream executor headers are not packaged in pip. I can take a look tomorrow
 		",3.0,albertz,2017-01-04T21:08:28Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L159>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L159</denchmark-link>
  it looks like we're missing tensorflow/stream_executor there
 I'm not sure if we're missing anything else, unfortunately, but I can add that change.
 		",02d2385b8c33e89b53e49bc89e646b54de920bad,Vijay Vasudevan,2017-01-04 15:28:42-08:00,MODIFY,0,tensorflow\tools\pip_package\setup.py,tensorflow\tools\pip_package\setup.py,0.0,160,,,,,,4.0,albertz,2017-07-11T09:45:23Z,"
 		This is not complete yet. When I try to include stream_executor.h:
 <denchmark-code>#include ""tensorflow/core/platform/stream_executor.h""
 </denchmark-code>
 
 I get:
 /u/zeyer/.local/lib/python2.7/site-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
 		",5.0,albertz,2017-08-11T22:16:45Z,"
 		I get the same error with stream_executor.h, when compiling a custom op with nvcc and 1.3.0 binaries:
 <denchmark-code>In file included from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/default/stream_executor.h:26:0,
                  from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/platform/stream_executor.h:24,
                  from /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/core/util/cuda_kernel_helper.h:26,
                  from roi_align_gpu.cu.cc:8:
 /usr/local/lib/python3.5/dist-packages/tensorflow/include/tensorflow/stream_executor/dso_loader.h:32:30: fatal error: cuda/cuda_config.h: No such file or directory
 </denchmark-code>
 
 		",6.0,albertz,2017-09-06T23:54:41Z,"
 		Same problem here with 1.3.0 compiled from source when I try to compile a custom op. Does anyone have a workaround?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,albertz,2017-09-14T00:36:54Z,"
 		<denchmark-link:https://github.com/pronobis>@pronobis</denchmark-link>
  it seems like you're suggesting a workaround yourself in <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12860>#12860</denchmark-link>
 :
 Copying cuda_config.h to /site-packages/tensorflow/include/tensorflow/stream_executor/cuda solves the problem.
 Are you asking for something else?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,albertz,2017-09-18T00:45:14Z,"
 		I suppose <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12860>#12860</denchmark-link>
  is there now to track this problem. Generally, I'm looking for a better permanent solution, but again, that's what <denchmark-link:https://github.com/tensorflow/tensorflow/issues/12860>#12860</denchmark-link>
  is for.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,albertz,2017-11-07T10:06:35Z,"
 		I wonder what the state is here? Wouldn't it be possible to just add cuda_config.h to the pip package? Wouldn't that solve the problem?
 		",10.0,albertz,2017-12-20T19:25:28Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",11.0,albertz,2017-12-20T23:44:18Z,"
 		<denchmark-link:https://github.com/av8ramit>@av8ramit</denchmark-link>
  could you check if we have all the dependencies of stream_executor.h as a part of the pip package, except for cuda headers?
 		",12.0,albertz,2017-12-21T21:29:20Z,"
 		<denchmark-link:https://github.com/gunan>@gunan</denchmark-link>
 , the dependencies are in the pip package
 		",13.0,albertz,2018-01-05T19:10:18Z,"
 		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",14.0,albertz,2018-01-05T21:34:31Z,"
 		All the dependencies are in the pip package.
 		",15.0,albertz,2018-01-05T22:02:48Z,"
 		Folding all the duplicates into <denchmark-link:https://github.com/tensorflow/tensorflow/issues/15002>#15002</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6717,lwohlhart,2017-01-08T00:29:44Z,2017-01-24T01:57:33Z,Incorrect gradient for categorical distribution entropy,"
 The Categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w.r.t. the input operators doesn't work.
 logits = tf.Variable(initial_value=[[1., 2., 3.], [2., 5., 1.]])
 
 probabilities = tf.nn.softmax(logits)
 log_probabilities = tf.nn.log_softmax(logits)
 entropy = - tf.reduce_sum(probabilities * log_probabilities, axis=-1)
 
 # using the actual distribution would be nicer but gradients seem buggy
 categorical_distribution = tf.contrib.distributions.Categorical(p=probabilities)
 categorical_distribution_entropy = categorical_distribution.entropy()
 
 # initialize
 init = tf.global_variables_initializer()
 sess = tf.Session()
 sess.run(init)
 
 # works
 print(sess.run(entropy))
 print(sess.run(tf.gradients(entropy, [logits])))
 
 # apparently loses gradient information
 print(sess.run(categorical_distribution_entropy))
 print(sess.run(tf.gradients(categorical_distribution_entropy, [logits])))
 In the outputs we see that the entropy calculation works but the gradients are somehow lost. This obviously also doesn't work when I try to maximize this entropy using any optimizer.
 <denchmark-code>[ 0.83239555  0.27431309]
 [array([[ 0.14181709,  0.14077036, -0.28258738],
        [ 0.13012242, -0.19513965,  0.06501719]], dtype=float32)]
 [ 0.83239555  0.27431309]
 [array([[ 0.,  0.,  0.],
        [ 0.,  0.,  0.]], dtype=float32)]
 </denchmark-code>
 
 	",1.0,lwohlhart,2017-01-09T02:25:16Z,"
 		I suspect this a numerical stability issue. <denchmark-link:https://gist.github.com/yaroslavvb/97504b8221a8529e7a51a50915206d68>Looking</denchmark-link>
  at your ""working"" graph, it uses . Whereas in the categorical distribution, it computes  and combines it with  <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 
 <denchmark-link:https://cloud.githubusercontent.com/assets/23068/21756041/dd734ea4-d5d0-11e6-82d0-001d9fab5bea.png></denchmark-link>
 
 		",2.0,lwohlhart,2017-01-18T16:37:43Z,"
 		Taking a look.
 		",3.0,lwohlhart,2017-01-18T19:33:38Z,"
 		For numerical stability, we use tf.nn.softmax_cross_entropy_with_logits to calculate the entropy.  I just found out that this operation does not perform backprop with respect to the ""labels"" argument (the exponentiated probabilities term of the entropy).  Thus the zeros.
 Probably the best way to fix this is for us to implement the numerically stable equivalent of your reduce_sum.
 		",b39773478542bf812a12715f7f753f9a88e5e86c,Eugene Brevdo,2017-01-18 15:50:31-08:00,MODIFY,1,tensorflow\contrib\distributions\python\kernel_tests\categorical_test.py,tensorflow\contrib\distributions\python\kernel_tests\categorical_test.py,1.0,"151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175",,MODIFY,2.0,tensorflow\contrib\distributions\python\ops\categorical.py,tensorflow\contrib\distributions\python\ops\categorical.py,4.0,lwohlhart,2017-01-18T19:53:16Z,"
 		Fix going into master hopefully today / tomorrow.
 		",5.0,lwohlhart,2017-01-24T01:56:15Z,"
 		thank you for the fix !
 i guess we can close this then
 		",,,,,1.0,"239,240,241",,_kl_categorical_categorical,"a,b,name",223,241,,,,,,,,,,,,,,,testEntropyGradient,self,151,175,,,,,,,,,,,,,,,,,,,,,,1.0,"212,213","212,213",_entropy,self,211,213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6738,cyanoboy,2017-01-09T12:01:43Z,2017-08-11T03:21:20Z,How to train Multibox object detector included in the TF Detect Android demo,"
 There is a way to train a custom model for the multibox object detector that is included in the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java>TF Detect Android demo</denchmark-link>
 ?
 	",1.0,cyanoboy,2017-01-09T16:58:33Z,"
 		<denchmark-link:https://github.com/cyanoboy>@cyanoboy</denchmark-link>
  We're preparing a walk through for exactly this; should be out sometime in the not-too-distant future.
 		",2.0,cyanoboy,2017-01-11T02:07:46Z,"
 		Assigning to <denchmark-link:https://github.com/andrewharp>@andrewharp</denchmark-link>
  assuming he wants to add  to the commit message which adds that walkthrough.
 		",3.0,cyanoboy,2017-03-07T23:06:10Z,"
 		<denchmark-link:https://github.com/andrewharp>@andrewharp</denchmark-link>
  can you give us an update about the walk through? Thanks for your awesome work 
 		",53aabd5cb0ffcc1fd33cbd00eb468dd8d8353df2,A. Unique TensorFlower,2017-08-09 18:22:04-07:00,MODIFY,0,WORKSPACE,WORKSPACE,0.0,"54,55,56,57,58,59,60,61,62,63",,MODIFY,0.0,tensorflow\core\framework\register_types.h,tensorflow\core\framework\register_types.h,4.0,cyanoboy,2017-03-28T17:30:24Z,"
 		<denchmark-link:https://github.com/lukeisontheroad>@lukeisontheroad</denchmark-link>
  We're still moving around stuff internally; it's ending up being a bit tricky to open-source. In the meantime there is the <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/TensorFlowYoloDetector.java>TF YOLO detector</denchmark-link>
  which you should be able to find training walkthroughs for online.
 		",5.0,cyanoboy,2017-04-13T08:42:42Z,"
 		Is there a trained version available for the custom inceptionV2 used for the feature extraction in the multibox object detector ?
 Thank you for your work on this demo
 		",6.0,cyanoboy,2017-06-16T19:18:40Z,"
 		hey Andrew, is this still forthcoming?
 		",0.0,"98,125","98,125",,,,,MODIFY,0.0,tensorflow\examples\android\BUILD,tensorflow\examples\android\BUILD,0.0,95,95,MODIFY,0.0,tensorflow\examples\android\README.md,tensorflow\examples\android\README.md,0.0,"27,28,29,30,31,34,154","27,28,29,32,152",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,cyanoboy,2017-06-23T20:36:44Z,"
 		This is still active. We're looking to get a model compatible with the <denchmark-link:https://github.com/tensorflow/models/tree/master/object_detection>TF Object Detection API</denchmark-link>
  out in the next 2 or 3 weeks. That should provide a much easier path to retraining.
 <denchmark-link:https://github.com/jch1>@jch1</denchmark-link>
 
 		",,,,,,,,,MODIFY,0.0,tensorflow\examples\android\download-models.gradle,tensorflow\examples\android\download-models.gradle,0.0,13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,tensorflow\examples\android\src\org\tensorflow\demo\DetectorActivity.java,tensorflow\examples\android\src\org\tensorflow\demo\DetectorActivity.java,1.0,"141,142,151,152,163,164,165,166,167,168,169,170,171,172,173,174,175,176,192,197","139,165,170",DetectorActivity::onPreviewSizeChosen,"size,rotation",132,258,1.0,384,,DetectorActivity::processImageRGBbytes,rgbBytes,384,384,MODIFY,0.0,tensorflow\examples\android\src\org\tensorflow\demo\TensorFlowMultiBoxDetector.java,tensorflow\examples\android\src\org\tensorflow\demo\TensorFlowMultiBoxDetector.java,0.0,44,44,,,,,8.0,cyanoboy,2017-06-24T06:42:29Z,"
 		Yup, this should be done soon, stay tuned!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"352,353,354,355,356,357,358,364","330,350",DetectorActivity::onImageAvailable,reader,263,382,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tensorflow\examples\android\src\org\tensorflow\demo\TensorFlowObjectDetectionAPIModel.java,,,,MODIFY,0.0,tensorflow\examples\android\src\org\tensorflow\demo\tracking\MultiBoxTracker.java,tensorflow\examples\android\src\org\tensorflow\demo\tracking\MultiBoxTracker.java,0.0,"62,63,64,65",62,9.0,cyanoboy,2017-06-24T13:49:26Z,"
 		sounds awesome!
 		",10.0,cyanoboy,2017-07-16T07:43:02Z,"
 		<denchmark-link:https://github.com/andrewharp>@andrewharp</denchmark-link>
  Thanks for your information!
 		",11.0,cyanoboy,2017-08-11T17:49:20Z,"
 		The default object detector in the Android Object Detection demo is now the 80-class <denchmark-link:https://github.com/tensorflow/models/tree/master/object_detection>Object Detection API</denchmark-link>
  SSD model thanks to <denchmark-link:https://github.com/jch1>@jch1</denchmark-link>
 's commit, so retraining is now possible following the tutorials there.
 The Yolo and original Multibox detectors remain available by modifying <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L89>DetectorActivity.java</denchmark-link>
  (the person-detecting multibox model  would also need to be added to <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/BUILD#L92>external_assets</denchmark-link>
  if reverting).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6766,jrosti,2017-01-10T12:06:06Z,2018-01-23T21:12:41Z,"softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument","
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System: Ubuntu 16.04
 Installed version of CUDA and cuDNN: CUDA-8.0, CUDNN 5.1.5
 Tensorflow version: 0.12.1 installed from
 <denchmark-link:https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl>https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl</denchmark-link>
 
 Reproduced also using tf-0.11.0, CUDA-7.5, CUDNN-5.1.3
 <denchmark-h:h3>Minimal reproducible example</denchmark-h>
 
 <denchmark-code>import tensorflow as tf
 y = tf.placeholder(""int64"", [None], ""y"")
 one_hot_y=tf.one_hot(y,10)
 ce = tf.nn.softmax_cross_entropy_with_logits(one_hot_y, one_hot_y)
 sess = tf.Session()
 sess.run(ce, {y: []})
 </denchmark-code>
 
 Result on GPU:
 <denchmark-code>E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
 W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
 F tensorflow/core/common_runtime/gpu/gpu_device.cc:104] EigenAllocator for GPU ran out of memory when allocating 0. See error logs for more detailed info.
 Aborted (core dumped)
 </denchmark-code>
 
 Result on CPU:
 <denchmark-code>array([], dtype=float32)
 </denchmark-code>
 
 	",1.0,jrosti,2017-01-11T05:10:08Z,"
 		<denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>
  Here's a GPU memory alloc issue. Reported both here and on Stack Overflow: <denchmark-link:http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator>http://stackoverflow.com/questions/41530966/memory-error-with-eigenallocator</denchmark-link>
 
 		",2.0,jrosti,2017-04-09T09:34:00Z,"
 		Hi, <denchmark-link:https://github.com/jrosti>@jrosti</denchmark-link>
 
 Have this problem been solved?
 I met the same problem in 
 		",3.0,jrosti,2017-04-19T17:20:12Z,"
 		I still have this problem which seems to arise when using tensorflow fold.
 		",07e6ca0ac7cdfcb6105fb3410fc68355c04df1d5,Yong Tang,2018-01-23 13:12:40-08:00,MODIFY,1,tensorflow\core\kernels\xent_op.cc,tensorflow\core\kernels\xent_op.cc,1.0,"70,71,72,73,74,75","70,71,72,73",MODIFY,1.0,tensorflow\python\kernel_tests\xent_op_test.py,tensorflow\python\kernel_tests\xent_op_test.py,4.0,jrosti,2017-06-02T04:06:18Z,"
 		Still getting this issue, as of 06/02/2017 when using tf.gather_nd(...) and softmax_cross_entropy_with_logits(...) together.
 		",5.0,jrosti,2017-06-24T23:02:07Z,"
 		Anyone looking into this? I'm also experiencing this issue using TensorFlow 1.2.0 (v1.2.0-rc2-21-g12f033d). In my case, a tf.while_loop is being used (in conjunction with tf.TensorArray); however, the same error occurs if I swap out the tf.while_loop for a while statement.
 Backtrace (abbr):
 <denchmark-code>2017-06-24 05:42:28.894580: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
 2017-06-24 05:42:28.894638: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
 2017-06-24 05:42:28.894647: E tensorflow/core/common_runtime/bfc_allocator.cc:244] tried to allocate 0 bytes
 2017-06-24 05:42:28.894657: W tensorflow/core/common_runtime/allocator_retry.cc:32] Request to allocate 0 bytes
 2017-06-24 05:42:28.895314: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
 2017-06-24 05:42:28.895406: E tensorflow/core/common_runtime/bfc_allocator.cc:378] tried to deallocate nullptr
 2017-06-24 05:42:28.896185: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](while/Reshape, while/Reshape_1)]]
 2017-06-24 05:42:28.896209: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](while/Reshape, while/Reshape_1)]]
 .
 .
 .
 2017-06-24 05:42:28.905449: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](while/Reshape, while/Reshape_1)]]
 2017-06-24 05:42:28.905669: W tensorflow/core/framework/op_kernel.cc:1158] Resource exhausted: Ran out of GPU memory when allocating 0 bytes for
          [[Node: while/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/gpu:0""](while/Reshape, while/Reshape_1)]]
 </denchmark-code>
 
 A bit further down,
 <denchmark-code>Caused by op 'while/SoftmaxCrossEntropyWithLogits', defined at:
   ...
   File ""scripts/gpu_experiment.py"", line 400, in build_backend
     backend['o'] = build_outputs(config, backend.get('o', None))
   File ""scripts/gpu_experiment.py"", line 363, in build_outputs
     loop_vars = tf.while_loop(loop_cond, _build_outputs, loop_vars)
     ...
     cross_ent = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)
 </denchmark-code>
 
 		",6.0,jrosti,2017-07-25T09:14:01Z,"
 		I'm also experiencing this issue and error message is same as  <denchmark-link:https://github.com/j-wilson>@j-wilson</denchmark-link>
    . I am fine tuning object detction api on faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017 model.ckpt.
 		",1.0,"243,244,245,246,247,248,249,250,251",,testZeroDimension,self,243,251,,,,,,,,,,,,,,,tensorflow::SoftmaxXentWithLogitsOp::Compute,context,41,76,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,jrosti,2017-08-24T11:27:15Z,"
 		Also having the same problem in tf 1.3. Would be great if it threw a more descriptive exception.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,jrosti,2017-09-17T17:38:02Z,"
 		I hit this issue.  After some investigating I realized I was feeding in an empty tensor.  Not sure if that is the only thing that can cause it, but that was my problem at least.
 It was easy to fix, but it certainly would be nice if it threw a more descriptive error.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,jrosti,2017-11-02T12:02:43Z,"
 		Massive thanks. Empty tensor is the cause of my problem too. <denchmark-link:https://github.com/metachi>@metachi</denchmark-link>
 
 		",10.0,jrosti,2017-12-20T19:25:48Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",11.0,jrosti,2018-01-04T19:18:11Z,"
 		It has been 14 days with no activity and the awaiting tensorflower label was assigned. Please update the label and/or status accordingly.
 		",12.0,jrosti,2018-01-10T18:25:03Z,"
 		I'm facing same problem even checking if tensor is empty or not:
 <denchmark-code>
  loss = K.switch(tf.size(y_true) > 0,
                     tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),
                     tf.constant(0.0))
 </denchmark-code>
 
 		",13.0,jrosti,2018-01-10T18:43:20Z,"
 		<denchmark-link:https://github.com/filipetrocadoferreira>@filipetrocadoferreira</denchmark-link>
  Have you tried to wrap the conditional branches into lambda functions?
 <denchmark-code>loss = K.switch(tf.size(y_true) > 0,
                     lambda: tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true, dim=1),
                     lambda: tf.constant(0.0))
 </denchmark-code>
 
 This should allow a lazy execution of the branches. (see <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/cond>tf.cond</denchmark-link>
 )
 		",14.0,jrosti,2018-01-11T09:19:23Z,"
 		wow, this seems to work. Can you link to an explanation?
 		",15.0,jrosti,2018-01-11T21:24:41Z,"
 		Added PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/16051>#16051</denchmark-link>
  for a fix.
 		",16.0,jrosti,2018-01-11T22:42:24Z,"
 		<denchmark-link:https://github.com/filipetrocadoferreira>@filipetrocadoferreira</denchmark-link>
  I can’t find any link with an extensive explanation (I tought it was explained in tf.cond ’s documentation but actually it is not). In short, if you don’t define the branches as functions, they will be both executed regardless of the condition. This explains why you still had the problem despite checking the tensor’s size.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6823,namrata-ibm,2017-01-13T08:40:52Z,2017-01-31T19:08:12Z,Upgrade HighwayHash,"
 The HighwayHash module which is downloaded as an external dependency in TensorFlow produces different hash results on big endian and little endian architectures. This causes the test  from  to fail on big endian. After raising an <denchmark-link:https://github.com/google/highwayhash/issues/35>issue</denchmark-link>
  with HighwayHash community, they have added a change to make hash values consistent across architectures through <denchmark-link:https://github.com/google/highwayhash/commit/cdde139127319cb5eb3917b635c4d2b182533cb4>commit</denchmark-link>
 .
 Will it be possible to pick this or higher commit of HighwayHash in TensorFlow?
 	",1.0,namrata-ibm,2017-01-13T20:20:49Z,"
 		This doesn't sound good. I'm going to write up a change that upgrades TensorFlow. I just hope this change won't cause problems for people with big endian CPUs who stored the result of the previous hash.
 		",2.0,namrata-ibm,2017-01-13T22:05:41Z,"
 		This upgrade is now blocked by <denchmark-link:https://github.com/google/highwayhash/issues/36>google/highwayhash#36</denchmark-link>
 .
 		",3.0,namrata-ibm,2017-01-24T17:45:38Z,"
 		This should now be unblocked. I'm now going to mail out a change.
 		",6ed5eaaac13540c1b9d5a7549b2c82aac91ba318,Justine Tunney,2017-01-25 10:48:48-08:00,MODIFY,0,tensorflow\workspace.bzl,tensorflow\workspace.bzl,0.0,"128,131,132,134,135,136","128,131,132,134,135",ADD,0.0,None,third_party\highwayhash.BUILD,4.0,namrata-ibm,2017-02-01T05:51:11Z,"
 		Thank you <denchmark-link:https://github.com/jart>@jart</denchmark-link>
  and <denchmark-link:https://github.com/rmlarsen>@rmlarsen</denchmark-link>
  for resolving this.
 		",5.0,namrata-ibm,2017-02-01T06:37:58Z,"
 		Pleasure to be of service.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7025,yaroslavvb,2017-01-23T23:39:53Z,2017-05-01T16:22:27Z,"Getting ""Dst tensor is not initialized."" when really the problem is out of GPU memory","
 This is the stack trace we sometimes get when trying to use TensorFlow on a GPU that's occupied by another process. It would help debugging if the error said something about memory.
 <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>
 
 tf.version: '0.12.1-1934-g27fca7d-dirty'
 (nightly from last week)
 <denchmark-code>W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
 W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
 name: TITAN X (Pascal)
 major: 6 minor: 1 memoryClockRate (GHz) 1.531
 pciBusID 0000:04:00.0
 Total memory: 11.90GiB
 Free memory: 381.44MiB
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)
 Traceback (most recent call last):
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1022, in _do_call
     return fn(*args)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1004, in _run_fn
     status, run_metadata)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/contextlib.py"", line 66, in __exit__
     next(self.gen)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
     pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""memory_test.py"", line 87, in <module>
     profile_densenet(False)
   File ""memory_test.py"", line 65, in profile_densenet
     sess.run(net.initializer, {net.x_init: trainx[:init_batch_size]})
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 767, in run
     run_metadata_ptr)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 965, in _run
     feed_dict_string, options, run_metadata)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
     target_list, options, run_metadata)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 Caused by op 'zeros_1266', defined at:
   File ""memory_test.py"", line 87, in <module>
     profile_densenet(False)
   File ""memory_test.py"", line 59, in profile_densenet
     net = densenet_lib.densenet(init_batch_size, batch_size, layers_per_block, filters_per_layer, save_memory=save_memory)
   File ""/home/yaroslav/openai.git/densenet/densenet.py"", line 183, in densenet
     optimizer = nn.adamax_updates(all_params, loss, lr=tf_lr)
   File ""/home/yaroslav/openai.git/densenet/nn.py"", line 41, in adamax_updates
     mg = tf.Variable(tf.zeros(int_shape(p)), p.name + '_adamax_mg')
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/ops/array_ops.py"", line 1376, in zeros
     output = constant(zero, shape=shape, dtype=dtype, name=name)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 169, in constant
     attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2395, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/home/yaroslav/.conda/envs/tim-jan17/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1264, in __init__
     self._traceback = _extract_stack()
 
 InternalError (see above for traceback): Dst tensor is not initialized.
 	 [[Node: zeros_1266 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [160] values: 0 0 0...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 </denchmark-code>
 
 	",1.0,yaroslavvb,2017-01-24T00:14:48Z,"
 		Classifying as ""docs"" but this is really an error message / error propagation issue.
 <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  do you by any chance have a PR in mind that you could submit?
 		",2.0,yaroslavvb,2017-01-24T00:24:57Z,"
 		I can change the message to read ""Dst tensor is not initialized (possibly caused by Out of Memory)"" unless <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>
  has a better idea. PS, this is not a hypothetical problem, we've had people thinking it's a regression in tensorflow/training scripts because of this error
 		",3.0,yaroslavvb,2017-01-24T01:10:56Z,"
 		My feeling is that this should fail at the place of memory allocation, not
 at the point of memory copy. There needs to be some digging to find out why
 it didn't fail earlier.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Mon, Jan 23, 2017 at 4:25 PM, Yaroslav Bulatov ***@***.***> wrote:
  I can change the message to read ""Dst tensor is not initialized (possibly
  caused by Out of Memory)"" unless @zheng-xq <https://github.com/zheng-xq>
  has a better idea
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#7025 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/APAgTtwKSbjLp9qQ0r0te9E38jX4uMb2ks5rVUUVgaJpZM4Lrr-s>
  .
 
 
 
 		",0a9b39caefd437fec742ae48b25061abd6e2699b,Vijay Vasudevan,2017-04-28 18:04:24-07:00,MODIFY,1,tensorflow\core\common_runtime\gpu\gpu_device.cc,tensorflow\core\common_runtime\gpu\gpu_device.cc,1.0,"468,469,470,471,472,473,474,475",,,,,,4.0,yaroslavvb,2017-01-26T18:32:35Z,"
 		Hi  <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  <denchmark-link:https://github.com/zheng-xq>@zheng-xq</denchmark-link>
 
 I'm getting this Dst Tensor Not Initialized error.
 (See my comment (the last one)  in this issue elsewhere: <denchmark-link:https://github.com/aymericdamien/TensorFlow-Examples/issues/38>aymericdamien/TensorFlow-Examples#38</denchmark-link>
 )
 I'm reproducing the stack trace here in case it helps diagnose the issue:
 <denchmark-code>▶ python imagenet_inference.py 
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally
 I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
 name: GeForce GT 750M
 major: 3 minor: 0 memoryClockRate (GHz) 0.9255
 pciBusID 0000:01:00.0
 Total memory: 2.00GiB
 Free memory: 305.92MiB
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
 I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State: 
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472
 I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024
 I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184
 I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size: 
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB
 I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats: 
 Limit:                   111063040
 InUse:                     9337856
 MaxInUse:                  9337856
 NumAllocs:                      11
 MaxAllocSize:              3538944
 
 W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________
 W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.
 W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.
 E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.
 	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 Traceback (most recent call last):
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1021, in _do_call
     return fn(*args)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1003, in _run_fn
     status, run_metadata)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py"", line 66, in __exit__
     next(self.gen)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
     pywrap_tensorflow.TF_GetCode(status))
 tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 During handling of the above exception, another exception occurred:
 
 Traceback (most recent call last):
   File ""imagenet_inference.py"", line 19, in <module>
     sess.run(init)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 766, in run
     run_metadata_ptr)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 964, in _run
     feed_dict_string, options, run_metadata)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
     target_list, options, run_metadata)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 Caused by op 'Variable_10/initial_value', defined at:
   File ""imagenet_inference.py"", line 16, in <module>
     probs = AlexNet(x, feature_extract=False)
   File ""/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py"", line 139, in AlexNet
     fc6W = tf.Variable(net_data[""fc6""][0])
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 224, in __init__
     expected_shape=expected_shape)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 333, in _init_from_args
     initial_value, name=""initial_value"", dtype=dtype)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
     ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
     return constant(v, dtype=dtype, name=name)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 169, in constant
     attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
     self._traceback = _extract_stack()
 
 InternalError (see above for traceback): Dst tensor is not initialized.
 	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 </denchmark-code>
 
 Here's deviceQuery successfully reporting seeing the GPU:
 <denchmark-code> py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ echo $CUDA_HOME 
 /usr/local/cuda
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ echo $CUDA_VISIBLE_DEVICES
 
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ ./deviceQuery 
 ./deviceQuery Starting...
 
  CUDA Device Query (Runtime API) version (CUDART static linking)
 
 Detected 1 CUDA Capable device(s)
 
 Device 0: ""GeForce GT 750M""
   CUDA Driver Version / Runtime Version          8.0 / 8.0
   CUDA Capability Major/Minor version number:    3.0
   Total amount of global memory:                 2048 MBytes (2147024896 bytes)
   ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores
   GPU Max Clock rate:                            926 MHz (0.93 GHz)
   Memory Clock rate:                             2508 Mhz
   Memory Bus Width:                              128-bit
   L2 Cache Size:                                 262144 bytes
   Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
   Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
   Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
   Total amount of constant memory:               65536 bytes
   Total amount of shared memory per block:       49152 bytes
   Total number of registers available per block: 65536
   Warp size:                                     32
   Maximum number of threads per multiprocessor:  2048
   Maximum number of threads per block:           1024
   Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
   Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
   Maximum memory pitch:                          2147483647 bytes
   Texture alignment:                             512 bytes
   Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
   Run time limit on kernels:                     Yes
   Integrated GPU sharing Host Memory:            No
   Support host page-locked memory mapping:       Yes
   Alignment requirement for Surfaces:            Yes
   Device has ECC support:                        Disabled
   Device supports Unified Addressing (UVA):      Yes
   Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
   Compute Mode:
      < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
 
 deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M
 Result = PASS
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ 
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ 
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ 
  py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ ./bandwidthTest 
 [CUDA Bandwidth Test] - Starting...
 Running on...
 
  Device 0: GeForce GT 750M
  Quick Mode
 
  Host to Device Bandwidth, 1 Device(s)
  PINNED Memory Transfers
    Transfer Size (Bytes)	Bandwidth(MB/s)
    33554432			3633.5
 
  Device to Host Bandwidth, 1 Device(s)
  PINNED Memory Transfers
    Transfer Size (Bytes)	Bandwidth(MB/s)
    33554432			6343.5
 
  Device to Device Bandwidth, 1 Device(s)
  PINNED Memory Transfers
    Transfer Size (Bytes)	Bandwidth(MB/s)
    33554432			42554.1
 
 Result = PASS
 
 NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
 </denchmark-code>
 
 My System:
 <denchmark-code>MacBook Pro (Retina, 15-inch, Late 2013)
 2.3 GHz Intel Core i7
 16 GB 1600 MHz DDR3
 NVIDIA GeForce GT 750M 2048 MB
 
 ---
 from System Report > Graphics
 NVIDIA GeForce GT 750M:
 
   Chipset Model:	NVIDIA GeForce GT 750M
   Type:	GPU
   Bus:	PCIe
   PCIe Lane Width:	x8
   VRAM (Total):	2048 MB
   Vendor:	NVIDIA (0x10de)
   Device ID:	0x0fe9
   Revision ID:	0x00a2
   ROM Revision:	3776
   gMux Version:	4.0.8 [3.2.8]
   Displays:
 Color LCD:
   Display Type:	Retina LCD
   Resolution:	2880 x 1800 Retina
   Retina:	Yes
   Pixel Depth:	32-Bit Color (ARGB8888)
   Main Display:	Yes
   Mirror:	Off
   Online:	Yes
   Built-In:	Yes
 </denchmark-code>
 
 		",5.0,yaroslavvb,2017-01-26T18:47:55Z,"
 		Sounds like you are running out of GPU memory
 <denchmark-link:#>…</denchmark-link>
 
 
 On Jan 26, 2017 10:33 AM, ""Atul Acharya"" ***@***.***> wrote:
  Hi @yaroslavvb <https://github.com/yaroslavvb> @zheng-xq
  <https://github.com/zheng-xq>
 
  I'm getting this Dst Tensor Not Initialized error.
 
  (See my comment (the last one) in this issue elsewhere:
  aymericdamien/TensorFlow-Examples#38
  <aymericdamien/TensorFlow-Examples#38>)
 
  I'm reproducing the stack trace here in case it helps diagnose the issue:
 
  ▶ python imagenet_inference.py
  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.dylib locally
  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.dylib locally
  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.dylib locally
  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.1.dylib locally
  I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.dylib locally
  I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] OS X does not support NUMA - returning NUMA node zero
  I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:
  name: GeForce GT 750M
  major: 3 minor: 0 memoryClockRate (GHz) 0.9255
  pciBusID 0000:01:00.0
  Total memory: 2.00GiB
  Free memory: 305.92MiB
  I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0
  I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y
  I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0)
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (256): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (512): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1024): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2048): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4096): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8192): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16384): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (32768): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (65536): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (131072): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (262144): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (524288): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (1048576): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (2097152): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (4194304): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (8388608): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (16777216): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (67108864): 	Total Chunks: 1, Chunks in use: 0 97.01MiB allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (134217728): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:643] Bin (268435456): 	Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.
  I tensorflow/core/common_runtime/bfc_allocator.cc:660] Bin for 144.00MiB was 128.00MiB, Chunk State:
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60000 of size 1280
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a60500 of size 139520
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82600 of size 512
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700a82800 of size 1228800
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700bae800 of size 1024
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700baec00 of size 3538944
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0ec00 of size 1536
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x700f0f200 of size 2654208
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197200 of size 1536
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701197800 of size 1769472
  I tensorflow/core/common_runtime/bfc_allocator.cc:678] Chunk at 0x701347800 of size 1024
  I tensorflow/core/common_runtime/bfc_allocator.cc:687] Free at 0x701347c00 of size 101725184
  I tensorflow/core/common_runtime/bfc_allocator.cc:693]      Summary of in-use Chunks by size:
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 512 totalling 512B
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1024 totalling 2.0KiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1280 totalling 1.2KiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 2 Chunks of size 1536 totalling 3.0KiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 139520 totalling 136.2KiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1228800 totalling 1.17MiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 1769472 totalling 1.69MiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2654208 totalling 2.53MiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3538944 totalling 3.38MiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 8.91MiB
  I tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:
  Limit:                   111063040
  InUse:                     9337856
  MaxInUse:                  9337856
  NumAllocs:                      11
  MaxAllocSize:              3538944
 
  W tensorflow/core/common_runtime/bfc_allocator.cc:274] *********___________________________________________________________________________________________
  W tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 144.00MiB.  See logs for memory state.
  W tensorflow/core/framework/op_kernel.cc:965] Internal: Dst tensor is not initialized.
  E tensorflow/core/common_runtime/executor.cc:390] Executor failed to create kernel. Internal: Dst tensor is not initialized.
  	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
  Traceback (most recent call last):
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1021, in _do_call
      return fn(*args)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1003, in _run_fn
      status, run_metadata)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/contextlib.py"", line 66, in __exit__
      next(self.gen)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py"", line 469, in raise_exception_on_not_ok_status
      pywrap_tensorflow.TF_GetCode(status))
  tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
  	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
  During handling of the above exception, another exception occurred:
 
  Traceback (most recent call last):
    File ""imagenet_inference.py"", line 19, in <module>
      sess.run(init)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 766, in run
      run_metadata_ptr)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 964, in _run
      feed_dict_string, options, run_metadata)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
      target_list, options, run_metadata)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
      raise type(e)(node_def, op, message)
  tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
  	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
  Caused by op 'Variable_10/initial_value', defined at:
    File ""imagenet_inference.py"", line 16, in <module>
      probs = AlexNet(x, feature_extract=False)
    File ""/Users/aa/Developer/courses/self_driving_carnd/traffic-signs/CarND-Alexnet-Feature-Extraction/alexnet.py"", line 139, in AlexNet
      fc6W = tf.Variable(net_data[""fc6""][0])
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 224, in __init__
      expected_shape=expected_shape)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/variables.py"", line 333, in _init_from_args
      initial_value, name=""initial_value"", dtype=dtype)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 669, in convert_to_tensor
      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 176, in _constant_tensor_conversion_function
      return constant(v, dtype=dtype, name=name)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py"", line 169, in constant
      attrs={""value"": tensor_value, ""dtype"": dtype_value}, name=name).outputs[0]
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
      original_op=self._default_original_op, op_def=op_def)
    File ""/Users/aa/Developer/miniconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
      self._traceback = _extract_stack()
 
  InternalError (see above for traceback): Dst tensor is not initialized.
  	 [[Node: Variable_10/initial_value = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [9216,4096] values: [-0.0043384791 -0.0071635786 -0.0067223078]...>, _device=""/job:localhost/replica:0/task:0/gpu:0""]()]]
 
 
  Here's deviceQuery successfully reporting seeing the GPU:
 
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ echo $CUDA_HOME
  /usr/local/cuda
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ echo $CUDA_VISIBLE_DEVICES
 
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ ./deviceQuery
  ./deviceQuery Starting...
 
   CUDA Device Query (Runtime API) version (CUDART static linking)
 
  Detected 1 CUDA Capable device(s)
 
  Device 0: ""GeForce GT 750M""
    CUDA Driver Version / Runtime Version          8.0 / 8.0
    CUDA Capability Major/Minor version number:    3.0
    Total amount of global memory:                 2048 MBytes (2147024896 bytes)
    ( 2) Multiprocessors, (192) CUDA Cores/MP:     384 CUDA Cores
    GPU Max Clock rate:                            926 MHz (0.93 GHz)
    Memory Clock rate:                             2508 Mhz
    Memory Bus Width:                              128-bit
    L2 Cache Size:                                 262144 bytes
    Maximum Texture Dimension Size (x,y,z)         1D=(65536), 2D=(65536, 65536), 3D=(4096, 4096, 4096)
    Maximum Layered 1D Texture Size, (num) layers  1D=(16384), 2048 layers
    Maximum Layered 2D Texture Size, (num) layers  2D=(16384, 16384), 2048 layers
    Total amount of constant memory:               65536 bytes
    Total amount of shared memory per block:       49152 bytes
    Total number of registers available per block: 65536
    Warp size:                                     32
    Maximum number of threads per multiprocessor:  2048
    Maximum number of threads per block:           1024
    Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
    Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
    Maximum memory pitch:                          2147483647 bytes
    Texture alignment:                             512 bytes
    Concurrent copy and kernel execution:          Yes with 1 copy engine(s)
    Run time limit on kernels:                     Yes
    Integrated GPU sharing Host Memory:            No
    Support host page-locked memory mapping:       Yes
    Alignment requirement for Surfaces:            Yes
    Device has ECC support:                        Disabled
    Device supports Unified Addressing (UVA):      Yes
    Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
    Compute Mode:
       < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >
 
  deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 8.0, CUDA Runtime Version = 8.0, NumDevs = 1, Device0 = GeForce GT 750M
  Result = PASS
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶
   py35 ▶ ~ ▶ Developer ❯ … ❯ x86_64 ❯ darwin ❯ release ▶ $ ▶ ./bandwidthTest
  [CUDA Bandwidth Test] - Starting...
  Running on...
 
   Device 0: GeForce GT 750M
   Quick Mode
 
   Host to Device Bandwidth, 1 Device(s)
   PINNED Memory Transfers
     Transfer Size (Bytes)	Bandwidth(MB/s)
     33554432			3633.5
 
   Device to Host Bandwidth, 1 Device(s)
   PINNED Memory Transfers
     Transfer Size (Bytes)	Bandwidth(MB/s)
     33554432			6343.5
 
   Device to Device Bandwidth, 1 Device(s)
   PINNED Memory Transfers
     Transfer Size (Bytes)	Bandwidth(MB/s)
     33554432			42554.1
 
  Result = PASS
 
  NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.
 
  My System:
 
  MacBook Pro (Retina, 15-inch, Late 2013)
  2.3 GHz Intel Core i7
  16 GB 1600 MHz DDR3
  NVIDIA GeForce GT 750M 2048 MB
 
  ---
  from System Report > Graphics
  NVIDIA GeForce GT 750M:
 
    Chipset Model:	NVIDIA GeForce GT 750M
    Type:	GPU
    Bus:	PCIe
    PCIe Lane Width:	x8
    VRAM (Total):	2048 MB
    Vendor:	NVIDIA (0x10de)
    Device ID:	0x0fe9
    Revision ID:	0x00a2
    ROM Revision:	3776
    gMux Version:	4.0.8 [3.2.8]
    Displays:
  Color LCD:
    Display Type:	Retina LCD
    Resolution:	2880 x 1800 Retina
    Retina:	Yes
    Pixel Depth:	32-Bit Color (ARGB8888)
    Main Display:	Yes
    Mirror:	Off
    Online:	Yes
    Built-In:	Yes
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#7025 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AABaHJK2XuJg9IHUT3Rb63Nbtahdgr8sks5rWObHgaJpZM4Lrr-s>
  .
 
 
 
 		",6.0,yaroslavvb,2017-04-06T22:27:56Z,"
 		I get the same error and pretty sure it is for the same reason. I am on tf 1.0.0 and using keras 2.0.2.
 		",,,,,,,,,,,,,,,,,,,,,,tensorflow::BaseGPUDevice::MakeTensorFromProto,"tensor_proto,alloc_attrs,tensor",447,487,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,yaroslavvb,2017-04-28T22:30:17Z,"
 		For now, I changed the message to ""Dst tensor is not initialized (are you out of memory?)."" (not yet on Github).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,yaroslavvb,2017-04-28T22:41:26Z,"
 		<denchmark-link:https://github.com/vrv>@vrv</denchmark-link>
  may have localized the problem to 
 , which doesn't check for errors and thus produces an uninitialized tensor.
 <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  If you can reproduce the situation, would you mind checking if  at that point detects the problem?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,yaroslavvb,2017-04-28T23:07:44Z,"
 		<denchmark-link:https://github.com/vrv>@vrv</denchmark-link>
  Has a CL that will hopefully fix this.
 		",10.0,yaroslavvb,2017-04-28T23:45:30Z,"
 		Yeah, verified I have a CL that fixes this, submitting it internally now.
 		",11.0,yaroslavvb,2018-03-15T15:42:14Z,"
 		I just updated TF to 1.6 and still experience the same issue.
 I am running the smallest possible model, Cart-Pole, on GTX1080 8MB. Is it a TensorFlow bug that can be fixed somehow or we are simply trying to fit too big models (overenthusiastic the batch size probably the main reason for that)?
 sebtac
 		",12.0,yaroslavvb,2018-05-21T18:11:49Z,"
 		Still experiencing this issue with recent version <denchmark-link:https://github.com/tensorflow/tensorflow/commit/068fd9c936dbf8c9ace9edae9e7bb9e64256d381>068fd9c</denchmark-link>
 .
 (I can confirm this is due to an OOM issue.)
 <denchmark-code>tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 	 [[Node: _arg_q_actions_0_1/_7 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_339__arg_q_actions_0_1"", tensor_type=DT_INT32, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
 	 [[Node: loss/assert_broadcastable/AssertGuard/Assert/Switch/_33 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_91_loss/assert_broadcastable/AssertGuard/Assert/Switch"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
 </denchmark-code>
 
 		",13.0,yaroslavvb,2018-08-01T14:39:13Z,"
 		Getting this in TF 1.9.0 when I increase the size of my test set to a large number of samples (possibly a memory issue).
 		",14.0,yaroslavvb,2018-09-04T17:51:28Z,"
 		I am also getting the same error in TF 1.8.0  also in the latest version. My machine is 3x NVidia Tesla P40 22GB
 raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InternalError: Dst tensor is not initialized.
 [[Node: conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer/_2025 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_3008_conv4_22_1x1_increase/Conv2D-0-1-TransposeNCHWToNHWC-LayoutOptimizer"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:1""</denchmark-link>
 ]]
 [[Node: adversarial/Mean/_3087 = _Recv<denchmark-link:>client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:1"", send_device_incarnation=1, tensor_name=""edge_5391_adversarial/Mean"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""</denchmark-link>
 ]]
 		",15.0,yaroslavvb,2018-12-14T08:44:15Z,"
 		If not solved yet... free memory... previously generated not-used embedding, models etc...
 del all_embs, (model_names...), (model_input_names)
 import gc; gc.collect()
 time.sleep(10)
 		",16.0,yaroslavvb,2018-12-22T17:55:19Z,"
 		Has anyone solved this?
 		",17.0,yaroslavvb,2018-12-26T15:05:48Z,"
 		Make sure multiple processes are not accessing GPU and see if that solves the problem. It seemed to be the solution for me. Especially if you're running your code in Jupyter notebook, kill any other running notebook kernel.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7065,SeguinBe,2017-01-25T18:40:58Z,2017-05-10T03:03:31Z,pytorch 2.5x faster on VGG16,"
 <denchmark-h:h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>
 
 Started on SO, and was told to post here (<denchmark-link:http://stackoverflow.com/questions/41832779/tensorflow-2-5x-slower-than-pytorch-on-vgg16-architecture?noredirect=1#comment70901342_41832779>SO post</denchmark-link>
 )
 <denchmark-h:h3>Environment info</denchmark-h>
 
 Operating System:
 Ubuntu 14.04 + Maxwell Titan X
 Installed version of CUDA and cuDNN:
 CUDA 8.0, cuDNN 5.1
 :~$ ls -l /usr/local/cuda/lib64/libcud*
 -rw-r--r-- 1 root root    558720 Jan 25 08:23 /usr/local/cuda/lib64/libcudadevrt.a
 lrwxrwxrwx 1 root root        16 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0
 lrwxrwxrwx 1 root root        19 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44
 -rwxr-xr-x 1 root root    415432 Jan 25 08:23 /usr/local/cuda/lib64/libcudart.so.8.0.44
 -rw-r--r-- 1 root root    775162 Jan 25 08:23 /usr/local/cuda/lib64/libcudart_static.a
 lrwxrwxrwx 1 1000 users       13 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so -> libcudnn.so.5
 lrwxrwxrwx 1 1000 users       17 Jul 27 07:55 /usr/local/cuda/lib64/libcudnn.so.5 -> libcudnn.so.5.1.5
 -rwxrwxr-x 1 1000 users 79337624 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn.so.5.1.5
 -rw-rw-r-- 1 1000 users 69756172 Jul 27 07:53 /usr/local/cuda/lib64/libcudnn_static.a
 Installed from binary pip package :
 
 https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-0.12.1-cp35-cp35m-linux_x86_64.whl with an Anaconda distribution
 The output from python -c ""import tensorflow; print(tensorflow.__version__)"":
 
 <denchmark-code>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
 0.12.1
 </denchmark-code>
 
 <denchmark-h:h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</denchmark-h>
 
 Using the following code to do a forward pass on a pretrained VGG16 :
 import tensorflow as tf
 from tensorflow.contrib import slim
 from tensorflow.contrib.slim import nets
 
 tf.reset_default_graph()
 # Use RNG to avoid the feed_dict argument
 input_images = tf.random_uniform((16, 224, 224, 3), maxval=255)  
 preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
 saver = tf.train.Saver()
 
 config = tf.ConfigProto(log_device_placement=True)
 sess = tf.InteractiveSession(config=config)
 saver.restore(sess, './vgg_16.ckpt')
 
 # With jupyter notebook magic
 %timeit sess.run(preds)
 Compared to the pytorch version on the same machine :
 import numpy as np
 import torch
 import torchvision.models as models
 from torch.autograd import Variable
 torch.backends.cudnn.benchmark = True
 
 net = models.vgg16()
 net.cuda()
 
 _in = Variable(torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda())
 
 # With jupyter notebook magic
 %timeit net(_in)
 I get the following results by comparing the frameworks. Surprisingly, there is a small difference with the more complicated resnet-50 while I get a huge gap for the VGG16 architecture which (almost) just uses 3x3 convolutions.
 
 
 
 Model
 TF
 pytorch
 
 
 
 
 VGG16
 160ms
 65ms
 
 
 resnet-50
 58ms
 48ms
 
 
 
 	",1.0,SeguinBe,2017-01-25T18:42:42Z,"
 		<denchmark-link:https://github.com/sguada>@sguada</denchmark-link>
  do you see anything that got added recently that could address performance gap? (maybe some new fused ops?)
 		",2.0,SeguinBe,2017-01-25T18:50:11Z,"
 		cc: <denchmark-link:https://github.com/vincentvanhoucke>@vincentvanhoucke</denchmark-link>
  in case he knows others working on VGG-like models
 		",3.0,SeguinBe,2017-01-25T21:18:26Z,"
 		Hi <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
 .  I am going to reproduce the result and get back to you.  Wanted to let you know we are looking at this issue.
 		",0318cf082ee88ff0e226a5bf7da0487f44d82182,A. Unique TensorFlower,2017-01-30 12:49:13-08:00,MODIFY,1,tensorflow\core\kernels\conv_grad_filter_ops.cc,tensorflow\core\kernels\conv_grad_filter_ops.cc,1.0,"468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495",,MODIFY,1.0,tensorflow\core\kernels\conv_grad_input_ops.cc,tensorflow\core\kernels\conv_grad_input_ops.cc,4.0,SeguinBe,2017-01-25T21:19:32Z,"
 		thanks
 ps: <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  is the affected party here
 		",5.0,SeguinBe,2017-01-25T22:36:18Z,"
 		One drive-by observation: the setup with
 <denchmark-code>input_images = tf.random_uniform(16, 224, 224, 3), maxval=255)
 </denchmark-code>
 
 ...might be slow because it's invoking the random number generator for every batch. In the PyTorch program, you run the RNG once, outside the timing loop (in the call to np.random.randn()), and reuse its results several times.
 The following might be a fairer comparison:
 <denchmark-code>input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))
 preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
 sess.run(tf.global_variable_initializer())
 # ...
 </denchmark-code>
 
 		",6.0,SeguinBe,2017-01-25T22:58:26Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  tried it already, it does not change the timing at all.
 import tensorflow as tf
 from tensorflow.contrib import slim
 from tensorflow.contrib.slim import nets
 
 tf.reset_default_graph()
 input_images = tf.Variable(tf.random_uniform((16, 224, 224, 3), maxval=255))
 preds = nets.vgg.vgg_16(input_images, is_training=False)[0]
 saver = tf.train.Saver(var_list=[v for v in tf.global_variables() if 'vgg_16' in v.name])
 init_op = tf.variables_initializer([input_images])
 
 config = tf.ConfigProto(log_device_placement=False)
 sess = tf.InteractiveSession(config=config)
 saver.restore(sess, './vgg_16.ckpt')
 sess.run(init_op)
 On a side note, the resnet-50 timing in the end is more like 50ms so basically the same as pytorch.
 		",1.0,"632,633,634,635,636,637,638,639,640,641,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661",,tensorflow::Conv2DSlowBackpropInputOp::Compute,context,547,888,MODIFY,2.0,tensorflow\core\kernels\conv_grad_ops_3d.cc,tensorflow\core\kernels\conv_grad_ops_3d.cc,1.0,"408,410,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450","408,410",MODIFY,2.0,tensorflow\core\kernels\conv_ops.cc,tensorflow\core\kernels\conv_ops.cc,1.0,"454,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,497,498,499,500,501,502,503,504,505,506,509",,tensorflow::Conv2DSlowBackpropFilterOp::Compute,context,374,711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,SeguinBe,2017-01-26T20:37:08Z,"
 		The tf version is using the slower NHWC data format. Changing it to NCHW will most likely speed things up.
 		","tensorflow::Conv3DBackpropInputOp<GPUDevice,T>::Compute",context,389,605,"tensorflow::LaunchConv2DOp<GPUDevice,T>::launch","ctx,use_cudnn,cudnn_use_autotune,input_param,filter,row_stride,col_stride,padding,output,data_format",434,721,MODIFY,1.0,tensorflow\core\kernels\conv_ops_3d.cc,tensorflow\core\kernels\conv_ops_3d.cc,1.0,"175,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204",175,"tensorflow::LaunchConvOp<GPUDevice,T>::launch","ctx,input_param,filter,strides,padding,output",138,353,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\core\kernels\conv_ops_using_gemm.cc,tensorflow\core\kernels\conv_ops_using_gemm.cc,1.0,"236,237,238,239,240,241,251,252,253,254,255,256,257,258,259,260,261,262,263","236,237,238,239",tensorflow::Im2ColConvFunctor::operator ( ),"context,input_data,input_batches,input_height,input_width,input_depth,filter_data,filter_height,filter_width,filter_count,stride_rows,stride_cols,padding,output_data,output_height,output_width",211,419,,,,,,,,MODIFY,3.0,tensorflow\python\kernel_tests\conv_ops_3d_test.py,tensorflow\python\kernel_tests\conv_ops_3d_test.py,1.0,"512,513,514,515,516,517,518,519,520,521,522,523,524,525",,testFilterGradientKernelSizeMatchesInputSize,self,512,525,8.0,SeguinBe,2017-01-27T01:41:58Z,"
 		Can you try?
 with slim.arg_scope([slim.conv2d], data_format='NCHW'):
 preds, _ = nets.vgg.vgg_16(input_images, is_training=False)
 		",,,,,,,,,,,,,,,1.0,"686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,712",,"tensorflow::Conv3DBackpropFilterOp<GPUDevice,T>::Compute",context,630,839,,,,,,,,,,,,,,,1.0,"67,68,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102","67,68",tensorflow::LaunchGeneric::launch,"ctx,input,filter,row_stride,col_stride,padding,output,data_format",61,109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"527,528,529,530,531,532,533,534,535,536,537,538,539,540",,testInputGradientKernelSizeMatchesInputSize,self,527,540,1.0,"245,246,247,248,249,250,251",,testKernelSizeMatchesInputSize,self,245,251,,,,,,,,MODIFY,5.0,tensorflow\python\kernel_tests\conv_ops_test.py,tensorflow\python\kernel_tests\conv_ops_test.py,1.0,"560,561,562,563,564,565,566,567,568,569,570,571,572",,,,,,,,,9.0,SeguinBe,2017-01-27T09:51:57Z,"
 		input_images = tf.Variable(tf.random_uniform((16, 3, 224, 224), maxval=255))
 with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):
     preds, _ = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
 init_op = tf.global_variables_initializer()
 
 config = tf.ConfigProto(log_device_placement=True)
 sess = tf.InteractiveSession(config=config)
 sess.run(init_op)
 Using data_format='NHWC' and size [X, 224, 224, 3] I still get 160ms and with the data_format='NCHW' it is slightly better at 150ms...
 I have to note that the fc6-fc7 are implemented as convolution in the TF version, will try if modifying them to pure matrix-multiplication changes anything
 		",10.0,SeguinBe,2017-01-27T10:16:59Z,"
 		So I think that was mainly the solution, the tensorflow definition of the network was using a convolution instead of the fully connected linear matrix multiplication for fc6 fc7 fc8 (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py#L116>here</denchmark-link>
 ). Did not think originally it would be a big problem but to recapitulate :
 
 
 
 Model
 Timing
 
 
 
 
 TF-slim default
 160ms
 
 
 TF-slim + NCHW
 150ms
 
 
 fc layers instead of conv
 94ms
 
 
 fc layers instead of conv + NCHW
 82ms
 
 
 pytorch
 65ms
 
 
 
 There is still a gap but it is definitely more acceptable, should we consider this as resolved?
 		",11.0,SeguinBe,2017-01-27T17:38:50Z,"
 		<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  are the models identical? One way of telling is initializing with same weights and running the computation through. Since they both rely on CuDNN they should get the same timing, 30% slower seems off.
 BTW, you can print out layers and their flops like this, this can sometimes help spot the difference
 <denchmark-code>  tf.contrib.tfprof.model_analyzer.print_model_analysis(
       tf.get_default_graph(),
       tfprof_options=tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS)
 </denchmark-code>
 
 		",12.0,SeguinBe,2017-01-27T20:35:03Z,"
 		Looped in by <denchmark-link:https://github.com/sguada>@sguada</denchmark-link>
 .
 The discrepancy between using convolutional vs. fully connected layers should be fixed.
 The convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format (<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439>see here</denchmark-link>
 ).
 Using  vs.  should not make a difference for 'fc7' or 'fc8' -- <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  can you verify that please?
 However, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when
 convolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC
 		",13.0,SeguinBe,2017-01-28T00:49:51Z,"
 		<denchmark-link:https://github.com/gpapan>@gpapan</denchmark-link>
  yeah it would be great if it works when the input size and the kernel size are the same and padding is 'VALID'.
 Also, it seems that the optimization won't work with data_format='NCWH', but it should also work, isn't it?
 		",14.0,SeguinBe,2017-01-28T18:09:18Z,"
 		<denchmark-link:https://github.com/gpapan>@gpapan</denchmark-link>
  Yes, setting fc7 or/and fc8 as 1x1 conv does not change the timing (both in NHWC and NCHW btw)
 <denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  It'd take a bit more time to properly transfer the weights from one framework to another. Though I think that I find the same proportional gap even in the 3x3 convolutional layers. I'll investigate more in the coming days.
 		",15.0,SeguinBe,2017-01-29T22:33:00Z,"
 		<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  Thanks, that's very informative, I will go ahead and submit a change to optimize the branch in which the filter and input activations have the same size.
 <denchmark-link:https://github.com/sguada>@sguada</denchmark-link>
  I think that cudnn natively supports the NCHW format and handles this special case internally. The last experiment by <denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  also hints in the same direction.
 		",16.0,SeguinBe,2017-01-31T20:00:27Z,"
 		Can someone try it out and get timings with the latest head?
 		",17.0,SeguinBe,2017-01-31T21:21:53Z,"
 		<denchmark-link:https://github.com/SeguinBe>@SeguinBe</denchmark-link>
  can you please verify that the fix just pushed solves the conv2d vs. fully connected discrepancy issue?
 		",18.0,SeguinBe,2017-02-01T12:43:52Z,"
 		Recent update of <denchmark-link:https://arxiv.org/abs/1608.07249>Benchmarking State-of-the-Art Deep Learning Software Tools</denchmark-link>
  shows some performance issues. For example, (see table 7)  is significantly (~ 10 times) slower in TF than in other frameworks, an it's even slower at GTX 980 than at GTX 1080. Also, ResNet-50 is ~5.5 times faster in MXNet. Those are most significant differences.
 In addition, LSTM is around 3 times faster in CNTK, and ResNet-56 is twice faster in MXNet.
 Version used was TensorFlow 0.11 (commit <denchmark-link:https://github.com/tensorflow/tensorflow/tree/47dd089db3cd16d76595791b2e8483e2fd0b0a25>47dd089</denchmark-link>
 ) with CUDA 8.0 and cuDNN 5.1
 		",19.0,SeguinBe,2017-02-01T14:18:28Z,"
 		<denchmark-link:https://github.com/Randl>@Randl</denchmark-link>
  -- thanks, <denchmark-link:https://github.com/annarev>@annarev</denchmark-link>
  has been looking at those differences (ps, additional details should probably go to a different github issue since the problems there are different from vgg difference which is almost solved)
 		",20.0,SeguinBe,2017-05-10T03:03:29Z,"
 		Closing this item.  Comments can still be made.  While it does not always make a difference, you can also try adding .  This seems to make a bigger difference on K80 and it is model dependent.  For Pascal I saw a 9% improvement when testing a Wide ResNet implementation.  Our <denchmark-link:https://www.tensorflow.org/performance/performance_models>benchmark scripts</denchmark-link>
  also include a VGG16 model, and I asked the team to check that implementation of VGG16 against the findings in this thread.
 		",21.0,SeguinBe,2018-09-07T14:44:19Z,"
 		So I'm coming back to some old issues and tried again this one:
 <denchmark-h:h2>Setup</denchmark-h>
 
 <denchmark-code>conda create -n deepl python=3.6
 conda activate deepl
 conda install tensorflow-gpu=1.9 jupyter
 conda install pytorch torchvision -c pytorch
     
 wget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz
 tar -xvf vgg_16_2016_08_28.tar.gz
 </denchmark-code>
 
 Ubuntu 18.04 + Titan X Maxwell
 TF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)
 <denchmark-h:h2>Commands</denchmark-h>
 
 <denchmark-h:h4>Tensorflow</denchmark-h>
 
 import os
 ## Does not change anything
 #os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'
 import tensorflow as tf
 from tensorflow.contrib import slim
 from tensorflow.contrib.slim import nets
 import numpy as np
 
 tf.reset_default_graph()
 if False:  # 'NHWC' format
     # Use RNG to avoid the feed_dict argument
     input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))
     net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
     conv_layer = net[1]['vgg_16/pool5']
     preds = net[0]
 else:  # 'NCHW' format
     input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))
     with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):
         net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)
         conv_layer = net[1]['vgg_16/pool5']
         preds = net[0]
 saver = tf.train.Saver()
 
 config = tf.ConfigProto(log_device_placement=True)
 sess = tf.InteractiveSession(config=config)
 saver.restore(sess, './vgg_16.ckpt')
 
 # With jupyter notebook magic
 %timeit sess.run(conv_layer)
 %timeit sess.run(preds)
 <denchmark-h:h4>pyTorch</denchmark-h>
 
 import numpy as np
 import torch
 import torchvision.models as models
 torch.backends.cudnn.benchmark = True
 
 net = models.vgg16()
 net.cuda()
 
 _in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()
 
 # With jupyter notebook magic
 %timeit net.features(_in).data.cpu().numpy()
 %timeit net(_in).data.cpu().numpy()
 <denchmark-h:h2>Results</denchmark-h>
 
 
 
 
 Framework
 TF-NHWC
 TF-NCHW
 pyTorch
 
 
 
 
 pool5 output
 72.5
 60.1
 59.1
 
 
 fc8 output
 73.6
 131.0
 60.8
 
 
 
 <denchmark-h:h3>Conclusion</denchmark-h>
 
 Performance is the same as long as the same data format is used.
 However, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testConv2DKernelSizeMatchesInputSizeBackpropInput,self,560,572,1.0,"700,701,702,703,704,705,706,707,708,709,710,711",,testConv2DKernelSizeMatchesInputSizeBackpropFilter,self,700,711,1.0,"1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019",,testInputGradientKernelSizeMatchesInputSize,self,1004,1019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"406,407,408,409,410,411,412",,testConv2DKernelSizeMatchesInputSize,self,406,412,1.0,"1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036",,testFilterGradientKernelSizeMatchesInputSize,self,1021,1036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7077,Mazecreator,2017-01-25T22:09:43Z,2017-01-26T19:25:19Z,TensorBoard ImportError: No module named werkzeug,"
 using the current HEAD of tensorflow, I have bumped into an issue when I execute tensorboard.
 Version is reported as: 0.12.head
 git rev-parse HEAD: <denchmark-link:https://github.com/tensorflow/tensorflow/commit/a12c7dc3d83049e10c1dca8903d73cc71d3cb7b2>a12c7dc</denchmark-link>
 
 Linux: Linux tensor 4.4.0-53-generic <denchmark-link:https://github.com/tensorflow/tensorflow/issues/74>#74</denchmark-link>
 -Ubuntu SMP Fri Dec 2 15:59:10 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux
 
 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
 2017-01-25 17:02:36: I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
 Traceback (most recent call last):
 File ""/usr/local/bin/tensorboard"", line 9, in 
 load_entry_point('tensorflow', 'console_scripts', 'tensorboard')()
 File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/init.py"", line 542, in load_entry_point
 return get_distribution(dist).load_entry_point(group, name)
 File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/init.py"", line 2575, in load_entry_point
 return ep.load()
 File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/init.py"", line 2235, in load
 return self.resolve()
 File ""/home/greg/.local/lib/python2.7/site-packages/pkg_resources/init.py"", line 2241, in resolve
 module = import(self.module_name, fromlist=['name'], level=0)
 File ""/home/greg/tensorflow/_python_build/tensorflow/tensorboard/tensorboard.py"", line 26, in 
 from werkzeug import serving
 ImportError: No module named werkzeug
 
 I see the werkzeug.BUILD file on my system so not sure why it cannot be found.
 	",1.0,Mazecreator,2017-01-25T23:17:45Z,"
 		Just to be clear, you ran configure, built the whl file, installed, and running from there?
 /cc <denchmark-link:https://github.com/dandelionmane>@dandelionmane</denchmark-link>
 
 		",2.0,Mazecreator,2017-01-25T23:29:19Z,"
 		<denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L38>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/pip_package/setup.py#L38</denchmark-link>
  also lists werkzeug, so it's not clear why pip installing the package would not install werkzeug too.
 		",3.0,Mazecreator,2017-01-25T23:36:35Z,"
 		Sorry,
 I ran configure and use TensorFlow from source, but don't build the whl, I am setup for ""Setting up TensorFlow for Development"".  So I use the following:
 <denchmark-code># To build with GPU support:
 bazel build -c opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
 
 mkdir _python_build
 cd _python_build 
 ln -s ../bazel-bin/tensorflow/tools/pip_package/build_pip_package.runfiles/org_tensorflow/* .
 ln -s ../tensorflow/tools/pip_package/* .
 python setup.py develop
 </denchmark-code>
 
 		",bc72653cf7968115fe9f714d7d2bc63004524479,Yifei Feng,2017-01-26 20:36:59-08:00,MODIFY,0,tensorflow\g3doc\get_started\os_setup.md,tensorflow\g3doc\get_started\os_setup.md,0.0,"926,927",,,,,,4.0,Mazecreator,2017-01-25T23:38:41Z,"
 		I am not super familiar with that style of development: I don't know why calling the setup.py with 'develop' mode doesn't install the required packages.
 You should be able to get around this by pip install werkzeug manually.
 		",5.0,Mazecreator,2017-01-26T00:09:44Z,"
 		I can try the PIP but a little concerned it might cause problems later as the PIP should be bundled with the development install.
 Here is the link to the ""setting up with TensorFlow for Development"":
 <denchmark-link:https://www.tensorflow.org/get_started/os_setup#setting_up_tensorflow_for_development>https://www.tensorflow.org/get_started/os_setup#setting_up_tensorflow_for_development</denchmark-link>
 
 EDIT: That did fix the error but unsure why we need to manually install via ""pip""
 		",6.0,Mazecreator,2017-01-26T00:34:43Z,"
 		If you are concerned about polluting your environment, you should run virtualenv. We need werkzeug to run.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,Mazecreator,2017-01-26T18:49:53Z,"
 		I understand, but wanted to report the break in the current HEAD development build.  This is the first include I have needed to PIP install that appears it should have been installed with bazel.  My only concern is I will not be able to detect when it is corrected in the future.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,Mazecreator,2017-01-26T19:02:56Z,"
 		Right, we are thinking that setup.py should have installed it, but for some reason didn't. Is it working for you now?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,Mazecreator,2017-01-26T19:15:40Z,"
 		Yes, I will remove the PIP package once you guys determine how to move forward as you could add the package as a prerequisite.
 		",10.0,Mazecreator,2017-01-26T19:25:19Z,"
 		Yes, it looks like it's a documentation issue. According to <denchmark-link:http://stackoverflow.com/questions/28509965/setuptools-development-requirements>stackoverflow</denchmark-link>
 , the  mode doesn't install the requirements. You have to install them separately. Closing a looping internally to update the doc.
 		",11.0,Mazecreator,2017-05-17T23:40:32Z,"
 		I also encountered the same problem, but I found the werkzeug package. But still did not solve the problem
 		",12.0,Mazecreator,2017-05-21T12:51:22Z,"
 		I have the same problem with TwistedW ,and still did not solve,too.
 		",13.0,Mazecreator,2017-11-24T19:51:50Z,"
 		I have the same problem and pip install does not solve it.
 		",14.0,Mazecreator,2017-11-27T21:36:38Z,"
 		This is strictly a TB dependency.
 You should reach out to tensorboard to look into fixing this dependency in tensorboard pip package.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7088,bazinac,2017-01-26T10:34:02Z,2017-01-28T08:48:20Z,Running optimized graph with two output nodes on android gives “Session was not created with a graph before Run()” error.,"
 Hi, please would you please be so kind and help me with one issue that prevents me from moving forward. I have graph with two output layers (final_result_orig – which is basically coming form retraining example,final_result_added – my custom layer) and I am unable to strip/optimize_on_inference it in order to run on android device (on pc it runs fine)
 When I run:
 bazel-bin/tensorflow/python/tools/optimize_for_inference 
 –input=/tmp/output.pb 
 –output=/tmp/optimized.pb 
 –input_names=Mul 
 –output_names=”final_result_orig,final_result_added”
 Then in my android application, I get “Session was not created with a graph before Run()” error, and both final_result_orig and final_result_added are not found.
 When I run:
 bazel-bin/tensorflow/python/tools/optimize_for_inference 
 –input=/tmp/output.pb 
 –output=/tmp/optimized.pb 
 –input_names=Mul 
 –output_names=”final_result_orig”
 It works fine, final_result_orig is available and works correctly, however final_result_added is obviously not found and not available for my app to use.
 And when I run:
 bazel-bin/tensorflow/python/tools/optimize_for_inference 
 –input=/tmp/output.pb 
 –output=/tmp/optimized.pb 
 –input_names=Mul 
 –output_names=”final_result_added”
 It does not work as well with “Session was not created with a graph before Run()” error, and both final_result_orig and final_result_added are not found.
 I do not understand what I am doing wrong – what could be wrong with “final_result_added”, as it works fine on PC and not android? Thank you very much.
 <denchmark-h:h2>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</denchmark-h>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/issues/5553>#5553</denchmark-link>
 
 <denchmark-h:h2>Environment info</denchmark-h>
 
 Ubuntu 16.04 + Android 6.0
 	",1.0,bazinac,2017-01-26T16:39:20Z,"
 		<denchmark-link:https://github.com/bazinac>@bazinac</denchmark-link>
  Are you using a recent version of TensorFlow? The loading code has been improved recently to give more descriptive errors when something goes wrong. I'd suggest syncing first and then seeing what it says.
 		",2.0,bazinac,2017-01-26T23:03:52Z,"
 		Unfortunatelly, after installing latest Tensorflow, I am strangely unable to even build optimize_for_inference script with
 `Traceback (most recent call last):
 File ""/home/poborak/SW/tensorflow-master/bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/python/tools/optimize_for_inference.py"", line 65, in 
 from tensorflow.python.framework import dtypes
 File ""/home/poborak/SW/tensorflow-master/bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/python/init.py"", line 72, in 
 raise ImportError(msg)
 ImportError: Traceback (most recent call last):
 File ""/home/poborak/SW/tensorflow-master/bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/python/init.py"", line 61, in 
 from tensorflow.python import pywrap_tensorflow
 File ""/home/poborak/SW/tensorflow-master/bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 28, in 
 _pywrap_tensorflow = swig_import_helper()
 File ""/home/poborak/SW/tensorflow-master/bazel-bin/tensorflow/python/tools/optimize_for_inference.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 20, in swig_import_helper
 import _pywrap_tensorflow
 ImportError: No module named _pywrap_tensorflow
 Failed to load the native TensorFlow runtime.
 `
 Note that otherwise tensorflow works properly, (i can even run ""import tensorflow as tf"" in python shell, everywhere else than in /tensorflow-master dir). I fought this whole evening and now I gave up and will hope that you somehow will be able to help me.  Otherwise, thanks for very good job with this project. It is fantastic that you make tool like tensorflow available to us.
 		",3.0,bazinac,2017-01-26T23:15:27Z,"
 		Perhaps <denchmark-link:https://github.com/tensorflow/tensorflow/issues/97>#97</denchmark-link>
  or <denchmark-link:https://github.com/tensorflow/tensorflow/issues/1013>#1013</denchmark-link>
  are relevant here? Have you run ./configure? What Python version are you using?
 My assumption is that you have a different version of tf installed that Python is finding when you import tf elsewhere, so that is probably misleading.
 		",c3df5d40ef8240ede980ccb740d6af87837d8eef,Andrew Harp,2017-01-27 15:12:31-08:00,MODIFY,0,tensorflow\core\kernels\BUILD,tensorflow\core\kernels\BUILD,0.0,3607,,,,,,4.0,bazinac,2017-01-26T23:30:29Z,"
 		I am using Python 3.5. I have tried to build tf from source, but it has failed to build Python wheel. I therefore opted to install latest (12.1) release using pip.
 I then ran ./configure in downloaded copy od latest tensorflow-master and built optimize_for_inference (I am sorry to provide misleading info on my previous comment). Then this abovementioned error arises when I try to run built optimize_for_inference...
 		",5.0,bazinac,2017-01-26T23:46:25Z,"
 		It sounds like <denchmark-link:https://github.com/andrewharp>@andrewharp</denchmark-link>
  is right. There are two versions in your system. I'm guessing that the best at this point is to go back and  tensorflow, then work through the issues building from source.
 		",6.0,bazinac,2017-01-27T18:08:29Z,"
 		I followed your hint and somehow managed to build from source. When I run it know, tensorflow is much more verbose and I can see that -
 <denchmark-code>E/native: tensorflow_inference_jni.cc:146 Could not create TensorFlow graph: Invalid argument: No OpKernel was registered to support Op 'Pow' with these attrs.
 
 [[Node: final_added_training_ops/final_result_added/pow = Pow[T=DT_FLOAT](final_added_training_ops/final_result_added/truediv, final_added_training_ops/final_result_added/pow/y)]]
 </denchmark-code>
 
 I google out somewhere that this can be surpassed by adding necessary OpKernel to Android operator sets in BUILD file. But I don't understand what OpKernel I should add to Android operator set in case of Op 'Pow' and how do to so. Could you please tell me.
 In addition to tf.sqrt I use also tf.reduce_mean, tf.truncated_normal...
 Sorry for being so annoying lately :D
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,bazinac,2017-01-27T18:39:27Z,"
 		On behalf of our users, thanks for helping us clarify things!
 You should be able to add ops from there:
 <denchmark-link:https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L3378>https://www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/BUILD#L3378</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,bazinac,2017-01-27T20:01:11Z,"
 		Could you try that and let us know?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,bazinac,2017-01-27T20:16:37Z,"
 		It looks like cwise_op_pow.cc needs to be added to android_extended_ops_group1.
 AFAIK there's no reason not to make this change -- we've just been adding ops as needed, so long as they actually compile for Android without adding undue size.
 I think it would also be informative to list registered ops in the case one isn't found; is there an easy way to do that?
 		",10.0,bazinac,2017-01-27T20:24:29Z,"
 		It's actually already included in the Makefile build, so not including it for Bazel seems to be an oversight. Sending a CL to fix.
 		",11.0,bazinac,2017-01-28T08:48:20Z,"
 		Yay after adding ""cwise_op_pow.cc"", it works. Thank you guys!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7186,maddin200,2017-02-01T10:55:36Z,2017-02-25T17:49:31Z,missing fclose,"
 \contrib\pi_examples\label_image\label_image.cc line 105
 missing fclose(infile);
 	",1.0,maddin200,2017-02-01T15:15:10Z,"
 		would you like to send a PR to fix this?
 		",2.0,maddin200,2017-02-01T15:35:42Z,"
 		BTW, PR for such small change is quite easy and you can do it mostly from browser:
 First, click ""Fork"" on github tensorflow page
 Then in Terminal:
 <denchmark-code>git remote add mine https://github.com/<username>/tensorflow
 git remote add tfmain https://github.com/tensorflow/tensorflow.git
 git fetch tfmain
 git checkout tfmain/master -b bugfix
 git push --set_upstream mine
 
 </denchmark-code>
 
 Then in bugfix branch on /tensorflow on Github, find file, click ""Edit"" button, edit file, then click commit, then click ""Pull Requests"" and select tensorflow/master on left and <username>/tensorflow:bugfix on right
 		",3.0,maddin200,2017-02-25T16:25:34Z,"
 		<denchmark-link:https://github.com/yaroslavvb>@yaroslavvb</denchmark-link>
  Looks like this issue has been resolved.
 		",82a0f347baf7284345cbf9e830310604da9b1b73,Rahul Kavi,2017-02-16 09:50:52-08:00,MODIFY,1,tensorflow\contrib\pi_examples\label_image\label_image.cc,tensorflow\contrib\pi_examples\label_image\label_image.cc,1.0,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,LoadJpegFile,"file_name,data,width,height,channels",86,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7300,PierreAndreNoel,2017-02-06T19:49:07Z,2017-04-13T22:00:27Z,"API documentation ""Core graph data structures""","
 A few things are wrong in the documentation around <denchmark-link:url>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/framework.md#tfgraphadd_to_collectionname-value-graphadd_to_collection</denchmark-link>
 .
 
 Near the end of tf.Graph.name_scope, the line about ValueError is incomplete.
 The paragraph at the very end of tf.Graph.name_scope should be in tf.Graph.add_to_collection.
 Both tf.Graph.add_to_collection and tf.Graph.add_to_collections (plural) exist.
 
 	",1.0,PierreAndreNoel,2017-02-07T15:55:46Z,"
 		The link you posted is broken.
 		",2.0,PierreAndreNoel,2017-02-07T16:03:36Z,"
 		Sorry. Please use <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/framework.md>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/framework.md</denchmark-link>
  then scroll down to the end of tf.Graph.name_scope .
 		",3.0,PierreAndreNoel,2017-02-07T16:11:40Z,"
 		<denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  Looks like you changed the documention in cl/126447618 to the rules are the
 		",dfe3bb7cdc888767c2fff32efea405c8d6aa5c88,Derek Murray,2017-02-07 15:46:03-08:00,MODIFY,1,tensorflow\python\framework\ops.py,tensorflow\python\framework\ops.py,1.0,"2902,2903",2902,,,,,4.0,PierreAndreNoel,2017-02-07T23:12:45Z,"
 		I'm submitting a fix for the broken docstring in tf.Graph.name_scope().
 The bad formatting between  and the paragraph describing collections is (I believe) a function of the doc generator, which <denchmark-link:https://github.com/josh11b>@josh11b</denchmark-link>
  touched last and will be best placed to fix.
 I don't know what the problem is about having tf.Graph.add_to_collection() and tf.Graph.add_to_collections(), other than it seems a bit redundant. Regardless, we're stuck with supporting both of those methods because of the 1.0 API freeze.
 		",5.0,PierreAndreNoel,2017-04-13T22:00:27Z,"
 		I'm closing this, as the formatting and paragraph whatnot is fixed on this new page:  <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope>https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope</denchmark-link>
 
 Please re-open or re-file if I'm missing something.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,name_scope,"self,name",2821,2928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7404,malmaud,2017-02-10T03:14:10Z,2018-03-29T18:11:56Z,No attribute 'outer_context' when calculating gradient from imported graph,"
 It seems when you import a graph with a ""while"" loop, you can't calculate gradients as you could on the original graph. e.g.
 import tensorflow as tf
 i=tf.constant(0, name=""input"")
 out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=""output"")
 graph_def = tf.get_default_graph().as_graph_def()
 
 g = tf.Graph()
 with g.as_default():
     tf.import_graph_def(graph_def)
 s = tf.Session(graph=g)
 i_imported = g.get_tensor_by_name(""import/input:0"")
 out_imported = g.get_tensor_by_name(""import/output/Exit:0"")
 tf.gradients(out_imported, i_imported)
 <denchmark-code>---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-12-e7e2b78684d3> in <module>()
 ----> 1 tf.gradients(out_imported, i_imported)
 
 /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method)
     439     pending_count, loop_state = _PendingCount(ops.get_default_graph(), to_ops,
     440                                               from_ops,
 --> 441                                               colocate_gradients_with_ops)
     442 
     443     # Iterate over the collected ops.
 
 
 /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.pyc in _PendingCount(graph, to_ops, from_ops, colocate_gradients_with_ops)
     184   # 'loop_state' is None if there are no while loops.
     185   loop_state = control_flow_ops.MaybeCreateControlFlowState(
 --> 186       between_op_list, between_ops, colocate_gradients_with_ops)
     187 
     188   # Initialize pending count for between ops.
 
 /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in MaybeCreateControlFlowState(between_op_list, between_ops, colocate_gradients_with_ops)
    1293           loop_state.AddWhileContext(op, between_op_list, between_ops)
    1294       else:
 -> 1295         loop_state.AddWhileContext(op, between_op_list, between_ops)
    1296   return loop_state
    1297 
 
 /Users/malmaud/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_ops.pyc in AddWhileContext(self, op, between_op_list, between_ops)
    1102     if grad_state is None:
    1103       # This is a new while loop so create a grad state for it.
 -> 1104       outer_forward_ctxt = forward_ctxt.outer_context
    1105       if outer_forward_ctxt:
    1106         outer_forward_ctxt = outer_forward_ctxt.GetWhileContext()
 
 AttributeError: 'NoneType' object has no attribute 'outer_context'
 </denchmark-code>
 
 	",1.0,malmaud,2017-02-10T16:43:52Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  Any idea what would be happening here?  Do while loop gradients depend on unserialized information?
 		",2.0,malmaud,2017-02-11T07:22:35Z,"
 		Yes, though I thought Yuan and Sherry had added the appropriate serialization to the metagraph.  Is this on master branch of tf?
 		",3.0,malmaud,2017-02-11T12:46:43Z,"
 		Ya, this is master.
 		",1d7c2fa60f717dea7239970d96f7d4bf96842039,ImSheridan,2018-03-29 11:11:55-07:00,MODIFY,1,tensorflow\python\ops\control_flow_ops.py,tensorflow\python\ops\control_flow_ops.py,1.0,"836,837,838",,,,,,4.0,malmaud,2017-02-11T17:14:36Z,"
 		OK, I see if I save and restore the metagraph (via tf.train.export_meta_graph and tf.train.import_meta_graph, then I can take the gradient in the restored session. So it's just a problem if you try serializing and importing the GraphDef itself.
 		",5.0,malmaud,2017-02-13T15:50:44Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  Is there an easy way to produce a nicer error message here, so that users don't have to guess the graph vs. metagraph issue?
 		",6.0,malmaud,2017-02-13T17:00:39Z,"
 		Yes; I believe that the forward_ctxt object here:
 
 forward_ctxt.outer_context
 
 should have a <denchmark-link:https://github.com/Property>@Property</denchmark-link>
  outer_context; and if no such internal object is
 set, it can raise a ValueError/InternalError that maybe you're trying to
 call gradients on a while loop without properly serializing your graph via
 MetaGraphDef.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Mon, Feb 13, 2017 at 7:51 AM, Geoffrey Irving ***@***.***> wrote:
  @ebrevdo <https://github.com/ebrevdo> Is there an easy way to produce a
  nicer error message here, so that users don't have to guess the graph vs.
  metagraph issue?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#7404 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/ABtimxlSpTQAQlztIVyjpp6fmXDWkZ60ks5rcHvvgaJpZM4L89DK>
  .
 
 
 
 		",,,,,,,,,,,,,,,,,,,,,,__init__,"self,forward_ctxt,outer_grad_state",800,883,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,malmaud,2017-11-10T16:58:13Z,"
 		Hi <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  , is there any update on this issue? Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,malmaud,2017-11-14T06:10:20Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 , any update on this issue or any workaround? Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,malmaud,2017-12-20T19:27:54Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",10.0,malmaud,2018-01-04T19:18:55Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",11.0,malmaud,2018-01-24T13:18:44Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",12.0,malmaud,2018-02-08T00:52:28Z,"
 		<denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
  should we mark this contributions welcome?
 		",13.0,malmaud,2018-02-08T02:26:46Z,"
 		Yes
 		",14.0,malmaud,2018-03-28T15:29:35Z,"
 		Created a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/18052>#18052</denchmark-link>
  to fix this.
 		",15.0,malmaud,2018-12-01T13:58:02Z,"
 		Hello, I came up with the same issue when I use tf.import_graph_def:
 AttributeError: 'NoneType' object has no attribute 'outer_context'
 Following the instructions above I switched to import_meta_graph like this:
 `
 def _make_model_and_ops(self, patch_val):
 start = time.time()
 <denchmark-code>    self.image_input_ = tf.placeholder(tf.float32, shape=(None, None, None, 3), name='image_input')
     saver = tf.train.import_meta_graph(PATH_TO_CKPT + '.meta',
                                        import_scope=""detection"",
                                        input_map={
                                            'ToFloat_3': self.image_input_
                                        })
     saver.restore(self.sess, PATH_TO_CKPT)
     self.graph = self.sess.graph
     
     with self.sess.graph.as_default():
         tf.set_random_seed(1234)
         
         # Tensors are post-fixed with an underscore!
         #self.image_input_shape_ = tf.placeholder(tf.int32, shape=(1,3), name=""image_input_shape"")
         
         self.eps_ = tf.placeholder(tf.float32, shape=(1), name='eps')
         
         # The following commented part is the original code using import_graph_def
         '''
         detection_graph_def = tf.GraphDef()
         with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
             serialized_graph = fid.read()
             detection_graph_def.ParseFromString(serialized_graph)
             tf.import_graph_def(detection_graph_def, name='detection',
                                 input_map={
                                            'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3': self.image_input_,
                                            'Preprocessor/map/TensorArrayStack_1/TensorArrayGatherV3': self.image_input_shape_
                                           })
         '''
         
         # Second-stage Class Loss
         self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')
         second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')
         self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')
         
         self.second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),
                                                                                    logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) 
        
         # Second-stage bounding boxes
         self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')
         
         grads = tf.gradients(self.second_stage_cls_losses_, [self.image_input_])
         print(grads)
         self.unclipped_adv_images_ = self.image_input_ + self.eps_ * tf.sign(grads)
         self.adv_images_ = tf.clip_by_value(self.unclipped_adv_images_, clip_value_min=0, clip_value_max=255)
 
 
     elapsed = time.time() - start
     print(""Finished loading the model, took {:.0f}s"".format(elapsed))           
 </denchmark-code>
 
 `
 But the problem still exist.
 Does this mean that the information required for tf.gradients does not appear in the check point? If so, what else can I do?
 Thanks, and sorry for my poor English....
 		",16.0,malmaud,2020-03-14T05:59:09Z,"
 		Having the same problem as <denchmark-link:https://github.com/ProjectDimlight>@ProjectDimlight</denchmark-link>
 , tried the suggestions above with no luck. Any updates on this?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7406,poweic,2017-02-10T04:31:14Z,2017-03-22T16:54:14Z,Should check whether n_class is zero before calling sample_n() in mixture.py,"
 <denchmark-h:h3>Problem Description</denchmark-h>
 
 Mixture model first use categorical to sample how much samples it need for each mixture components (this is variable  at <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L308>line 308</denchmark-link>
 , but it actually means ), and then it pass  to .
 The problem is  could be 0 and you can't pass  to , which is used in Beta distribution. (see <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/mixture.py#L310>line 310</denchmark-link>
  in mixture.py)
 <denchmark-h:h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</denchmark-h>
 
 It's easy to reproduce: just create a mixture of Beta + Uniform with 50/50 probability. Half of the time it'll sample from uniform, and half of the time it'll sample from Beta.
 #!/usr/bin/python
 import tensorflow as tf
 ds = tf.contrib.distributions
 
 # Create mixture distribution of Beta + Uniform
 components = [ds.Beta(2., 2.), ds.Uniform(a=0., b=1.)]
 cat = ds.Categorical(p=[0.5, 0.5])
 mix = ds.Mixture(cat=cat, components=components)
 
 # get ONLY 1 sample
 x = mix.sample_n(1)
 
 with tf.Session() as sess:
     sess.run(tf.global_variables_initializer())    
     # repeats until crash
     for i in range(1000):
         print sess.run(x)
 <denchmark-h:h3>What other attempted solutions have you tried?</denchmark-h>
 
 Two possible solutions:
 
 
 Add a conditional branch in mixture.py like this (tested with the above script):
 # INSTEAD OF DOING
 # samples_class_c = self.components[c].sample_n(n_class, seed=seed)
 # DO THIS
 samples_class_c = control_flow_ops.cond(
     math_ops.equal(n_class, 0),
     lambda: array_ops.zeros(0, self.components[c].dtype),
     lambda: self.components[c].sample_n(n_class, seed=seed)
 )
 Just create a zero tensor with shape 0 when n_class is 0 and let the reshape operator at line 330 to worry about the shape.
 
 
 Support shape=0 in random_gamma(shape, alpha, ...). Personally I think it's a bad idea. It already caused InvalidArgumentError exception, which means the one who implemented this might already considered this problem before.
 
 
 <denchmark-h:h3>Logs or other output that would be helpful</denchmark-h>
 
 (If logs are large, please upload as attachment or provide link).
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally
 I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally
 I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
 name: TITAN X (Pascal)
 major: 6 minor: 1 memoryClockRate (GHz) 1.531
 pciBusID 0000:02:00.0
 Total memory: 11.90GiB
 Free memory: 337.50MiB
 W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x4190110
 I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
 name: GeForce GTX 980 Ti
 major: 5 minor: 2 memoryClockRate (GHz) 1.076
 pciBusID 0000:01:00.0
 Total memory: 5.93GiB
 Free memory: 5.08GiB
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y 
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
 I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 980 Ti, pci bus id: 0000:01:00.0)
 [ 0.32805401]
 [ 0.2802822]
 W tensorflow/core/framework/op_kernel.cc:975] Invalid argument: Input shape should have non-zero element count, got: 0
 	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]
 Traceback (most recent call last):
   File ""bug.py"", line 21, in <module>
     print sess.run(x)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
     run_metadata_ptr)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
     feed_dict_string, options, run_metadata)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
     target_list, options, run_metadata)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
     raise type(e)(node_def, op, message)
 tensorflow.python.framework.errors_impl.InvalidArgumentError: Input shape should have non-zero element count, got: 0
 	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]
 
 Caused by op u'Beta/sample_n/random_gamma_1/RandomGamma', defined at:
   File ""bug.py"", line 15, in <module>
     x = mix.sample_n(1)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py"", line 574, in sample_n
     x = self._sample_n(n, seed, **condition_kwargs)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/mixture.py"", line 313, in _sample_n
     samples_class_c = self.components[c].sample_n(n_class, seed=seed)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/distribution.py"", line 574, in sample_n
     x = self._sample_n(n, seed, **condition_kwargs)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/distributions/python/ops/beta.py"", line 205, in _sample_n
     seed=distribution_util.gen_new_seed(seed, ""beta""))
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/random_ops.py"", line 437, in random_gamma
     seed2=seed2) / beta
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_random_ops.py"", line 122, in _random_gamma
     seed=seed, seed2=seed2, name=name)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 759, in apply_op
     op_def=op_def)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2240, in create_op
     original_op=self._default_original_op, op_def=op_def)
   File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1128, in __init__
     self._traceback = _extract_stack()
 
 InvalidArgumentError (see above for traceback): Input shape should have non-zero element count, got: 0
 	 [[Node: Beta/sample_n/random_gamma_1/RandomGamma = RandomGamma[S=DT_INT32, T=DT_FLOAT, seed=0, seed2=0, _device=""/job:localhost/replica:0/task:0/cpu:0""](Beta/sample_n/random_gamma_1/shape/_25, Beta/sample_n/random_gamma_1/add/_27)]]
 P.S. The variable name n_class confused me for a while.
 	",1.0,poweic,2017-02-10T16:47:55Z,"
 		The bug here is that  doesn't allow zero samples.  <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op.cc#L306>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/random_op.cc#L306</denchmark-link>
  should be fixed to exit early in that case rather than crashing.  <denchmark-link:https://github.com/brianwa84>@brianwa84</denchmark-link>
 : Can you confirm this interpretation?
 @botonchou Pending Brian's ack, would you be interested in submitting a pull request?
 		",2.0,poweic,2017-02-10T17:08:25Z,"
 		Returning a zero shape tensor is OK for the zero sample use-case. Feel free
 to submit the pull request.
 <denchmark-link:#>…</denchmark-link>
 
 
 On Fri, Feb 10, 2017 at 11:49 AM, Geoffrey Irving ***@***.***> wrote:
  The bug here is that RandomGamma doesn't allow zero samples.
  https://github.com/tensorflow/tensorflow/blob/master/
  tensorflow/core/kernels/random_op.cc#L306 should be fixed to exit early
  in that case rather than crashing. @brianwa84
  <https://github.com/brianwa84>: Can you confirm this interpretation?
 
  @botonchou <https://github.com/botonchou> Pending Brian's ack, would you
  be interested in submitting a pull request?
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#7406 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/AVJZI8fwfqaPDIeg0TDDN-fBUXmQdIpAks5rbJURgaJpZM4L8_pY>
  .
 
 
 
 		",3.0,poweic,2017-02-10T17:11:08Z,"
 		<denchmark-link:https://github.com/brianwa84>@brianwa84</denchmark-link>
  Not to pry, but did you really intend to post your phone number in a Github comment?
 		",3116fa80450735e907907bc57a6834e9e212570a,Mycosynth,2017-03-22 09:54:14-07:00,MODIFY,1,tensorflow\core\kernels\random_op.cc,tensorflow\core\kernels\random_op.cc,1.0,306,"306,307,308,309",,,,,4.0,poweic,2017-02-10T18:14:25Z,"
 		heh thx. email signature fail
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tensorflow::RandomGammaOp::Compute,ctx,285,455,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7906,karltk,2017-02-26T22:21:45Z,2017-03-01T00:52:56Z,Erroneous number of channels in the Guide to TF Layers tutorial.,"
 The tutorial A Guide to TF Layers: Building a Convolutional Neural Network seems to have the wrong number of channels in the tensor dimensions given at the end of the paragraph Convolutional Layer #1:
 
 Our output tensor produced by conv2d() has a shape of [batch_size, 28, 28, 1]: the same width and height dimensions as the input, but now with 32 channels holding the output from each of the filters.
 
 To be consistent with the rest of the tutorial, it should probably say ""shape of [batch_size, 28, 28, 32]"", since the rightmost dimension denotes the number of channels.
 	",1.0,karltk,2017-03-01T00:52:56Z,"
 		<denchmark-link:https://github.com/karltk>@karltk</denchmark-link>
  Good point, thanks for filing this issue!  I'll get it fixed up.
 		",,,,,,,,,4e63540076921d2c08d03aa9efb76fd483920593,A. Unique TensorFlower,2017-02-28 18:49:11-08:00,MODIFY,0,tensorflow\docs_src\tutorials\layers.md,tensorflow\docs_src\tutorials\layers.md,0.0,284,284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8011,pannous,2017-03-02T13:59:38Z,2018-02-08T00:57:22Z,TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;,"
 Feature request for a better error description OR for better summary handling:
 The following code works fine if some summaries where defined before:
 <denchmark-code>ops=[] 
 ops += [tf.summary.merge_all()]
 session.run(ops)
 </denchmark-code>
 
 However if there were no summaries we get:
 TypeError: Fetch argument None has invalid type <class 'NoneType'>
 Which is really saying:  ""One of the session.run ops where empty, which is forbidden.""
 Alternatively let merge_all return a NoOp if there are no summaries.
 	",1.0,pannous,2017-03-03T00:15:24Z,"
 		<denchmark-link:https://github.com/pannous>@pannous</denchmark-link>
  Thanks for the concise and clear issue!
 <denchmark-link:https://github.com/dandelionmane>@dandelionmane</denchmark-link>
  the suggestion sounds reasonable, can you comment?
 		",2.0,pannous,2017-06-16T18:57:15Z,"
 		It should just work and return a tf.no_op.
 		",3.0,pannous,2017-06-16T20:30:18Z,"
 		On it.
 		",cffd79f4b102c2082cbcc258abf7ed06df8c141c,Sourabh Bajaj,2017-12-01 12:22:16-08:00,MODIFY,5,tensorflow\core\kernels\slice_op.cc,tensorflow\core\kernels\slice_op.cc,1.0,"216,217,218,219,220,221,222,224,225,226,227",,MODIFY,4.0,tensorflow\core\kernels\slice_op.h,tensorflow\core\kernels\slice_op.h,4.0,pannous,2017-09-14T04:58:21Z,"
 		bump - this one just cost me 5 minutes of my life :P
 		",5.0,pannous,2017-12-22T07:47:56Z,"
 		It has been 14 days with no activity and this issue has an assignee.Please update the label and/or status accordingly.
 		",6.0,pannous,2018-01-05T19:13:40Z,"
 		Nagging Assigneee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",1.0,,"79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99",tensorflow::internal::SliceUsingEigen,"d,out,in,slice_indices,slice_sizes",79,99,MODIFY,2.0,tensorflow\core\kernels\slice_op_gpu.cu.cc,tensorflow\core\kernels\slice_op_gpu.cu.cc,1.0,,"32,33,34,35,36,37,38,39,40,41,42,43,44,45,46",MODIFY,1.0,tensorflow\core\kernels\strided_slice_op_impl.h,tensorflow\core\kernels\strided_slice_op_impl.h,1.0,"87,88,90,91,93,94,95,96","87,89,91,92,93,94,95,96",tensorflow::SliceOp::HandleCase,"context,begin,size,result",216,228,1.0,"267,268,269,270,271,279,283,284","251,252,253,254,255,256,257,258,259,260,271,272,273,274",tensorflow::MklSliceOp::Compute,context,237,286,1.0,"193,194,195,196,198,205,209,210,211,212","193,194,195,196,197,208,209,210",tensorflow::SliceOp::Compute,context,163,212,1.0,"332,341,342,406,407,408,409,410,411,412,413,414,415","394,395,403,404,405,406,407,408,409",tensorflow::MklSliceOp::HandleCase,"context,begin,size,result",332,416,1.0,"30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,47","30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49",tensorflow::functor::Slice::operator ( ),"d,output,input,slice_indices,slice_sizes",30,49,1.0,"38,39,40,41,42,43,44,45,47","38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76",tensorflow::internal::SliceSimple,"d,out,in,slice_indices",38,76,1.0,,"109,110,111,112,113,115,116,117,118,119",tensorflow::functor::Slice::operator ( ),"d,out,in,slice_indices,slice_sizes",109,121,,,,,,,,,,,,,,,7.0,pannous,2018-01-24T13:20:52Z,"
 		Nagging Assignee: It has been 14 days with no activity and this issue has an assignee. Please update the label and/or status accordingly.
 		",tensorflow::internal::SliceKernel,"nthreads,src,buf,ndims,dst",32,46,tensorflow::HandleStridedSliceCase,"context,begin,end,strides,processing_shape,is_simple_slice,result",77,112,MODIFY,4.0,tensorflow\core\kernels\strided_slice_op_test.cc,tensorflow\core\kernels\strided_slice_op_test.cc,1.0,,"80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123",tensorflow::Dim8SliceHelper,"iters,size",80,123,1.0,,141,tensorflow::BM_SliceBFloat16,"iters,dim2",139,142,1.0,,127,tensorflow::BM_SliceFloat,"iters,dim2",125,128,1.0,,134,tensorflow::BM_SliceComplex64,"iters,dim2",132,135,,,,,,,,,,,,,,,MODIFY,1.0,tensorflow\python\kernel_tests\slice_op_test.py,tensorflow\python\kernel_tests\slice_op_test.py,1.0,230,"220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242",testRandomHighRank,self,220,242,,,,,,,,,,,,,,,,,,,8.0,pannous,2018-02-08T00:57:22Z,"
 		Automatically closing due to lack of recent activity. Please update the issue when new information becomes available, and we will reopen the issue. Thanks!
 		",1.0,"331,332,341,342","321,330,394,395",tensorflow::MklSliceOp::HandleCase4D,"context,begin,size,result",321,396,,,,,,,,1.0,,"49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80",tensorflow::internal::SliceSimpleGpu,"d,out,in,slice_indices",49,80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8364,BlackHC,2017-03-13T17:27:43Z,2017-03-24T14:03:16Z,Documentation formatting broken,"
 See <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/copy_graph/copy_op_to_graph>https://www.tensorflow.org/api_docs/python/tf/contrib/copy_graph/copy_op_to_graph</denchmark-link>
  (source code formatting leaks into general text)
 or <denchmark-link:https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor/copy>https://www.tensorflow.org/api_docs/python/tf/contrib/graph_editor/copy</denchmark-link>
  (Returns: and Raises: get folded into parameters).
 Either the doc generator needs to understand python doc comments better or the doc comments need to be updated to work better with markdown (extra newlines etc). What do you think?
 Thanks,
 Andreas
 	",1.0,BlackHC,2017-03-14T13:29:33Z,"
 		<denchmark-link:https://github.com/josh11b>@josh11b</denchmark-link>
  <denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
  for docs
 		",,,,,,,,,4b86783ac6c3af9c35d3af36d4db0e9bc11f21c0,Mark Daoust,2017-03-22 18:07:47-07:00,MODIFY,2,tensorflow\contrib\copy_graph\python\util\copy_elements.py,tensorflow\contrib\copy_graph\python\util\copy_elements.py,1.0,"246,247,248,251","246,247,248,251",MODIFY,1.0,tensorflow\tools\docs\parser.py,tensorflow\tools\docs\parser.py,,,,,,,,,,,,,1.0,406,"406,407",_parse_function_details,docstring,361,425,,,,,,,,,,,,,,,get_copied_op,"org_instance,graph,scope",237,261,1.0,"47,48,49,52,55","47,48,49,52,55",copy_variable_to_graph,"org_instance,to_graph,scope",41,98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8718,kanitw,2017-03-25T18:04:17Z,2017-06-13T16:46:21Z,Incorrect reference for tf.learn in Linear Model tutorials,"
 This <denchmark-link:https://www.tensorflow.org/tutorials/linear>page</denchmark-link>
  and <denchmark-link:https://www.tensorflow.org/tutorials/wide>this</denchmark-link>
  mentions  multiple times.
 However, looking at the links and source code, I believe that  should actually be either  or , not .
 (I guess tf.learn was the old name, but got renamed but the tutorial is still outdated?)
 	",1.0,kanitw,2017-03-28T19:40:20Z,"
 		Could you fix or delegate this <denchmark-link:https://github.com/wolffg>@wolffg</denchmark-link>
 ?
 		",2.0,kanitw,2017-06-12T19:17:20Z,"
 		Added a PR <denchmark-link:https://github.com/tensorflow/tensorflow/pull/10658>#10658</denchmark-link>
  for that.
 		",,,,,4f52ce514bc83c5adbfdcd8a342a0fa0f42e55d0,Yong Tang,2017-06-13 09:46:20-07:00,MODIFY,0,tensorflow\docs_src\tutorials\linear.md,tensorflow\docs_src\tutorials\linear.md,0.0,"3,9,10,13,20,55,58,64,89,91,151,190,200,225","3,9,10,13,20,55,58,64,89,91,151,190,200,225",MODIFY,0.0,tensorflow\docs_src\tutorials\wide.md,tensorflow\docs_src\tutorials\wide.md,,,,,,,,,,,,,0.0,"3,19,139,141,214,364,470","3,19,139,141,214,364,470",,,,,MODIFY,0.0,tensorflow\docs_src\tutorials\wide_and_deep.md,tensorflow\docs_src\tutorials\wide_and_deep.md,0.0,"12,26,45","12,26,45",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8809,unnir,2017-03-29T15:41:30Z,2017-05-01T16:22:26Z,an update for the tf.contrib.learn Quickstart example is needed,"
 Hi all,
 just tried to start the script from here: <denchmark-link:https://www.tensorflow.org/get_started/tflearn>https://www.tensorflow.org/get_started/tflearn</denchmark-link>
 
 found one issue for python3 users:
 import urllib
 raw = urllib.urlopen(IRIS_TRAINING_URL).read()
 this returns:
 ---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-32-2005abd64a8b> in <module>()
 ----> 1 raw = urllib.urlopen(IRIS_TRAINING_URL).read()
 
 AttributeError: module 'urllib' has no attribute 'urlopen'
 
 the solution:
 import urllib.request as ur
 raw = ur.urlopen(IRIS_TRAINING_URL).read()
 But, the main reason for this issue-ticket is warning messages like:
 <denchmark-code>WARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
 Instructions for updating:
 Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported. 
 </denchmark-code>
 
 or this
 <denchmark-code>WARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.
 Instructions for updating:
 Estimator is decoupled from Scikit Learn interface by moving into
 separate class SKCompat. Arguments x, y and batch_size are only
 available in the SKCompat class, Estimator will only accept input_fn.
 Example conversion:
   est = Estimator(...) -> est = SKCompat(Estimator(...))
 WARNING:tensorflow:From <ipython-input-28-221e14b2595e>:1: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.
 Instructions for updating:
 Estimator is decoupled from Scikit Learn interface by moving into
 separate class SKCompat. Arguments x, y and batch_size are only
 available in the SKCompat class, Estimator will only accept input_fn.
 Example conversion:
   est = Estimator(...) -> est = SKCompat(Estimator(...))
 </denchmark-code>
 
 or like this:
 <denchmark-code>WARNING:tensorflow:From /Users/vadimborisov/anaconda/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/head.py:1362: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.
 Instructions for updating:
 Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.
 INFO:tensorflow:Starting evaluation at 2017-03-29-15:28:31
 INFO:tensorflow:Evaluation [1/1]
 INFO:tensorflow:Finished evaluation at 2017-03-29-15:28:32
 INFO:tensorflow:Saving dict for global step 4000: accuracy = 0.966667, auc = 0.998333, global_step = 4000, loss = 0.0793921
 WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.
 </denchmark-code>
 
 So, I believe, the introduction tutorials should be free from warnings.
 I can try to fix the warnings, if someone is interesting.
 I'm using Python 3.5.2 |Anaconda 4.3.1 (x86_64)| (default, Jul  2 2016, 17:52:12) .
 	",1.0,unnir,2017-03-31T09:25:01Z,"
 		Also please add DynamicRnnEstimator example. Looks like it a have different interface. Not like LinearRegressor and that is very confusing.
 		",2.0,unnir,2017-03-31T18:11:36Z,"
 		<denchmark-link:https://github.com/martinwicke>@martinwicke</denchmark-link>
 , <denchmark-link:https://github.com/wolfffg>@wolfffg</denchmark-link>
 , are we planning an update here? Could you redirect to the new author?
 		",3.0,unnir,2017-03-31T21:07:46Z,"
 		<denchmark-link:https://github.com/wolffg>@wolffg</denchmark-link>
 ​
 		",0c08c585591152046ca1e6781d1f2fa573427dfc,Mark Daoust,2017-04-28 10:28:49-07:00,MODIFY,0,tensorflow\docs_src\deploy\distributed.md,tensorflow\docs_src\deploy\distributed.md,0.0,181,181,MODIFY,0.0,tensorflow\docs_src\programmers_guide\reading_data.md,tensorflow\docs_src\programmers_guide\reading_data.md,,,,,,,,,,,,,0.0,"136,173","136,173",,,,,MODIFY,0.0,tensorflow\docs_src\tutorials\deep_cnn.md,tensorflow\docs_src\tutorials\deep_cnn.md,0.0,"86,90,91,92,93,94,100","86,90,91,92,93,94,100",MODIFY,0.0,tensorflow\docs_src\tutorials\word2vec.md,tensorflow\docs_src\tutorials\word2vec.md,0.0,"26,344,360,388,394","26,344,360,388,394",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9047,taion,2017-04-07T16:08:11Z,2017-04-13T20:27:35Z,beta2_power is applied incorrectly in Adam optimizer,"
 In adam.py and in the ApplyAdam op, the denominator is effectively:
 <denchmark-code>(tf.sqrt(v_t) + epsilon_t) / tf.sqrt(1 - beta2_power)
 </denchmark-code>
 
 However, this appears incorrect – per the paper, the correct EMA adjustment should give:
 <denchmark-code>tf.sqrt(v_t / (1 - beta2_power)) + epsilon_t
 </denchmark-code>
 
 Otherwise, when epsilon_t is large relative to tf.sqrt(v_t), the effective epsilon used in the denominator is also scaled up by the correction factor, which doesn't match what's in the paper.
 Does this seem right, or am I missing something here?
 	",1.0,taion,2017-04-07T22:39:53Z,"
 		<denchmark-link:https://github.com/georgedahl>@georgedahl</denchmark-link>
  <denchmark-link:https://github.com/allenlavoie>@allenlavoie</denchmark-link>
  <denchmark-link:https://github.com/alextp>@alextp</denchmark-link>
  : Any comments on this (or know whom to redirect to?)
 		",2.0,taion,2017-04-07T23:24:47Z,"
 		Just so I'm clear, you do agree that the TensorFlow implementation matches the expression just before Section 2.1, with the ""epsilon hat"" in it? It's just that ""epsilon hat"" is a scaled version of the epsilon in Algorithm 1 in the paper, right?
 Is there a use-case for having the un-scaled epsilon as an option?
 		",3.0,taion,2017-04-07T23:25:20Z,"
 		I think this is correct. If we look in the paper <denchmark-link:https://arxiv.org/pdf/1412.6980.pdf>https://arxiv.org/pdf/1412.6980.pdf</denchmark-link>
  at the bottom of section 2 (Algorithm), it says ""the efficiency of algorithm 1 can, at the expense of clarity, be improved..."" and lists an update like what we are doing, no?
 <denchmark-link:https://cloud.githubusercontent.com/assets/994930/24822992/edac7cae-1bc7-11e7-9a15-0f4ca0250a5e.png></denchmark-link>
 
 		",1926b9d3b91375f5a4433303a27b49b4da53f64e,A. Unique TensorFlower,2017-04-12 15:04:53-07:00,MODIFY,0,tensorflow\python\training\adam.py,tensorflow\python\training\adam.py,0.0,"64,65,66,67,84,85,86","64,81",,,,,4.0,taion,2017-04-07T23:32:10Z,"
 		Ah, the ""epsilon hat"" vs the regular epsilon is what I was missing (though ""epsilon hat"" is never defined that I can see, I assume it is a scaled version). I agree this is probably a bug.
 		",5.0,taion,2017-04-07T23:49:17Z,"
 		Oops, yeah – I think it should be something like:
 epsilon_hat = epsilon_t * tf.sqrt(1 - beta2_power)
 Otherwise without this scaling the effective epsilon is scaled by 1. / tf.sqrt(1 - beta2_power).
 With the default settings it probably barely matters (so you get an epsilon of ~3e-7 instead of 1e-8... big deal), but if you follow the recommendations in the comments and e.g. set epsilon to 1 or 0.1, then you effectively will barely update the weights for the first few iterations until beta2_power gets closer to 0 – like if you set epsilon to 1, then the effective ""epsilon"" you'd use for the first iteration would be ~31.
 I can't really think of a reason why you'd want to incorrectly scale the epsilon in this case.
 		",6.0,taion,2017-04-08T00:40:56Z,"
 		On the other hand, the recommendation in that comment is definitely referring to epsilon_hat being 1 or 0.1, not epsilon (i.e. the person who wrote it was using TensorFlow's implementation of Adam).
 <denchmark-link:https://github.com/skywaLKer518>@skywaLKer518</denchmark-link>
 : Any thoughts on epsilon vs. epsilon_hat? It looks like you authored that bit of advice on large values of epsilon for Adam.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,taion,2017-04-11T17:25:39Z,"
 		It is very possible that the optimizer gets modified after I tested and added the comments (I'm not exactly sure now -- the code is definitely at least re-organized and I do not recall the use of epsilon_hat clearly) so that the comment might not apply any more. But at the time we test it on Inception (summer 2015), we observe much better performance with large epsilon (e.g. 1.0) than the default values like 1e-8 (which sometimes even causes objective divergence if I remember correctly).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,taion,2017-04-11T17:45:46Z,"
 		epsilon and epsilon_hat end up approximately the same after a few thousand iterations anyway – not using epsilon_hat just means that training will start out very slow at the beginning when using a large value of epsilon.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9.0,taion,2017-04-11T17:49:43Z,"
 		Interesting. I'm inclined to keep behavior as-is, since it probably doesn't matter for the on-label use of avoiding short-term numerical instability due to near-zero gradients. In fact we'd need to be careful that a correction didn't introduce numerical issues.
 The off-label advice for making long-term training more stable has developed for epsilon_hat, so we'd make people re-tune their hyperparameters for questionable benefit by changing the default (i.e. maybe very small updates for the first few thousand iterations is desirable).
 However, the documentation should certainly be updated to let people know that it's epsilon_hat rather than epsilon that they're setting.
 <denchmark-link:https://github.com/taion>@taion</denchmark-link>
 : Does that sound reasonable? I'm happy to put together the documentation changes.
 		",10.0,taion,2017-04-11T18:00:53Z,"
 		Just checked a few other packages –
 
 Keras uses the same incorrect formulation as here: https://github.com/fchollet/keras/blob/7f58b6fbe702c1936e88a878002ee6e9c469bc77/keras/optimizers.py#L389-L400
 PyTorch uses the same incorrect formulation as here: https://github.com/pytorch/pytorch/blob/f17cfe42936310a2e3fd573e1f4dec8c684d4003/torch/optim/adam.py#L68-L72
 Lasagne uses the correct implementation, but in the unoptimized form: https://github.com/Lasagne/Lasagne/blob/45bb5689f0b2edb7114608e88305e8074d29bbe7/lasagne/updates.py#L620-L622
 
 I mean, I don't know. The correct version seems unlikely to cause issues for the on-label case, or people using Lasagne would have had problems.
 I don't really want to train an InceptionNet from scratch to see what this does for the off-label case; I will just note that when I tried using a larger epsilon a while ago, my model seemed to not train at all even when I used a higher learning rate, which would suggest that the current implementation makes things worse in the off-label case of using a higher epsilon.
 		",11.0,taion,2017-04-12T17:47:02Z,"
 		Thank you for taking a look at other frameworks! That's an interesting list.
 In terms of numerical issues, I'm particularly worried about float16 users. AdamOptimizer already requires tuning the default epsilon_hat of 1e-8 to at least 1e-7 to avoid errors. If we naively divided it by anything more than 3, it would underflow. Specifying epsilon_hat directly at least makes reasoning about precision issues a bit easier.
 		",12.0,taion,2017-04-12T18:45:51Z,"
 		Oh! I didn't think of the float16 case.
 Maybe the first-best would be to rename the current epsilon to epsilon_hat (with a deprecation notice for the moment), then possibly add a proper epsilon later on?
 		",13.0,taion,2017-04-12T20:55:49Z,"
 		The current formulation is sufficiently useful for numerical stability that we'll want to keep it around. I'll do the documentation fix and mark this as closed once that propagates. Feel free to open a feature request for a second epsilon parameter (""epsilon_nohat""?), or work on a pull request, but I don't think it's going to be a priority without experimental evidence that it's useful.
 		",14.0,taion,2017-04-12T21:04:37Z,"
 		Sounds good, thanks.
 		",15.0,taion,2017-04-12T21:06:36Z,"
 		Fix is submitting, should be synced within a day or so. Thank you for the report!
 		",16.0,taion,2017-04-13T20:32:42Z,"
 		Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9089,yaroslavvb,2017-04-09T19:07:13Z,2017-04-11T20:26:50Z,cpp protobuf instructions out-of-date for MacOS,"
 Instructions to upgrade to cpp protobuf implementation on Mac from <denchmark-link:https://www.tensorflow.org/install/install_mac#protobuf_pip_package_31>https://www.tensorflow.org/install/install_mac#protobuf_pip_package_31</denchmark-link>
  don't work work, makes TF fails with following stacktrace
 <denchmark-code>Traceback (most recent call last):
   File ""kronecker_benchmark.py"", line 3, in <module>
     import tensorflow as tf
   File ""/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
     from tensorflow.python import *
   File ""/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 54, in <module>
     from tensorflow.core.framework.graph_pb2 import *
   File ""/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/tensorflow/core/framework/graph_pb2.py"", line 6, in <module>
     from google.protobuf import descriptor as _descriptor
   File ""/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/descriptor.py"", line 46, in <module>
     from google.protobuf.pyext import _message
 ImportError: dlopen(/Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-darwin.so, 2): Library not loaded: /usr/local/lib/libprotobuf.10.dylib
   Referenced from: /Users/yaroslav/anaconda/envs/mar1/lib/python3.5/site-packages/google/protobuf/pyext/_message.cpython-35m-darwin.so
   Reason: image not found
 
 </denchmark-code>
 
 Work-around is to use older link:
 pip install --upgrade <denchmark-link:https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl>https://storage.googleapis.com/tensorflow/mac/cpu/protobuf-3.1.0-cp35-none-macosx_10_11_x86_64.whl</denchmark-link>
 
 Check that it works
 python -c ""from google.protobuf.internal import api_implementation; print(api_implementation._default_implementation_type)"" 
 MacOS: 10.12.4 (16E195), TensorFlow, latest nightly from today installed as:
 pip install --upgrade https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=mac-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.1.0rc1-py3-none-any.whl 
 	",1.0,yaroslavvb,2017-04-10T18:10:08Z,"
 		<denchmark-link:https://github.com/jhseu>@jhseu</denchmark-link>
  : Any suggestions for the appropriate update here?
 		",2.0,yaroslavvb,2017-04-10T19:29:04Z,"
 		Sent a fix for the links in the docs (internally, should be reflected soon).
 Also filing a request with the protobuf team to upload optimized mac builds to pypi.
 		",,,,,3dc4907a0d7ae113547fcf716f618e3c8e1b9c77,Jonathan Hseu,2017-04-10 14:34:44-07:00,MODIFY,0,tensorflow\docs_src\install\install_mac.md,tensorflow\docs_src\install\install_mac.md,0.0,"662,667","662,667",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9103,karpkarp,2017-04-10T13:04:15Z,2017-04-11T20:26:50Z,BUG: tensorflow.placeholder shape does not serialize with protobuf,"
 Profobuf serialization(json)
 {
 ""attr"": {
 ""dtype"": {
 ""type"": ""DT_FLOAT""
 },
 ""shape"": {
 ""shape"": {}
 }
 },
 ""name"": ""x"",
 ""op"": ""Placeholder""
 },
 Tensorflow code
 x = tf.placeholder(tf.float32, shape=None, name=""x"")
 	",1.0,karpkarp,2017-04-10T16:57:01Z,"
 		Could you elaborate on the bug here? There is a <denchmark-link:https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/core/ops/array_ops.cc#L2751>known issue with the Placeholder op</denchmark-link>
  where it cannot distinguish between an unknown and a scalar shape, but it does serialize all other shapes correctly.
 There is some work underway to figure out if that bug can be fixed without requiring the PlaceholderV2 operation, but all other shapes should be fine regardless.
 Could you elaborate on your concern here?
 		",2.0,karpkarp,2017-04-10T17:17:46Z,"
 		Sorry, I copied the wrong line from Python as I was testing.
 When a placeholder of shape [None, 784] was serialized, the corresponding element in the profobuf json serialization does not contain a shape attribute.
 This is the python code:
     x = tf.placeholder(tf.float32, shape=[None, 784], name=""x"")
     y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y_"")
     with tf.name_scope(""first_layer""):
 
         W = tf.Variable(tf.zeros([784,10]), name=""W"")
         b = tf.Variable(tf.zeros([10]), name=""b"")
     # Output
         y = tf.matmul(x,W) + b
 
     with tf.name_scope(""softmax_layer""):
     # Loss Function
         softmax = tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_)
     with tf.name_scope(""error_check""):
         cross_entropy = tf.reduce_mean(softmax)
 
     with tf.name_scope(""accuracy_check""):
     #Accuracy Calc
         correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
 
     sess.run(tf.global_variables_initializer())
 
     outfile_txt = json_format.MessageToJson(sess.graph_def)
     outfile = open(""outfile.json"", 'w')
     outfile.write(outfile_txt)
 Select elements from the output json file:
      {
       ""attr"": {
         ""dtype"": {
           ""type"": ""DT_FLOAT""
         },
         ""shape"": {
           ""shape"": {}
         }
       },
       ""name"": ""x"",
       ""op"": ""Placeholder""
     },
 {
       ""attr"": {
         ""shape"": {
           ""shape"": {
             ""dim"": [
               {
                 ""size"": ""784""
               },
               {
                 ""size"": ""10""
               }
             ]
           }
         },
         ""shared_name"": {
           ""s"": """"
         },
         ""container"": {
           ""s"": """"
         },
         ""dtype"": {
           ""type"": ""DT_FLOAT""
         }
       },
       ""name"": ""first_layer/W"",
       ""op"": ""VariableV2""
     }
 Let me know if this is because of what you said earlier. Im using tensorflow-gpu installed from pip3 on windows.
 		",3.0,karpkarp,2017-04-10T19:51:27Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  do you know if there's an issue tracking that work?
 		",24a95ae389e1c76e771ac33d66e0ec40a236260f,Vijay Vasudevan,2017-04-10 16:50:29-07:00,MODIFY,1,tensorflow\cc\client\client_session_test.cc,tensorflow\cc\client\client_session_test.cc,1.0,52,52,MODIFY,5.0,tensorflow\cc\framework\cc_op_gen.cc,tensorflow\cc\framework\cc_op_gen.cc,4.0,karpkarp,2017-04-10T20:01:55Z,"
 		<denchmark-link:https://github.com/karpkarp>@karpkarp</denchmark-link>
  : Thanks for the sample code. It seems that if any of the dimensions are unknown is when we end up with an empty shape in the , which is broader than the problem  is going to address.
 I'll dig in a bit more.
 CC  <denchmark-link:https://github.com/vrv>@vrv</denchmark-link>
 
 		",5.0,karpkarp,2017-04-10T20:27:35Z,"
 		Actually I'm trying to change Placeholder itself so no new V2 is needed, but this is precisely correct.  We currently lose shape information when you serialize and deserialize partially known placeholder shapes.  This is fixed in V2 which I am trying to backport to v1.
 		",6.0,karpkarp,2017-04-10T21:56:51Z,"
 		It seems in array_ops.py, it sets a requirement for the shape to be fully defined with shape.is_fully_defined() in the placeholder function. Any particular reason for this? Does this mean that a placeholder of shape of [None, SomeNum] will not be enforced?
 In any case, I removed the condition where the Placeholder shape has to be fully defined and the serialization issues are fixed. This does break placeholders with no defined shape so I added two additional function in python/framework/tensor_shape.py
 python/framework/tensor_shape
  def is_partially_defined(self):
     return self._dims is not None
 
   def assert_is_partially_defined(self):
     if not self.is_partially_defined(self):
       raise ValueError(""Shape %s is not partially defined"" % self)
 python\ops\array_ops.py
 def placeholder(dtype, shape=None, name=None):
   shape = tensor_shape.as_shape(shape)
   if shape.is_partially_defined():
     dim_list = shape.as_list()
   else:
     dim_list = []
   ret = gen_array_ops._placeholder(
       dtype=dtype,
       shape=dim_list,
       name=name)
   ret.set_shape(shape)
   return ret
   ""versions"": {
     ""producer"": 21
   },
   ""node"": [
     {
       ""op"": ""Placeholder"",
       ""name"": ""x"",
       ""attr"": {
         ""shape"": {
           ""shape"": {
             ""dim"": [
               {
                 ""size"": ""-1""
               },
               {
                 ""size"": ""784""
               }
             ]
           }
         },
         ""dtype"": {
           ""type"": ""DT_FLOAT""
         }
       }
     },
     {
       ""op"": ""Placeholder"",
       ""name"": ""y_"",
       ""attr"": {
         ""shape"": {
           ""shape"": {
             ""dim"": [
               {
                 ""size"": ""-1""
               },
               {
                 ""size"": ""10""
               }
             ]
           }
         },
         ""dtype"": {
           ""type"": ""DT_FLOAT""
         }
       }
     },
     {
       ""op"": ""NoOp"",
       ""name"": ""init""
     }
   ]
 }
 		",1.0,"129,130,131,132,133",129,tensorflow::PrintTensorShape,shape_proto,129,141,MODIFY,1.0,tensorflow\core\framework\partial_tensor_shape.cc,tensorflow\core\framework\partial_tensor_shape.cc,1.0,"85,86,87,88,89,90,91,92,93,94",,MODIFY,0.0,tensorflow\core\framework\partial_tensor_shape.h,tensorflow\core\framework\partial_tensor_shape.h,0.0,"49,50,51",49,tensorflow::TEST,"ClientSessionTest,Extend",50,64,,,,,,,,,,,,,,,,,,,,,,1.0,"195,196,197,198,199",,tensorflow::PrintTensorProto,proto,195,199,1.0,"308,309","295,296",tensorflow::AttrTypeName,attr_type,294,321,1.0,"216,218,251,252,253,254,255,256,257,258","206,208,209,210,211,244,245",tensorflow::PrintAttrValue,"op,attr_value",201,269,1.0,"129,130,131,132,133",129,tensorflow::PrintTensorShape,shape,129,137,,,,,,,,,,,,tensorflow::PartialTensorShape::PartialTensorShape,shape,85,94,,,,,MODIFY,0.0,tensorflow\core\ops\array_ops.cc,tensorflow\core\ops\array_ops.cc,0.0,"2741,2749","2741,2749",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tensorflow\core\ops\array_ops_test.cc,tensorflow\core\ops\array_ops_test.cc,1.0,"789,796,814,816","789,796,812,813,814,815,816,817,818,819,820,821,822",tensorflow::TEST,"ArrayOpsTest,Placeholder_ShapeFn",776,822,1.0,,"832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,870,872",tensorflow::TEST,"ArrayOpsTest,PlaceholderV2_ShapeFn",832,878,MODIFY,0.0,tensorflow\core\public\version.h,tensorflow\core\public\version.h,0.0,"87,88,89,93",90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,9.0,tensorflow\python\kernel_tests\constant_op_test.py,tensorflow\python\kernel_tests\constant_op_test.py,1.0,"677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751","688,700,706,707,719,728,731,734,737,738",MODIFY,1.0,tensorflow\python\ops\array_ops.py,tensorflow\python\ops\array_ops.py,1.0,1512,1512,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testOldGraph,self,677,751,1.0,638,"636,637,638,639,640",testScalarShape,self,636,640,1.0,"668,671,674",671,testTensorStr,self,667,675,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,644,"642,643,644,645,646,647,648,649,650,651,652",testPartialShape,self,642,652,1.0,656,"654,655,656,657,658,659,660,661",testControlDependency,self,654,661,1.0,,"592,593,594,595,596,597,598,599,600,601",testDtype,self,591,601,1.0,609,"611,612,613,614,615,616,617,618,619,620,621,622",testShape,self,607,622,1.0,665,"663,664",testBadShape,self,663,665,1.0,626,"624,625,626,627,628,629,630,631,632,633,634",testUnknownShape,self,624,634,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,placeholder,"dtype,shape,name",1482,1512,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9136,jdonier,2017-04-11T12:47:39Z,2017-04-20T20:42:34Z,Issues when using Queues + tf.train.Server,"
 NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.
 <denchmark-h:h3>You must complete this information or else your issue will be closed</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?: Yes
 TensorFlow installed from (source or binary)?: binary
 TensorFlow version: 1.0.0 CPU / 1.0.1 (CPU and GPU enabled) / 1.1.0rc1 (CPU)
 Bazel version (if compiling from source):
 CUDA/cuDNN version: N/A
 GPU Model and Memory: N/A
 Exact command to reproduce: cf below.
 
 This problem has been reproduced on both Linux and various Mac OS machines.
 <denchmark-h:h3>Describe the problem clearly</denchmark-h>
 
 We seem to experience issues when using both queues + tf.train.Server. When executed in a simple python 3.5.3 console, the following script hangs:
 <denchmark-code>import tensorflow as tf
 import time
 
 cluster = tf.train.ClusterSpec({""cpu1"" : ['localhost:2222']})
 server = tf.train.Server(cluster, job_name=""cpu1"", task_index=0)
 
 with tf.Graph().as_default() as graph:
     # Queue
     input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))
 
     # Useless variable
     variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=""variable"")
 
     # Session and queue runners
     session = tf.Session(target=server.target)
     session.run(tf.global_variables_initializer())
     tf.train.start_queue_runners(session)
 
 print(session.run(variable))  # this works
 print(session.run(tf.assign(variable, 2)))  # this also works, but only if called directly
 
 # any pause between creating and running the session breaks it
 time.sleep(1)
 
 print(session.run(variable))  # retrieving a variable still works, but...
 print(session.run(tf.assign(variable, 3)))  # ... assigning a variable will make the program hang.
 </denchmark-code>
 
 It outputs:
 <denchmark-code>1
 2
 2
 </denchmark-code>
 
 and then hangs forever. The problem vanishes when either commenting the input_queue=... line, or when writing session = tf.Session() instead of passing the server.target.
 The problems seems to happen not only with variable assignments, but also saving the model using tf.train.Saver().save(session, 'my_model') for instance (and possibly other operations). Note that reading a variable works fine.
 In the example script, the time.sleepcommand simulates a pause between creating the session and running it to set a variable. The same effect is achieved, for example, when splitting session creation and running code across two Jupyter notebook cells. When executing the whole code in one cell, it works fine.
 <denchmark-h:h3>Source Code / Logs</denchmark-h>
 
 The source code to reproduce the problem is displayed above. I have attached a traceback using gdb, which shows that the program is hanging while trying to acquire a lock.
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/913097/tf-issue-gdb-bt.txt>tf-issue-gdb-bt.txt</denchmark-link>
 
 <denchmark-link:https://github.com/tensorflow/tensorflow/files/913102/tf-issue-gdb-stack-threads.txt>tf-issue-gdb-stack-threads.txt</denchmark-link>
 
 	",1.0,jdonier,2017-04-14T18:09:20Z,"
 		Thanks for the detailed report and stacktraces, this helps a lot and is much appreciated.
 <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  pointed out that we might have a bug when graphs are extended in a distributed session while some operations (in this case the enqueue operation) are in progress (See <denchmark-link:https://github.com/tensorflow/tensorflow/blob/87cdfafd44ff5e332fd820608783432fea83a4c9/tensorflow/core/distributed_runtime/master_session.cc#L1038>master_session.cc:1038</denchmark-link>
  - that code predates the queue runners).
 <denchmark-link:https://github.com/suharshs>@suharshs</denchmark-link>
  : Would you have the bandwidth to look into that TODO?
 <denchmark-link:https://github.com/jdonier>@jdonier</denchmark-link>
  : In the mean time, a workaround for you would be to ensure that the graph isn't modified after the queue runners are started. For example, your snippet above could be rewritten as:
 import tensorflow as tf
 import time
 
 cluster = tf.train.ClusterSpec({""cpu1"" : ['localhost:2222']})
 server = tf.train.Server(cluster, job_name=""cpu1"", task_index=0)
 
 with tf.Graph().as_default() as graph:
     # Queue
     input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))
 
     # Useless variable
     variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=""variable"")
 
     # Session and queue runners
     session = tf.Session(target=server.target)
     session.run(tf.global_variables_initializer())
 
     # CHANGE FROM PREVIOUS SNIPPET: Assign operations
     assign2 = tf.assign(variable, 2)
     assign3 = tf.assign(variable, 3)
 
     tf.train.start_queue_runners(session)
 
 print(session.run(variable))
 print(session.run(assign2))
 
 # Freely sleep
 time.sleep(1)
 
 print(session.run(variable))
 print(session.run(assign3))
 FYI <denchmark-link:https://github.com/jhseu>@jhseu</denchmark-link>
  <denchmark-link:https://github.com/saeta>@saeta</denchmark-link>
  who might like to know about this too.
 		",2.0,jdonier,2017-04-14T18:44:28Z,"
 		<denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  the reason why I want to modify the graph after the queue runners are started is to change some parameters during / after training (e.g. the training rate during training, or dropout rates between training and inference) so this needs to be done after the queue runners have been started. I guess I could define them as placeholders but it's a bit weird to have to feed these values for every computation...
 About the problem with model saving: I was creating a tf.train.Saver() at saving time, which was causing the problem, consistent with your explanation. It all works fine if I define it when I create the graph -- so thanks a lot!
 		",3.0,jdonier,2017-04-18T23:59:21Z,"
 		I have a change coming soon that should fix this. (Thanks <denchmark-link:https://github.com/mrry>@mrry</denchmark-link>
  and <denchmark-link:https://github.com/asimshankar>@asimshankar</denchmark-link>
  for flagging!)
 		",1f210ad7c2a81fe27196dd1a85c9bb92f19bc94a,Brennan Saeta,2017-04-19 10:31:38-07:00,MODIFY,12,tensorflow\core\distributed_runtime\master_session.cc,tensorflow\core\distributed_runtime\master_session.cc,1.0,"1005,1006,1007,1008,1009",,MODIFY,0.0,tensorflow\core\distributed_runtime\master_session.h,tensorflow\core\distributed_runtime\master_session.h,,,,,,,,,,,,,0.0,119,119,,,,,MODIFY,1.0,tensorflow\python\training\server_lib_test.py,tensorflow\python\training\server_lib_test.py,1.0,"230,231,232,233,234,235,236,237,238,239,240,241,242",,,,,,,,,tensorflow::MasterSession::Create,graph_def,994,1011,1.0,"798,799","771,772,783",tensorflow::MasterSession::ReffedClientGraph::ProcessDeviceStats,"ph,execution_state,ds,is_rpc",770,812,1.0,482,"501,502",tensorflow::MasterSession::ReffedClientGraph::RunPartitions,"env,step_id,execution_count,pss,call_opts,req,resp,cm,is_last_partial_run",480,637,1.0,"1268,1269,1270,1271,1272,1285,1286,1294","1292,1293,1306,1307",tensorflow::MasterSession::DoPartialRun,"opts,req,resp",1201,1307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,testExtendAfterQueueRunners,self,230,242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"76,77,78","76,77,78,79,80",tensorflow::MasterSession::ReffedClientGraph::ReffedClientGraph,"handle,bopts,cg,session_opts,stats_publisher_factory,execution_state,is_partial,worker_cache",60,80,1.0,"706,707,708,709,710,722,728","726,727,728,729,741,747",tensorflow::MasterSession::ReffedClientGraph::ProcessStats,"step_id,pss,ph,options,resp",706,749,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1353,1354,1359,1360","1316,1317,1376,1377,1378",tensorflow::MasterSession::DoRunWithLocalExecution,"opts,req,resp",1309,1382,,,,,,,,1.0,,"501,502",tensorflow::MasterSession::ReffedClientGraph::RunPartitions,"env,step_id,execution_count,execution_state,pss,call_opts,req,resp,cm,is_last_partial_run",499,657,1.0,,"1038,1039,1040,1041,1042,1043,1044,1045,1046",tensorflow::MasterSession::Extend,"req,resp",1028,1064,1.0,"728,752,763","726,727,728,729,741,747",tensorflow::MasterSession::ReffedClientGraph::ProcessStats,"step_id,pss,execution_state,ph,options,resp",726,768,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"752,763","771,772,783",tensorflow::MasterSession::ReffedClientGraph::ProcessDeviceStats,"ph,ds,is_rpc",751,792,1.0,"133,134,135","149,150",tensorflow::MasterSession::ReffedClientGraph::RetrieveLogs,"step_id,ss",119,156,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9161,ayat-khairy,2017-04-12T12:03:39Z,2017-04-12T21:15:31Z,"Hi, I am unable to access the documentation","
 NOTE: Issues that are not bugs or feature requests will be closed. Please ask usage questions on StackOverflow.
 <denchmark-h:h3>You must complete this information or else your issue will be closed</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?:
 TensorFlow installed from (source or binary)?:
 TensorFlow version:
 Bazel version (if compiling from source):
 CUDA/cuDNN version:
 GPU Model and Memory:
 Exact command to reproduce:
 
 <denchmark-h:h3>Describe the problem clearly</denchmark-h>
 
 <denchmark-h:h3>Source Code / Logs</denchmark-h>
 
 Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full-traceback. Large logs and files should be attached. Try to reproducible test-case code the bare-minimum necessary to generate the problem
 	",1.0,ayat-khairy,2017-04-12T12:54:17Z,"
 		<denchmark-link:https://www.tensorflow.org/get_started/get_started>https://www.tensorflow.org/get_started/get_started</denchmark-link>
 
 		",2.0,ayat-khairy,2017-04-12T13:02:22Z,"
 		Thanks! Sorry, I mean the documentation of TensorFlow Debugger (tfdbg) <denchmark-link:https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/debug/examples>https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/debug/examples</denchmark-link>
 
 		",3.0,ayat-khairy,2017-04-12T14:50:54Z,"
 		I can not see too, is there any document of tensorflow which is whriten by java or scala?
 		",9cc21f04ed051d6a6d3a21e909aa547110aa8b0d,Shanqing Cai,2017-04-12 09:52:44-07:00,MODIFY,0,tensorflow\python\debug\examples\README.md,tensorflow\python\debug\examples\README.md,0.0,"3,4,5,6,7,8,9","3,4",,,,,4.0,ayat-khairy,2017-04-12T15:21:17Z,"
 		<denchmark-link:https://github.com/ayat-khairy>@ayat-khairy</denchmark-link>
  : Thanks for pointing out that broken link. In the mean time you can find documentation for  in <denchmark-link:https://www.tensorflow.org/programmers_guide/debugger>https://www.tensorflow.org/programmers_guide/debugger</denchmark-link>
  and <denchmark-link:https://www.tensorflow.org/programmers_guide/tfdbg-tflearn>https://www.tensorflow.org/programmers_guide/tfdbg-tflearn</denchmark-link>
 
 <denchmark-link:https://github.com/caisq>@caisq</denchmark-link>
  : Do you want to update the link in the README?
 <denchmark-link:https://github.com/geektcp>@geektcp</denchmark-link>
  : I'm not sure I understand your question. If you have a separate bug report about documentation, please file an issue with more details. Or if this comment answers your question, great.
 Thanks!
 		",5.0,ayat-khairy,2017-04-12T19:00:19Z,"
 		A fix has been submitted internally and will be pushed to GtiHub soon.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9312,Songweiping,2017-04-19T13:59:31Z,2017-04-20T03:57:18Z,Typo in seq2seq.attention_wrapper.py,"
 Hi,
 I think there is a small typo in contrib.seq2seq.attention_wrapper.py, would someone like to check it?
 code url: <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L471%5D>https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L471]</denchmark-link>
 
 I guess it should be  rather than  to be checked.
 Thanks.
 	",1.0,Songweiping,2017-04-19T16:56:51Z,"
 		Yes, could you send a PR to fix this?
 /CC: <denchmark-link:https://github.com/ebrevdo>@ebrevdo</denchmark-link>
 
 		",2.0,Songweiping,2017-04-19T17:42:02Z,"
 		yes please!
 <denchmark-link:#>…</denchmark-link>
 
 
 On Wed, Apr 19, 2017 at 9:57 AM, drpngx ***@***.***> wrote:
  Yes, could you send a PR to fix this?
 
  /CC: @ebrevdo <https://github.com/ebrevdo>
 
  —
  You are receiving this because you were mentioned.
  Reply to this email directly, view it on GitHub
  <#9312 (comment)>,
  or mute the thread
  <https://github.com/notifications/unsubscribe-auth/ABtim3R_dvUJtttYzVdb_zSrz70zJrTgks5rxjzugaJpZM4NBuTr>
  .
 
 
 
 		",3.0,Songweiping,2017-04-20T02:31:28Z,"
 		I have sent a PR, please let me know if any questions.
 		",0557e8f90c1a2f027f4561c70558c6c836138058,weipingpku,2017-04-19 20:56:43-07:00,MODIFY,0,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py,0.0,471,471,,,,,4.0,Songweiping,2017-04-20T03:57:30Z,"
 		Merged.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9633,tpet,2017-05-03T15:49:22Z,2017-05-05T18:20:38Z,SIGSEGV with sparse_add and broadcasting,"
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
 yes, enclosed below
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
 Ubuntu 16.04
 TensorFlow installed from (source or binary):
 binary via pip
 TensorFlow version (use command below):
 ('v1.0.0-65-g4763edf-dirty', '1.0.1')
 Bazel version (if compiling from source):
 N/A, using pip installation
 CUDA/cuDNN version:
 N/A, CPU-only
 GPU model and memory:
 none
 Exact command to reproduce:
 
 <denchmark-code>from __future__ import print_function
 import numpy as np
 import tensorflow as tf
 
 dense_sz = [1, 1000, 1000]
 dense = tf.constant(1.0, shape=dense_sz, dtype=tf.float32)
 
 sparse_sz = [10, 1000, 1000]
 nnz = 100
 nz_ind = np.random.choice(np.prod(sparse_sz), size=nnz, replace=False)
 nz_ind = np.unravel_index(nz_ind, dims=sparse_sz)
 nz_ind = np.array(nz_ind).T
 assert np.all(nz_ind < np.array(sparse_sz)[None, :])
 # Ensure canonical ordering.
 ind = np.lexsort([nz_ind[:, i].flatten() for i in reversed(range(nz_ind.shape[1]))])
 nz_ind = nz_ind[ind, :]
 print('nz_ind\n', nz_ind)
 
 sparse_plc = tf.sparse_placeholder(tf.float32)
 sparse_sum = tf.sparse_add(dense, sparse_plc)
 init = tf.global_variables_initializer()
 
 with tf.Session() as sess:
     sess.run(init)
     print('after init')
     res = sess.run(sparse_sum, feed_dict={sparse_plc: tf.SparseTensorValue(nz_ind, np.ones((nnz,)), sparse_sz)})
     print('sum\n', res)
 </denchmark-code>
 
 <denchmark-h:h3>Describe the problem</denchmark-h>
 
 Running the code above results in
 <denchmark-code>[...]
 after init
 
 Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)
 </denchmark-code>
 
 For lower values of nnz, (nnz = 1) it finishes fine quite often.
 <denchmark-code>[...]
 after init
 sum
  [[[ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   ..., 
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]
   [ 1.  1.  1. ...,  1.  1.  1.]]]
 
 Process finished with exit code 0
 </denchmark-code>
 
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 See above.
 	",1.0,tpet,2017-05-03T17:29:05Z,"
 		<denchmark-link:https://github.com/concretevitamin>@concretevitamin</denchmark-link>
  can you take a look at this?
 Here's the stacktrace:
 <denchmark-code>PC: @     0x7f37662d027d  (unknown)  raise
     @         0x243417e6       1120  FailureSignalHandler()
     @     0x7f37662d03d0       1472  (unknown)
     @     0x7f3764db3191        128  faulthandler_fatal_error
     @     0x7f37662d03d0  (unknown)  (unknown)
     @         0x2217d353       1664  tensorflow::SparseTensorDenseAddOp<>::Compute()
     @         0x22f76586         96  tensorflow::ThreadPoolDevice::Compute()
     @         0x22f0764b       2464  tensorflow::(anonymous namespace)::ExecutorState::Process()
     @         0x22f13cb1        160  std::_Mem_fn<>::operator()<>()
     @         0x22f13bc3         96  std::_Bind<>::__call<>()
     @         0x22f13b06         64  std::_Bind<>::operator()<>()
     @         0x22f136fd         32  std::_Function_handler<>::_M_invoke()
     @         0x13172ede         32  std::function<>::operator()()
     @         0x2325fcc8        128  tensorflow::thread::EigenEnvironment::ExecuteTask()
     @         0x2325f019        208  Eigen::ThreadPoolTempl<>::WorkerLoop()
     @         0x2325ec5e         32  Eigen::ThreadPoolTempl<>::ThreadPoolTempl()::{lambda()#1}::operator()()
     @         0x2325eacd         32  std::_Function_handler<>::_M_invoke()
     @         0x13172ede         32  std::function<>::operator()()
     @         0x2325e934         48  tensorflow::thread::EigenEnvironment::CreateThread()::{lambda()#1}::operator()()
     @         0x2325e76d         32  std::_Function_handler<>::_M_invoke()
     @         0x13172ede         32  std::function<>::operator()()
     @         0x23296f0c         32  tensorflow::(anonymous namespace)::GoogleThread::FuncThread::Run()
     @         0x23d41427        448  Thread::ThreadBody()
     @     0x7f37662c6890        176  start_thread
     @     0x7f3765d2237d  (unknown)  clone
 </denchmark-code>
 
 		",2.0,tpet,2017-05-03T23:10:18Z,"
 		Did you modify anything else in the code? The version shows dirty.
 <denchmark-link:https://github.com/yifeif>@yifeif</denchmark-link>
  <denchmark-link:https://github.com/av8ramit>@av8ramit</denchmark-link>
  are the pip installs built from a dirty git repo?
 		",3.0,tpet,2017-05-04T01:14:38Z,"
 		I'm taking a look.
 		",50b836addfed6b49fc823987e9301f1b6eeef90c,Zongheng Yang,2017-05-04 12:30:04-07:00,MODIFY,1,tensorflow\core\kernels\sparse_tensor_dense_add_op.cc,tensorflow\core\kernels\sparse_tensor_dense_add_op.cc,1.0,"50,51,52,57,58,59,60,61,62,63,64,65,66,67,68,69,95,96,97","50,51,56,57,58,59,85,86",MODIFY,1.0,tensorflow\python\ops\sparse_ops.py,tensorflow\python\ops\sparse_ops.py,4.0,tpet,2017-05-04T10:56:25Z,"
 		The results look the same with tf pip-upgraded to ('v1.1.0-rc0-61-g1ec6ed5', '1.1.0').
 		",5.0,tpet,2017-05-04T18:59:16Z,"
 		Thanks for the report <denchmark-link:https://github.com/tpet>@tpet</denchmark-link>
 .
 I am submitting a ""fix"" that instead of segfaulting, return a proper error status that requires both operands have matching shapes.  This has always been the assumption in the , but was <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse_tensor_dense_add_op.cc#L56>not enforced</denchmark-link>
 .  The patch should show up in master within a day or two.
 Do you exactly require the functionality of ""sparse + dense -> dense, with dense-to-sparse broadcast""?  If so, I'd like to mark this as contributions welcome (the current kernels do not support this broadcast pattern).
 However, if you can get away with ""sparse + dense -> sparse, with dense-to-sparse broadcast"", we already have <denchmark-link:https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/sparse_ops.py#L316>sparse_dense_cwise_add()</denchmark-link>
  that does this.  Let us know, and we can expose this function as a public method.
 		",6.0,tpet,2017-05-05T10:40:45Z,"
 		<denchmark-link:https://github.com/concretevitamin>@concretevitamin</denchmark-link>
  I agree raising an exception is much better. The ""sparse + dense -> dense, with dense-to-sparse broadcast"" really seems not that much useful, compared to ""sparse + dense -> sparse, with dense-to-sparse broadcast"". Now I actually don't need this particular thing.
 My initial use case was a bit different. In the process of trying to get some reasonable behavior I happened to find the segfault and created this example.
 My use case is this:
 D + reduce_sum(a * S)
 where D and the result is dense [1 n2 n3 n4]
 S is sparse [n1 n2 n3 n4]
 a is dense [n1 1 1 1] and broadcasts to S.
 So far I hasn't been able to get to some reasonable performance with this.
 		",1.0,"244,245",,sparse_add,"a,b,thresh",236,315,,,,,,,,,,,,,,,tensorflow::SparseTensorDenseAddOp::Compute,ctx,37,102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,tpet,2017-05-05T10:50:52Z,"
 		I'm using a pip installation so it will take some time until it propagates down to me so feel free to close the issue if you think it is resolved. Thanks.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,8.0,tpet,2017-05-05T18:20:38Z,"
 		Okay, closing for now.  For the reduce, take a look at tf.sparse_reduce_sum() and/or tf.sparse_reduce_sum_reduce().
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9931,galeone,2017-05-16T08:52:31Z,2017-05-17T20:01:53Z,Go: SIGSEGV when using int32 instead of int64 and missing error in Resize functions,"
 <denchmark-h:h2>Problem</denchmark-h>
 
 In Go, some operation causes a SIGSEGV when using an int32 instead of an int64 (and I have reasons to believe that the same will happen when using float instead of double and vice-versa).
 The Resize* operations don't define the output shape correctly when the input is not a ""batch"": they just let the dimensions undefined instead of raising some errors.
 The tests below are commented so I hope that's enough to let you understand what the problems are.
 <denchmark-h:h3>Source code / logs</denchmark-h>
 
 package poc_test
 
 import (
         ""fmt""
         //tf ""github.com/tensorflow/tensorflow/tensorflow/go""
         ""github.com/tensorflow/tensorflow/tensorflow/go/op""
         ""testing""
 )
 
 func TestResizeWithoutBatchIsNoSense(t *testing.T) {
         // Create root scope
         root := op.NewScope()
 
         // Define graph
 
         // 1: read image content
         imagePath := ""test.jpg""
         contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
 
         // 2: decode Jpeg
         value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
 
         // I'd like to add noise to the image, so I'd like to define a nose tensor with the same shape of the image
         // Just to be sure that the image shape is fully defined, I resize it
         resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), value, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
 
         // If the size parameter is an int32, no error is raised but the operation is no sense
         // Because it returns ? instead of [80, 80, 3]
         // The reason is taht Resize* methods requires a batch of images: should raise an error?
         fmt.Println(""Shape with int32: "", resize1.Shape().String())
         if dims64, err := resize1.Shape().ToSlice(); err != nil {                                                                                                                                                                                                              
                 fmt.Println(dims64)                                                                                                                                                                                                                                            
         } else {                                                                                                                                                                                                                                                               
                 t.Error(""Error: "", err.Error())                                                                                                                                                                                                                                
         }                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                
         // I expect a fully defined shape                                                                                                                                                                                                                                      
         if !resize1.Shape().IsFullySpecified() {                                                                                                                                                                                                                               
                 t.Error(""Not defined shape"")                                                                                                                                                                                                                                   
         }                                                                                                                                                                                                                                                                      
                                                                                                                                                                                                                                                                                
         // create the batch and see how things changes                                                                                                                                                                                                                         
         batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))                                                                                                                                                                    
         resize1 = op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))                                                                                                                                 
         fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())                                                                                                                                                                                       
         if dims64, err := resize1.Shape().ToSlice(); err == nil {                                                                                                                                                                                                              
                 fmt.Println(dims64)                                                                                                                                                                                                                                            
         } else {                                                                                                                                                                                                                                                               
                 fmt.Println(""Error: "", err.Error())                                                                                                                                                                                                                            
         }                                                                                                                                                                                                                                                                      
         // Now the things have sense and the shape is defined and equals to [ 1, 80, 80, 3]                                                                                                                                                                                    
 }                                                                                                                                                                                                                                                                              
                                                                                                                                                                                                                                                                                
 func TestResizeWithIn64ShapeSigSegvs(t *testing.T) {                                                                                                                                                                                                                           
         defer func() {                                                                                                                                                                                                                                                         
                 if r := recover(); r != nil {                                                                                                                                                                                                                                  
                         t.Error(""Panic!"")                                                                                                                                                                                                                                      
                 }                                                                                                                                                                                                                                                              
         }()
         // Create root scope
         root := op.NewScope()
 
         // Define graph
 
         // 1: read image content
         imagePath := ""test.jpg""
         contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
 
         // 2: decode Jpeg
         value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
 
         // However, changing int32 with int64 breaks everyting (no matter if I use `batch` or `value`)
         resize2 := op.ResizeArea(root.SubScope(""ResizeArea2""), value, op.Const(root.SubScope(""size2""), []int64{int64(80), int64(80)}))
         // This operation causes a SIGSEGV
         fmt.Println(""Shape value: "", resize2.Shape())
         fmt.Println(""Shape with int64: "", resize2.Shape().String())
 
         // In short, chaning int32 with int64 causes SIGSEGV. It looks like kernels are not registered to handle both types
 
         // This can bring the code to be a mess to debug, because If I'd like to, for example, add noise to an image
         // I have to generate a set of values with the same shape of the input images.
         // Using the one with the defined shape (the batch) I'd like to use the output of Shape().ToSlice()
         // But I can't.
 }
 
 func TestGenerateNoiseWithInt32Shape(t *testing.T) {
         defer func() {
                 if r := recover(); r != nil {
                         t.Error(""Panic!"")
                 }
         }()
         // Create root scope
         root := op.NewScope()
 
         // Define graph
 
         // 1: read image content
         imagePath := ""test.jpg""
         contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
         // 2: decode Jpeg
         value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
         batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))
         resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
         fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())
         if dims64, err := resize1.Shape().ToSlice(); err != nil {
                 fmt.Println(dims64)
         } else {
                 fmt.Println(""Error: "", err.Error())
         }
 
         dims64, _ := resize1.Shape().ToSlice()
         noise := op.ParameterizedTruncatedNormal(root.SubScope(""ParameterizedTruncatedNormal""),
                 op.Const(root.SubScope(""shape""), dims64),
                 op.Const(root.SubScope(""means""), 0.),
                 op.Const(root.SubScope(""stddev""), 1.),
                 op.Const(root.SubScope(""minvals""), 0.),
                 op.Const(root.SubScope(""maxvals""), 1.))
         fmt.Println(noise)
 
         // ^ This operation causes SIGSEGV
         // I have to convert dims64 to a slice of int32 and then the operation works
 
 }
 
 func TestGenerateNoiseWithInt64Shape(t *testing.T) {
         // Create root scope
         root := op.NewScope()
 
         // Define graph
 
         // 1: read image content
         imagePath := ""test.jpg""
         contents := op.ReadFile(root.SubScope(""ReadFile""), op.Const(root.SubScope(""filename""), imagePath))
         // 2: decode Jpeg
         value := op.DecodeJpeg(root.SubScope(""DecodeJpeg""), contents, op.DecodeJpegChannels(3))
         batch := op.ExpandDims(root.SubScope(""expand""), value, op.Const(root.SubScope(""axis""), []int32{0}))
         resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), batch, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
         fmt.Println(""Shape with int32 and input as a batch: "", resize1.Shape().String())
         if dims64, err := resize1.Shape().ToSlice(); err != nil {
                 fmt.Println(dims64)
         } else {
                 fmt.Println(""Error: "", err.Error())
         }
 
         dims64, _ := resize1.Shape().ToSlice()
 
         var dims []int32 = make([]int32, len(dims64))
         for i, dim := range dims64 {
                 dims[i] = int32(dim)
         }
 
         noise := op.ParameterizedTruncatedNormal(root.SubScope(""ParameterizedTruncatedNormal""),
                 op.Const(root.SubScope(""shape""), dims64),
                 op.Const(root.SubScope(""means""), 0.),
                 op.Const(root.SubScope(""stddev""), 1.),
                 op.Const(root.SubScope(""minvals""), 0.),
                 op.Const(root.SubScope(""maxvals""), 1.))
         fmt.Println(noise.Shape().String())
 }
 <denchmark-h:h3>System information</denchmark-h>
 
 
 Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
 OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Archlinux
 TensorFlow installed from (source or binary): source
 TensorFlow version (use command below): 1.1.0-rc2
 Bazel version (if compiling from source): 0.4.5
 CUDA/cuDNN version: cuda 8, cudnn 5.1
 GPU model and memory:  GeForce GTX 1080
 Exact command to reproduce: go test
 
 	",1.0,galeone,2017-05-16T16:13:55Z,"
 		Thanks for the report <denchmark-link:https://github.com/galeone>@galeone</denchmark-link>
  , there are multiple things going on here.
 
 
 There was a bug in the underlying C API where it was suppressing errors during graph construction. That is fixed at head in 7d785f1, so it will be available with the 1.2 release of the TensorFlow C API
 
 
 The Scope type follows the ""builder pattern"" for graph construction. So, while op.ResizeNearestNeighbor(Scope*, ...) doesn't return an error, errors are collected in Scope.Err. This allows for more compact graph definitions (see the package example for the op package), at the cost of forgetting to check the error. Setting aside comments on the merits of this design, before using any of the returned tf.Outputs from the functions in the op package, one is encouraged to check the error. So, in line 30 of the snippet above for example, I'd suggest:
 resize1 := op.ResizeNearestNeighbor(root.SubScope(""ResizeArea""), value, op.Const(root.SubScope(""size""), []int32{int32(80), int32(80)}))
 if err := root.Err(); err != nil {
     t.Fatal(err)
 }
 
 // If the size parameter is an int32, no error is raised but the operation is no sense
 // Because it returns ? instead of [80, 80, 3]
 // The reason is taht Resize* methods requires a batch of images: should raise an error?
 fmt.Println(""Shape with int32: "", resize1.Shape().String())
 
 
 Regardless, the Go API should never end up with segfaults from the underlying C API (in this case, providing an invalid TF_Operation pointer). So I'm going to send a fix for that.
 
 
 Long story short: A couple of fixes will ensure error messages/panics that are more useful than the cryptic segfaults. Additionally, it's good practice to check the error on the Scope object.
 Hope that helps (will update this issue with the fix mentioned above). Comments/thoughts welcome.
 		",2.0,galeone,2017-05-16T17:59:55Z,"
 		Thank you for the suggestion on how to properly check for errors!
 Is somewhere documented the Scope follows the builder pattern? I guess it would be nice to put this in Scope's go-doc.
 About the point 3: what is the problem? Why it segfaults? That op has a kernel registered to handle both types or the fix is to register the missing type?
 		",3.0,galeone,2017-05-16T19:33:41Z,"
 		<denchmark-link:https://github.com/galeone>@galeone</denchmark-link>
  : I'm probably misusing the phrase ""builder pattern"" :), but yeah, it does want you to check for errors. If you'd like to contribute or suggest some changes to  or other files to help improve documentation, we'd be more than happy to look at them.
 Regarding point 3: It isn't that the kernel isn't registered, it's that the operation requires that the  argument be an int32 Tensor (see <denchmark-link:https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go/op#ResizeNearestNeighbor>godoc</denchmark-link>
 ).
 Between <denchmark-link:https://github.com/tensorflow/tensorflow/commit/7d785f1e18af9d22d940f18aac6e8c9ffd268b22>7d785f1</denchmark-link>
  and a fix I'm about to make on the Go side, you shouldn't see any segfaults in the C API - but you may see a nil-pointer dereference in Go (because  will return nil and an error after the fix).
 Hope that helps (the fix should be in sometime tomorrow I think)
 		",fe41d05e7c8343ed53fc788d6c312792b390f679,Asim Shankar,2017-05-16 13:11:36-07:00,MODIFY,1,tensorflow\go\graph.go,tensorflow\go\graph.go,1.0,"188,189,190,192","188,189,190,192",MODIFY,2.0,tensorflow\go\op\op_test.go,tensorflow\go\op\op_test.go,4.0,galeone,2017-05-16T20:08:11Z,"
 		I'll be happy to contribute! In the next days, as soon as I have time, I'm going to give a better look.
 However, since every size argument of every function requires an int32, why the .Shape() method returns a []int64? I mean, it seems not coherent. The documentation talks about int32 almost everywhere and this int64 just makes operations like the creation of a new tensor with a predefined shape (like the noise in my previous example) a problem because of the required casts.
 Isn't just better to use an int32 everywhere and thus avoid the problem? I'm pretty sure that no one will create a tensor with a dimension > 2^32 -1
 		",5.0,galeone,2017-05-16T20:22:50Z,"
 		I believe the use of int32 in many operations predates efforts to accept int64 as well (see for example <denchmark-link:https://github.com/tensorflow/tensorflow/commit/91ce95d497ec2957535b2ce6a965cd8269d723e5>91ce95d</denchmark-link>
 ). So, I think some of these ops need to be updated to accept int64 as well.
 In general, having Tensors that reach > 2^32-1 dimensions isn't unheard of, especially when reshaping large batches of multi-dimensional tensors.
 		",6.0,galeone,2017-05-17T07:11:10Z,"
 		Alright, thus instead of using int32 everywhere I suggest using int64 everywhere in order to maintain consistency. Or, if not everywhere, at leat use the same type for attributes that works togeather, i.e. shape and dimensions (as input and/or output parameters) should be both int64.
 		",1.0,"47,48,49,50,51,52,53,54,55,56,57",,,,47,57,MODIFY,1.0,tensorflow\go\operation.go,tensorflow\go\operation.go,1.0,"116,117,118,119,120",,,,,,,,,AddOperation,OpSpec,147,193,,,,,,,,,,,,,,,,,,,,,,1.0,"38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60",,TestAddOperationFailure,T,38,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,galeone,2017-05-17T20:01:52Z,"
 		This issue should be resolved at head (and with C library compiled from head), so I'm going to close this out.
 Contributions for making more operations accept int64 for shape similar to <denchmark-link:https://github.com/tensorflow/tensorflow/commit/91ce95d497ec2957535b2ce6a965cd8269d723e5>91ce95d</denchmark-link>
  are welcome.
 <denchmark-link:https://github.com/galeone>@galeone</denchmark-link>
  : Hope that helps. Feel free to open a new issue if you run into more trouble. Thanks!
 		",c,,115,122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
