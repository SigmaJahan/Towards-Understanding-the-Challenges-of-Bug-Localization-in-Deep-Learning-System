{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#JSON to Processed CSV \n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Flatten function for JSON processing\n",
    "def flatten(obj, flat, prefix=''):\n",
    "    for key, val in obj.items():\n",
    "        if isinstance(val, dict):\n",
    "            flatten(val, flat, prefix + key + '.')\n",
    "        else:\n",
    "            flat[prefix + key] = val\n",
    "\n",
    "# Process JSON files and convert them to CSV\n",
    "def process_json_to_csv(dirs):\n",
    "    csv_files = []\n",
    "    for dirname in dirs:\n",
    "        data = []\n",
    "        for filename in os.listdir(dirname):\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "            with open(filepath, 'r', encoding='utf8') as f:\n",
    "                flat = {}\n",
    "                flatten(json.load(f), flat)\n",
    "                data.append(flat)\n",
    "        # Extract the specific part of the directory name\n",
    "        specific_dir_name = os.path.basename(dirname)\n",
    "        csv_filename = f\"{specific_dir_name}_raw.csv\"\n",
    "        df = pd.DataFrame(data)\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "        csv_files.append(csv_filename)\n",
    "    return csv_files\n",
    "\n",
    "# Data cleaning and formatting functions\n",
    "def cleanhtml(raw_html):\n",
    "    CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(CLEANR, '', raw_html)\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
    "    text = re.sub('\\w*\\f\\w*', ' ', text)\n",
    "    text = re.sub('\\(.*?\\)', ' ', text)\n",
    "    text = re.sub('\\[.*]\\)', ' ', text)\n",
    "    text = re.sub(r'https?://[A-Za-z0-9./]+',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    return text\n",
    "\n",
    "def clean_text_round2(text):\n",
    "    text = re.sub('[‘’“”…]', ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\t', ' ', text)\n",
    "    return text\n",
    "\n",
    "def process_and_format_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Select and rename columns\n",
    "    df = df[['BR.BR_id', 'BR.BR_text.BRsummary', 'BR.BR_text.BRdescription', \n",
    "             'commit.commit_id', 'commit.changed_files.file_0.file_new_name']]\n",
    "    df.columns = ['BR_id', 'BRsummary', 'BRdescription', 'commit_id', 'file_new_name']\n",
    "\n",
    "    # Data cleaning steps\n",
    "    df = df.drop_duplicates().dropna(subset=['BRdescription'])\n",
    "    df['BRdescription'] = df['BRdescription'].apply(cleanhtml)\n",
    "    df['BRdescription'] = df['BRdescription'].apply(clean_text_round1)\n",
    "    df['BRdescription'] = df['BRdescription'].apply(clean_text_round2)\n",
    "\n",
    "    # Truncate descriptions to 1000 characters\n",
    "    df['BRdescription'] = df['BRdescription'].apply(lambda x: x[:1000] if len(x) >= 1000 else x)\n",
    "\n",
    "    # Save the formatted dataframe\n",
    "    formatted_csv = file_path.replace('.csv', '_cleaned.csv')\n",
    "    df.to_csv(formatted_csv, index=False)\n",
    "\n",
    "# Define directories for JSON processing\n",
    "json_dirs = [\"D:/ICSME-2023/Dataset/Denchmark/JSonSet/simplejson/tensorflow+tensorflow\"] \n",
    "\n",
    "# Process JSON files and get CSV file paths\n",
    "csv_file_paths = process_json_to_csv(json_dirs)\n",
    "\n",
    "# Process and format each CSV file\n",
    "for csv_file in csv_file_paths:\n",
    "    process_and_format_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sigma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# CSV to TSV \n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from dateutil import parser\n",
    "import csv\n",
    "import os\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def format_datetime(datetime_str):\n",
    "    # Adjust this function to handle different datetime formats and timezone info\n",
    "    dt = parser.parse(datetime_str)\n",
    "    if dt.tzinfo is not None:\n",
    "        dt = dt.astimezone(pytz.utc)  # Convert to UTC if timezone info is present\n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def convert_to_timestamp(datetime_str):\n",
    "    # Convert datetime string to UNIX timestamp, adjusting for timezones if necessary\n",
    "    dt = parser.parse(datetime_str)\n",
    "    if dt.tzinfo is not None:\n",
    "        dt = dt.astimezone(pytz.utc)  # Convert to UTC if timezone info is present\n",
    "    return int(dt.timestamp())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove HTML tags\n",
    "    no_html = re.sub('<.*?>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    no_urls = re.sub(r'http\\S+', '', no_html)\n",
    "    \n",
    "    # Remove special characters\n",
    "    no_special_chars = re.sub('[^A-Za-z0-9]+', ' ', no_urls)\n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    words = nltk.word_tokenize(no_special_chars)\n",
    "    \n",
    "    # Remove stopwords from the text\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Apply lemmatization to the remaining words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    no_punct = [word for word in lemmatized_words if word not in string.punctuation]\n",
    "    \n",
    "    # Remove digits\n",
    "    no_digits = [word for word in no_punct if not re.match('\\d+', word)]\n",
    "    \n",
    "    # Lowercase all words\n",
    "    lowercase_words = [word.lower() for word in no_digits]\n",
    "    \n",
    "    # Join the cleaned words back into a string\n",
    "    cleaned_text = \" \".join(lowercase_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def process_csv(file):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    br_id = df['BR.BR_id']\n",
    "    report_time = df['BR.BRopenT']\n",
    "    summary = df['BR.BR_text.BRsummary']\n",
    "    description = df['BR.BR_text.BRdescription']\n",
    "    commit = df['commit.commit_id']\n",
    "    commit_time = df['commit.commitT']\n",
    "    new_file_selected_cols = df.filter(regex='^commit\\.changed_files\\.file_\\d+\\.file_new_name$')\n",
    "    new_file_selected_cols = df.apply(lambda row: ' '.join(filter(None, [str(row[col]) if str(row[col]) != 'None' and str(row[col]) != 'nan' else ' ' for col in new_file_selected_cols.columns])), axis=1)\n",
    "    final_df = pd.DataFrame({\"bug_id\": br_id, 'report_time': report_time, 'report_timestamp': report_time, 'summary': summary, 'description': description, 'commit': commit, 'commit_timestamp': commit_time, 'files': new_file_selected_cols})\n",
    "    final_df['status'] = 'resolved fixed'\n",
    "    final_df['report_time'] = final_df['report_time'].apply(format_datetime)\n",
    "    final_df['report_timestamp'] = final_df['report_timestamp'].apply(convert_to_timestamp)\n",
    "    final_df['commit_timestamp'] = final_df['commit_timestamp'].apply(convert_to_timestamp)\n",
    "    final_df['description'] = final_df['description'].apply(lambda x: preprocess_text(x))\n",
    "    \n",
    "    final_df = final_df[['bug_id', 'summary', 'description', 'report_time', 'report_timestamp', \n",
    "                         'status', 'commit', 'commit_timestamp', 'files']]\n",
    "    final_df.to_csv(file[:-4] + '_preprocessed.csv', index_label='id')\n",
    "\n",
    "def convert_csv_to_tsv(input_file):\n",
    "    output_file = input_file.split('.')[0] + '.tsv'\n",
    "    with open(input_file, 'r', encoding='utf8') as csvfile, open(output_file, 'w', newline='', encoding='utf8') as tsvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n",
    "\n",
    "files = ['tensorflow+tensorflow_raw.csv']\n",
    "\n",
    "for file in files:\n",
    "    process_csv(file)\n",
    "    convert_csv_to_tsv(file[:-4] + '_preprocessed.csv')\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def convert_tsv_to_txt(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as tsv_file, open(output_file, 'w', encoding='utf-8') as txt_file:\n",
    "        reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        \n",
    "        for row in reader:\n",
    "            txt_file.write('\\t'.join(row) + '\\n')\n",
    "\n",
    "# Example usage\n",
    "tsv_files = ['pytorchlightning+pytorch-lightning_raw_preprocessed.tsv']   # Replace with your TSV filenames\n",
    "\n",
    "for tsv_file in tsv_files:\n",
    "    output_txt = tsv_file.replace('.tsv', '.txt')  # Generates a .txt filename based on the .tsv filename\n",
    "    convert_tsv_to_txt(tsv_file, output_txt)\n",
    "\n",
    "print(\"Conversion complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
