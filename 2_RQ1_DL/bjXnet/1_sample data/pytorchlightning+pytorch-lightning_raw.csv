BR.BR_id,BR.BR_author,BR.BRopenT,BR.BRcloseT,BR.BR_text.BRsummary,BR.BR_text.BRdescription,commit.commit_id,commit.commit_author,commit.commitT,commit.changed_files.file_0.file_change_type,commit.changed_files.file_0.file_Nmethod,commit.changed_files.file_0.file_old_name,commit.changed_files.file_0.file_new_name,commit.changed_files.file_0.hunks.hunk_0.Ismethod,commit.changed_files.file_0.hunks.hunk_0.added_lines,commit.changed_files.file_0.hunks.hunk_0.deleted_lines,BR.comments.comments_0.comment_id,BR.comments.comments_0.comment_author,BR.comments.comments_0.commentT,BR.comments.comments_0.comment_text,BR.comments.comments_1.comment_id,BR.comments.comments_1.comment_author,BR.comments.comments_1.commentT,BR.comments.comments_1.comment_text,BR.comments.comments_2.comment_id,BR.comments.comments_2.comment_author,BR.comments.comments_2.commentT,BR.comments.comments_2.comment_text,BR.comments.comments_3.comment_id,BR.comments.comments_3.comment_author,BR.comments.comments_3.commentT,BR.comments.comments_3.comment_text,commit.changed_files.file_1.file_change_type,commit.changed_files.file_1.file_Nmethod,commit.changed_files.file_1.file_old_name,commit.changed_files.file_1.file_new_name,commit.changed_files.file_1.hunks.hunk_0.Ismethod,commit.changed_files.file_1.hunks.hunk_0.added_lines,commit.changed_files.file_1.hunks.hunk_0.deleted_lines,commit.changed_files.file_1.hunks.hunk_0.method_info.method_name,commit.changed_files.file_1.hunks.hunk_0.method_info.method_params,commit.changed_files.file_1.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_2.file_change_type,commit.changed_files.file_2.file_Nmethod,commit.changed_files.file_2.file_old_name,commit.changed_files.file_2.file_new_name,commit.changed_files.file_2.hunks.hunk_0.Ismethod,commit.changed_files.file_2.hunks.hunk_0.added_lines,commit.changed_files.file_2.hunks.hunk_0.deleted_lines,commit.changed_files.file_3.file_change_type,commit.changed_files.file_3.file_Nmethod,commit.changed_files.file_3.file_old_name,commit.changed_files.file_3.file_new_name,commit.changed_files.file_3.hunks.hunk_0.Ismethod,commit.changed_files.file_3.hunks.hunk_0.added_lines,commit.changed_files.file_3.hunks.hunk_0.deleted_lines,commit.changed_files.file_3.hunks.hunk_0.method_info.method_name,commit.changed_files.file_3.hunks.hunk_0.method_info.method_params,commit.changed_files.file_3.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_4.file_change_type,commit.changed_files.file_4.file_Nmethod,commit.changed_files.file_4.file_old_name,commit.changed_files.file_4.file_new_name,commit.changed_files.file_4.hunks.hunk_0.Ismethod,commit.changed_files.file_4.hunks.hunk_0.added_lines,commit.changed_files.file_4.hunks.hunk_0.deleted_lines,commit.changed_files.file_4.hunks.hunk_0.method_info.method_name,commit.changed_files.file_4.hunks.hunk_0.method_info.method_params,commit.changed_files.file_4.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_0.method_info.method_endline,BR.comments.comments_4.comment_id,BR.comments.comments_4.comment_author,BR.comments.comments_4.commentT,BR.comments.comments_4.comment_text,BR.comments.comments_5.comment_id,BR.comments.comments_5.comment_author,BR.comments.comments_5.commentT,BR.comments.comments_5.comment_text,BR.comments.comments_6.comment_id,BR.comments.comments_6.comment_author,BR.comments.comments_6.commentT,BR.comments.comments_6.comment_text,BR.comments.comments_7.comment_id,BR.comments.comments_7.comment_author,BR.comments.comments_7.commentT,BR.comments.comments_7.comment_text,BR.comments.comments_8.comment_id,BR.comments.comments_8.comment_author,BR.comments.comments_8.commentT,BR.comments.comments_8.comment_text,BR.comments.comments_9.comment_id,BR.comments.comments_9.comment_author,BR.comments.comments_9.commentT,BR.comments.comments_9.comment_text,BR.comments.comments_10.comment_id,BR.comments.comments_10.comment_author,BR.comments.comments_10.commentT,BR.comments.comments_10.comment_text,BR.comments.comments_11.comment_id,BR.comments.comments_11.comment_author,BR.comments.comments_11.commentT,BR.comments.comments_11.comment_text,BR.comments.comments_12.comment_id,BR.comments.comments_12.comment_author,BR.comments.comments_12.commentT,BR.comments.comments_12.comment_text,BR.comments.comments_13.comment_id,BR.comments.comments_13.comment_author,BR.comments.comments_13.commentT,BR.comments.comments_13.comment_text,BR.comments.comments_14.comment_id,BR.comments.comments_14.comment_author,BR.comments.comments_14.commentT,BR.comments.comments_14.comment_text,commit.changed_files.file_2.hunks.hunk_0.method_info.method_name,commit.changed_files.file_2.hunks.hunk_0.method_info.method_params,commit.changed_files.file_2.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_0.method_info.method_name,commit.changed_files.file_0.hunks.hunk_0.method_info.method_params,commit.changed_files.file_0.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_1.Ismethod,commit.changed_files.file_4.hunks.hunk_1.added_lines,commit.changed_files.file_4.hunks.hunk_1.deleted_lines,commit.changed_files.file_4.hunks.hunk_1.method_info.method_name,commit.changed_files.file_4.hunks.hunk_1.method_info.method_params,commit.changed_files.file_4.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_2.Ismethod,commit.changed_files.file_4.hunks.hunk_2.added_lines,commit.changed_files.file_4.hunks.hunk_2.deleted_lines,commit.changed_files.file_4.hunks.hunk_2.method_info.method_name,commit.changed_files.file_4.hunks.hunk_2.method_info.method_params,commit.changed_files.file_4.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_5.file_change_type,commit.changed_files.file_5.file_Nmethod,commit.changed_files.file_5.file_old_name,commit.changed_files.file_5.file_new_name,commit.changed_files.file_5.hunks.hunk_0.Ismethod,commit.changed_files.file_5.hunks.hunk_0.added_lines,commit.changed_files.file_5.hunks.hunk_0.deleted_lines,commit.changed_files.file_5.hunks.hunk_0.method_info.method_name,commit.changed_files.file_5.hunks.hunk_0.method_info.method_params,commit.changed_files.file_5.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_1.Ismethod,commit.changed_files.file_5.hunks.hunk_1.added_lines,commit.changed_files.file_5.hunks.hunk_1.deleted_lines,commit.changed_files.file_5.hunks.hunk_1.method_info.method_name,commit.changed_files.file_5.hunks.hunk_1.method_info.method_params,commit.changed_files.file_5.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_6.file_change_type,commit.changed_files.file_6.file_Nmethod,commit.changed_files.file_6.file_old_name,commit.changed_files.file_6.file_new_name,commit.changed_files.file_3.hunks.hunk_1.Ismethod,commit.changed_files.file_3.hunks.hunk_1.added_lines,commit.changed_files.file_3.hunks.hunk_1.deleted_lines,commit.changed_files.file_3.hunks.hunk_1.method_info.method_name,commit.changed_files.file_3.hunks.hunk_1.method_info.method_params,commit.changed_files.file_3.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_0.Ismethod,commit.changed_files.file_6.hunks.hunk_0.added_lines,commit.changed_files.file_6.hunks.hunk_0.deleted_lines,commit.changed_files.file_6.hunks.hunk_0.method_info.method_name,commit.changed_files.file_6.hunks.hunk_0.method_info.method_params,commit.changed_files.file_6.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_1.Ismethod,commit.changed_files.file_6.hunks.hunk_1.added_lines,commit.changed_files.file_6.hunks.hunk_1.deleted_lines,commit.changed_files.file_6.hunks.hunk_1.method_info.method_name,commit.changed_files.file_6.hunks.hunk_1.method_info.method_params,commit.changed_files.file_6.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_7.file_change_type,commit.changed_files.file_7.file_Nmethod,commit.changed_files.file_7.file_old_name,commit.changed_files.file_7.file_new_name,commit.changed_files.file_7.hunks.hunk_0.Ismethod,commit.changed_files.file_7.hunks.hunk_0.added_lines,commit.changed_files.file_7.hunks.hunk_0.deleted_lines,commit.changed_files.file_1.hunks.hunk_1.Ismethod,commit.changed_files.file_1.hunks.hunk_1.added_lines,commit.changed_files.file_1.hunks.hunk_1.deleted_lines,commit.changed_files.file_1.hunks.hunk_1.method_info.method_name,commit.changed_files.file_1.hunks.hunk_1.method_info.method_params,commit.changed_files.file_1.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_2.Ismethod,commit.changed_files.file_1.hunks.hunk_2.added_lines,commit.changed_files.file_1.hunks.hunk_2.deleted_lines,commit.changed_files.file_1.hunks.hunk_2.method_info.method_name,commit.changed_files.file_1.hunks.hunk_2.method_info.method_params,commit.changed_files.file_1.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_1.Ismethod,commit.changed_files.file_0.hunks.hunk_1.added_lines,commit.changed_files.file_0.hunks.hunk_1.deleted_lines,commit.changed_files.file_0.hunks.hunk_1.method_info.method_name,commit.changed_files.file_0.hunks.hunk_1.method_info.method_params,commit.changed_files.file_0.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_2.Ismethod,commit.changed_files.file_0.hunks.hunk_2.added_lines,commit.changed_files.file_0.hunks.hunk_2.deleted_lines,commit.changed_files.file_0.hunks.hunk_2.method_info.method_name,commit.changed_files.file_0.hunks.hunk_2.method_info.method_params,commit.changed_files.file_0.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_3.Ismethod,commit.changed_files.file_0.hunks.hunk_3.added_lines,commit.changed_files.file_0.hunks.hunk_3.deleted_lines,commit.changed_files.file_0.hunks.hunk_3.method_info.method_name,commit.changed_files.file_0.hunks.hunk_3.method_info.method_params,commit.changed_files.file_0.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_1.Ismethod,commit.changed_files.file_2.hunks.hunk_1.added_lines,commit.changed_files.file_2.hunks.hunk_1.deleted_lines,commit.changed_files.file_2.hunks.hunk_1.method_info.method_name,commit.changed_files.file_2.hunks.hunk_1.method_info.method_params,commit.changed_files.file_2.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_2.Ismethod,commit.changed_files.file_2.hunks.hunk_2.added_lines,commit.changed_files.file_2.hunks.hunk_2.deleted_lines,commit.changed_files.file_2.hunks.hunk_2.method_info.method_name,commit.changed_files.file_2.hunks.hunk_2.method_info.method_params,commit.changed_files.file_2.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_2.Ismethod,commit.changed_files.file_3.hunks.hunk_2.added_lines,commit.changed_files.file_3.hunks.hunk_2.deleted_lines,commit.changed_files.file_3.hunks.hunk_2.method_info.method_name,commit.changed_files.file_3.hunks.hunk_2.method_info.method_params,commit.changed_files.file_3.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_3.Ismethod,commit.changed_files.file_3.hunks.hunk_3.added_lines,commit.changed_files.file_3.hunks.hunk_3.deleted_lines,commit.changed_files.file_3.hunks.hunk_3.method_info.method_name,commit.changed_files.file_3.hunks.hunk_3.method_info.method_params,commit.changed_files.file_3.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_4.Ismethod,commit.changed_files.file_3.hunks.hunk_4.added_lines,commit.changed_files.file_3.hunks.hunk_4.deleted_lines,commit.changed_files.file_3.hunks.hunk_4.method_info.method_name,commit.changed_files.file_3.hunks.hunk_4.method_info.method_params,commit.changed_files.file_3.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_5.Ismethod,commit.changed_files.file_3.hunks.hunk_5.added_lines,commit.changed_files.file_3.hunks.hunk_5.deleted_lines,commit.changed_files.file_3.hunks.hunk_5.method_info.method_name,commit.changed_files.file_3.hunks.hunk_5.method_info.method_params,commit.changed_files.file_3.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_6.Ismethod,commit.changed_files.file_3.hunks.hunk_6.added_lines,commit.changed_files.file_3.hunks.hunk_6.deleted_lines,commit.changed_files.file_3.hunks.hunk_6.method_info.method_name,commit.changed_files.file_3.hunks.hunk_6.method_info.method_params,commit.changed_files.file_3.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_7.Ismethod,commit.changed_files.file_3.hunks.hunk_7.added_lines,commit.changed_files.file_3.hunks.hunk_7.deleted_lines,commit.changed_files.file_3.hunks.hunk_7.method_info.method_name,commit.changed_files.file_3.hunks.hunk_7.method_info.method_params,commit.changed_files.file_3.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_8.Ismethod,commit.changed_files.file_3.hunks.hunk_8.added_lines,commit.changed_files.file_3.hunks.hunk_8.deleted_lines,commit.changed_files.file_3.hunks.hunk_8.method_info.method_name,commit.changed_files.file_3.hunks.hunk_8.method_info.method_params,commit.changed_files.file_3.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_9.Ismethod,commit.changed_files.file_3.hunks.hunk_9.added_lines,commit.changed_files.file_3.hunks.hunk_9.deleted_lines,commit.changed_files.file_3.hunks.hunk_9.method_info.method_name,commit.changed_files.file_3.hunks.hunk_9.method_info.method_params,commit.changed_files.file_3.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_10.Ismethod,commit.changed_files.file_3.hunks.hunk_10.added_lines,commit.changed_files.file_3.hunks.hunk_10.deleted_lines,commit.changed_files.file_3.hunks.hunk_10.method_info.method_name,commit.changed_files.file_3.hunks.hunk_10.method_info.method_params,commit.changed_files.file_3.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_2.Ismethod,commit.changed_files.file_5.hunks.hunk_2.added_lines,commit.changed_files.file_5.hunks.hunk_2.deleted_lines,commit.changed_files.file_5.hunks.hunk_2.method_info.method_name,commit.changed_files.file_5.hunks.hunk_2.method_info.method_params,commit.changed_files.file_5.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_0.method_info.method_name,commit.changed_files.file_7.hunks.hunk_0.method_info.method_params,commit.changed_files.file_7.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_1.Ismethod,commit.changed_files.file_7.hunks.hunk_1.added_lines,commit.changed_files.file_7.hunks.hunk_1.deleted_lines,commit.changed_files.file_7.hunks.hunk_1.method_info.method_name,commit.changed_files.file_7.hunks.hunk_1.method_info.method_params,commit.changed_files.file_7.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_8.file_change_type,commit.changed_files.file_8.file_Nmethod,commit.changed_files.file_8.file_old_name,commit.changed_files.file_8.file_new_name,commit.changed_files.file_8.hunks.hunk_0.Ismethod,commit.changed_files.file_8.hunks.hunk_0.added_lines,commit.changed_files.file_8.hunks.hunk_0.deleted_lines,commit.changed_files.file_8.hunks.hunk_0.method_info.method_name,commit.changed_files.file_8.hunks.hunk_0.method_info.method_params,commit.changed_files.file_8.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_1.Ismethod,commit.changed_files.file_8.hunks.hunk_1.added_lines,commit.changed_files.file_8.hunks.hunk_1.deleted_lines,commit.changed_files.file_8.hunks.hunk_1.method_info.method_name,commit.changed_files.file_8.hunks.hunk_1.method_info.method_params,commit.changed_files.file_8.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_2.Ismethod,commit.changed_files.file_8.hunks.hunk_2.added_lines,commit.changed_files.file_8.hunks.hunk_2.deleted_lines,commit.changed_files.file_8.hunks.hunk_2.method_info.method_name,commit.changed_files.file_8.hunks.hunk_2.method_info.method_params,commit.changed_files.file_8.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_3.Ismethod,commit.changed_files.file_8.hunks.hunk_3.added_lines,commit.changed_files.file_8.hunks.hunk_3.deleted_lines,commit.changed_files.file_8.hunks.hunk_3.method_info.method_name,commit.changed_files.file_8.hunks.hunk_3.method_info.method_params,commit.changed_files.file_8.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_4.Ismethod,commit.changed_files.file_8.hunks.hunk_4.added_lines,commit.changed_files.file_8.hunks.hunk_4.deleted_lines,commit.changed_files.file_8.hunks.hunk_4.method_info.method_name,commit.changed_files.file_8.hunks.hunk_4.method_info.method_params,commit.changed_files.file_8.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_5.Ismethod,commit.changed_files.file_8.hunks.hunk_5.added_lines,commit.changed_files.file_8.hunks.hunk_5.deleted_lines,commit.changed_files.file_8.hunks.hunk_5.method_info.method_name,commit.changed_files.file_8.hunks.hunk_5.method_info.method_params,commit.changed_files.file_8.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_6.Ismethod,commit.changed_files.file_8.hunks.hunk_6.added_lines,commit.changed_files.file_8.hunks.hunk_6.deleted_lines,commit.changed_files.file_8.hunks.hunk_6.method_info.method_name,commit.changed_files.file_8.hunks.hunk_6.method_info.method_params,commit.changed_files.file_8.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_7.Ismethod,commit.changed_files.file_8.hunks.hunk_7.added_lines,commit.changed_files.file_8.hunks.hunk_7.deleted_lines,commit.changed_files.file_8.hunks.hunk_7.method_info.method_name,commit.changed_files.file_8.hunks.hunk_7.method_info.method_params,commit.changed_files.file_8.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_9.file_change_type,commit.changed_files.file_9.file_Nmethod,commit.changed_files.file_9.file_old_name,commit.changed_files.file_9.file_new_name,commit.changed_files.file_9.hunks.hunk_0.Ismethod,commit.changed_files.file_9.hunks.hunk_0.added_lines,commit.changed_files.file_9.hunks.hunk_0.deleted_lines,commit.changed_files.file_9.hunks.hunk_0.method_info.method_name,commit.changed_files.file_9.hunks.hunk_0.method_info.method_params,commit.changed_files.file_9.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_1.Ismethod,commit.changed_files.file_9.hunks.hunk_1.added_lines,commit.changed_files.file_9.hunks.hunk_1.deleted_lines,commit.changed_files.file_9.hunks.hunk_1.method_info.method_name,commit.changed_files.file_9.hunks.hunk_1.method_info.method_params,commit.changed_files.file_9.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_2.Ismethod,commit.changed_files.file_9.hunks.hunk_2.added_lines,commit.changed_files.file_9.hunks.hunk_2.deleted_lines,commit.changed_files.file_9.hunks.hunk_2.method_info.method_name,commit.changed_files.file_9.hunks.hunk_2.method_info.method_params,commit.changed_files.file_9.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_3.Ismethod,commit.changed_files.file_9.hunks.hunk_3.added_lines,commit.changed_files.file_9.hunks.hunk_3.deleted_lines,commit.changed_files.file_9.hunks.hunk_3.method_info.method_name,commit.changed_files.file_9.hunks.hunk_3.method_info.method_params,commit.changed_files.file_9.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_10.file_change_type,commit.changed_files.file_10.file_Nmethod,commit.changed_files.file_10.file_old_name,commit.changed_files.file_10.file_new_name,commit.changed_files.file_10.hunks.hunk_0.Ismethod,commit.changed_files.file_10.hunks.hunk_0.added_lines,commit.changed_files.file_10.hunks.hunk_0.deleted_lines,commit.changed_files.file_11.file_change_type,commit.changed_files.file_11.file_Nmethod,commit.changed_files.file_11.file_old_name,commit.changed_files.file_11.file_new_name,commit.changed_files.file_11.hunks.hunk_0.Ismethod,commit.changed_files.file_11.hunks.hunk_0.added_lines,commit.changed_files.file_11.hunks.hunk_0.deleted_lines,commit.changed_files.file_11.hunks.hunk_0.method_info.method_name,commit.changed_files.file_11.hunks.hunk_0.method_info.method_params,commit.changed_files.file_11.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_1.Ismethod,commit.changed_files.file_11.hunks.hunk_1.added_lines,commit.changed_files.file_11.hunks.hunk_1.deleted_lines,commit.changed_files.file_11.hunks.hunk_1.method_info.method_name,commit.changed_files.file_11.hunks.hunk_1.method_info.method_params,commit.changed_files.file_11.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_12.file_change_type,commit.changed_files.file_12.file_Nmethod,commit.changed_files.file_12.file_old_name,commit.changed_files.file_12.file_new_name,commit.changed_files.file_12.hunks.hunk_0.Ismethod,commit.changed_files.file_12.hunks.hunk_0.added_lines,commit.changed_files.file_12.hunks.hunk_0.deleted_lines,commit.changed_files.file_12.hunks.hunk_0.method_info.method_name,commit.changed_files.file_12.hunks.hunk_0.method_info.method_params,commit.changed_files.file_12.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_12.hunks.hunk_1.Ismethod,commit.changed_files.file_12.hunks.hunk_1.added_lines,commit.changed_files.file_12.hunks.hunk_1.deleted_lines,commit.changed_files.file_12.hunks.hunk_1.method_info.method_name,commit.changed_files.file_12.hunks.hunk_1.method_info.method_params,commit.changed_files.file_12.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_12.hunks.hunk_2.Ismethod,commit.changed_files.file_12.hunks.hunk_2.added_lines,commit.changed_files.file_12.hunks.hunk_2.deleted_lines,commit.changed_files.file_12.hunks.hunk_2.method_info.method_name,commit.changed_files.file_12.hunks.hunk_2.method_info.method_params,commit.changed_files.file_12.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_12.hunks.hunk_3.Ismethod,commit.changed_files.file_12.hunks.hunk_3.added_lines,commit.changed_files.file_12.hunks.hunk_3.deleted_lines,commit.changed_files.file_12.hunks.hunk_3.method_info.method_name,commit.changed_files.file_12.hunks.hunk_3.method_info.method_params,commit.changed_files.file_12.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_12.hunks.hunk_4.Ismethod,commit.changed_files.file_12.hunks.hunk_4.added_lines,commit.changed_files.file_12.hunks.hunk_4.deleted_lines,commit.changed_files.file_12.hunks.hunk_4.method_info.method_name,commit.changed_files.file_12.hunks.hunk_4.method_info.method_params,commit.changed_files.file_12.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_12.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_13.file_change_type,commit.changed_files.file_13.file_Nmethod,commit.changed_files.file_13.file_old_name,commit.changed_files.file_13.file_new_name,commit.changed_files.file_13.hunks.hunk_0.Ismethod,commit.changed_files.file_13.hunks.hunk_0.added_lines,commit.changed_files.file_13.hunks.hunk_0.deleted_lines,commit.changed_files.file_14.file_change_type,commit.changed_files.file_14.file_Nmethod,commit.changed_files.file_14.file_old_name,commit.changed_files.file_14.file_new_name,commit.changed_files.file_14.hunks.hunk_0.Ismethod,commit.changed_files.file_14.hunks.hunk_0.added_lines,commit.changed_files.file_14.hunks.hunk_0.deleted_lines,commit.changed_files.file_14.hunks.hunk_0.method_info.method_name,commit.changed_files.file_14.hunks.hunk_0.method_info.method_params,commit.changed_files.file_14.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_14.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_14.hunks.hunk_1.Ismethod,commit.changed_files.file_14.hunks.hunk_1.added_lines,commit.changed_files.file_14.hunks.hunk_1.deleted_lines,commit.changed_files.file_14.hunks.hunk_1.method_info.method_name,commit.changed_files.file_14.hunks.hunk_1.method_info.method_params,commit.changed_files.file_14.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_14.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_13.hunks.hunk_0.method_info.method_name,commit.changed_files.file_13.hunks.hunk_0.method_info.method_params,commit.changed_files.file_13.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_13.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_15.file_change_type,commit.changed_files.file_15.file_Nmethod,commit.changed_files.file_15.file_old_name,commit.changed_files.file_15.file_new_name,commit.changed_files.file_16.file_change_type,commit.changed_files.file_16.file_Nmethod,commit.changed_files.file_16.file_old_name,commit.changed_files.file_16.file_new_name,commit.changed_files.file_17.file_change_type,commit.changed_files.file_17.file_Nmethod,commit.changed_files.file_17.file_old_name,commit.changed_files.file_17.file_new_name,commit.changed_files.file_17.hunks.hunk_0.Ismethod,commit.changed_files.file_17.hunks.hunk_0.added_lines,commit.changed_files.file_17.hunks.hunk_0.deleted_lines,commit.changed_files.file_2.hunks.hunk_3.Ismethod,commit.changed_files.file_2.hunks.hunk_3.added_lines,commit.changed_files.file_2.hunks.hunk_3.deleted_lines,commit.changed_files.file_2.hunks.hunk_3.method_info.method_name,commit.changed_files.file_2.hunks.hunk_3.method_info.method_params,commit.changed_files.file_2.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_4.Ismethod,commit.changed_files.file_2.hunks.hunk_4.added_lines,commit.changed_files.file_2.hunks.hunk_4.deleted_lines,commit.changed_files.file_2.hunks.hunk_4.method_info.method_name,commit.changed_files.file_2.hunks.hunk_4.method_info.method_params,commit.changed_files.file_2.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_5.Ismethod,commit.changed_files.file_2.hunks.hunk_5.added_lines,commit.changed_files.file_2.hunks.hunk_5.deleted_lines,commit.changed_files.file_2.hunks.hunk_5.method_info.method_name,commit.changed_files.file_2.hunks.hunk_5.method_info.method_params,commit.changed_files.file_2.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_3.Ismethod,commit.changed_files.file_4.hunks.hunk_3.added_lines,commit.changed_files.file_4.hunks.hunk_3.deleted_lines,commit.changed_files.file_4.hunks.hunk_3.method_info.method_name,commit.changed_files.file_4.hunks.hunk_3.method_info.method_params,commit.changed_files.file_4.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_4.Ismethod,commit.changed_files.file_4.hunks.hunk_4.added_lines,commit.changed_files.file_4.hunks.hunk_4.deleted_lines,commit.changed_files.file_4.hunks.hunk_4.method_info.method_name,commit.changed_files.file_4.hunks.hunk_4.method_info.method_params,commit.changed_files.file_4.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_2.Ismethod,commit.changed_files.file_6.hunks.hunk_2.added_lines,commit.changed_files.file_6.hunks.hunk_2.deleted_lines,commit.changed_files.file_6.hunks.hunk_2.method_info.method_name,commit.changed_files.file_6.hunks.hunk_2.method_info.method_params,commit.changed_files.file_6.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_3.Ismethod,commit.changed_files.file_6.hunks.hunk_3.added_lines,commit.changed_files.file_6.hunks.hunk_3.deleted_lines,commit.changed_files.file_6.hunks.hunk_3.method_info.method_name,commit.changed_files.file_6.hunks.hunk_3.method_info.method_params,commit.changed_files.file_6.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_2.Ismethod,commit.changed_files.file_7.hunks.hunk_2.added_lines,commit.changed_files.file_7.hunks.hunk_2.deleted_lines,commit.changed_files.file_7.hunks.hunk_2.method_info.method_name,commit.changed_files.file_7.hunks.hunk_2.method_info.method_params,commit.changed_files.file_7.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_3.Ismethod,commit.changed_files.file_7.hunks.hunk_3.added_lines,commit.changed_files.file_7.hunks.hunk_3.deleted_lines,commit.changed_files.file_7.hunks.hunk_3.method_info.method_name,commit.changed_files.file_7.hunks.hunk_3.method_info.method_params,commit.changed_files.file_7.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_4.Ismethod,commit.changed_files.file_7.hunks.hunk_4.added_lines,commit.changed_files.file_7.hunks.hunk_4.deleted_lines,commit.changed_files.file_7.hunks.hunk_4.method_info.method_name,commit.changed_files.file_7.hunks.hunk_4.method_info.method_params,commit.changed_files.file_7.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_0.method_info.method_name,commit.changed_files.file_10.hunks.hunk_0.method_info.method_params,commit.changed_files.file_10.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_14.hunks.hunk_2.Ismethod,commit.changed_files.file_14.hunks.hunk_2.added_lines,commit.changed_files.file_14.hunks.hunk_2.deleted_lines,commit.changed_files.file_14.hunks.hunk_2.method_info.method_name,commit.changed_files.file_14.hunks.hunk_2.method_info.method_params,commit.changed_files.file_14.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_14.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_14.hunks.hunk_3.Ismethod,commit.changed_files.file_14.hunks.hunk_3.added_lines,commit.changed_files.file_14.hunks.hunk_3.deleted_lines,commit.changed_files.file_14.hunks.hunk_3.method_info.method_name,commit.changed_files.file_14.hunks.hunk_3.method_info.method_params,commit.changed_files.file_14.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_14.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_14.hunks.hunk_4.Ismethod,commit.changed_files.file_14.hunks.hunk_4.added_lines,commit.changed_files.file_14.hunks.hunk_4.deleted_lines,commit.changed_files.file_14.hunks.hunk_4.method_info.method_name,commit.changed_files.file_14.hunks.hunk_4.method_info.method_params,commit.changed_files.file_14.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_14.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_15.hunks.hunk_0.Ismethod,commit.changed_files.file_15.hunks.hunk_0.added_lines,commit.changed_files.file_15.hunks.hunk_0.deleted_lines,commit.changed_files.file_15.hunks.hunk_0.method_info.method_name,commit.changed_files.file_15.hunks.hunk_0.method_info.method_params,commit.changed_files.file_15.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_15.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_15.hunks.hunk_1.Ismethod,commit.changed_files.file_15.hunks.hunk_1.added_lines,commit.changed_files.file_15.hunks.hunk_1.deleted_lines,commit.changed_files.file_15.hunks.hunk_1.method_info.method_name,commit.changed_files.file_15.hunks.hunk_1.method_info.method_params,commit.changed_files.file_15.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_15.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_0.Ismethod,commit.changed_files.file_16.hunks.hunk_0.added_lines,commit.changed_files.file_16.hunks.hunk_0.deleted_lines,commit.changed_files.file_16.hunks.hunk_0.method_info.method_name,commit.changed_files.file_16.hunks.hunk_0.method_info.method_params,commit.changed_files.file_16.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_1.Ismethod,commit.changed_files.file_16.hunks.hunk_1.added_lines,commit.changed_files.file_16.hunks.hunk_1.deleted_lines,commit.changed_files.file_16.hunks.hunk_1.method_info.method_name,commit.changed_files.file_16.hunks.hunk_1.method_info.method_params,commit.changed_files.file_16.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_2.Ismethod,commit.changed_files.file_16.hunks.hunk_2.added_lines,commit.changed_files.file_16.hunks.hunk_2.deleted_lines,commit.changed_files.file_16.hunks.hunk_2.method_info.method_name,commit.changed_files.file_16.hunks.hunk_2.method_info.method_params,commit.changed_files.file_16.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_0.method_info.method_name,commit.changed_files.file_17.hunks.hunk_0.method_info.method_params,commit.changed_files.file_17.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_1.Ismethod,commit.changed_files.file_17.hunks.hunk_1.added_lines,commit.changed_files.file_17.hunks.hunk_1.deleted_lines,commit.changed_files.file_17.hunks.hunk_1.method_info.method_name,commit.changed_files.file_17.hunks.hunk_1.method_info.method_params,commit.changed_files.file_17.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_2.Ismethod,commit.changed_files.file_17.hunks.hunk_2.added_lines,commit.changed_files.file_17.hunks.hunk_2.deleted_lines,commit.changed_files.file_17.hunks.hunk_2.method_info.method_name,commit.changed_files.file_17.hunks.hunk_2.method_info.method_params,commit.changed_files.file_17.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_3.Ismethod,commit.changed_files.file_17.hunks.hunk_3.added_lines,commit.changed_files.file_17.hunks.hunk_3.deleted_lines,commit.changed_files.file_17.hunks.hunk_3.method_info.method_name,commit.changed_files.file_17.hunks.hunk_3.method_info.method_params,commit.changed_files.file_17.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_18.file_change_type,commit.changed_files.file_18.file_Nmethod,commit.changed_files.file_18.file_old_name,commit.changed_files.file_18.file_new_name,commit.changed_files.file_18.hunks.hunk_0.Ismethod,commit.changed_files.file_18.hunks.hunk_0.added_lines,commit.changed_files.file_18.hunks.hunk_0.deleted_lines,commit.changed_files.file_18.hunks.hunk_0.method_info.method_name,commit.changed_files.file_18.hunks.hunk_0.method_info.method_params,commit.changed_files.file_18.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_18.hunks.hunk_1.Ismethod,commit.changed_files.file_18.hunks.hunk_1.added_lines,commit.changed_files.file_18.hunks.hunk_1.deleted_lines,commit.changed_files.file_18.hunks.hunk_1.method_info.method_name,commit.changed_files.file_18.hunks.hunk_1.method_info.method_params,commit.changed_files.file_18.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_18.hunks.hunk_2.Ismethod,commit.changed_files.file_18.hunks.hunk_2.added_lines,commit.changed_files.file_18.hunks.hunk_2.deleted_lines,commit.changed_files.file_18.hunks.hunk_2.method_info.method_name,commit.changed_files.file_18.hunks.hunk_2.method_info.method_params,commit.changed_files.file_18.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_18.hunks.hunk_3.Ismethod,commit.changed_files.file_18.hunks.hunk_3.added_lines,commit.changed_files.file_18.hunks.hunk_3.deleted_lines,commit.changed_files.file_18.hunks.hunk_3.method_info.method_name,commit.changed_files.file_18.hunks.hunk_3.method_info.method_params,commit.changed_files.file_18.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_18.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_19.file_change_type,commit.changed_files.file_19.file_Nmethod,commit.changed_files.file_19.file_old_name,commit.changed_files.file_19.file_new_name,commit.changed_files.file_20.file_change_type,commit.changed_files.file_20.file_Nmethod,commit.changed_files.file_20.file_old_name,commit.changed_files.file_20.file_new_name,commit.changed_files.file_20.hunks.hunk_0.Ismethod,commit.changed_files.file_20.hunks.hunk_0.added_lines,commit.changed_files.file_20.hunks.hunk_0.deleted_lines,commit.changed_files.file_20.hunks.hunk_0.method_info.method_name,commit.changed_files.file_20.hunks.hunk_0.method_info.method_params,commit.changed_files.file_20.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_0.method_info.method_endline,BR.comments.comments_15.comment_id,BR.comments.comments_15.comment_author,BR.comments.comments_15.commentT,BR.comments.comments_15.comment_text,BR.comments.comments_16.comment_id,BR.comments.comments_16.comment_author,BR.comments.comments_16.commentT,BR.comments.comments_16.comment_text,BR.comments.comments_17.comment_id,BR.comments.comments_17.comment_author,BR.comments.comments_17.commentT,BR.comments.comments_17.comment_text,BR.comments.comments_18.comment_id,BR.comments.comments_18.comment_author,BR.comments.comments_18.commentT,BR.comments.comments_18.comment_text,commit.changed_files.file_1.hunks.hunk_3.Ismethod,commit.changed_files.file_1.hunks.hunk_3.added_lines,commit.changed_files.file_1.hunks.hunk_3.deleted_lines,commit.changed_files.file_1.hunks.hunk_3.method_info.method_name,commit.changed_files.file_1.hunks.hunk_3.method_info.method_params,commit.changed_files.file_1.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_4.Ismethod,commit.changed_files.file_1.hunks.hunk_4.added_lines,commit.changed_files.file_1.hunks.hunk_4.deleted_lines,commit.changed_files.file_1.hunks.hunk_4.method_info.method_name,commit.changed_files.file_1.hunks.hunk_4.method_info.method_params,commit.changed_files.file_1.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_5.Ismethod,commit.changed_files.file_1.hunks.hunk_5.added_lines,commit.changed_files.file_1.hunks.hunk_5.deleted_lines,commit.changed_files.file_1.hunks.hunk_5.method_info.method_name,commit.changed_files.file_1.hunks.hunk_5.method_info.method_params,commit.changed_files.file_1.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_4.Ismethod,commit.changed_files.file_6.hunks.hunk_4.added_lines,commit.changed_files.file_6.hunks.hunk_4.deleted_lines,commit.changed_files.file_6.hunks.hunk_4.method_info.method_name,commit.changed_files.file_6.hunks.hunk_4.method_info.method_params,commit.changed_files.file_6.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_5.Ismethod,commit.changed_files.file_6.hunks.hunk_5.added_lines,commit.changed_files.file_6.hunks.hunk_5.deleted_lines,commit.changed_files.file_6.hunks.hunk_5.method_info.method_name,commit.changed_files.file_6.hunks.hunk_5.method_info.method_params,commit.changed_files.file_6.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_6.Ismethod,commit.changed_files.file_1.hunks.hunk_6.added_lines,commit.changed_files.file_1.hunks.hunk_6.deleted_lines,commit.changed_files.file_1.hunks.hunk_6.method_info.method_name,commit.changed_files.file_1.hunks.hunk_6.method_info.method_params,commit.changed_files.file_1.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_7.Ismethod,commit.changed_files.file_1.hunks.hunk_7.added_lines,commit.changed_files.file_1.hunks.hunk_7.deleted_lines,commit.changed_files.file_1.hunks.hunk_7.method_info.method_name,commit.changed_files.file_1.hunks.hunk_7.method_info.method_params,commit.changed_files.file_1.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_8.Ismethod,commit.changed_files.file_1.hunks.hunk_8.added_lines,commit.changed_files.file_1.hunks.hunk_8.deleted_lines,commit.changed_files.file_1.hunks.hunk_8.method_info.method_name,commit.changed_files.file_1.hunks.hunk_8.method_info.method_params,commit.changed_files.file_1.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_9.Ismethod,commit.changed_files.file_1.hunks.hunk_9.added_lines,commit.changed_files.file_1.hunks.hunk_9.deleted_lines,commit.changed_files.file_1.hunks.hunk_9.method_info.method_name,commit.changed_files.file_1.hunks.hunk_9.method_info.method_params,commit.changed_files.file_1.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_10.Ismethod,commit.changed_files.file_1.hunks.hunk_10.added_lines,commit.changed_files.file_1.hunks.hunk_10.deleted_lines,commit.changed_files.file_1.hunks.hunk_10.method_info.method_name,commit.changed_files.file_1.hunks.hunk_10.method_info.method_params,commit.changed_files.file_1.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_11.Ismethod,commit.changed_files.file_1.hunks.hunk_11.added_lines,commit.changed_files.file_1.hunks.hunk_11.deleted_lines,commit.changed_files.file_1.hunks.hunk_11.method_info.method_name,commit.changed_files.file_1.hunks.hunk_11.method_info.method_params,commit.changed_files.file_1.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_12.Ismethod,commit.changed_files.file_1.hunks.hunk_12.added_lines,commit.changed_files.file_1.hunks.hunk_12.deleted_lines,commit.changed_files.file_1.hunks.hunk_12.method_info.method_name,commit.changed_files.file_1.hunks.hunk_12.method_info.method_params,commit.changed_files.file_1.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_13.Ismethod,commit.changed_files.file_1.hunks.hunk_13.added_lines,commit.changed_files.file_1.hunks.hunk_13.deleted_lines,commit.changed_files.file_1.hunks.hunk_13.method_info.method_name,commit.changed_files.file_1.hunks.hunk_13.method_info.method_params,commit.changed_files.file_1.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_4.Ismethod,commit.changed_files.file_0.hunks.hunk_4.added_lines,commit.changed_files.file_0.hunks.hunk_4.deleted_lines,commit.changed_files.file_0.hunks.hunk_4.method_info.method_name,commit.changed_files.file_0.hunks.hunk_4.method_info.method_params,commit.changed_files.file_0.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_5.Ismethod,commit.changed_files.file_0.hunks.hunk_5.added_lines,commit.changed_files.file_0.hunks.hunk_5.deleted_lines,commit.changed_files.file_0.hunks.hunk_5.method_info.method_name,commit.changed_files.file_0.hunks.hunk_5.method_info.method_params,commit.changed_files.file_0.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_6.Ismethod,commit.changed_files.file_0.hunks.hunk_6.added_lines,commit.changed_files.file_0.hunks.hunk_6.deleted_lines,commit.changed_files.file_0.hunks.hunk_6.method_info.method_name,commit.changed_files.file_0.hunks.hunk_6.method_info.method_params,commit.changed_files.file_0.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_7.Ismethod,commit.changed_files.file_0.hunks.hunk_7.added_lines,commit.changed_files.file_0.hunks.hunk_7.deleted_lines,commit.changed_files.file_0.hunks.hunk_7.method_info.method_name,commit.changed_files.file_0.hunks.hunk_7.method_info.method_params,commit.changed_files.file_0.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_8.Ismethod,commit.changed_files.file_0.hunks.hunk_8.added_lines,commit.changed_files.file_0.hunks.hunk_8.deleted_lines,commit.changed_files.file_0.hunks.hunk_8.method_info.method_name,commit.changed_files.file_0.hunks.hunk_8.method_info.method_params,commit.changed_files.file_0.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_9.Ismethod,commit.changed_files.file_0.hunks.hunk_9.added_lines,commit.changed_files.file_0.hunks.hunk_9.deleted_lines,commit.changed_files.file_0.hunks.hunk_9.method_info.method_name,commit.changed_files.file_0.hunks.hunk_9.method_info.method_params,commit.changed_files.file_0.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_10.Ismethod,commit.changed_files.file_0.hunks.hunk_10.added_lines,commit.changed_files.file_0.hunks.hunk_10.deleted_lines,commit.changed_files.file_0.hunks.hunk_10.method_info.method_name,commit.changed_files.file_0.hunks.hunk_10.method_info.method_params,commit.changed_files.file_0.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_0.hunks.hunk_11.Ismethod,commit.changed_files.file_0.hunks.hunk_11.added_lines,commit.changed_files.file_0.hunks.hunk_11.deleted_lines,commit.changed_files.file_0.hunks.hunk_11.method_info.method_name,commit.changed_files.file_0.hunks.hunk_11.method_info.method_params,commit.changed_files.file_0.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_0.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_1.Ismethod,commit.changed_files.file_10.hunks.hunk_1.added_lines,commit.changed_files.file_10.hunks.hunk_1.deleted_lines,commit.changed_files.file_10.hunks.hunk_1.method_info.method_name,commit.changed_files.file_10.hunks.hunk_1.method_info.method_params,commit.changed_files.file_10.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_6.Ismethod,commit.changed_files.file_2.hunks.hunk_6.added_lines,commit.changed_files.file_2.hunks.hunk_6.deleted_lines,commit.changed_files.file_2.hunks.hunk_6.method_info.method_name,commit.changed_files.file_2.hunks.hunk_6.method_info.method_params,commit.changed_files.file_2.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_7.Ismethod,commit.changed_files.file_2.hunks.hunk_7.added_lines,commit.changed_files.file_2.hunks.hunk_7.deleted_lines,commit.changed_files.file_2.hunks.hunk_7.method_info.method_name,commit.changed_files.file_2.hunks.hunk_7.method_info.method_params,commit.changed_files.file_2.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_8.Ismethod,commit.changed_files.file_2.hunks.hunk_8.added_lines,commit.changed_files.file_2.hunks.hunk_8.deleted_lines,commit.changed_files.file_2.hunks.hunk_8.method_info.method_name,commit.changed_files.file_2.hunks.hunk_8.method_info.method_params,commit.changed_files.file_2.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_9.Ismethod,commit.changed_files.file_2.hunks.hunk_9.added_lines,commit.changed_files.file_2.hunks.hunk_9.deleted_lines,commit.changed_files.file_2.hunks.hunk_9.method_info.method_name,commit.changed_files.file_2.hunks.hunk_9.method_info.method_params,commit.changed_files.file_2.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_10.Ismethod,commit.changed_files.file_2.hunks.hunk_10.added_lines,commit.changed_files.file_2.hunks.hunk_10.deleted_lines,commit.changed_files.file_2.hunks.hunk_10.method_info.method_name,commit.changed_files.file_2.hunks.hunk_10.method_info.method_params,commit.changed_files.file_2.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_5.Ismethod,commit.changed_files.file_7.hunks.hunk_5.added_lines,commit.changed_files.file_7.hunks.hunk_5.deleted_lines,commit.changed_files.file_7.hunks.hunk_5.method_info.method_name,commit.changed_files.file_7.hunks.hunk_5.method_info.method_params,commit.changed_files.file_7.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_6.Ismethod,commit.changed_files.file_7.hunks.hunk_6.added_lines,commit.changed_files.file_7.hunks.hunk_6.deleted_lines,commit.changed_files.file_7.hunks.hunk_6.method_info.method_name,commit.changed_files.file_7.hunks.hunk_6.method_info.method_params,commit.changed_files.file_7.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_7.Ismethod,commit.changed_files.file_7.hunks.hunk_7.added_lines,commit.changed_files.file_7.hunks.hunk_7.deleted_lines,commit.changed_files.file_7.hunks.hunk_7.method_info.method_name,commit.changed_files.file_7.hunks.hunk_7.method_info.method_params,commit.changed_files.file_7.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_2.Ismethod,commit.changed_files.file_11.hunks.hunk_2.added_lines,commit.changed_files.file_11.hunks.hunk_2.deleted_lines,commit.changed_files.file_11.hunks.hunk_2.method_info.method_name,commit.changed_files.file_11.hunks.hunk_2.method_info.method_params,commit.changed_files.file_11.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_3.Ismethod,commit.changed_files.file_11.hunks.hunk_3.added_lines,commit.changed_files.file_11.hunks.hunk_3.deleted_lines,commit.changed_files.file_11.hunks.hunk_3.method_info.method_name,commit.changed_files.file_11.hunks.hunk_3.method_info.method_params,commit.changed_files.file_11.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_4.Ismethod,commit.changed_files.file_11.hunks.hunk_4.added_lines,commit.changed_files.file_11.hunks.hunk_4.deleted_lines,commit.changed_files.file_11.hunks.hunk_4.method_info.method_name,commit.changed_files.file_11.hunks.hunk_4.method_info.method_params,commit.changed_files.file_11.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_5.Ismethod,commit.changed_files.file_11.hunks.hunk_5.added_lines,commit.changed_files.file_11.hunks.hunk_5.deleted_lines,commit.changed_files.file_11.hunks.hunk_5.method_info.method_name,commit.changed_files.file_11.hunks.hunk_5.method_info.method_params,commit.changed_files.file_11.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_6.Ismethod,commit.changed_files.file_11.hunks.hunk_6.added_lines,commit.changed_files.file_11.hunks.hunk_6.deleted_lines,commit.changed_files.file_11.hunks.hunk_6.method_info.method_name,commit.changed_files.file_11.hunks.hunk_6.method_info.method_params,commit.changed_files.file_11.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_7.Ismethod,commit.changed_files.file_11.hunks.hunk_7.added_lines,commit.changed_files.file_11.hunks.hunk_7.deleted_lines,commit.changed_files.file_11.hunks.hunk_7.method_info.method_name,commit.changed_files.file_11.hunks.hunk_7.method_info.method_params,commit.changed_files.file_11.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_8.Ismethod,commit.changed_files.file_11.hunks.hunk_8.added_lines,commit.changed_files.file_11.hunks.hunk_8.deleted_lines,commit.changed_files.file_11.hunks.hunk_8.method_info.method_name,commit.changed_files.file_11.hunks.hunk_8.method_info.method_params,commit.changed_files.file_11.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_9.Ismethod,commit.changed_files.file_11.hunks.hunk_9.added_lines,commit.changed_files.file_11.hunks.hunk_9.deleted_lines,commit.changed_files.file_11.hunks.hunk_9.method_info.method_name,commit.changed_files.file_11.hunks.hunk_9.method_info.method_params,commit.changed_files.file_11.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_10.Ismethod,commit.changed_files.file_11.hunks.hunk_10.added_lines,commit.changed_files.file_11.hunks.hunk_10.deleted_lines,commit.changed_files.file_11.hunks.hunk_10.method_info.method_name,commit.changed_files.file_11.hunks.hunk_10.method_info.method_params,commit.changed_files.file_11.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_11.Ismethod,commit.changed_files.file_11.hunks.hunk_11.added_lines,commit.changed_files.file_11.hunks.hunk_11.deleted_lines,commit.changed_files.file_11.hunks.hunk_11.method_info.method_name,commit.changed_files.file_11.hunks.hunk_11.method_info.method_params,commit.changed_files.file_11.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_12.Ismethod,commit.changed_files.file_11.hunks.hunk_12.added_lines,commit.changed_files.file_11.hunks.hunk_12.deleted_lines,commit.changed_files.file_11.hunks.hunk_12.method_info.method_name,commit.changed_files.file_11.hunks.hunk_12.method_info.method_params,commit.changed_files.file_11.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_13.Ismethod,commit.changed_files.file_11.hunks.hunk_13.added_lines,commit.changed_files.file_11.hunks.hunk_13.deleted_lines,commit.changed_files.file_11.hunks.hunk_13.method_info.method_name,commit.changed_files.file_11.hunks.hunk_13.method_info.method_params,commit.changed_files.file_11.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_14.Ismethod,commit.changed_files.file_11.hunks.hunk_14.added_lines,commit.changed_files.file_11.hunks.hunk_14.deleted_lines,commit.changed_files.file_11.hunks.hunk_14.method_info.method_name,commit.changed_files.file_11.hunks.hunk_14.method_info.method_params,commit.changed_files.file_11.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_15.Ismethod,commit.changed_files.file_11.hunks.hunk_15.added_lines,commit.changed_files.file_11.hunks.hunk_15.deleted_lines,commit.changed_files.file_11.hunks.hunk_15.method_info.method_name,commit.changed_files.file_11.hunks.hunk_15.method_info.method_params,commit.changed_files.file_11.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_16.Ismethod,commit.changed_files.file_11.hunks.hunk_16.added_lines,commit.changed_files.file_11.hunks.hunk_16.deleted_lines,commit.changed_files.file_11.hunks.hunk_16.method_info.method_name,commit.changed_files.file_11.hunks.hunk_16.method_info.method_params,commit.changed_files.file_11.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_17.Ismethod,commit.changed_files.file_11.hunks.hunk_17.added_lines,commit.changed_files.file_11.hunks.hunk_17.deleted_lines,commit.changed_files.file_11.hunks.hunk_17.method_info.method_name,commit.changed_files.file_11.hunks.hunk_17.method_info.method_params,commit.changed_files.file_11.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_18.Ismethod,commit.changed_files.file_11.hunks.hunk_18.added_lines,commit.changed_files.file_11.hunks.hunk_18.deleted_lines,commit.changed_files.file_11.hunks.hunk_18.method_info.method_name,commit.changed_files.file_11.hunks.hunk_18.method_info.method_params,commit.changed_files.file_11.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_19.Ismethod,commit.changed_files.file_11.hunks.hunk_19.added_lines,commit.changed_files.file_11.hunks.hunk_19.deleted_lines,commit.changed_files.file_11.hunks.hunk_19.method_info.method_name,commit.changed_files.file_11.hunks.hunk_19.method_info.method_params,commit.changed_files.file_11.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_20.Ismethod,commit.changed_files.file_11.hunks.hunk_20.added_lines,commit.changed_files.file_11.hunks.hunk_20.deleted_lines,commit.changed_files.file_11.hunks.hunk_20.method_info.method_name,commit.changed_files.file_11.hunks.hunk_20.method_info.method_params,commit.changed_files.file_11.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_21.Ismethod,commit.changed_files.file_11.hunks.hunk_21.added_lines,commit.changed_files.file_11.hunks.hunk_21.deleted_lines,commit.changed_files.file_11.hunks.hunk_21.method_info.method_name,commit.changed_files.file_11.hunks.hunk_21.method_info.method_params,commit.changed_files.file_11.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_22.Ismethod,commit.changed_files.file_11.hunks.hunk_22.added_lines,commit.changed_files.file_11.hunks.hunk_22.deleted_lines,commit.changed_files.file_11.hunks.hunk_22.method_info.method_name,commit.changed_files.file_11.hunks.hunk_22.method_info.method_params,commit.changed_files.file_11.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_23.Ismethod,commit.changed_files.file_11.hunks.hunk_23.added_lines,commit.changed_files.file_11.hunks.hunk_23.deleted_lines,commit.changed_files.file_11.hunks.hunk_23.method_info.method_name,commit.changed_files.file_11.hunks.hunk_23.method_info.method_params,commit.changed_files.file_11.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_24.Ismethod,commit.changed_files.file_11.hunks.hunk_24.added_lines,commit.changed_files.file_11.hunks.hunk_24.deleted_lines,commit.changed_files.file_11.hunks.hunk_24.method_info.method_name,commit.changed_files.file_11.hunks.hunk_24.method_info.method_params,commit.changed_files.file_11.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_25.Ismethod,commit.changed_files.file_11.hunks.hunk_25.added_lines,commit.changed_files.file_11.hunks.hunk_25.deleted_lines,commit.changed_files.file_11.hunks.hunk_25.method_info.method_name,commit.changed_files.file_11.hunks.hunk_25.method_info.method_params,commit.changed_files.file_11.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_26.Ismethod,commit.changed_files.file_11.hunks.hunk_26.added_lines,commit.changed_files.file_11.hunks.hunk_26.deleted_lines,commit.changed_files.file_11.hunks.hunk_26.method_info.method_name,commit.changed_files.file_11.hunks.hunk_26.method_info.method_params,commit.changed_files.file_11.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_27.Ismethod,commit.changed_files.file_11.hunks.hunk_27.added_lines,commit.changed_files.file_11.hunks.hunk_27.deleted_lines,commit.changed_files.file_11.hunks.hunk_27.method_info.method_name,commit.changed_files.file_11.hunks.hunk_27.method_info.method_params,commit.changed_files.file_11.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_28.Ismethod,commit.changed_files.file_11.hunks.hunk_28.added_lines,commit.changed_files.file_11.hunks.hunk_28.deleted_lines,commit.changed_files.file_11.hunks.hunk_28.method_info.method_name,commit.changed_files.file_11.hunks.hunk_28.method_info.method_params,commit.changed_files.file_11.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_29.Ismethod,commit.changed_files.file_11.hunks.hunk_29.added_lines,commit.changed_files.file_11.hunks.hunk_29.deleted_lines,commit.changed_files.file_11.hunks.hunk_29.method_info.method_name,commit.changed_files.file_11.hunks.hunk_29.method_info.method_params,commit.changed_files.file_11.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_2.Ismethod,commit.changed_files.file_10.hunks.hunk_2.added_lines,commit.changed_files.file_10.hunks.hunk_2.deleted_lines,commit.changed_files.file_10.hunks.hunk_2.method_info.method_name,commit.changed_files.file_10.hunks.hunk_2.method_info.method_params,commit.changed_files.file_10.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_11.Ismethod,commit.changed_files.file_2.hunks.hunk_11.added_lines,commit.changed_files.file_2.hunks.hunk_11.deleted_lines,commit.changed_files.file_2.hunks.hunk_11.method_info.method_name,commit.changed_files.file_2.hunks.hunk_11.method_info.method_params,commit.changed_files.file_2.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_5.Ismethod,commit.changed_files.file_4.hunks.hunk_5.added_lines,commit.changed_files.file_4.hunks.hunk_5.deleted_lines,commit.changed_files.file_4.hunks.hunk_5.method_info.method_name,commit.changed_files.file_4.hunks.hunk_5.method_info.method_params,commit.changed_files.file_4.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_6.Ismethod,commit.changed_files.file_4.hunks.hunk_6.added_lines,commit.changed_files.file_4.hunks.hunk_6.deleted_lines,commit.changed_files.file_4.hunks.hunk_6.method_info.method_name,commit.changed_files.file_4.hunks.hunk_6.method_info.method_params,commit.changed_files.file_4.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_7.Ismethod,commit.changed_files.file_4.hunks.hunk_7.added_lines,commit.changed_files.file_4.hunks.hunk_7.deleted_lines,commit.changed_files.file_4.hunks.hunk_7.method_info.method_name,commit.changed_files.file_4.hunks.hunk_7.method_info.method_params,commit.changed_files.file_4.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_8.Ismethod,commit.changed_files.file_4.hunks.hunk_8.added_lines,commit.changed_files.file_4.hunks.hunk_8.deleted_lines,commit.changed_files.file_4.hunks.hunk_8.method_info.method_name,commit.changed_files.file_4.hunks.hunk_8.method_info.method_params,commit.changed_files.file_4.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_8.method_info.method_endline,BR.comments.comments_19.comment_id,BR.comments.comments_19.comment_author,BR.comments.comments_19.commentT,BR.comments.comments_19.comment_text,BR.comments.comments_20.comment_id,BR.comments.comments_20.comment_author,BR.comments.comments_20.commentT,BR.comments.comments_20.comment_text,BR.comments.comments_21.comment_id,BR.comments.comments_21.comment_author,BR.comments.comments_21.commentT,BR.comments.comments_21.comment_text,commit.changed_files.file_10.hunks.hunk_3.Ismethod,commit.changed_files.file_10.hunks.hunk_3.added_lines,commit.changed_files.file_10.hunks.hunk_3.deleted_lines,commit.changed_files.file_10.hunks.hunk_3.method_info.method_name,commit.changed_files.file_10.hunks.hunk_3.method_info.method_params,commit.changed_files.file_10.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_4.Ismethod,commit.changed_files.file_10.hunks.hunk_4.added_lines,commit.changed_files.file_10.hunks.hunk_4.deleted_lines,commit.changed_files.file_10.hunks.hunk_4.method_info.method_name,commit.changed_files.file_10.hunks.hunk_4.method_info.method_params,commit.changed_files.file_10.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_5.Ismethod,commit.changed_files.file_10.hunks.hunk_5.added_lines,commit.changed_files.file_10.hunks.hunk_5.deleted_lines,commit.changed_files.file_10.hunks.hunk_5.method_info.method_name,commit.changed_files.file_10.hunks.hunk_5.method_info.method_params,commit.changed_files.file_10.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_6.Ismethod,commit.changed_files.file_10.hunks.hunk_6.added_lines,commit.changed_files.file_10.hunks.hunk_6.deleted_lines,commit.changed_files.file_10.hunks.hunk_6.method_info.method_name,commit.changed_files.file_10.hunks.hunk_6.method_info.method_params,commit.changed_files.file_10.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_4.Ismethod,commit.changed_files.file_17.hunks.hunk_4.added_lines,commit.changed_files.file_17.hunks.hunk_4.deleted_lines,commit.changed_files.file_17.hunks.hunk_4.method_info.method_name,commit.changed_files.file_17.hunks.hunk_4.method_info.method_params,commit.changed_files.file_17.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_5.Ismethod,commit.changed_files.file_17.hunks.hunk_5.added_lines,commit.changed_files.file_17.hunks.hunk_5.deleted_lines,commit.changed_files.file_17.hunks.hunk_5.method_info.method_name,commit.changed_files.file_17.hunks.hunk_5.method_info.method_params,commit.changed_files.file_17.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_6.Ismethod,commit.changed_files.file_17.hunks.hunk_6.added_lines,commit.changed_files.file_17.hunks.hunk_6.deleted_lines,commit.changed_files.file_17.hunks.hunk_6.method_info.method_name,commit.changed_files.file_17.hunks.hunk_6.method_info.method_params,commit.changed_files.file_17.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_7.Ismethod,commit.changed_files.file_17.hunks.hunk_7.added_lines,commit.changed_files.file_17.hunks.hunk_7.deleted_lines,commit.changed_files.file_17.hunks.hunk_7.method_info.method_name,commit.changed_files.file_17.hunks.hunk_7.method_info.method_params,commit.changed_files.file_17.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_8.Ismethod,commit.changed_files.file_17.hunks.hunk_8.added_lines,commit.changed_files.file_17.hunks.hunk_8.deleted_lines,commit.changed_files.file_17.hunks.hunk_8.method_info.method_name,commit.changed_files.file_17.hunks.hunk_8.method_info.method_params,commit.changed_files.file_17.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_9.Ismethod,commit.changed_files.file_17.hunks.hunk_9.added_lines,commit.changed_files.file_17.hunks.hunk_9.deleted_lines,commit.changed_files.file_17.hunks.hunk_9.method_info.method_name,commit.changed_files.file_17.hunks.hunk_9.method_info.method_params,commit.changed_files.file_17.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_17.hunks.hunk_10.Ismethod,commit.changed_files.file_17.hunks.hunk_10.added_lines,commit.changed_files.file_17.hunks.hunk_10.deleted_lines,commit.changed_files.file_17.hunks.hunk_10.method_info.method_name,commit.changed_files.file_17.hunks.hunk_10.method_info.method_params,commit.changed_files.file_17.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_17.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_19.hunks.hunk_0.Ismethod,commit.changed_files.file_19.hunks.hunk_0.added_lines,commit.changed_files.file_19.hunks.hunk_0.deleted_lines,commit.changed_files.file_19.hunks.hunk_0.method_info.method_name,commit.changed_files.file_19.hunks.hunk_0.method_info.method_params,commit.changed_files.file_19.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_19.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_21.file_change_type,commit.changed_files.file_21.file_Nmethod,commit.changed_files.file_21.file_old_name,commit.changed_files.file_21.file_new_name,commit.changed_files.file_21.hunks.hunk_0.Ismethod,commit.changed_files.file_21.hunks.hunk_0.added_lines,commit.changed_files.file_21.hunks.hunk_0.deleted_lines,commit.changed_files.file_21.hunks.hunk_0.method_info.method_name,commit.changed_files.file_21.hunks.hunk_0.method_info.method_params,commit.changed_files.file_21.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_21.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_21.hunks.hunk_1.Ismethod,commit.changed_files.file_21.hunks.hunk_1.added_lines,commit.changed_files.file_21.hunks.hunk_1.deleted_lines,commit.changed_files.file_21.hunks.hunk_1.method_info.method_name,commit.changed_files.file_21.hunks.hunk_1.method_info.method_params,commit.changed_files.file_21.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_21.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_21.hunks.hunk_2.Ismethod,commit.changed_files.file_21.hunks.hunk_2.added_lines,commit.changed_files.file_21.hunks.hunk_2.deleted_lines,commit.changed_files.file_21.hunks.hunk_2.method_info.method_name,commit.changed_files.file_21.hunks.hunk_2.method_info.method_params,commit.changed_files.file_21.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_21.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_22.file_change_type,commit.changed_files.file_22.file_Nmethod,commit.changed_files.file_22.file_old_name,commit.changed_files.file_22.file_new_name,commit.changed_files.file_22.hunks.hunk_0.Ismethod,commit.changed_files.file_22.hunks.hunk_0.added_lines,commit.changed_files.file_22.hunks.hunk_0.deleted_lines,commit.changed_files.file_22.hunks.hunk_0.method_info.method_name,commit.changed_files.file_22.hunks.hunk_0.method_info.method_params,commit.changed_files.file_22.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_22.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_22.hunks.hunk_1.Ismethod,commit.changed_files.file_22.hunks.hunk_1.added_lines,commit.changed_files.file_22.hunks.hunk_1.deleted_lines,commit.changed_files.file_22.hunks.hunk_1.method_info.method_name,commit.changed_files.file_22.hunks.hunk_1.method_info.method_params,commit.changed_files.file_22.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_22.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_22.hunks.hunk_2.Ismethod,commit.changed_files.file_22.hunks.hunk_2.added_lines,commit.changed_files.file_22.hunks.hunk_2.deleted_lines,commit.changed_files.file_22.hunks.hunk_2.method_info.method_name,commit.changed_files.file_22.hunks.hunk_2.method_info.method_params,commit.changed_files.file_22.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_22.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_22.hunks.hunk_3.Ismethod,commit.changed_files.file_22.hunks.hunk_3.added_lines,commit.changed_files.file_22.hunks.hunk_3.deleted_lines,commit.changed_files.file_22.hunks.hunk_3.method_info.method_name,commit.changed_files.file_22.hunks.hunk_3.method_info.method_params,commit.changed_files.file_22.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_22.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_23.file_change_type,commit.changed_files.file_23.file_Nmethod,commit.changed_files.file_23.file_old_name,commit.changed_files.file_23.file_new_name,commit.changed_files.file_23.hunks.hunk_0.Ismethod,commit.changed_files.file_23.hunks.hunk_0.added_lines,commit.changed_files.file_23.hunks.hunk_0.deleted_lines,commit.changed_files.file_23.hunks.hunk_0.method_info.method_name,commit.changed_files.file_23.hunks.hunk_0.method_info.method_params,commit.changed_files.file_23.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_23.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_23.hunks.hunk_1.Ismethod,commit.changed_files.file_23.hunks.hunk_1.added_lines,commit.changed_files.file_23.hunks.hunk_1.deleted_lines,commit.changed_files.file_23.hunks.hunk_1.method_info.method_name,commit.changed_files.file_23.hunks.hunk_1.method_info.method_params,commit.changed_files.file_23.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_23.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_23.hunks.hunk_2.Ismethod,commit.changed_files.file_23.hunks.hunk_2.added_lines,commit.changed_files.file_23.hunks.hunk_2.deleted_lines,commit.changed_files.file_23.hunks.hunk_2.method_info.method_name,commit.changed_files.file_23.hunks.hunk_2.method_info.method_params,commit.changed_files.file_23.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_23.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_23.hunks.hunk_3.Ismethod,commit.changed_files.file_23.hunks.hunk_3.added_lines,commit.changed_files.file_23.hunks.hunk_3.deleted_lines,commit.changed_files.file_23.hunks.hunk_3.method_info.method_name,commit.changed_files.file_23.hunks.hunk_3.method_info.method_params,commit.changed_files.file_23.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_23.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_24.file_change_type,commit.changed_files.file_24.file_Nmethod,commit.changed_files.file_24.file_old_name,commit.changed_files.file_24.file_new_name,commit.changed_files.file_24.hunks.hunk_0.Ismethod,commit.changed_files.file_24.hunks.hunk_0.added_lines,commit.changed_files.file_24.hunks.hunk_0.deleted_lines,commit.changed_files.file_24.hunks.hunk_0.method_info.method_name,commit.changed_files.file_24.hunks.hunk_0.method_info.method_params,commit.changed_files.file_24.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_24.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_25.file_change_type,commit.changed_files.file_25.file_Nmethod,commit.changed_files.file_25.file_old_name,commit.changed_files.file_25.file_new_name,commit.changed_files.file_25.hunks.hunk_0.Ismethod,commit.changed_files.file_25.hunks.hunk_0.added_lines,commit.changed_files.file_25.hunks.hunk_0.deleted_lines,commit.changed_files.file_25.hunks.hunk_0.method_info.method_name,commit.changed_files.file_25.hunks.hunk_0.method_info.method_params,commit.changed_files.file_25.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_25.hunks.hunk_0.method_info.method_endline,BR.comments.comments_22.comment_id,BR.comments.comments_22.comment_author,BR.comments.comments_22.commentT,BR.comments.comments_22.comment_text,BR.comments.comments_23.comment_id,BR.comments.comments_23.comment_author,BR.comments.comments_23.commentT,BR.comments.comments_23.comment_text,BR.comments.comments_24.comment_id,BR.comments.comments_24.comment_author,BR.comments.comments_24.commentT,BR.comments.comments_24.comment_text,BR.comments.comments_25.comment_id,BR.comments.comments_25.comment_author,BR.comments.comments_25.commentT,BR.comments.comments_25.comment_text,BR.comments.comments_26.comment_id,BR.comments.comments_26.comment_author,BR.comments.comments_26.commentT,BR.comments.comments_26.comment_text,BR.comments.comments_27.comment_id,BR.comments.comments_27.comment_author,BR.comments.comments_27.commentT,BR.comments.comments_27.comment_text,commit.changed_files.file_8.hunks.hunk_8.Ismethod,commit.changed_files.file_8.hunks.hunk_8.added_lines,commit.changed_files.file_8.hunks.hunk_8.deleted_lines,commit.changed_files.file_8.hunks.hunk_8.method_info.method_name,commit.changed_files.file_8.hunks.hunk_8.method_info.method_params,commit.changed_files.file_8.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_9.Ismethod,commit.changed_files.file_8.hunks.hunk_9.added_lines,commit.changed_files.file_8.hunks.hunk_9.deleted_lines,commit.changed_files.file_8.hunks.hunk_9.method_info.method_name,commit.changed_files.file_8.hunks.hunk_9.method_info.method_params,commit.changed_files.file_8.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_10.Ismethod,commit.changed_files.file_8.hunks.hunk_10.added_lines,commit.changed_files.file_8.hunks.hunk_10.deleted_lines,commit.changed_files.file_8.hunks.hunk_10.method_info.method_name,commit.changed_files.file_8.hunks.hunk_10.method_info.method_params,commit.changed_files.file_8.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_11.Ismethod,commit.changed_files.file_8.hunks.hunk_11.added_lines,commit.changed_files.file_8.hunks.hunk_11.deleted_lines,commit.changed_files.file_8.hunks.hunk_11.method_info.method_name,commit.changed_files.file_8.hunks.hunk_11.method_info.method_params,commit.changed_files.file_8.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_19.hunks.hunk_1.Ismethod,commit.changed_files.file_19.hunks.hunk_1.added_lines,commit.changed_files.file_19.hunks.hunk_1.deleted_lines,commit.changed_files.file_19.hunks.hunk_1.method_info.method_name,commit.changed_files.file_19.hunks.hunk_1.method_info.method_params,commit.changed_files.file_19.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_19.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_1.Ismethod,commit.changed_files.file_20.hunks.hunk_1.added_lines,commit.changed_files.file_20.hunks.hunk_1.deleted_lines,commit.changed_files.file_20.hunks.hunk_1.method_info.method_name,commit.changed_files.file_20.hunks.hunk_1.method_info.method_params,commit.changed_files.file_20.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_2.Ismethod,commit.changed_files.file_20.hunks.hunk_2.added_lines,commit.changed_files.file_20.hunks.hunk_2.deleted_lines,commit.changed_files.file_20.hunks.hunk_2.method_info.method_name,commit.changed_files.file_20.hunks.hunk_2.method_info.method_params,commit.changed_files.file_20.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_3.Ismethod,commit.changed_files.file_20.hunks.hunk_3.added_lines,commit.changed_files.file_20.hunks.hunk_3.deleted_lines,commit.changed_files.file_20.hunks.hunk_3.method_info.method_name,commit.changed_files.file_20.hunks.hunk_3.method_info.method_params,commit.changed_files.file_20.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_4.Ismethod,commit.changed_files.file_20.hunks.hunk_4.added_lines,commit.changed_files.file_20.hunks.hunk_4.deleted_lines,commit.changed_files.file_20.hunks.hunk_4.method_info.method_name,commit.changed_files.file_20.hunks.hunk_4.method_info.method_params,commit.changed_files.file_20.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_5.Ismethod,commit.changed_files.file_20.hunks.hunk_5.added_lines,commit.changed_files.file_20.hunks.hunk_5.deleted_lines,commit.changed_files.file_20.hunks.hunk_5.method_info.method_name,commit.changed_files.file_20.hunks.hunk_5.method_info.method_params,commit.changed_files.file_20.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_6.Ismethod,commit.changed_files.file_20.hunks.hunk_6.added_lines,commit.changed_files.file_20.hunks.hunk_6.deleted_lines,commit.changed_files.file_20.hunks.hunk_6.method_info.method_name,commit.changed_files.file_20.hunks.hunk_6.method_info.method_params,commit.changed_files.file_20.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_7.Ismethod,commit.changed_files.file_20.hunks.hunk_7.added_lines,commit.changed_files.file_20.hunks.hunk_7.deleted_lines,commit.changed_files.file_20.hunks.hunk_7.method_info.method_name,commit.changed_files.file_20.hunks.hunk_7.method_info.method_params,commit.changed_files.file_20.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_8.Ismethod,commit.changed_files.file_20.hunks.hunk_8.added_lines,commit.changed_files.file_20.hunks.hunk_8.deleted_lines,commit.changed_files.file_20.hunks.hunk_8.method_info.method_name,commit.changed_files.file_20.hunks.hunk_8.method_info.method_params,commit.changed_files.file_20.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_9.Ismethod,commit.changed_files.file_20.hunks.hunk_9.added_lines,commit.changed_files.file_20.hunks.hunk_9.deleted_lines,commit.changed_files.file_20.hunks.hunk_9.method_info.method_name,commit.changed_files.file_20.hunks.hunk_9.method_info.method_params,commit.changed_files.file_20.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_10.Ismethod,commit.changed_files.file_20.hunks.hunk_10.added_lines,commit.changed_files.file_20.hunks.hunk_10.deleted_lines,commit.changed_files.file_20.hunks.hunk_10.method_info.method_name,commit.changed_files.file_20.hunks.hunk_10.method_info.method_params,commit.changed_files.file_20.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_11.Ismethod,commit.changed_files.file_20.hunks.hunk_11.added_lines,commit.changed_files.file_20.hunks.hunk_11.deleted_lines,commit.changed_files.file_20.hunks.hunk_11.method_info.method_name,commit.changed_files.file_20.hunks.hunk_11.method_info.method_params,commit.changed_files.file_20.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_12.Ismethod,commit.changed_files.file_20.hunks.hunk_12.added_lines,commit.changed_files.file_20.hunks.hunk_12.deleted_lines,commit.changed_files.file_20.hunks.hunk_12.method_info.method_name,commit.changed_files.file_20.hunks.hunk_12.method_info.method_params,commit.changed_files.file_20.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_13.Ismethod,commit.changed_files.file_20.hunks.hunk_13.added_lines,commit.changed_files.file_20.hunks.hunk_13.deleted_lines,commit.changed_files.file_20.hunks.hunk_13.method_info.method_name,commit.changed_files.file_20.hunks.hunk_13.method_info.method_params,commit.changed_files.file_20.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_14.Ismethod,commit.changed_files.file_20.hunks.hunk_14.added_lines,commit.changed_files.file_20.hunks.hunk_14.deleted_lines,commit.changed_files.file_20.hunks.hunk_14.method_info.method_name,commit.changed_files.file_20.hunks.hunk_14.method_info.method_params,commit.changed_files.file_20.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_15.Ismethod,commit.changed_files.file_20.hunks.hunk_15.added_lines,commit.changed_files.file_20.hunks.hunk_15.deleted_lines,commit.changed_files.file_20.hunks.hunk_15.method_info.method_name,commit.changed_files.file_20.hunks.hunk_15.method_info.method_params,commit.changed_files.file_20.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_16.Ismethod,commit.changed_files.file_20.hunks.hunk_16.added_lines,commit.changed_files.file_20.hunks.hunk_16.deleted_lines,commit.changed_files.file_20.hunks.hunk_16.method_info.method_name,commit.changed_files.file_20.hunks.hunk_16.method_info.method_params,commit.changed_files.file_20.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_17.Ismethod,commit.changed_files.file_20.hunks.hunk_17.added_lines,commit.changed_files.file_20.hunks.hunk_17.deleted_lines,commit.changed_files.file_20.hunks.hunk_17.method_info.method_name,commit.changed_files.file_20.hunks.hunk_17.method_info.method_params,commit.changed_files.file_20.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_18.Ismethod,commit.changed_files.file_20.hunks.hunk_18.added_lines,commit.changed_files.file_20.hunks.hunk_18.deleted_lines,commit.changed_files.file_20.hunks.hunk_18.method_info.method_name,commit.changed_files.file_20.hunks.hunk_18.method_info.method_params,commit.changed_files.file_20.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_19.Ismethod,commit.changed_files.file_20.hunks.hunk_19.added_lines,commit.changed_files.file_20.hunks.hunk_19.deleted_lines,commit.changed_files.file_20.hunks.hunk_19.method_info.method_name,commit.changed_files.file_20.hunks.hunk_19.method_info.method_params,commit.changed_files.file_20.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_20.Ismethod,commit.changed_files.file_20.hunks.hunk_20.added_lines,commit.changed_files.file_20.hunks.hunk_20.deleted_lines,commit.changed_files.file_20.hunks.hunk_20.method_info.method_name,commit.changed_files.file_20.hunks.hunk_20.method_info.method_params,commit.changed_files.file_20.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_21.Ismethod,commit.changed_files.file_20.hunks.hunk_21.added_lines,commit.changed_files.file_20.hunks.hunk_21.deleted_lines,commit.changed_files.file_20.hunks.hunk_21.method_info.method_name,commit.changed_files.file_20.hunks.hunk_21.method_info.method_params,commit.changed_files.file_20.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_20.hunks.hunk_22.Ismethod,commit.changed_files.file_20.hunks.hunk_22.added_lines,commit.changed_files.file_20.hunks.hunk_22.deleted_lines,commit.changed_files.file_20.hunks.hunk_22.method_info.method_name,commit.changed_files.file_20.hunks.hunk_22.method_info.method_params,commit.changed_files.file_20.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_20.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_6.Ismethod,commit.changed_files.file_6.hunks.hunk_6.added_lines,commit.changed_files.file_6.hunks.hunk_6.deleted_lines,commit.changed_files.file_6.hunks.hunk_6.method_info.method_name,commit.changed_files.file_6.hunks.hunk_6.method_info.method_params,commit.changed_files.file_6.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_6.hunks.hunk_7.Ismethod,commit.changed_files.file_6.hunks.hunk_7.added_lines,commit.changed_files.file_6.hunks.hunk_7.deleted_lines,commit.changed_files.file_6.hunks.hunk_7.method_info.method_name,commit.changed_files.file_6.hunks.hunk_7.method_info.method_params,commit.changed_files.file_6.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_6.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_4.Ismethod,commit.changed_files.file_9.hunks.hunk_4.added_lines,commit.changed_files.file_9.hunks.hunk_4.deleted_lines,commit.changed_files.file_9.hunks.hunk_4.method_info.method_name,commit.changed_files.file_9.hunks.hunk_4.method_info.method_params,commit.changed_files.file_9.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_9.hunks.hunk_5.Ismethod,commit.changed_files.file_9.hunks.hunk_5.added_lines,commit.changed_files.file_9.hunks.hunk_5.deleted_lines,commit.changed_files.file_9.hunks.hunk_5.method_info.method_name,commit.changed_files.file_9.hunks.hunk_5.method_info.method_params,commit.changed_files.file_9.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_9.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_11.Ismethod,commit.changed_files.file_3.hunks.hunk_11.added_lines,commit.changed_files.file_3.hunks.hunk_11.deleted_lines,commit.changed_files.file_3.hunks.hunk_11.method_info.method_name,commit.changed_files.file_3.hunks.hunk_11.method_info.method_params,commit.changed_files.file_3.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_12.Ismethod,commit.changed_files.file_3.hunks.hunk_12.added_lines,commit.changed_files.file_3.hunks.hunk_12.deleted_lines,commit.changed_files.file_3.hunks.hunk_12.method_info.method_name,commit.changed_files.file_3.hunks.hunk_12.method_info.method_params,commit.changed_files.file_3.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_13.Ismethod,commit.changed_files.file_3.hunks.hunk_13.added_lines,commit.changed_files.file_3.hunks.hunk_13.deleted_lines,commit.changed_files.file_3.hunks.hunk_13.method_info.method_name,commit.changed_files.file_3.hunks.hunk_13.method_info.method_params,commit.changed_files.file_3.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_7.Ismethod,commit.changed_files.file_10.hunks.hunk_7.added_lines,commit.changed_files.file_10.hunks.hunk_7.deleted_lines,commit.changed_files.file_10.hunks.hunk_7.method_info.method_name,commit.changed_files.file_10.hunks.hunk_7.method_info.method_params,commit.changed_files.file_10.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_8.Ismethod,commit.changed_files.file_10.hunks.hunk_8.added_lines,commit.changed_files.file_10.hunks.hunk_8.deleted_lines,commit.changed_files.file_10.hunks.hunk_8.method_info.method_name,commit.changed_files.file_10.hunks.hunk_8.method_info.method_params,commit.changed_files.file_10.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_10.hunks.hunk_9.Ismethod,commit.changed_files.file_10.hunks.hunk_9.added_lines,commit.changed_files.file_10.hunks.hunk_9.deleted_lines,commit.changed_files.file_10.hunks.hunk_9.method_info.method_name,commit.changed_files.file_10.hunks.hunk_9.method_info.method_params,commit.changed_files.file_10.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_10.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_26.file_change_type,commit.changed_files.file_26.file_Nmethod,commit.changed_files.file_26.file_old_name,commit.changed_files.file_26.file_new_name,commit.changed_files.file_27.file_change_type,commit.changed_files.file_27.file_Nmethod,commit.changed_files.file_27.file_old_name,commit.changed_files.file_27.file_new_name,commit.changed_files.file_28.file_change_type,commit.changed_files.file_28.file_Nmethod,commit.changed_files.file_28.file_old_name,commit.changed_files.file_28.file_new_name,commit.changed_files.file_28.hunks.hunk_0.Ismethod,commit.changed_files.file_28.hunks.hunk_0.added_lines,commit.changed_files.file_28.hunks.hunk_0.deleted_lines,commit.changed_files.file_28.hunks.hunk_0.method_info.method_name,commit.changed_files.file_28.hunks.hunk_0.method_info.method_params,commit.changed_files.file_28.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_28.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_28.hunks.hunk_1.Ismethod,commit.changed_files.file_28.hunks.hunk_1.added_lines,commit.changed_files.file_28.hunks.hunk_1.deleted_lines,commit.changed_files.file_28.hunks.hunk_1.method_info.method_name,commit.changed_files.file_28.hunks.hunk_1.method_info.method_params,commit.changed_files.file_28.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_28.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_28.hunks.hunk_2.Ismethod,commit.changed_files.file_28.hunks.hunk_2.added_lines,commit.changed_files.file_28.hunks.hunk_2.deleted_lines,commit.changed_files.file_28.hunks.hunk_2.method_info.method_name,commit.changed_files.file_28.hunks.hunk_2.method_info.method_params,commit.changed_files.file_28.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_28.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_28.hunks.hunk_3.Ismethod,commit.changed_files.file_28.hunks.hunk_3.added_lines,commit.changed_files.file_28.hunks.hunk_3.deleted_lines,commit.changed_files.file_28.hunks.hunk_3.method_info.method_name,commit.changed_files.file_28.hunks.hunk_3.method_info.method_params,commit.changed_files.file_28.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_28.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_29.file_change_type,commit.changed_files.file_29.file_Nmethod,commit.changed_files.file_29.file_old_name,commit.changed_files.file_29.file_new_name,commit.changed_files.file_29.hunks.hunk_0.Ismethod,commit.changed_files.file_29.hunks.hunk_0.added_lines,commit.changed_files.file_29.hunks.hunk_0.deleted_lines,commit.changed_files.file_29.hunks.hunk_0.method_info.method_name,commit.changed_files.file_29.hunks.hunk_0.method_info.method_params,commit.changed_files.file_29.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_29.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_29.hunks.hunk_1.Ismethod,commit.changed_files.file_29.hunks.hunk_1.added_lines,commit.changed_files.file_29.hunks.hunk_1.deleted_lines,commit.changed_files.file_29.hunks.hunk_1.method_info.method_name,commit.changed_files.file_29.hunks.hunk_1.method_info.method_params,commit.changed_files.file_29.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_29.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_29.hunks.hunk_2.Ismethod,commit.changed_files.file_29.hunks.hunk_2.added_lines,commit.changed_files.file_29.hunks.hunk_2.deleted_lines,commit.changed_files.file_29.hunks.hunk_2.method_info.method_name,commit.changed_files.file_29.hunks.hunk_2.method_info.method_params,commit.changed_files.file_29.hunks.hunk_2.method_info.method_startline,commit.changed_files.file_29.hunks.hunk_2.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_12.Ismethod,commit.changed_files.file_2.hunks.hunk_12.added_lines,commit.changed_files.file_2.hunks.hunk_12.deleted_lines,commit.changed_files.file_2.hunks.hunk_12.method_info.method_name,commit.changed_files.file_2.hunks.hunk_12.method_info.method_params,commit.changed_files.file_2.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_13.Ismethod,commit.changed_files.file_2.hunks.hunk_13.added_lines,commit.changed_files.file_2.hunks.hunk_13.deleted_lines,commit.changed_files.file_2.hunks.hunk_13.method_info.method_name,commit.changed_files.file_2.hunks.hunk_13.method_info.method_params,commit.changed_files.file_2.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_14.Ismethod,commit.changed_files.file_3.hunks.hunk_14.added_lines,commit.changed_files.file_3.hunks.hunk_14.deleted_lines,commit.changed_files.file_3.hunks.hunk_14.method_info.method_name,commit.changed_files.file_3.hunks.hunk_14.method_info.method_params,commit.changed_files.file_3.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_15.Ismethod,commit.changed_files.file_3.hunks.hunk_15.added_lines,commit.changed_files.file_3.hunks.hunk_15.deleted_lines,commit.changed_files.file_3.hunks.hunk_15.method_info.method_name,commit.changed_files.file_3.hunks.hunk_15.method_info.method_params,commit.changed_files.file_3.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_16.Ismethod,commit.changed_files.file_3.hunks.hunk_16.added_lines,commit.changed_files.file_3.hunks.hunk_16.deleted_lines,commit.changed_files.file_3.hunks.hunk_16.method_info.method_name,commit.changed_files.file_3.hunks.hunk_16.method_info.method_params,commit.changed_files.file_3.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_12.Ismethod,commit.changed_files.file_8.hunks.hunk_12.added_lines,commit.changed_files.file_8.hunks.hunk_12.deleted_lines,commit.changed_files.file_8.hunks.hunk_12.method_info.method_name,commit.changed_files.file_8.hunks.hunk_12.method_info.method_params,commit.changed_files.file_8.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_13.Ismethod,commit.changed_files.file_8.hunks.hunk_13.added_lines,commit.changed_files.file_8.hunks.hunk_13.deleted_lines,commit.changed_files.file_8.hunks.hunk_13.method_info.method_name,commit.changed_files.file_8.hunks.hunk_13.method_info.method_params,commit.changed_files.file_8.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_14.Ismethod,commit.changed_files.file_8.hunks.hunk_14.added_lines,commit.changed_files.file_8.hunks.hunk_14.deleted_lines,commit.changed_files.file_8.hunks.hunk_14.method_info.method_name,commit.changed_files.file_8.hunks.hunk_14.method_info.method_params,commit.changed_files.file_8.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_15.Ismethod,commit.changed_files.file_8.hunks.hunk_15.added_lines,commit.changed_files.file_8.hunks.hunk_15.deleted_lines,commit.changed_files.file_8.hunks.hunk_15.method_info.method_name,commit.changed_files.file_8.hunks.hunk_15.method_info.method_params,commit.changed_files.file_8.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_16.Ismethod,commit.changed_files.file_8.hunks.hunk_16.added_lines,commit.changed_files.file_8.hunks.hunk_16.deleted_lines,commit.changed_files.file_8.hunks.hunk_16.method_info.method_name,commit.changed_files.file_8.hunks.hunk_16.method_info.method_params,commit.changed_files.file_8.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_17.Ismethod,commit.changed_files.file_8.hunks.hunk_17.added_lines,commit.changed_files.file_8.hunks.hunk_17.deleted_lines,commit.changed_files.file_8.hunks.hunk_17.method_info.method_name,commit.changed_files.file_8.hunks.hunk_17.method_info.method_params,commit.changed_files.file_8.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_18.Ismethod,commit.changed_files.file_8.hunks.hunk_18.added_lines,commit.changed_files.file_8.hunks.hunk_18.deleted_lines,commit.changed_files.file_8.hunks.hunk_18.method_info.method_name,commit.changed_files.file_8.hunks.hunk_18.method_info.method_params,commit.changed_files.file_8.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_19.Ismethod,commit.changed_files.file_8.hunks.hunk_19.added_lines,commit.changed_files.file_8.hunks.hunk_19.deleted_lines,commit.changed_files.file_8.hunks.hunk_19.method_info.method_name,commit.changed_files.file_8.hunks.hunk_19.method_info.method_params,commit.changed_files.file_8.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_20.Ismethod,commit.changed_files.file_8.hunks.hunk_20.added_lines,commit.changed_files.file_8.hunks.hunk_20.deleted_lines,commit.changed_files.file_8.hunks.hunk_20.method_info.method_name,commit.changed_files.file_8.hunks.hunk_20.method_info.method_params,commit.changed_files.file_8.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_21.Ismethod,commit.changed_files.file_8.hunks.hunk_21.added_lines,commit.changed_files.file_8.hunks.hunk_21.deleted_lines,commit.changed_files.file_8.hunks.hunk_21.method_info.method_name,commit.changed_files.file_8.hunks.hunk_21.method_info.method_params,commit.changed_files.file_8.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_22.Ismethod,commit.changed_files.file_8.hunks.hunk_22.added_lines,commit.changed_files.file_8.hunks.hunk_22.deleted_lines,commit.changed_files.file_8.hunks.hunk_22.method_info.method_name,commit.changed_files.file_8.hunks.hunk_22.method_info.method_params,commit.changed_files.file_8.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_23.Ismethod,commit.changed_files.file_8.hunks.hunk_23.added_lines,commit.changed_files.file_8.hunks.hunk_23.deleted_lines,commit.changed_files.file_8.hunks.hunk_23.method_info.method_name,commit.changed_files.file_8.hunks.hunk_23.method_info.method_params,commit.changed_files.file_8.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_24.Ismethod,commit.changed_files.file_8.hunks.hunk_24.added_lines,commit.changed_files.file_8.hunks.hunk_24.deleted_lines,commit.changed_files.file_8.hunks.hunk_24.method_info.method_name,commit.changed_files.file_8.hunks.hunk_24.method_info.method_params,commit.changed_files.file_8.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_25.Ismethod,commit.changed_files.file_8.hunks.hunk_25.added_lines,commit.changed_files.file_8.hunks.hunk_25.deleted_lines,commit.changed_files.file_8.hunks.hunk_25.method_info.method_name,commit.changed_files.file_8.hunks.hunk_25.method_info.method_params,commit.changed_files.file_8.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_26.Ismethod,commit.changed_files.file_8.hunks.hunk_26.added_lines,commit.changed_files.file_8.hunks.hunk_26.deleted_lines,commit.changed_files.file_8.hunks.hunk_26.method_info.method_name,commit.changed_files.file_8.hunks.hunk_26.method_info.method_params,commit.changed_files.file_8.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_8.hunks.hunk_27.Ismethod,commit.changed_files.file_8.hunks.hunk_27.added_lines,commit.changed_files.file_8.hunks.hunk_27.deleted_lines,commit.changed_files.file_8.hunks.hunk_27.method_info.method_name,commit.changed_files.file_8.hunks.hunk_27.method_info.method_params,commit.changed_files.file_8.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_8.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_3.Ismethod,commit.changed_files.file_5.hunks.hunk_3.added_lines,commit.changed_files.file_5.hunks.hunk_3.deleted_lines,commit.changed_files.file_5.hunks.hunk_3.method_info.method_name,commit.changed_files.file_5.hunks.hunk_3.method_info.method_params,commit.changed_files.file_5.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_4.Ismethod,commit.changed_files.file_5.hunks.hunk_4.added_lines,commit.changed_files.file_5.hunks.hunk_4.deleted_lines,commit.changed_files.file_5.hunks.hunk_4.method_info.method_name,commit.changed_files.file_5.hunks.hunk_4.method_info.method_params,commit.changed_files.file_5.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_5.Ismethod,commit.changed_files.file_5.hunks.hunk_5.added_lines,commit.changed_files.file_5.hunks.hunk_5.deleted_lines,commit.changed_files.file_5.hunks.hunk_5.method_info.method_name,commit.changed_files.file_5.hunks.hunk_5.method_info.method_params,commit.changed_files.file_5.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_5.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_6.Ismethod,commit.changed_files.file_5.hunks.hunk_6.added_lines,commit.changed_files.file_5.hunks.hunk_6.deleted_lines,commit.changed_files.file_5.hunks.hunk_6.method_info.method_name,commit.changed_files.file_5.hunks.hunk_6.method_info.method_params,commit.changed_files.file_5.hunks.hunk_6.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_6.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_7.Ismethod,commit.changed_files.file_5.hunks.hunk_7.added_lines,commit.changed_files.file_5.hunks.hunk_7.deleted_lines,commit.changed_files.file_5.hunks.hunk_7.method_info.method_name,commit.changed_files.file_5.hunks.hunk_7.method_info.method_params,commit.changed_files.file_5.hunks.hunk_7.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_7.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_8.Ismethod,commit.changed_files.file_5.hunks.hunk_8.added_lines,commit.changed_files.file_5.hunks.hunk_8.deleted_lines,commit.changed_files.file_5.hunks.hunk_8.method_info.method_name,commit.changed_files.file_5.hunks.hunk_8.method_info.method_params,commit.changed_files.file_5.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_14.Ismethod,commit.changed_files.file_1.hunks.hunk_14.added_lines,commit.changed_files.file_1.hunks.hunk_14.deleted_lines,commit.changed_files.file_1.hunks.hunk_14.method_info.method_name,commit.changed_files.file_1.hunks.hunk_14.method_info.method_params,commit.changed_files.file_1.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_15.Ismethod,commit.changed_files.file_1.hunks.hunk_15.added_lines,commit.changed_files.file_1.hunks.hunk_15.deleted_lines,commit.changed_files.file_1.hunks.hunk_15.method_info.method_name,commit.changed_files.file_1.hunks.hunk_15.method_info.method_params,commit.changed_files.file_1.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_16.Ismethod,commit.changed_files.file_1.hunks.hunk_16.added_lines,commit.changed_files.file_1.hunks.hunk_16.deleted_lines,commit.changed_files.file_1.hunks.hunk_16.method_info.method_name,commit.changed_files.file_1.hunks.hunk_16.method_info.method_params,commit.changed_files.file_1.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_17.Ismethod,commit.changed_files.file_1.hunks.hunk_17.added_lines,commit.changed_files.file_1.hunks.hunk_17.deleted_lines,commit.changed_files.file_1.hunks.hunk_17.method_info.method_name,commit.changed_files.file_1.hunks.hunk_17.method_info.method_params,commit.changed_files.file_1.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_18.Ismethod,commit.changed_files.file_1.hunks.hunk_18.added_lines,commit.changed_files.file_1.hunks.hunk_18.deleted_lines,commit.changed_files.file_1.hunks.hunk_18.method_info.method_name,commit.changed_files.file_1.hunks.hunk_18.method_info.method_params,commit.changed_files.file_1.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_19.Ismethod,commit.changed_files.file_1.hunks.hunk_19.added_lines,commit.changed_files.file_1.hunks.hunk_19.deleted_lines,commit.changed_files.file_1.hunks.hunk_19.method_info.method_name,commit.changed_files.file_1.hunks.hunk_19.method_info.method_params,commit.changed_files.file_1.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_20.Ismethod,commit.changed_files.file_1.hunks.hunk_20.added_lines,commit.changed_files.file_1.hunks.hunk_20.deleted_lines,commit.changed_files.file_1.hunks.hunk_20.method_info.method_name,commit.changed_files.file_1.hunks.hunk_20.method_info.method_params,commit.changed_files.file_1.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_21.Ismethod,commit.changed_files.file_1.hunks.hunk_21.added_lines,commit.changed_files.file_1.hunks.hunk_21.deleted_lines,commit.changed_files.file_1.hunks.hunk_21.method_info.method_name,commit.changed_files.file_1.hunks.hunk_21.method_info.method_params,commit.changed_files.file_1.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_22.Ismethod,commit.changed_files.file_1.hunks.hunk_22.added_lines,commit.changed_files.file_1.hunks.hunk_22.deleted_lines,commit.changed_files.file_1.hunks.hunk_22.method_info.method_name,commit.changed_files.file_1.hunks.hunk_22.method_info.method_params,commit.changed_files.file_1.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_23.Ismethod,commit.changed_files.file_1.hunks.hunk_23.added_lines,commit.changed_files.file_1.hunks.hunk_23.deleted_lines,commit.changed_files.file_1.hunks.hunk_23.method_info.method_name,commit.changed_files.file_1.hunks.hunk_23.method_info.method_params,commit.changed_files.file_1.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_24.Ismethod,commit.changed_files.file_1.hunks.hunk_24.added_lines,commit.changed_files.file_1.hunks.hunk_24.deleted_lines,commit.changed_files.file_1.hunks.hunk_24.method_info.method_name,commit.changed_files.file_1.hunks.hunk_24.method_info.method_params,commit.changed_files.file_1.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_25.Ismethod,commit.changed_files.file_1.hunks.hunk_25.added_lines,commit.changed_files.file_1.hunks.hunk_25.deleted_lines,commit.changed_files.file_1.hunks.hunk_25.method_info.method_name,commit.changed_files.file_1.hunks.hunk_25.method_info.method_params,commit.changed_files.file_1.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_26.Ismethod,commit.changed_files.file_1.hunks.hunk_26.added_lines,commit.changed_files.file_1.hunks.hunk_26.deleted_lines,commit.changed_files.file_1.hunks.hunk_26.method_info.method_name,commit.changed_files.file_1.hunks.hunk_26.method_info.method_params,commit.changed_files.file_1.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_27.Ismethod,commit.changed_files.file_1.hunks.hunk_27.added_lines,commit.changed_files.file_1.hunks.hunk_27.deleted_lines,commit.changed_files.file_1.hunks.hunk_27.method_info.method_name,commit.changed_files.file_1.hunks.hunk_27.method_info.method_params,commit.changed_files.file_1.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_28.Ismethod,commit.changed_files.file_1.hunks.hunk_28.added_lines,commit.changed_files.file_1.hunks.hunk_28.deleted_lines,commit.changed_files.file_1.hunks.hunk_28.method_info.method_name,commit.changed_files.file_1.hunks.hunk_28.method_info.method_params,commit.changed_files.file_1.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_29.Ismethod,commit.changed_files.file_1.hunks.hunk_29.added_lines,commit.changed_files.file_1.hunks.hunk_29.deleted_lines,commit.changed_files.file_1.hunks.hunk_29.method_info.method_name,commit.changed_files.file_1.hunks.hunk_29.method_info.method_params,commit.changed_files.file_1.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_30.Ismethod,commit.changed_files.file_1.hunks.hunk_30.added_lines,commit.changed_files.file_1.hunks.hunk_30.deleted_lines,commit.changed_files.file_1.hunks.hunk_30.method_info.method_name,commit.changed_files.file_1.hunks.hunk_30.method_info.method_params,commit.changed_files.file_1.hunks.hunk_30.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_30.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_31.Ismethod,commit.changed_files.file_1.hunks.hunk_31.added_lines,commit.changed_files.file_1.hunks.hunk_31.deleted_lines,commit.changed_files.file_1.hunks.hunk_31.method_info.method_name,commit.changed_files.file_1.hunks.hunk_31.method_info.method_params,commit.changed_files.file_1.hunks.hunk_31.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_31.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_32.Ismethod,commit.changed_files.file_1.hunks.hunk_32.added_lines,commit.changed_files.file_1.hunks.hunk_32.deleted_lines,commit.changed_files.file_1.hunks.hunk_32.method_info.method_name,commit.changed_files.file_1.hunks.hunk_32.method_info.method_params,commit.changed_files.file_1.hunks.hunk_32.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_32.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_33.Ismethod,commit.changed_files.file_1.hunks.hunk_33.added_lines,commit.changed_files.file_1.hunks.hunk_33.deleted_lines,commit.changed_files.file_1.hunks.hunk_33.method_info.method_name,commit.changed_files.file_1.hunks.hunk_33.method_info.method_params,commit.changed_files.file_1.hunks.hunk_33.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_33.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_34.Ismethod,commit.changed_files.file_1.hunks.hunk_34.added_lines,commit.changed_files.file_1.hunks.hunk_34.deleted_lines,commit.changed_files.file_1.hunks.hunk_34.method_info.method_name,commit.changed_files.file_1.hunks.hunk_34.method_info.method_params,commit.changed_files.file_1.hunks.hunk_34.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_34.method_info.method_endline,commit.changed_files.file_1.hunks.hunk_35.Ismethod,commit.changed_files.file_1.hunks.hunk_35.added_lines,commit.changed_files.file_1.hunks.hunk_35.deleted_lines,commit.changed_files.file_1.hunks.hunk_35.method_info.method_name,commit.changed_files.file_1.hunks.hunk_35.method_info.method_params,commit.changed_files.file_1.hunks.hunk_35.method_info.method_startline,commit.changed_files.file_1.hunks.hunk_35.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_9.Ismethod,commit.changed_files.file_4.hunks.hunk_9.added_lines,commit.changed_files.file_4.hunks.hunk_9.deleted_lines,commit.changed_files.file_4.hunks.hunk_9.method_info.method_name,commit.changed_files.file_4.hunks.hunk_9.method_info.method_params,commit.changed_files.file_4.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_10.Ismethod,commit.changed_files.file_4.hunks.hunk_10.added_lines,commit.changed_files.file_4.hunks.hunk_10.deleted_lines,commit.changed_files.file_4.hunks.hunk_10.method_info.method_name,commit.changed_files.file_4.hunks.hunk_10.method_info.method_params,commit.changed_files.file_4.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_14.Ismethod,commit.changed_files.file_2.hunks.hunk_14.added_lines,commit.changed_files.file_2.hunks.hunk_14.deleted_lines,commit.changed_files.file_2.hunks.hunk_14.method_info.method_name,commit.changed_files.file_2.hunks.hunk_14.method_info.method_params,commit.changed_files.file_2.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_15.Ismethod,commit.changed_files.file_2.hunks.hunk_15.added_lines,commit.changed_files.file_2.hunks.hunk_15.deleted_lines,commit.changed_files.file_2.hunks.hunk_15.method_info.method_name,commit.changed_files.file_2.hunks.hunk_15.method_info.method_params,commit.changed_files.file_2.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_16.Ismethod,commit.changed_files.file_2.hunks.hunk_16.added_lines,commit.changed_files.file_2.hunks.hunk_16.deleted_lines,commit.changed_files.file_2.hunks.hunk_16.method_info.method_name,commit.changed_files.file_2.hunks.hunk_16.method_info.method_params,commit.changed_files.file_2.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_17.Ismethod,commit.changed_files.file_2.hunks.hunk_17.added_lines,commit.changed_files.file_2.hunks.hunk_17.deleted_lines,commit.changed_files.file_2.hunks.hunk_17.method_info.method_name,commit.changed_files.file_2.hunks.hunk_17.method_info.method_params,commit.changed_files.file_2.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_18.Ismethod,commit.changed_files.file_2.hunks.hunk_18.added_lines,commit.changed_files.file_2.hunks.hunk_18.deleted_lines,commit.changed_files.file_2.hunks.hunk_18.method_info.method_name,commit.changed_files.file_2.hunks.hunk_18.method_info.method_params,commit.changed_files.file_2.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_19.Ismethod,commit.changed_files.file_2.hunks.hunk_19.added_lines,commit.changed_files.file_2.hunks.hunk_19.deleted_lines,commit.changed_files.file_2.hunks.hunk_19.method_info.method_name,commit.changed_files.file_2.hunks.hunk_19.method_info.method_params,commit.changed_files.file_2.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_20.Ismethod,commit.changed_files.file_2.hunks.hunk_20.added_lines,commit.changed_files.file_2.hunks.hunk_20.deleted_lines,commit.changed_files.file_2.hunks.hunk_20.method_info.method_name,commit.changed_files.file_2.hunks.hunk_20.method_info.method_params,commit.changed_files.file_2.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_21.Ismethod,commit.changed_files.file_2.hunks.hunk_21.added_lines,commit.changed_files.file_2.hunks.hunk_21.deleted_lines,commit.changed_files.file_2.hunks.hunk_21.method_info.method_name,commit.changed_files.file_2.hunks.hunk_21.method_info.method_params,commit.changed_files.file_2.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_22.Ismethod,commit.changed_files.file_2.hunks.hunk_22.added_lines,commit.changed_files.file_2.hunks.hunk_22.deleted_lines,commit.changed_files.file_2.hunks.hunk_22.method_info.method_name,commit.changed_files.file_2.hunks.hunk_22.method_info.method_params,commit.changed_files.file_2.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_23.Ismethod,commit.changed_files.file_2.hunks.hunk_23.added_lines,commit.changed_files.file_2.hunks.hunk_23.deleted_lines,commit.changed_files.file_2.hunks.hunk_23.method_info.method_name,commit.changed_files.file_2.hunks.hunk_23.method_info.method_params,commit.changed_files.file_2.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_24.Ismethod,commit.changed_files.file_2.hunks.hunk_24.added_lines,commit.changed_files.file_2.hunks.hunk_24.deleted_lines,commit.changed_files.file_2.hunks.hunk_24.method_info.method_name,commit.changed_files.file_2.hunks.hunk_24.method_info.method_params,commit.changed_files.file_2.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_25.Ismethod,commit.changed_files.file_2.hunks.hunk_25.added_lines,commit.changed_files.file_2.hunks.hunk_25.deleted_lines,commit.changed_files.file_2.hunks.hunk_25.method_info.method_name,commit.changed_files.file_2.hunks.hunk_25.method_info.method_params,commit.changed_files.file_2.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_26.Ismethod,commit.changed_files.file_2.hunks.hunk_26.added_lines,commit.changed_files.file_2.hunks.hunk_26.deleted_lines,commit.changed_files.file_2.hunks.hunk_26.method_info.method_name,commit.changed_files.file_2.hunks.hunk_26.method_info.method_params,commit.changed_files.file_2.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_27.Ismethod,commit.changed_files.file_2.hunks.hunk_27.added_lines,commit.changed_files.file_2.hunks.hunk_27.deleted_lines,commit.changed_files.file_2.hunks.hunk_27.method_info.method_name,commit.changed_files.file_2.hunks.hunk_27.method_info.method_params,commit.changed_files.file_2.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_28.Ismethod,commit.changed_files.file_2.hunks.hunk_28.added_lines,commit.changed_files.file_2.hunks.hunk_28.deleted_lines,commit.changed_files.file_2.hunks.hunk_28.method_info.method_name,commit.changed_files.file_2.hunks.hunk_28.method_info.method_params,commit.changed_files.file_2.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_29.Ismethod,commit.changed_files.file_2.hunks.hunk_29.added_lines,commit.changed_files.file_2.hunks.hunk_29.deleted_lines,commit.changed_files.file_2.hunks.hunk_29.method_info.method_name,commit.changed_files.file_2.hunks.hunk_29.method_info.method_params,commit.changed_files.file_2.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_2.hunks.hunk_30.Ismethod,commit.changed_files.file_2.hunks.hunk_30.added_lines,commit.changed_files.file_2.hunks.hunk_30.deleted_lines,commit.changed_files.file_2.hunks.hunk_30.method_info.method_name,commit.changed_files.file_2.hunks.hunk_30.method_info.method_params,commit.changed_files.file_2.hunks.hunk_30.method_info.method_startline,commit.changed_files.file_2.hunks.hunk_30.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_30.Ismethod,commit.changed_files.file_11.hunks.hunk_30.added_lines,commit.changed_files.file_11.hunks.hunk_30.deleted_lines,commit.changed_files.file_11.hunks.hunk_30.method_info.method_name,commit.changed_files.file_11.hunks.hunk_30.method_info.method_params,commit.changed_files.file_11.hunks.hunk_30.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_30.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_31.Ismethod,commit.changed_files.file_11.hunks.hunk_31.added_lines,commit.changed_files.file_11.hunks.hunk_31.deleted_lines,commit.changed_files.file_11.hunks.hunk_31.method_info.method_name,commit.changed_files.file_11.hunks.hunk_31.method_info.method_params,commit.changed_files.file_11.hunks.hunk_31.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_31.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_32.Ismethod,commit.changed_files.file_11.hunks.hunk_32.added_lines,commit.changed_files.file_11.hunks.hunk_32.deleted_lines,commit.changed_files.file_11.hunks.hunk_32.method_info.method_name,commit.changed_files.file_11.hunks.hunk_32.method_info.method_params,commit.changed_files.file_11.hunks.hunk_32.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_32.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_33.Ismethod,commit.changed_files.file_11.hunks.hunk_33.added_lines,commit.changed_files.file_11.hunks.hunk_33.deleted_lines,commit.changed_files.file_11.hunks.hunk_33.method_info.method_name,commit.changed_files.file_11.hunks.hunk_33.method_info.method_params,commit.changed_files.file_11.hunks.hunk_33.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_33.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_34.Ismethod,commit.changed_files.file_11.hunks.hunk_34.added_lines,commit.changed_files.file_11.hunks.hunk_34.deleted_lines,commit.changed_files.file_11.hunks.hunk_34.method_info.method_name,commit.changed_files.file_11.hunks.hunk_34.method_info.method_params,commit.changed_files.file_11.hunks.hunk_34.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_34.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_35.Ismethod,commit.changed_files.file_11.hunks.hunk_35.added_lines,commit.changed_files.file_11.hunks.hunk_35.deleted_lines,commit.changed_files.file_11.hunks.hunk_35.method_info.method_name,commit.changed_files.file_11.hunks.hunk_35.method_info.method_params,commit.changed_files.file_11.hunks.hunk_35.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_35.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_36.Ismethod,commit.changed_files.file_11.hunks.hunk_36.added_lines,commit.changed_files.file_11.hunks.hunk_36.deleted_lines,commit.changed_files.file_11.hunks.hunk_36.method_info.method_name,commit.changed_files.file_11.hunks.hunk_36.method_info.method_params,commit.changed_files.file_11.hunks.hunk_36.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_36.method_info.method_endline,commit.changed_files.file_11.hunks.hunk_37.Ismethod,commit.changed_files.file_11.hunks.hunk_37.added_lines,commit.changed_files.file_11.hunks.hunk_37.deleted_lines,commit.changed_files.file_11.hunks.hunk_37.method_info.method_name,commit.changed_files.file_11.hunks.hunk_37.method_info.method_params,commit.changed_files.file_11.hunks.hunk_37.method_info.method_startline,commit.changed_files.file_11.hunks.hunk_37.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_8.Ismethod,commit.changed_files.file_7.hunks.hunk_8.added_lines,commit.changed_files.file_7.hunks.hunk_8.deleted_lines,commit.changed_files.file_7.hunks.hunk_8.method_info.method_name,commit.changed_files.file_7.hunks.hunk_8.method_info.method_params,commit.changed_files.file_7.hunks.hunk_8.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_8.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_9.Ismethod,commit.changed_files.file_7.hunks.hunk_9.added_lines,commit.changed_files.file_7.hunks.hunk_9.deleted_lines,commit.changed_files.file_7.hunks.hunk_9.method_info.method_name,commit.changed_files.file_7.hunks.hunk_9.method_info.method_params,commit.changed_files.file_7.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_10.Ismethod,commit.changed_files.file_7.hunks.hunk_10.added_lines,commit.changed_files.file_7.hunks.hunk_10.deleted_lines,commit.changed_files.file_7.hunks.hunk_10.method_info.method_name,commit.changed_files.file_7.hunks.hunk_10.method_info.method_params,commit.changed_files.file_7.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_11.Ismethod,commit.changed_files.file_7.hunks.hunk_11.added_lines,commit.changed_files.file_7.hunks.hunk_11.deleted_lines,commit.changed_files.file_7.hunks.hunk_11.method_info.method_name,commit.changed_files.file_7.hunks.hunk_11.method_info.method_params,commit.changed_files.file_7.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_7.hunks.hunk_12.Ismethod,commit.changed_files.file_7.hunks.hunk_12.added_lines,commit.changed_files.file_7.hunks.hunk_12.deleted_lines,commit.changed_files.file_7.hunks.hunk_12.method_info.method_name,commit.changed_files.file_7.hunks.hunk_12.method_info.method_params,commit.changed_files.file_7.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_7.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_11.Ismethod,commit.changed_files.file_4.hunks.hunk_11.added_lines,commit.changed_files.file_4.hunks.hunk_11.deleted_lines,commit.changed_files.file_4.hunks.hunk_11.method_info.method_name,commit.changed_files.file_4.hunks.hunk_11.method_info.method_params,commit.changed_files.file_4.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_12.Ismethod,commit.changed_files.file_4.hunks.hunk_12.added_lines,commit.changed_files.file_4.hunks.hunk_12.deleted_lines,commit.changed_files.file_4.hunks.hunk_12.method_info.method_name,commit.changed_files.file_4.hunks.hunk_12.method_info.method_params,commit.changed_files.file_4.hunks.hunk_12.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_12.method_info.method_endline,commit.changed_files.file_26.hunks.hunk_0.Ismethod,commit.changed_files.file_26.hunks.hunk_0.added_lines,commit.changed_files.file_26.hunks.hunk_0.deleted_lines,commit.changed_files.file_27.hunks.hunk_0.Ismethod,commit.changed_files.file_27.hunks.hunk_0.added_lines,commit.changed_files.file_27.hunks.hunk_0.deleted_lines,commit.changed_files.file_30.file_change_type,commit.changed_files.file_30.file_Nmethod,commit.changed_files.file_30.file_old_name,commit.changed_files.file_30.file_new_name,commit.changed_files.file_30.hunks.hunk_0.Ismethod,commit.changed_files.file_30.hunks.hunk_0.added_lines,commit.changed_files.file_30.hunks.hunk_0.deleted_lines,commit.changed_files.file_30.hunks.hunk_0.method_info.method_name,commit.changed_files.file_30.hunks.hunk_0.method_info.method_params,commit.changed_files.file_30.hunks.hunk_0.method_info.method_startline,commit.changed_files.file_30.hunks.hunk_0.method_info.method_endline,commit.changed_files.file_31.file_change_type,commit.changed_files.file_31.file_Nmethod,commit.changed_files.file_31.file_old_name,commit.changed_files.file_31.file_new_name,commit.changed_files.file_31.hunks.hunk_0.Ismethod,commit.changed_files.file_31.hunks.hunk_0.added_lines,commit.changed_files.file_31.hunks.hunk_0.deleted_lines,commit.changed_files.file_32.file_change_type,commit.changed_files.file_32.file_Nmethod,commit.changed_files.file_32.file_old_name,commit.changed_files.file_32.file_new_name,commit.changed_files.file_32.hunks.hunk_0.Ismethod,commit.changed_files.file_32.hunks.hunk_0.added_lines,commit.changed_files.file_32.hunks.hunk_0.deleted_lines,commit.changed_files.file_33.file_change_type,commit.changed_files.file_33.file_Nmethod,commit.changed_files.file_33.file_old_name,commit.changed_files.file_33.file_new_name,commit.changed_files.file_33.hunks.hunk_0.Ismethod,commit.changed_files.file_33.hunks.hunk_0.added_lines,commit.changed_files.file_33.hunks.hunk_0.deleted_lines,commit.changed_files.file_34.file_change_type,commit.changed_files.file_34.file_Nmethod,commit.changed_files.file_34.file_old_name,commit.changed_files.file_34.file_new_name,commit.changed_files.file_34.hunks.hunk_0.Ismethod,commit.changed_files.file_34.hunks.hunk_0.added_lines,commit.changed_files.file_34.hunks.hunk_0.deleted_lines,commit.changed_files.file_3.hunks.hunk_17.Ismethod,commit.changed_files.file_3.hunks.hunk_17.added_lines,commit.changed_files.file_3.hunks.hunk_17.deleted_lines,commit.changed_files.file_3.hunks.hunk_17.method_info.method_name,commit.changed_files.file_3.hunks.hunk_17.method_info.method_params,commit.changed_files.file_3.hunks.hunk_17.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_17.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_18.Ismethod,commit.changed_files.file_3.hunks.hunk_18.added_lines,commit.changed_files.file_3.hunks.hunk_18.deleted_lines,commit.changed_files.file_3.hunks.hunk_18.method_info.method_name,commit.changed_files.file_3.hunks.hunk_18.method_info.method_params,commit.changed_files.file_3.hunks.hunk_18.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_18.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_19.Ismethod,commit.changed_files.file_3.hunks.hunk_19.added_lines,commit.changed_files.file_3.hunks.hunk_19.deleted_lines,commit.changed_files.file_3.hunks.hunk_19.method_info.method_name,commit.changed_files.file_3.hunks.hunk_19.method_info.method_params,commit.changed_files.file_3.hunks.hunk_19.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_19.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_20.Ismethod,commit.changed_files.file_3.hunks.hunk_20.added_lines,commit.changed_files.file_3.hunks.hunk_20.deleted_lines,commit.changed_files.file_3.hunks.hunk_20.method_info.method_name,commit.changed_files.file_3.hunks.hunk_20.method_info.method_params,commit.changed_files.file_3.hunks.hunk_20.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_20.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_21.Ismethod,commit.changed_files.file_3.hunks.hunk_21.added_lines,commit.changed_files.file_3.hunks.hunk_21.deleted_lines,commit.changed_files.file_3.hunks.hunk_21.method_info.method_name,commit.changed_files.file_3.hunks.hunk_21.method_info.method_params,commit.changed_files.file_3.hunks.hunk_21.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_21.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_22.Ismethod,commit.changed_files.file_3.hunks.hunk_22.added_lines,commit.changed_files.file_3.hunks.hunk_22.deleted_lines,commit.changed_files.file_3.hunks.hunk_22.method_info.method_name,commit.changed_files.file_3.hunks.hunk_22.method_info.method_params,commit.changed_files.file_3.hunks.hunk_22.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_22.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_23.Ismethod,commit.changed_files.file_3.hunks.hunk_23.added_lines,commit.changed_files.file_3.hunks.hunk_23.deleted_lines,commit.changed_files.file_3.hunks.hunk_23.method_info.method_name,commit.changed_files.file_3.hunks.hunk_23.method_info.method_params,commit.changed_files.file_3.hunks.hunk_23.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_23.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_24.Ismethod,commit.changed_files.file_3.hunks.hunk_24.added_lines,commit.changed_files.file_3.hunks.hunk_24.deleted_lines,commit.changed_files.file_3.hunks.hunk_24.method_info.method_name,commit.changed_files.file_3.hunks.hunk_24.method_info.method_params,commit.changed_files.file_3.hunks.hunk_24.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_24.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_25.Ismethod,commit.changed_files.file_3.hunks.hunk_25.added_lines,commit.changed_files.file_3.hunks.hunk_25.deleted_lines,commit.changed_files.file_3.hunks.hunk_25.method_info.method_name,commit.changed_files.file_3.hunks.hunk_25.method_info.method_params,commit.changed_files.file_3.hunks.hunk_25.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_25.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_26.Ismethod,commit.changed_files.file_3.hunks.hunk_26.added_lines,commit.changed_files.file_3.hunks.hunk_26.deleted_lines,commit.changed_files.file_3.hunks.hunk_26.method_info.method_name,commit.changed_files.file_3.hunks.hunk_26.method_info.method_params,commit.changed_files.file_3.hunks.hunk_26.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_26.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_27.Ismethod,commit.changed_files.file_3.hunks.hunk_27.added_lines,commit.changed_files.file_3.hunks.hunk_27.deleted_lines,commit.changed_files.file_3.hunks.hunk_27.method_info.method_name,commit.changed_files.file_3.hunks.hunk_27.method_info.method_params,commit.changed_files.file_3.hunks.hunk_27.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_27.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_28.Ismethod,commit.changed_files.file_3.hunks.hunk_28.added_lines,commit.changed_files.file_3.hunks.hunk_28.deleted_lines,commit.changed_files.file_3.hunks.hunk_28.method_info.method_name,commit.changed_files.file_3.hunks.hunk_28.method_info.method_params,commit.changed_files.file_3.hunks.hunk_28.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_28.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_29.Ismethod,commit.changed_files.file_3.hunks.hunk_29.added_lines,commit.changed_files.file_3.hunks.hunk_29.deleted_lines,commit.changed_files.file_3.hunks.hunk_29.method_info.method_name,commit.changed_files.file_3.hunks.hunk_29.method_info.method_params,commit.changed_files.file_3.hunks.hunk_29.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_29.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_30.Ismethod,commit.changed_files.file_3.hunks.hunk_30.added_lines,commit.changed_files.file_3.hunks.hunk_30.deleted_lines,commit.changed_files.file_3.hunks.hunk_30.method_info.method_name,commit.changed_files.file_3.hunks.hunk_30.method_info.method_params,commit.changed_files.file_3.hunks.hunk_30.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_30.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_31.Ismethod,commit.changed_files.file_3.hunks.hunk_31.added_lines,commit.changed_files.file_3.hunks.hunk_31.deleted_lines,commit.changed_files.file_3.hunks.hunk_31.method_info.method_name,commit.changed_files.file_3.hunks.hunk_31.method_info.method_params,commit.changed_files.file_3.hunks.hunk_31.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_31.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_32.Ismethod,commit.changed_files.file_3.hunks.hunk_32.added_lines,commit.changed_files.file_3.hunks.hunk_32.deleted_lines,commit.changed_files.file_3.hunks.hunk_32.method_info.method_name,commit.changed_files.file_3.hunks.hunk_32.method_info.method_params,commit.changed_files.file_3.hunks.hunk_32.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_32.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_33.Ismethod,commit.changed_files.file_3.hunks.hunk_33.added_lines,commit.changed_files.file_3.hunks.hunk_33.deleted_lines,commit.changed_files.file_3.hunks.hunk_33.method_info.method_name,commit.changed_files.file_3.hunks.hunk_33.method_info.method_params,commit.changed_files.file_3.hunks.hunk_33.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_33.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_34.Ismethod,commit.changed_files.file_3.hunks.hunk_34.added_lines,commit.changed_files.file_3.hunks.hunk_34.deleted_lines,commit.changed_files.file_3.hunks.hunk_34.method_info.method_name,commit.changed_files.file_3.hunks.hunk_34.method_info.method_params,commit.changed_files.file_3.hunks.hunk_34.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_34.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_35.Ismethod,commit.changed_files.file_3.hunks.hunk_35.added_lines,commit.changed_files.file_3.hunks.hunk_35.deleted_lines,commit.changed_files.file_3.hunks.hunk_35.method_info.method_name,commit.changed_files.file_3.hunks.hunk_35.method_info.method_params,commit.changed_files.file_3.hunks.hunk_35.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_35.method_info.method_endline,commit.changed_files.file_3.hunks.hunk_36.Ismethod,commit.changed_files.file_3.hunks.hunk_36.added_lines,commit.changed_files.file_3.hunks.hunk_36.deleted_lines,commit.changed_files.file_3.hunks.hunk_36.method_info.method_name,commit.changed_files.file_3.hunks.hunk_36.method_info.method_params,commit.changed_files.file_3.hunks.hunk_36.method_info.method_startline,commit.changed_files.file_3.hunks.hunk_36.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_13.Ismethod,commit.changed_files.file_4.hunks.hunk_13.added_lines,commit.changed_files.file_4.hunks.hunk_13.deleted_lines,commit.changed_files.file_4.hunks.hunk_13.method_info.method_name,commit.changed_files.file_4.hunks.hunk_13.method_info.method_params,commit.changed_files.file_4.hunks.hunk_13.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_13.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_14.Ismethod,commit.changed_files.file_4.hunks.hunk_14.added_lines,commit.changed_files.file_4.hunks.hunk_14.deleted_lines,commit.changed_files.file_4.hunks.hunk_14.method_info.method_name,commit.changed_files.file_4.hunks.hunk_14.method_info.method_params,commit.changed_files.file_4.hunks.hunk_14.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_14.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_15.Ismethod,commit.changed_files.file_4.hunks.hunk_15.added_lines,commit.changed_files.file_4.hunks.hunk_15.deleted_lines,commit.changed_files.file_4.hunks.hunk_15.method_info.method_name,commit.changed_files.file_4.hunks.hunk_15.method_info.method_params,commit.changed_files.file_4.hunks.hunk_15.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_15.method_info.method_endline,commit.changed_files.file_4.hunks.hunk_16.Ismethod,commit.changed_files.file_4.hunks.hunk_16.added_lines,commit.changed_files.file_4.hunks.hunk_16.deleted_lines,commit.changed_files.file_4.hunks.hunk_16.method_info.method_name,commit.changed_files.file_4.hunks.hunk_16.method_info.method_params,commit.changed_files.file_4.hunks.hunk_16.method_info.method_startline,commit.changed_files.file_4.hunks.hunk_16.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_9.Ismethod,commit.changed_files.file_5.hunks.hunk_9.added_lines,commit.changed_files.file_5.hunks.hunk_9.deleted_lines,commit.changed_files.file_5.hunks.hunk_9.method_info.method_name,commit.changed_files.file_5.hunks.hunk_9.method_info.method_params,commit.changed_files.file_5.hunks.hunk_9.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_9.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_10.Ismethod,commit.changed_files.file_5.hunks.hunk_10.added_lines,commit.changed_files.file_5.hunks.hunk_10.deleted_lines,commit.changed_files.file_5.hunks.hunk_10.method_info.method_name,commit.changed_files.file_5.hunks.hunk_10.method_info.method_params,commit.changed_files.file_5.hunks.hunk_10.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_10.method_info.method_endline,commit.changed_files.file_5.hunks.hunk_11.Ismethod,commit.changed_files.file_5.hunks.hunk_11.added_lines,commit.changed_files.file_5.hunks.hunk_11.deleted_lines,commit.changed_files.file_5.hunks.hunk_11.method_info.method_name,commit.changed_files.file_5.hunks.hunk_11.method_info.method_params,commit.changed_files.file_5.hunks.hunk_11.method_info.method_startline,commit.changed_files.file_5.hunks.hunk_11.method_info.method_endline,commit.changed_files.file_13.hunks.hunk_1.Ismethod,commit.changed_files.file_13.hunks.hunk_1.added_lines,commit.changed_files.file_13.hunks.hunk_1.deleted_lines,commit.changed_files.file_13.hunks.hunk_1.method_info.method_name,commit.changed_files.file_13.hunks.hunk_1.method_info.method_params,commit.changed_files.file_13.hunks.hunk_1.method_info.method_startline,commit.changed_files.file_13.hunks.hunk_1.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_3.Ismethod,commit.changed_files.file_16.hunks.hunk_3.added_lines,commit.changed_files.file_16.hunks.hunk_3.deleted_lines,commit.changed_files.file_16.hunks.hunk_3.method_info.method_name,commit.changed_files.file_16.hunks.hunk_3.method_info.method_params,commit.changed_files.file_16.hunks.hunk_3.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_3.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_4.Ismethod,commit.changed_files.file_16.hunks.hunk_4.added_lines,commit.changed_files.file_16.hunks.hunk_4.deleted_lines,commit.changed_files.file_16.hunks.hunk_4.method_info.method_name,commit.changed_files.file_16.hunks.hunk_4.method_info.method_params,commit.changed_files.file_16.hunks.hunk_4.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_4.method_info.method_endline,commit.changed_files.file_16.hunks.hunk_5.Ismethod,commit.changed_files.file_16.hunks.hunk_5.added_lines,commit.changed_files.file_16.hunks.hunk_5.deleted_lines,commit.changed_files.file_16.hunks.hunk_5.method_info.method_name,commit.changed_files.file_16.hunks.hunk_5.method_info.method_params,commit.changed_files.file_16.hunks.hunk_5.method_info.method_startline,commit.changed_files.file_16.hunks.hunk_5.method_info.method_endline
1092,jeremyjordan,2020-03-08T15:46:05Z,2020-03-19T13:23:19Z,tqdm fails in notebook for versions tqdm&lt; 4.41.0,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 tqdm_notebook doesn't have a  method until <denchmark-link:https://github.com/tqdm/tqdm/releases/tag/v4.41.0>version 4.41.0</denchmark-link>
 . Training fails in notebooks if you have a lower version of tqdm (current requirements enforce >=4.35.0).
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Train any model with tqdm<4.41.0 in a Colab notebook.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-code>AttributeError: 'tqdm_notebook' object has no attribute 'reset'
 </denchmark-code>
 
 	",36274bed49cf40c0d9bdabb3058674879badf1e4,Jeremy Jordan,2020-03-19 09:23:18-04:00,MODIFY,0,requirements.txt,requirements.txt,0.0,1,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1114,alexeykarnachev,2020-03-10T22:15:58Z,2020-03-16T18:35:11Z,ReduceLROnPlateau scheduler type check,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Incorrect type check for scheduler of class ReduceLROnPlateau.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 713
       in
       bc01b9a
 
 
 
 
 
 
  isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau) 
 
 
 
 
 
 I believe, this check:
 isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau)
 must look like this:
 isinstance(scheduler['scheduler'], optim.lr_scheduler.ReduceLROnPlateau)
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Create a scheduler of type optim.lr_scheduler.ReduceLROnPlateau in the configure_optimizers method of a LightningModule class.
 Return an optimizer and scheduler from this method. Place them in lists: return [optimizer], [scheduler].
 Execute the trainer.fit(module).
 Put a break-point here: 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 712
       in
       bc01b9a
 
 
 
 
 
 
  scheduler['reduce_on_plateau'] = \ 
 
 
 
 
 
 Make sure that the condition is never True.
 
 	",384e124490f7a629dc677fc5b658b69afade0a04,Nicki Skafte,2020-03-16 14:35:10-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,31,31,1.0,alexeykarnachev,2020-03-10T22:16:40Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,alexeykarnachev,2020-03-11T23:17:27Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  could you pls check it? as it comes from your PR :]
 		",3.0,alexeykarnachev,2020-03-12T11:23:00Z,"
 		I agree with <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  that this is indeed a bug, that I introduced with the recent rework of the learning rate schedulers. I can send a new PR with the bug fix and include a test for  schedulers
 		",4.0,alexeykarnachev,2020-03-12T11:38:41Z,"
 		Thx guys!
 <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  minds send fix PR?
 		",MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"710,711","710,711",configure_schedulers,"self,list",698,724,MODIFY,0.0,tests\models\__init__.py,tests\models\__init__.py,0.0,"27,28",27,MODIFY,1.0,tests\models\mixins.py,tests\models\mixins.py,1.0,"682,683,684,685,686,687,688",,configure_optimizers,self,682,688,MODIFY,1.0,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,"152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181",,test_reduce_lr_on_plateau_scheduling,tmpdir,152,181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1116,bkkaggle,2020-03-11T02:47:18Z,2020-03-30T22:45:07Z,Wandb logger doesn't upload saved model checkpoint for final epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training a model on the TPU and using the wandb logger, the checkpoint for the last epoch trained doesn't get uploaded to wandb.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Colab notebook: <denchmark-link:https://colab.research.google.com/drive/1oPaRWGZcz6YEol012xFADN42LV-jowtT>https://colab.research.google.com/drive/1oPaRWGZcz6YEol012xFADN42LV-jowtT</denchmark-link>
 
 	",a707d4bea1a78a98265fd1ea5b7a7a6cadc37fb9,Bilal Khan,2020-03-30 18:45:06-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,35,,1.0,bkkaggle,2020-03-11T13:40:37Z,"
 		Hello, could you be mo specific what is not working and ho are you using it?
 I see in the notebook for example if __name__ == ""__main__"": parser = argparse.ArgumentParser() which is not correct
 		",2.0,bkkaggle,2020-03-11T23:42:53Z,"
 		I'm finetuning distilgpt2 on 8 cores of a colab TPU and using the wandb logger. The main problem seems to be that at the end of training, each tpu process and the wandb logger shut down before the built in ModelCheckpoint callback finishes saving the checkpoint for the epoch to disk.
 Relevant part of stack trace:
 <denchmark-code>Epoch 1: 100%|████| 327/327 [01:39<00:00,  1.65it/s, loss=3.468, v_num=2211mbhk]
 Epoch 2:  91%|███▌| 296/327 [01:15<01:51,  3.59s/it, loss=3.529, v_num=2211mbhk]
 Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]
 Epoch 2:  91%|███▋| 297/327 [01:24<02:31,  5.05s/it, loss=3.529, v_num=2211mbhk]
 Epoch 2:  92%|███▋| 301/327 [01:24<01:32,  3.54s/it, loss=3.529, v_num=2211mbhk]
 Epoch 2:  94%|███▊| 307/327 [01:24<00:49,  2.48s/it, loss=3.529, v_num=2211mbhk]
 Epoch 2:  96%|███▊| 314/327 [01:24<00:22,  1.74s/it, loss=3.529, v_num=2211mbhk]
 Epoch 2:  98%|███▉| 321/327 [01:24<00:07,  1.23s/it, loss=3.529, v_num=2211mbhk]
 wandb: Waiting for W&B process to finish, PID 2784
 wandb: Program ended successfully.
 
 wandb: Waiting for W&B process to finish, PID 2784
 
 wandb: Waiting for W&B process to finish, PID 2784
 
 wandb: Waiting for W&B process to finish, PID 2784
 Epoch 2: 100%|████| 327/327 [01:24<00:00,  1.23s/it, loss=3.529, v_num=2211mbhk]
                                                                                 
 wandb: Waiting for W&B process to finish, PID 2784
 
 wandb: Waiting for W&B process to finish, PID 2784
 
 wandb: Waiting for W&B process to finish, PID 2784
 wandb: Run summary:
 wandb:        global_step 591
 wandb:           _runtime 242.3862087726593
 wandb:      learning_rate 3.7664783427495294e-07
 wandb:         train_loss 3.7114081382751465
 wandb:              _step 61
 wandb:         _timestamp 1583893453.517989
 wandb:           val_loss 3.4930191040039062
 wandb:            val_ppl 32.88510513305664
 wandb:   adjusted_val_ppl 55.14464569091797
 wandb: Syncing files in wandb/run-20200311_022014-2211mbhk:
 wandb:   code/finetune.py
 wandb:   epoch=0.ckpt
 wandb:   epoch=1.ckpt.part
 wandb: plus 7 W&B file(s) and 2 media file(s)
 </denchmark-code>
 
 		",3.0,bkkaggle,2020-03-14T02:05:16Z,"
 		<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>
  <denchmark-link:https://github.com/calclavia>@calclavia</denchmark-link>
  pls ^^
 		",4.0,bkkaggle,2020-03-14T03:16:48Z,"
 		<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>
  does it happen also if you use a jupyter notebook or is it specific to collab?
 wandb has also a way to let you run experiments offline and sync afterwards.
 		",MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,,"105,106,107,108,109,110",finalize,"self,str",105,110,MODIFY,1.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,,"33,34,35,36,37,38,39,40,41,42,43,44,45,46",,,,,,,,,,,,,,,,,,,,,,,5.0,bkkaggle,2020-03-14T23:13:43Z,"
 		<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>
  I just took a look at this, couple things to note.
 
 Never put your api key in a notebook.  Instead use:
 
 import wandb
 wandb.login()
 And it will prompt you for your api key if it's not in the notebook env yet.
 
 Could this be related to the early stopping callback?  I'm not familiar with the mechanics of lighting and TPU's but this seems to be caused by triggering the finalize action of the WandbLogger before the model has actually been finalized.
 
 		",6.0,bkkaggle,2020-03-15T16:41:49Z,"
 		I've set the early stopping callback to have a patience of 10 epochs with
 early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)
 so I don't think it should be the problem. I also updated the code to use wandb.login() so the API key isn't visible anymore.
 		",7.0,bkkaggle,2020-03-15T18:07:33Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 I overwrote the collab by removing the finalize method and just tracking calls:
 class WandbLogger(WandbLogger):
     @rank_zero_only
     def finalize(self, status: str = 'success') -> None:
         print('Calling finalize')
         '''try:
             exit_code = 0 if status == 'success' else 1
             wandb.join(exit_code)
         except TypeError:
             wandb.join()'''
         print('Called finalize')
 I get following error stack:
 wandb: Tracking run with wandb version 0.8.29
 wandb: Run data is saved locally in wandb/run-20200315_175429-1ig7n4xi
 wandb: Syncing run smooth-leaf-2
 wandb: ⭐️ View project at https://app.wandb.ai/borisd13/lm-finetuning
 wandb: 🚀 View run at https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi
 wandb: Run `wandb off` to turn off syncing.
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:82: UserWarning: Checkpoint directory /content/wandb/run-20200315_175429-1ig7n4xi exists and is not empty with save_top_k != 0.All files in this directory will be deleted when a checkpoint is saved!
   f""Checkpoint directory {filepath} exists and is not empty with save_top_k != 0.""
 100%|█████████████████████████████████████████████| 1/1 [00:09<00:00,  9.95s/it]
 Dataset created in 10 seconds
 Dataset length: 9444
 Num tokens: 2417664 | Num original tokens: 2088674
 2020-03-15 17:54:45.061374: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.080550: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.099582: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.119609: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.139724: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.159427: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 2020-03-15 17:54:45.179451: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 E0315 17:54:52.681799302    8753 server_chttp2.cc:40]        {""created"":""@1584294892.681777447"",""description"":""Only 1 addresses added out of total 2 resolved"",""file"":""external/com_github_grpc_grpc/src/core/ext/transport/chttp2/server/chttp2_server.cc"",""file_line"":404,""referenced_errors"":[{""created"":""@1584294892.681774375"",""description"":""Address family not supported by protocol"",""errno"":97,""file"":""external/com_github_grpc_grpc/src/core/lib/iomgr/socket_utils_common_posix.cc"",""file_line"":420,""os_error"":""Address family not supported by protocol"",""syscall"":""socket"",""target_address"":""[::1]:50549""}]}
 2020-03-15 17:54:53.917624: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:53.918498: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:54.150331: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:54.281601: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:54.655386: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:54.830783: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:55.214524: I tensorflow/compiler/xla/xla_client/computation_client.cc:197] Fetching mesh configuration for worker tpu_worker:0 from mesh service at localhost:50549
 2020-03-15 17:54:59.626295: I tensorflow/compiler/xla/xla_client/mesh_service.cc:208] Waiting to connect to client mesh master (300 seconds) localhost:50549
 0it [00:00, ?it/s]
   0%|                                                     | 0/1 [00:00<?, ?it/s]
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.41it/s]Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.40it/s]
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.36it/s]
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]
 Dataset created in 0 seconds
 Dataset length: 976
 Num tokens: 249856 | Num original tokens: 217646
 Epoch 1:   0%|                                          | 0/327 [00:00<?, ?it/s]/pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 /pytorch/torch/csrc/utils/python_arg_parser.cpp:739: UserWarning: This overload of add_ is deprecated:
 	add_(Number alpha, Tensor other)
 Consider using one of the following signatures instead:
 	add_(Tensor other, Number alpha)
 Epoch 1:  91%|███▌| 296/327 [01:13<00:21,  1.47it/s, loss=3.468, v_num=1ig7n4xi]
 Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]
 Epoch 1:  91%|███▋| 297/327 [01:21<01:24,  2.82s/it, loss=3.468, v_num=1ig7n4xi]
 Epoch 1:  91%|███▋| 298/327 [01:23<01:11,  2.47s/it, loss=3.468, v_num=1ig7n4xi]
 Epoch 1:  93%|███▋| 305/327 [01:23<00:38,  1.73s/it, loss=3.468, v_num=1ig7n4xi]
 Epoch 1:  95%|███▊| 312/327 [01:23<00:18,  1.22s/it, loss=3.468, v_num=1ig7n4xi]
 Epoch 1:  98%|███▉| 319/327 [01:23<00:06,  1.17it/s, loss=3.468, v_num=1ig7n4xi]
 Epoch 1: 100%|████| 327/327 [01:24<00:00,  1.65it/s, loss=3.468, v_num=1ig7n4xi]
                                                                                 /usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.
   warnings.warn(SAVE_STATE_WARNING, UserWarning)
 tcmalloc: large alloc 1704353792 bytes == 0x151728000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e307c 0x4e3120 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9
 Epoch 1: 100%|████| 327/327 [01:41<00:00,  1.65it/s, loss=3.468, v_num=1ig7n4xi]tcmalloc: large alloc 2568052736 bytes == 0x1d72fe000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48
 Epoch 2:  91%|███▌| 296/327 [01:13<01:47,  3.47s/it, loss=3.529, v_num=1ig7n4xi]
 Validating:   0%|                                        | 0/31 [00:00<?, ?it/s]
 Epoch 2:  91%|███▋| 297/327 [01:21<02:26,  4.88s/it, loss=3.529, v_num=1ig7n4xi]
 Epoch 2:  92%|███▋| 300/327 [01:21<01:32,  3.43s/it, loss=3.529, v_num=1ig7n4xi]
 Epoch 2:  94%|███▊| 307/327 [01:21<00:48,  2.40s/it, loss=3.529, v_num=1ig7n4xi]
 Epoch 2:  95%|███▊| 312/327 [01:21<00:25,  1.69s/it, loss=3.529, v_num=1ig7n4xi]
 Epoch 2:  98%|███▉| 319/327 [01:21<00:09,  1.19s/it, loss=3.529, v_num=1ig7n4xi]
 Epoch 2: 100%|███▉| 326/327 [01:21<00:00,  1.20it/s, loss=3.529, v_num=1ig7n4xi]Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Calling finalize
 Called finalize
 Epoch 2: 100%|████| 327/327 [01:21<00:00,  1.20it/s, loss=3.529, v_num=1ig7n4xi]
                                                                                 tcmalloc: large alloc 1704116224 bytes == 0x151728000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x4e307c 0x4e3120 0x4e3024 0x4e34a4 0x4e307c 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9
 tcmalloc: large alloc 2567823360 bytes == 0x1e993e000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48
 Epoch 2: 100%|████| 327/327 [01:50<00:00,  2.95it/s, loss=3.529, v_num=1ig7n4xi]
 Calling finalize
 Called finalize
 tcmalloc: large alloc 2567823360 bytes == 0x1e993e000 @  0x7fd8343af2a4 0x592727 0x4dddf7 0x4ddece 0x4e24ad 0x4e25eb 0x4e1430 0x4e2edb 0x4e284a 0x4e14d8 0x4e2c5b 0x4e27b2 0x4e1430 0x4e2c5b 0x5ec012 0x4e1683 0x4e2c5b 0x4e3120 0x4e3ac6 0x5ebdc2 0x50a94c 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x508245 0x50a080 0x50aa7d 0x50c5b9 0x509d48
 Traceback (most recent call last):
   File ""finetune.py"", line 387, in <module>
     trainer.fit(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 615, in fit
     self.load_spawn_weights(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 366, in load_spawn_weights
     loaded_model = original_model.__class__.load_from_checkpoint(path)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/lightning.py"", line 1389, in load_from_checkpoint
     model = cls._load_model_state(checkpoint)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/lightning.py"", line 1419, in _load_model_state
     model = cls(*model_args)
 TypeError: __init__() missing 1 required positional argument: 'args'
 
 wandb: Waiting for W&B process to finish, PID 8665
 wandb: Program failed with code 1. Press ctrl-c to abort syncing.
 wandb: Run summary:
 wandb:         train_loss 3.7114081382751465
 wandb:      learning_rate 3.7664783427495294e-07
 wandb:         _timestamp 1584295103.1529658
 wandb:        global_step 591
 wandb:              _step 61
 wandb:           _runtime 236.35386848449707
 wandb:            val_ppl 32.88510513305664
 wandb:           val_loss 3.4930191040039062
 wandb:   adjusted_val_ppl 55.14464569091797
 wandb: Syncing files in wandb/run-20200315_175429-1ig7n4xi:
 wandb:   code/finetune.py
 wandb:   epoch=0.ckpt
 wandb:   epoch=1.ckpt
 wandb: plus 7 W&B file(s) and 2 media file(s)
 wandb:                                                                                
 wandb: Synced smooth-leaf-2: https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi
 We can see 2 issues (I believe unrelated to wandb):
 
 WandbLogger.finalize method is called before ""Epoch 2"" is finished -> could be related to reaching the end of dataloaders?
 Another error appears with load_spawn_weights
 
 Note that the checkpoints get synchronized correctly with W&B now that we ignore the  call: <denchmark-link:https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi>https://app.wandb.ai/borisd13/lm-finetuning/runs/1ig7n4xi</denchmark-link>
 
 You are probably more familiar with the internals of pytorch_lightning if it's related to it but let me know if I can be of any further help.
 		",8.0,bkkaggle,2020-03-15T21:11:53Z,"
 		Thanks for the fix.
 The reason why the error message TypeError: __init__() missing 1 required positional argument: 'args' is probably happening is because I pass a custom parameter to my LightningModel
 <denchmark-code>class LM(pl.LightningModule):
     def __init__(self, args):
         super(LM, self).__init__()
 
         self.args = args
 </denchmark-code>
 
 I'll change my code to follow pytorch-lightning's recommended way of setting hyperparameters (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html>https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html</denchmark-link>
 ) so this error message should go away.
 Should I close the issue now or wait until a fix is pushed up?
 		",9.0,bkkaggle,2020-03-15T22:06:05Z,"
 		<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>
  thx for your help and tracing back the issues. 
 about the  I think that we have been fixing it in the last release
 the  is explicitly called in the cleaning phase after killing the training
 
 
 		",10.0,bkkaggle,2020-03-15T22:47:12Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  What is strange in this instance is that  seems to be called while Epoch 2 is still running (as we get few more prints after).
 However it may be related to some issues with the implementation of this specific example.
 		",11.0,bkkaggle,2020-03-16T22:55:53Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>
  any thought?
 		",12.0,bkkaggle,2020-03-17T01:16:52Z,"
 		should we only set self.logger.finalize(""success"") on the main process? (and after all other nodes are finished)
 		",13.0,bkkaggle,2020-03-18T22:39:55Z,"
 		Hi, i've updated the colab notebook with <denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>
  's override of 's  method and updated the  to use take command line args as so it now runs without errors.
 WandbLogger already has the @rank_zero_only decorator applied to its finalize method, so shouldn't it already only be called on the main process?
     @rank_zero_only
     def finalize(self, status: str = 'success') -> None:
 		",14.0,bkkaggle,2020-03-18T23:06:50Z,"
 		<denchmark-link:https://github.com/bkkaggle>@bkkaggle</denchmark-link>
  mind sending a PR?
 		",15.0,bkkaggle,2020-03-19T01:21:33Z,"
 		Sure
 		",test_wandb_logger,wandb,13,48,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1119,xingzhaolee,2020-03-11T13:02:06Z,2020-03-12T14:50:01Z,Checkpoint fails in single node multi-GPU mode using  DDP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Checkpoint fails in single node multi-GPU mode using  DDP.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 python pl_examples/basic_examples/gpu_template.py --distributed_backend ddp --gpus 2
 Epoch 2: : 700it [00:28, 42.69it/s, l/home/xz/anaconda3/envs/x/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown                                                                                                                                                                                                                 
   len(cache))
 Traceback (most recent call last):
   File ""gpu_template.py"", line 79, in <module>
     main(hyperparams)
   File ""gpu_template.py"", line 40, in main
     trainer.fit(model)
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 590, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 171, in spawn
     while not spawn_context.join():
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 118, in join
     raise Exception(msg)
 Exception: 
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
     fn(i, *args)
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 342, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 830, in run_pretrain_routine
     self.train()
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 343, in train
     self.run_training_epoch()
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 452, in run_training_epoch
     self.call_checkpoint_callback()
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 737, in call_checkpoint_callback
     self.checkpoint_callback.on_validation_end(self, self.get_model())
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 204, in on_validation_end
     self._do_check_save(filepath, current, epoch)
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 221, in _do_check_save
     self._del_model(delpath)
   File ""/home/xz/anaconda3/envs/x/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 121, in _del_model
     os.remove(filepath)
 FileNotFoundError: [Errno 2] No such file or directory: '/home/xz/pytorch-lightning/pl_examples/basic_examples/lightning_logs/version_1/checkpoints/epoch=0.ckpt'
 	",b4d4e489bf413ebf3288d29c5905d2292ce18d58,Xing Zhao LEE,2020-03-12 15:50:00+01:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"178,179,180,181",,1.0,xingzhaolee,2020-03-11T13:43:26Z,"
 		yeah we shall run all examples in CI too
 		",2.0,xingzhaolee,2020-03-11T20:39:10Z,"
 		I believe this happens with multiple gpus as well. And it only seems to happen if ModelCheckpoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...
 		",3.0,xingzhaolee,2020-03-12T02:19:25Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  fixed. part of the code that caused the bug was removed a few commits back.
 		",4.0,xingzhaolee,2020-03-12T03:28:12Z,"
 		fix is in master?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,xingzhaolee,2020-03-12T03:32:04Z,"
 		fix for DDP checkpoint is in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1125>#1125</denchmark-link>
 , still waiting for it to be reviewed and merged.
 
 I believe this happens with multiple gpus as well. And it only seems to happen if ModelChckepoint(save_top_k) is set greater than 1. Still converting models to 0.7.1 but wanted to share this ...
 
 as for this issue, on my side it seems to work fine. can you double check?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",177,218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1131,stathius,2020-03-12T23:01:45Z,2020-03-30T22:43:54Z,Better message when DataLoader is wrong,"
 On the verge between bug and improvement.
 There was a bug in my Validation DataLoader and was returning irrelevant staff. Accidentally the length was 0.  Probably an edge case combination. The error I was getting during the validation sanity check was quite cryptic:
 <denchmark-code>Traceback (most recent call last):
   File ""UNet_WaveProp.py"", line 174, in <module>
     trainer.fit(model)
   File ""/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 629, in fit
     self.run_pretrain_routine(model)
   File ""/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 809, in run_pretrain_routine
     False)
   File ""/mnt/RDS/home/code/pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py"", line 300, in evaluate
     eval_results = model.validation_epoch_end(outputs)
   File ""UNet_WaveProp.py"", line 138, in validation_epoch_end
     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
 RuntimeError: stack expects a non-empty TensorList
 </denchmark-code>
 
 I had to go through the code of pytorch-lightning for few hours to understand what was happening.
 Maybe a more informative message would make more sense?
 One thing would be to check if the DataLoader's size is 0.
 What do you think? I could take a stab at a PR.
 	",2ccc7456ca421afcdfba0c4482635c99e2593f70,Nicki Skafte,2020-03-30 18:43:53-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,21,,1.0,stathius,2020-03-12T23:02:29Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,stathius,2020-03-14T01:07:39Z,"
 		yeah, better error messing always helps, thx and PR is welcome! :]
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"29,30,33,34,35",31,_has_len,DataLoader,28,38,MODIFY,0.0,tests\base\__init__.py,tests\base\__init__.py,0.0,"28,29",28,MODIFY,1.0,tests\base\mixins.py,tests\base\mixins.py,1.0,"258,259,260,261,262",,train_dataloader,self,258,262,MODIFY,1.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484",,test_error_on_zero_len_dataloader,tmpdir,464,484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1139,alexeykarnachev,2020-03-13T12:22:58Z,2020-03-24T18:55:29Z,Can't cast Trainer automatically generated args to their required types,"
 <denchmark-h:h2>❓ Questions and Help</denchmark-h>
 
 <denchmark-h:h4>What is your question?</denchmark-h>
 
 I'm not sure, that this is a bug, so I put it like a question.
 The problem is: if I want to add the Trainer arguments to my custom ArgumentParser object, I call the add_argparse_args Trainer classmethod. But this method doesn't cast the Trainer arguments to their required types.
 It forces me to cast the arguments by myself. Like so:
 trainer_args.update(
         {
             'accumulate_grad_batches': int(trainer_args['accumulate_grad_batches']),
             'train_percent_check': float(trainer_args['train_percent_check']),
             'val_percent_check': float(trainer_args['val_percent_check']),
             'val_check_interval': int(trainer_args['val_check_interval']),
             'track_grad_norm': int(trainer_args['track_grad_norm']),
             'max_epochs': int(trainer_args['max_epochs']),
             'precision': int(trainer_args['precision']),
             'gradient_clip_val': float(trainer_args['gradient_clip_val']),
         }
 )
 And after that, I can pass updated arguments to the Trainer:
 trainer = pytorch_lightning.Trainer(
         **trainer_args
 )
 And I can't find a central place, where the Trainer handles an automatically generated arguments (their types).
 <denchmark-h:h4>What have you tried?</denchmark-h>
 
 I've tried to pass arguments to the trainer without handling their types. For instance, if i'll not cast accumulate_grad_batches to the integer type, the exception will be raised:
 <denchmark-code>TypeError: Gradient accumulation supports only int and dict types
 </denchmark-code>
 
 <denchmark-h:h4>What's your environment?</denchmark-h>
 
 
 OS: [Linux]
 Packaging [pip]
 Version [0.7.1]
 
 	",ced662fc2790058f5a55ca20244b31003c970ee5,Alexey Karnachev,2020-03-24 14:55:27-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"32,33,34",32,1.0,alexeykarnachev,2020-03-13T13:08:27Z,"
 		<denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  could you pls give a complete example, I am missing which is the ... here it seems like you want to update the  by itself
 <denchmark-code>trainer_args.update({'accumulate_grad_batches': int(trainer_args['accumulate_grad_batches']), ...)
 </denchmark-code>
 
 if you need the default values, pls use default_attributes
 		",2.0,alexeykarnachev,2020-03-13T13:37:10Z,"
 		Yes, sorry for the insufficient details from my side.
 Here is a gist:
 <denchmark-link:https://gist.github.com/alexeykarnachev/fd010b797d92873905780c856e9334d5>https://gist.github.com/alexeykarnachev/fd010b797d92873905780c856e9334d5</denchmark-link>
 
 It's not runnable script, because I've simplified it a lot. But it contains the problem i've described.
 If I'll execute this script with any Trainer argument, for example:
 python script.py --my_custom_arg_1=1 --my_custom_arg_n=2 --accumulate_grad_batches=1
 It will fail without the types casting, because accumulate_grad_batches will be interpreted as a string, and Trainer will fail.
 		",3.0,alexeykarnachev,2020-03-14T01:57:13Z,"
 		if do understand your use-case you want to have some parameters to be passed from CMD and/but some Trainer arguments to be fixed - forced to have a certain value, right?
 <denchmark-code>fixed_args = {
             'logger': tb_logger_callback,
             'checkpoint_callback': model_checkpoint_callback,
             'show_progress_bar': True,
             'progress_bar_refresh_rate': 1,
             'row_log_interval': 1,
 }
 trainer_args = vars(args)
 trainer_args.update(fixed_args)
 </denchmark-code>
 
 but in the case of accumulate_grad_batches (and some others) we shall do some casting eg with eval
 Woudl you mind sending a PR to fix Trainer.from_argparse_args(...) and/or Trainer.add_argparse_args(...) containing also type?
 		",4.0,alexeykarnachev,2020-03-14T08:30:48Z,"
 		Yes, I'll do it.
 But do you have any suggestions of how can it be performed ?
 I see it like this:
 in the Trainer.add_argparse_args classmethod there is a loop:
 <denchmark-code>        for arg in trainer_default_params:
             parser.add_argument(
                 f'--{arg}',
                 default=trainer_default_params[arg],
                 dest=arg,
                 help='autogenerated by pl.Trainer'
             )
 </denchmark-code>
 
 And it's possible to pass a type argument to the parser.add_argument, but this ""type"" first needs to be obtained somehow.
 I suppose, that it can be done via the signature inspection in the Trainer.default_attributes classmethod.
 		",MODIFY,0.0,pl_examples\full_examples\semantic_segmentation\models\unet\model.py,pl_examples\full_examples\semantic_segmentation\models\unet\model.py,0.0,16,,,,,,MODIFY,0.0,pl_examples\full_examples\semantic_segmentation\models\unet\parts.py,pl_examples\full_examples\semantic_segmentation\models\unet\parts.py,0.0,"11,31,49",,MODIFY,0.0,pl_examples\full_examples\semantic_segmentation\semseg.py,pl_examples\full_examples\semantic_segmentation\semseg.py,0.0,"37,124",,,,,,MODIFY,3.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513","472,474,477,478,479,480,481,482,483",get_init_arguments_and_types,cls,472,513,5.0,alexeykarnachev,2020-03-14T16:34:34Z,"
 		Please, consider a PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1147>#1147</denchmark-link>
 
 		",6.0,alexeykarnachev,2020-03-17T11:31:00Z,"
 		don’t we already have support for this?
 		",7.0,alexeykarnachev,2020-03-17T11:32:09Z,"
 		<denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.7.1/hyperparameters.html#trainer-args>https://pytorch-lightning.readthedocs.io/en/0.7.1/hyperparameters.html#trainer-args</denchmark-link>
 
 		",8.0,alexeykarnachev,2020-03-17T12:22:25Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  this issue is about assigning types for the automatically added arguments. By default argparser assumes that they are strings. And these string typed args will be passed to the trainer constructor, and will break it. So, this issue is about types casting, but not about any new functionality
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"516,517,518,519,520,521,522",,get_deprecated_arg_names,cls,516,522,1.0,"526,528,529,530,531,532,533,534,535,536,538,539,540,542,543,544,545,546,547,548,549,550,551,552,553,554,555",,add_argparse_args,"cls,ArgumentParser",525,557,MODIFY,2.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,,"631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646",test_default_args,tmpdir,631,646,1.0,253,,test_model_checkpoint_options,tmpdir,251,398,ADD,0.0,None,tests\trainer\test_trainer_cli.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1143,rmrao,2020-03-13T17:21:41Z,2020-04-06T12:13:25Z,`use_amp` is broken in 0.7.0,"
 I see that use_amp is deprecated but since the Trainer still accepts it as an argument, I believe this is still a bug.
 The issue is that the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/3c2fd560aa4d31b4f48ee225b83361deec53d9c7/pytorch_lightning/core/hooks.py#L136-L143>hook that deals with this</denchmark-link>
  specifically checks if . However, the Trainer does not set  if .
 Proposed solution:
 if use_amp is passed in, set precision = 16 and raise a deprecation warning.
 	",4ed3027309fe1882554e9b7ffe33f1aa92c88106,Roshan Rao,2020-04-06 08:13:24-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"84,85",,1.0,rmrao,2020-03-14T00:51:07Z,"
 		that s a good catch, thx! could send a fix PR? 🤖
 		",2.0,rmrao,2020-03-14T01:01:24Z,"
 		Submitted!
 		",,,,,,,,,MODIFY,1.0,pl_examples\basic_examples\gpu_template.py,pl_examples\basic_examples\gpu_template.py,1.0,35,35,main,hparams,18,41,MODIFY,1.0,pl_examples\domain_templates\imagenet.py,pl_examples\domain_templates\imagenet.py,1.0,229,229,MODIFY,2.0,pytorch_lightning\trainer\auto_mix_precision.py,pytorch_lightning\trainer\auto_mix_precision.py,1.0,"21,27,28,29,30","20,21,22,23,25",init_amp,"self,use_amp",19,30,MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"155,156",,use_amp,self,155,156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,main,hparams,218,234,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"389,390",,use_amp,self,389,390,,,,,,,,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"33,34",,use_amp,self,33,34,1.0,"123,124","91,125",__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,float,int,int,str,None,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,str,int,None,None,None,bool,bool,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,kwargs",77,126,1.0,"123,124","91,125",__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,float,int,int,str,None,None,None,show_progress_bar,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,str,int,None,None,None,bool,bool,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,nb_sanity_val_steps,kwargs",78,127,MODIFY,0.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,0.0,,203,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1155,qmeeus,2020-03-15T13:43:17Z,2020-05-03T23:15:57Z,No validation checks when overfit_pct is set,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When setting the overfit_pct to any value between 0 and 1 (exclusive) in trainer, the validation checks are disabled.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 I have worked on a minimal example to reproduce the bug:
 import pytorch_lightning as pl
 import torch
 
 class Dataset(torch.utils.data.Dataset):
 
     def __init__(self, input_dim, output_dim):
         super(Dataset, self).__init__()
         self.input_dim = input_dim
         self.output_dim = output_dim
 
     def __getitem__(self, idx):
         X = torch.rand(1, self.input_dim)
         y = torch.randint(0, self.output_dim, (1,))
         return X, y
 
     def __len__(self):
         return 1000
 
 class Model(pl.LightningModule):
 
     def __init__(self, input_dim, output_dim):
         super(Model, self).__init__()
         self.layer = torch.nn.Linear(input_dim, output_dim)
         self.dataset = Dataset(input_dim, output_dim)
 
     def forward(self, x, y):
         yhat = torch.softmax(self.layer(x), -1)
         return F.nll_loss(logits, y)
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(self.dataset, batch_size=64)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         loss = self.forward(*batch)
         return {'loss': loss, 'log': {'loss': loss}}
 
     def validation_step(self, batch, batch_idx):
         loss = self.forward(*batch)
         return {'val_loss': loss, 'log': {'val_loss': loss}}
 
 
 if __name__ == '__main__':
     model = Model(100, 10)
     trainer = pl.Trainer(overfit_pct=.01)
     trainer.fit(model)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Validation checks occur normally
 <denchmark-h:h3>Environment</denchmark-h>
 
 PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Manjaro Linux
 GCC version: (GCC) 8.3.0
 CMake version: Could not collect
 
 Python version: 3.7
 Is CUDA available: No
 CUDA runtime version: 10.2.89
 GPU models and configuration: Could not collect
 Nvidia driver version: Could not collect
 cuDNN version: /usr/lib/libcudnn.so.7.6.5
 
 Versions of relevant libraries:
 [pip] numpy==1.18.1
 [pip] pytorch-lightning==0.7.1
 [pip] torch==1.4.0
 [pip] torchvision==0.5.0
 [conda] mkl                       2020.0                      166  
 [conda] pytorch                   1.4.0           py3.7_cuda10.1.243_cudnn7.6.3_0    pytorch
 [conda] pytorch-lightning         0.7.1                    pypi_0    pypi
 [conda] torchvision               0.5.0                py37_cu101    pytorch
 	",d735055e6fb6225ad11c566e3711888c0cb4a21e,Adrian Wälchli,2020-03-24 14:49:11-04:00,MODIFY,0,pytorch_lightning\trainer\__init__.py,pytorch_lightning\trainer\__init__.py,0.0,"552,553,563,564,565,566,567,568,569,570,571,572,573,574,575","552,553",1.0,qmeeus,2020-03-15T13:43:56Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,qmeeus,2020-03-18T21:49:24Z,"
 		<denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>
  <denchmark-link:https://github.com/hadim>@hadim</denchmark-link>
  <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  mind check?
 		",3.0,qmeeus,2020-03-21T04:01:07Z,"
 		, but I had to fix <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>
 's code sample to make it visible.
 The sanity validation checks run, but the validation at the end of the epoch doesn't.
 When setting , validation checks work as expected.
 Here is the fixed minimal code sample:
 <denchmark-code>import pytorch_lightning as pl
 import torch
 import torch.nn.functional as F
 
 
 class Dataset(torch.utils.data.Dataset):
 
     def __init__(self, input_dim, output_dim):
         super(Dataset, self).__init__()
         self.input_dim = input_dim
         self.output_dim = output_dim
 
     def __getitem__(self, idx):
         X = torch.rand(self.input_dim)
         y = torch.randint(0, self.output_dim, (1,))
         return X, y
 
     def __len__(self):
         return 1000
 
 
 class Model(pl.LightningModule):
 
     def __init__(self, input_dim, output_dim):
         super(Model, self).__init__()
         self.layer = torch.nn.Linear(input_dim, output_dim)
         self.dataset = Dataset(input_dim, output_dim)
 
     def forward(self, x, y):
         logits = torch.softmax(self.layer(x), -1)
         return F.nll_loss(logits, y.flatten(0))
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(self.dataset, batch_size=64)
 
     def val_dataloader(self):
         return torch.utils.data.DataLoader(self.dataset, batch_size=64)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         loss = self.forward(*batch)
         return {'loss': loss, 'log': {'loss': loss}}
 
     def validation_step(self, batch, batch_idx):
         loss = self.forward(*batch)
         print('see that validation runs only in sanity check')
         return {'val_loss': loss, 'log': {'val_loss': loss}}
 
     def validation_end(self, outputs):
         loss = torch.stack([output['val_loss'] for output in outputs]).mean()
         return {'val_loss': loss, 'log': {'val_loss': loss}}
 
 
 if __name__ == '__main__':
     model = Model(100, 10)
     trainer = pl.Trainer(overfit_pct=0.1, max_epochs=10)
     trainer.fit(model)
 </denchmark-code>
 
 For the record, <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>
  your code had these issues:
 
 No val_dataloader defined
 Wrong shapes returned in dataloader
 Wrong shape for nll_loss labels
 
 		",4.0,qmeeus,2020-03-21T04:14:27Z,"
 		Actually overfit_pct argument is not documented in the Trainer class. We should fix that and say that setting overfit_pct is the same as setting train_percent_check, val_percent_check and test_percent_check.
 		",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"166,167",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,qmeeus,2020-03-21T06:26:11Z,"
 		False alarm! Turns out it is simply because you chose a too small value for overfit_pct.
 Your dataset has size 1000, and dataloader has batch_size 64.
 1000 / 64 ~= 15 batches
 When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.
 <denchmark-link:https://github.com/qmeeus>@qmeeus</denchmark-link>
  Please let me know if it isn't clear. I think the behaviour of is correct.
 		",6.0,qmeeus,2020-03-21T06:30:40Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Should we make it so that does not round to 0 batches?
 		",7.0,qmeeus,2020-03-21T10:42:11Z,"
 		
 False alarm! Turns out it is simply because you chose a too small value for overfit_pct.
 Your dataset has size 1000, and dataloader has batch_size 64.
 1000 / 64 ~= 15 batches
 When you choose overfit_pct = .01, then that gives 15 * 0.01 < 1 batch.
 @qmeeus Please let me know if it isn't clear. I think the behaviour of overfit_pct is correct.
 
 Awesome, thanks !
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1156,LucFrachon,2020-03-15T15:03:34Z,2020-06-05T10:28:30Z,ReduceLROnPlateau does not recognise val_loss despite progress_bar dict,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training my model, I get the following message:
 <denchmark-code>  File ""C:\Users\Luc\Miniconda3\envs\pytorch\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 371, in train
     raise MisconfigurationException(m)
 pytorch_lightning.utilities.debugging.MisconfigurationException: ReduceLROnPlateau conditioned on metric val_loss which is not available. Available metrics are: loss
 </denchmark-code>
 
 Ihis is similar to #321for instance, but I definitely return a progress_bar dict with a val_loss key in it (see code below).
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>  def training_step(self, batch, batch_idx):
        z, y_true = batch
        y_pred = self.forward(z)
        loss_val = self.loss_function(y_pred, y_true)
        return {'loss': loss_val.sqrt()}
 
    def validation_step(self, batch, batch_idx):
        z, y_true = batch
        lr = torch.tensor(self.optim.param_groups[0]['lr'])
        y_pred = self.forward(z)
        loss_val = self.loss_function(y_pred, y_true)
        return {'val_loss': loss_val.sqrt(), 'lr': lr}
 
    def validation_epoch_end(self, outputs):
        val_loss_mean = torch.stack([x['val_loss'] for x in outputs]).mean()
        lr = outputs[-1]['lr']
        logs = {'val_loss': val_loss_mean, 'lr': lr}
        return {'val_loss': val_loss_mean, 'progress_bar': logs, 'log': logs}
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The val_loss value should be picked up by the progress bar.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.4.0
 OS (e.g., Linux): Windows 10
 How you installed PyTorch (conda, pip, source): pip
 Python version: 3.6.10
 CUDA/cuDNN version: 10
 GPU models and configuration: 1070Ti x 1
 Any other relevant information:
 
 	",711892a0a293f7c7f951eba0907e1c0ccd2b37d8,authman,2020-03-19 09:22:29-04:00,MODIFY,0,docs\source\optimizers.rst,docs\source\optimizers.rst,0.0,"22,23,24,25,26,27,28,29,30,31,32,33,34,35",,1.0,LucFrachon,2020-03-16T14:07:38Z,"
 		Actually, ReduceLROnPlateau lr schedulers are conditioned on callback_metrics, see here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 717 to 726
       in
       774d9be
 
 
 
 
 
 
  if lr_scheduler['reduce_on_plateau']: 
 
 
 
  monitor_key = lr_scheduler['monitor'] 
 
 
 
  monitor_val = self.callback_metrics.get(monitor_key) 
 
 
 
  if monitor_val is None: 
 
 
 
  avail_metrics = ','.join(list(self.callback_metrics.keys())) 
 
 
 
  m = f'ReduceLROnPlateau conditioned on metric {monitor_key} ' \ 
 
 
 
  f'which is not available. Available metrics are: {avail_metrics}. ' \ 
 
 
 
  'Condition can be set using `monitor` key in lr scheduler dict' 
 
 
 
  raise MisconfigurationException(m) 
 
 
 
  lr_scheduler['scheduler'].step(monitor_val) 
 
 
 
 
 
 and these values are everything not defined in 'progress_bar', 'log' and 'hidden', see here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/logging.py
 
 
         Lines 106 to 109
       in
       774d9be
 
 
 
 
 
 
  callback_metrics = {} 
 
 
 
  for k, v in output.items(): 
 
 
 
  if k not in ['progress_bar', 'log', 'hiddens']: 
 
 
 
  callback_metrics[k] = v 
 
 
 
 
 
 since you are returning 'val_loss' as a separate key this should however not be the problem.
 Since only 'loss' is available as a callback_metrics it seems that your lr scheduler is called before your validation data is processed. What does your configure_optimizers() look like?
 		",2.0,LucFrachon,2020-03-16T15:24:11Z,"
 		Thanks for your answer. If I understand correctly, ReduceLROnPlateau is the only LR scheduler that does not use values from progress_bar?
 Here's configure_optimizers():
 <denchmark-code>    def configure_optimizers(self):
         self.optim = torch.optim.AdamW(self.parameters(), lr=self.hp.lr_ini, eps=1.e-4)
         self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, 'max', factor=self.hp.lr_reduce_factor,
                                                                     patience=self.hp.lr_reduce_patience,
                                                                     min_lr=self.hp.lr_min)
         return [self.optim], [self.scheduler]
 </denchmark-code>
 
 And here is the Trainer definition:
 <denchmark-code>trainer = pl.Trainer(gpus=-1, early_stop_callback=None, max_epochs=stgs.PRED_HPARAMS['max_epochs'])
 </denchmark-code>
 
 		",3.0,LucFrachon,2020-03-17T14:04:20Z,"
 		Only ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.
 I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.
 		",4.0,LucFrachon,2020-03-17T18:32:55Z,"
 		I have the same issue. That's how I deal with it:
     def training_step(self, batch, batch_idx):
         loss, lm_loss, mc_loss = self.forward(batch)
         lr = self.trainer.optimizers[0].param_groups[0]['lr']
         lr = torch.tensor(lr).unsqueeze(0).to(loss.device) if len(loss.size()) else lr
         log = {
             'MC-Loss/train': mc_loss,
             'LM-Loss/train': lm_loss,
             'Learning-Rate': lr
         }
         # Set up placeholders for valid metrics.
         if self.trainer.global_step == 0:
             log.update({'LM-Loss/valid': np.inf})
 
         return {'loss': loss, 'log': log}
 here I check if self.trainer.global_step == 0 and if yes, I set up a placeholder.
 <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  maybe you can advice a better way
 		",MODIFY,0.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,0.0,"945,947,949,950,951,953,955,957,958,959,960,961,962,963,964,965","945,946,948,949,951,952,953,954,955,956,957,959,960,962,963,965,966",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,LucFrachon,2020-03-17T22:04:57Z,"
 		
 Only ReduceLROnPlateau schedulers are conditioned on some value. All other LR schedulers have nothing to do with callback_metrics or progress_bar. Hope this answers your question.
 I don't see any obvious mistakes in your code. As a said in my earlier comment, my best guess is that your ReduceLROnlr scheduler is called before val_loss` is actually calculated. You probably need to check this.
 
 <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  Not sure I follow you. Maybe I misunderstood, but doesn't PL automatically handle calls to the scheduler? Doesn't it defeat (some of) the purpose of PL if I have to call it myself?
 <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  Thanks, I'll try that, but maybe there is a less hacky way...?
 		",6.0,LucFrachon,2020-03-19T15:05:06Z,"
 		<denchmark-link:https://github.com/LucFrachon>@LucFrachon</denchmark-link>
  sorry, I must have misinterpret what you asked about. Yes, PL automatically calls the scheduler you define. Sorry for the confusion.
 <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  I think that indicates that there is some bug here, since you need to setup a placeholder for the first step. I will see if I can come up with a more permanent solution.
 		",7.0,LucFrachon,2020-03-22T12:43:02Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  , btw, the trick:
         if self.trainer.global_step == 0:
             log.update({'Loss/valid': np.inf})
 doesn't help in case of warm start from checkpoint, because in warm start, the global_step is never equal to 0 :)
 For now I did it like this:
         # Set up placeholders for valid metrics.
         if not self._valid_metrics_patched:
             log.update({'Loss/valid': np.inf})
             self._valid_metrics_patched = True
 		",8.0,LucFrachon,2020-03-23T14:31:40Z,"
 		<denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  I have a hard time reproducing this bug, it seems to be a very corner case. Do you have a simple model that I can reproduce?
 		",9.0,LucFrachon,2020-03-23T19:36:37Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
 
 Yes, sure:
 <denchmark-link:https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25>https://gist.github.com/alexeykarnachev/efbd40f0ff0cfcd1e324de044a802c25</denchmark-link>
 
 		",10.0,LucFrachon,2020-03-25T15:40:46Z,"
 		Okay, after looking at your code <denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
 , this does not seems to be a bug. When you set  you are calling the  method for  after each batch and it therefore makes complete sense that no  is calculated yet. If you really want to do something like this, you need to set  in the  construction to a number lower than  in the scheduler construction. In this way  will be calculated before  is called.
 		",11.0,LucFrachon,2020-03-26T10:09:51Z,"
 		Thx, now it's more clear.
 Does it mean, that I can't use a schedulers, which are depend on some valid metric with interval=""step""?
 For instance, I have a custom reduce on plateau scheduler, which just ignores nan metric values, that's why I need to set a placeholder for valid metric (like in my first example).
 		",12.0,LucFrachon,2020-03-30T12:23:25Z,"
 		I do not think it is possible just out of the box. However, if you configure your scheduler correctly, then it should be possible. For example, if I initialize my Trainer as
 trainer = Trainer(val_check_interval=50)
 and initialize my scheduler as
 <denchmark-code>scheduler = {
     'schduler': ReduceLROnPlateau(optimizer, mode, factor, patience),
     'interval': 'step',
     'frequency': 100
 }
 </denchmark-code>
 
 it should work (not tested), since val_loss will be created every 50 steps but the scheduler will first be called after 100 steps.
 		",13.0,LucFrachon,2020-03-30T13:32:50Z,"
 		oh, right, I missed the frequency argument. Totally forgot about it. I'll try it. Thank you!
 		",14.0,LucFrachon,2020-05-29T14:54:40Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",15.0,LucFrachon,2020-06-05T10:05:23Z,"
 		shall be fixed with <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1989>#1989</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1161,sneiman,2020-03-16T18:09:55Z,2020-03-30T16:13:35Z,multi-gpu ddp calls validation and testing loops too many times,"
 When using ddp with multiple gpus, each validation and test loop is called with the entire validation dataset for each gpu.
 Expected behavior is that the dataset is divided appropriately across the gpus.
 I am using current master (cloned Mar 14), Ubuntu 19.10, Cuda 10.1, python 3.7.5, pytorch 1.4, venv environment.
 The problem appears to be in auto_add_sampler() in data_loading.py. It does not create a DistributedSampler for validation or test datasets.
 	",6dfe9951e132bdc9896926557138e7a21d4cd000,sneiman,2020-03-30 12:13:34-04:00,MODIFY,1,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"94,95,99","94,95,96,97,98",1.0,sneiman,2020-03-16T21:24:51Z,"
 		Latest pull - 1 hour ago, no longer this behavior. Closing.
 		",2.0,sneiman,2020-03-17T00:22:10Z,"
 		Sorry - this issue still exists in some configurations. My proposed fix is not the total picture. Still investigating - will provide reproducible example.
 		",3.0,sneiman,2020-03-17T03:29:44Z,"
 		Testing underway. Will make PR tomorrow.
 		",4.0,sneiman,2020-03-17T23:18:43Z,"
 		Dont want to clutter up PR world if no one is interested in this. Let me know ...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,sneiman,2020-03-18T21:54:08Z,"
 		that sounds a good contribution to me... mind send a PR?
 Any suggestion <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
 ?
 in a technical note when you refer some master state pls use coit hash as there can be multiple commits each day...
 		",6.0,sneiman,2020-03-18T22:05:28Z,"
 		will do on both pr, and hash ref
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,auto_add_sampler,"self,DataLoader,bool",72,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1181,TevenLeScao,2020-03-18T15:42:43Z,2020-04-02T09:41:56Z,Additional dataloader created and discarded when training with reload_dataloaders_every_epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I am training with reload_dataloaders_every_epoch and I've noticed it instantiates an extra DataLoader before training for which nothing is run. This is an issue for me as I am training with chunks that get loaded every epoch and it is messing with the order I load them in especially if I reload a checkpoint; it would be an issue for people that generate a new dataset every epoch as they waste computation. The tqdm bar also keeps the information of the first, discarded DataLoader (in the screenshot, the number of iterations is the same for both whereas they should be different sizes)
 <denchmark-link:https://user-images.githubusercontent.com/26709476/76968780-be917200-6929-11ea-82c5-6850a6f6e678.png></denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run the code sample below, which runs for one epoch and displays a message every time a DataLoader is created.
 A DataLoader gets instantiated a first time line 286 in training_loop.py outside of the epoch loop (that's the usual time it gets instantiated when not reloading every epoch. Then when using reload_dataloaders_every_epoch another one is created at the start of every epoch line 386, inside the loop, so for the first epoch there's an extra one.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import torch
 import pytorch_lightning as pl
 from torch.utils.data import DataLoader, Dataset
 from time import sleep
 
 class MinimalDataset(Dataset):
 
     def __init__(self, index):
         self.data = torch.Tensor(64 * index, 1024)
 
     def __getitem__(self, item):
         return self.data[item]
 
     def __len__(self):
         return len(self.data)
 
 class MinimalModule(pl.LightningModule):
 
     def __init__(self):
         super(MinimalModule, self).__init__()
         self.nn = torch.nn.Linear(1024, 1)
         self.current_index = 0
 
     def forward(self, batch):
         return self.nn(batch)
 
     def training_step(self, batch, batch_idx):
         sleep(0.1)
         loss = self.nn(batch)[0]
         return {'loss': loss}
 
     def validation_step(self, batch, batch_idx):
         loss = self.nn(batch)[0]
         return {'val_loss': loss}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.01)
 
     def train_dataloader(self):
         # REQUIRED
         self.current_index += 1
         print(f""initializing DataLoader n{self.current_index}"")
         data_loader = DataLoader(MinimalDataset(self.current_index))
         return data_loader
     
 model = MinimalModule()
 trainer = pl.Trainer(reload_dataloaders_every_epoch=True, num_sanity_val_steps=0, val_check_interval=8, max_epochs=1)
 
 trainer.fit(model)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Only one dataloader should be created; two are. The tqdm bar should show 128 iterations as that is the dataset size the second time; but it shows 64 instead (I added the sleep(0.1) to leave time to observe that)
 <denchmark-h:h3>Environment</denchmark-h>
 
 PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 OS: Ubuntu 18.04.4 LTS
 GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
 CMake version: Could not collect
 Python version: 3.6
 Is CUDA available: Yes
 CUDA runtime version: Could not collectepoch_end
 GPU models and configuration: GPU 0: GeForce RTX 2070 with Max-Q Design
 Nvidia driver version: 435.21
 cuDNN version: Could not collect
 Versions of relevant libraries:
 [pip3] numpy==1.18.1
 [pip3] pytorch-lightning==0.7.1
 [pip3] torch==1.4.0
 [pip3] torchvision==0.4.2
 [conda] Could not collect
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",04935ea7184a50d535af96dd85a58fdc43a659b8,Teven,2020-04-02 11:41:56+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,46,,1.0,TevenLeScao,2020-03-18T15:43:24Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"341,348,402,403,404,405,406,407,408,409,410","341,348",run_evaluation,"self,bool",322,413,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"321,322",277,MODIFY,2.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,,"397,398,399,400",run_training_epoch,self,386,499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"293,294,295,311,312,313",293,train,self,285,389,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1201,Dunrar,2020-03-20T22:35:06Z,2020-03-31T06:24:27Z,Early stopping not working on 0.7.1,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Early stopping does not work anymore. When I downgrade from 0.7.1 or the current dev version to 0.6.0 early stopping works again, with the same code.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>def main(hparams):
     if hparams.early_stopping == 'yes':
         early_stopping = EarlyStopping(
             monitor='batch/mean_absolute_loss',
             min_delta=hparams.min_delta,
             patience=hparams.patience,
             mode='min'
         )
     else:
         early_stopping = False
 
     model = MemoryTest(hparams)
     trainer = pl.Trainer(
         val_percent_check=0,
         early_stop_callback=early_stopping,
         default_save_path=src.settings.LOG_DIR,
         max_epochs=hparams.epochs
     )
 
     trainer.fit(model)
 </denchmark-code>
 
 <denchmark-code>class MemoryTest(pl.LightningModule):
     # Main Testing Unit for Experiments on Recurrent Cells
     def __init__(self, hp):
         super(MemoryTest, self).__init__()
         self.predict_col = hp.predict_col
         self.n_datasamples = hp.n_datasamples
         self.dataset = hp.dataset
         if self.dataset is 'rand':
             self.seq_len = None
         else:
             self.seq_len = hp.seq_len
         self.hparams = hp
         self.learning_rate = hp.learning_rate
         self.training_losses = []
         self.final_loss = None
 
         self.model = RecurrentModel(1, hp.n_cells, hp.n_layers, celltype=hp.celltype)
 
     def forward(self, input, input_len):
         return self.model(input, input_len)
 
     def training_step(self, batch, batch_idx):
         x, y, input_len = batch
         features_y = self.forward(x, input_len)
 
         loss = F.mse_loss(features_y, y)
         mean_absolute_loss = F.l1_loss(features_y, y)
 
         self.training_losses.append(mean_absolute_loss.item())
 
         neptune_logs = {'batch/train_loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss}
         return {'loss': loss, 'batch/mean_absolute_loss': mean_absolute_loss, 'log': neptune_logs}
 
     def on_epoch_end(self):
         train_loss_mean = np.mean(self.training_losses)
         self.final_loss = train_loss_mean
         self.training_losses = []  # reset for next epoch
 
     def configure_optimizers(self):
         return torch.optim.SGD(self.parameters(), lr=self.learning_rate)
 
     @pl.data_loader
     def train_dataloader(self):
         train_dataset = dg.RandomDataset(self.predict_col, self.n_datasamples)
         if self.dataset == 'rand_fix':
             train_dataset = dg.RandomDatasetFix(self.predict_col, self.n_datasamples, self.seq_len)
         if self.dataset == 'correlated':
             train_dataset = dg.CorrelatedDataset(self.predict_col, self.n_datasamples)
         train_loader = DataLoader(dataset=train_dataset, batch_size=1)
         return train_loader
 
     @staticmethod
     def add_model_specific_args(parent_parser):
         # MODEL specific
         model_parser = ArgumentParser(parents=[parent_parser])
         model_parser.add_argument('--learning_rate', default=1e-2, type=float)
         model_parser.add_argument('--n_layers', default=1, type=int)
         model_parser.add_argument('--n_cells', default=5, type=int)
         model_parser.add_argument('--celltype', default='LSTM', type=str)
 
         # training specific (for this model)
         model_parser.add_argument('--epochs', default=500, type=int)
         model_parser.add_argument('--patience', default=5, type=int)
         model_parser.add_argument('--min_delta', default=0.1, type=float)
         model_parser.add_argument('--early_stopping', default='yes', type=str)
 
         # data specific
         model_parser.add_argument('--n_datasamples', default=1000, type=int)
         model_parser.add_argument('--seq_len', default=10, type=int)
         model_parser.add_argument('--dataset', default='rand', type=str)
         model_parser.add_argument('--predict_col', default=1, type=int)
 
         return model_parser
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Early-stopping to take effect again.
 	",1aba411da96ed95419d13ec1f86a0d38a232f73e,Adrian Wälchli,2020-03-31 06:24:26+00:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,46,,1.0,Dunrar,2020-03-20T22:44:23Z,"
 		<denchmark-link:https://github.com/Dunrar>@Dunrar</denchmark-link>
  would you check it on actual master?
 		",2.0,Dunrar,2020-03-21T13:22:03Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  do you mean the bleeding edge version via ?
 		",3.0,Dunrar,2020-03-21T14:44:39Z,"
 		Okay, I tried that but early stopping still does not work
 		",4.0,Dunrar,2020-03-21T15:16:18Z,"
 		The code sample you provide does not define a validation step/end/dataloader.
 I would expect that early stopping does not work without it. How could it?
 		",MODIFY,0.0,docs\source\early_stopping.rst,docs\source\early_stopping.rst,0.0,"7,8,33,34,35,36,37,38,39,40,43,44","7,8,35",,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"370,371","370,371",MODIFY,2.0,tests\trainer\test_callbacks.py,tests\trainer\test_callbacks.py,1.0,"162,163,164,165,166",,test_early_stopping_without_val_step.training_step,"self,args,kwargs",162,166,,,,,,,,,,,,5.0,Dunrar,2020-03-21T15:19:00Z,"
 		if no val step is present, it uses the training step for early stopping
 		",6.0,Dunrar,2020-03-21T15:20:04Z,"
 		oh, my bad! Then I will have a closer look at this issue.
 		",7.0,Dunrar,2020-03-24T12:55:29Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  little update. In training_loop.py the line  is to blame. Just deleting the self.disable_validation and is_val_epoch checks solves the problem in my case, but there is probably more to take into consideration.
 		",8.0,Dunrar,2020-03-24T13:05:25Z,"
 		I also came to that point when I looked at it 2 days ago, will have more time to look at it soon. If I remember correctly, the tests didnt pass and I was tracking down at which point the change was introduced to figure out the reason it is there.
 		",9.0,Dunrar,2020-03-31T06:50:43Z,"
 		<denchmark-link:https://github.com/Dunrar>@Dunrar</denchmark-link>
  Thanks for the help. Your suggestion worked and I was able to make a test so that it doesn't break in the future :)
 cheers!
 		",10.0,Dunrar,2020-03-31T06:54:26Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Thank you!
 		",,,,,,,,,,,,,,,,,,,,,train,self,284,383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183",,test_early_stopping_without_val_step,tmpdir,156,183,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1213,Ir1d,2020-03-23T11:26:40Z,2020-07-10T01:27:31Z,Testing in dp mode uses only one of the GPUs,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run a test without training
 <denchmark-link:https://user-images.githubusercontent.com/10709657/77311956-1706ac00-6d3c-11ea-89fe-3cf2156babe2.png></denchmark-link>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Modified from the conference-seed repo
 trainer = Trainer(
             gpus=""-1"",
             distributed_backend='dp',
         )
 trainer.test(model)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 pl version: 0.6.0
 PyTorch Version (e.g., 1.0): 1.2
 OS (e.g., Linux): Ubuntu
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.6
 CUDA/cuDNN version: 10.1
 GPU models and configuration:
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",c869dd8b8f6301f3726df84535a3da4e9acf04ec,Jirka Borovec,2020-03-30 12:14:27-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,23,23,1.0,Ir1d,2020-03-23T11:29:51Z,"
 		ummm yeah, that's a bug. it should run via dp. <denchmark-link:https://github.com/Ir1d>@Ir1d</denchmark-link>
  want to submit a PR?
 <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
  any one else experience this?
 		",2.0,Ir1d,2020-03-23T11:31:21Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I tried wrapping the model in  like the training loop did, but it tells me that LightningDataParallel object doen't have a . How do I debug this?
 		",3.0,Ir1d,2020-03-23T11:40:28Z,"
 		you wouldn’t wrap it yourself ever haha.
 the trainer does the wrapping for you.
 the trainer needs to be modified to run the test on the correct method when done this way
 		",4.0,Ir1d,2020-03-23T11:46:52Z,"
 		I was trying to wrap it in evaluate in pytorch_lightning/trainer/evaluation_loop.py . Do you have any idea where to wrap this func?
 		",MODIFY,3.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,220,220,_evaluate,"self,LightningModule,dataloaders,int,bool",220,320,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"896,897,898,899","896,897,898,899",MODIFY,1.0,tests\test_deprecated.py,tests\test_deprecated.py,1.0,"98,109","98,109",test_tbd_remove_in_v1_0_0_model_hooks,,87,110,,,,,,,,,,,,5.0,Ir1d,2020-03-23T11:48:20Z,"
 		Anyway, we've find one possible workround here:
 After defining a torch model, and before sending it into PL model, wrap it with nn.dataparallel.
 		",6.0,Ir1d,2020-03-23T12:09:23Z,"
 		evaluate is private... you're not meant to call it directly.
 call .test()
 		",7.0,Ir1d,2020-03-23T12:15:31Z,"
 		lightning does the wrapping by itself...
 the fact that this doesn't work, is a bug.
 model = MyLightningModule.load_from_checkpoint(...)
 trainer = Trainer(
             gpus=""-1"",
             distributed_backend='dp',
         )
 trainer.test(model)
 The bug needs to be addressed correctly.
 It's weird because we have tests for this... double check that this is really not working for you.
 		",8.0,Ir1d,2020-03-23T12:38:02Z,"
 		
 evaluate is private... you're not meant to call it directly.
 call .test()
 
 so let's rename it starting with _ to be clear that it is private from it name
 		",9.0,Ir1d,2020-03-23T12:41:39Z,"
 		I was calling .test and its not working
 		",10.0,Ir1d,2020-03-27T12:30:40Z,"
 		<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  may you have look at this multi GPU issue?
 		",11.0,Ir1d,2020-06-08T12:41:21Z,"
 		<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  ping :)
 		",12.0,Ir1d,2020-06-26T13:46:56Z,"
 		looking at this with next sprint
 		",13.0,Ir1d,2020-07-10T01:27:31Z,"
 		fixed! (0.8.5)
 		",,,,,,,,,run_pretrain_routine,"self,LightningModule",817,920,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,368,368,run_evaluation,"self,bool",322,404,1.0,220,220,evaluate,"self,LightningModule,dataloaders,int,bool",220,320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
122,williamFalcon,2019-08-15T14:03:39Z,2019-08-15T15:45:14Z,Fix appveyor build,"
 windows support is not a priority. If the badge can be fixed today we'll keep appveyor. Otherwise we'll drop it from the project.
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 	",6f1d2c45fe72d7a8a637290c2e78973deb1637e0,Jirka Borovec,2019-08-15 11:45:03-04:00,MODIFY,0,appveyor.yml,appveyor.yml,0.0,48,"48,49,50",1.0,williamFalcon,2019-08-15T14:29:30Z,"
 		it failed because we were installing 1.1.0 directly but in  is now required 1.2.0
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/83b1646e457542ecde572fefc78252ec0e902464/appveyor.yml#L48>https://github.com/williamFalcon/pytorch-lightning/blob/83b1646e457542ecde572fefc78252ec0e902464/appveyor.yml#L48</denchmark-link>
 
 Seems that the fix is
 <denchmark-code>pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html
 </denchmark-code>
 
 just testing in a local branch and I will create PR if it solves the issue...
 		",2.0,williamFalcon,2019-08-15T15:25:50Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  you may use ""Assignees"" instead of simply mentioning next time... :)
 		",3.0,williamFalcon,2019-08-15T15:29:19Z,"
 		i tried. it doesn’t let me assign people...
 		",4.0,williamFalcon,2019-08-15T15:54:40Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  you still shall update the badge to point to your project so it is automatically updated according to your actual status, not my fork... see <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/123#issuecomment-521679440>#123 (comment)</denchmark-link>
 
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blame/master/README.md#L13>https://github.com/williamFalcon/pytorch-lightning/blame/master/README.md#L13</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,williamFalcon,2019-08-15T17:08:18Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 . something weird happening with the build on 3.6
 		",6.0,williamFalcon,2019-08-15T20:18:02Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  could you pass me a link to the build so I may have look?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1223,lobantseff,2020-03-24T14:24:01Z,2020-05-31T12:31:22Z,gan.py multi-gpu running problems,"
 Running <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/domain_templates/gan.py>gan.py</denchmark-link>
  example with Trainer(ngpus=2) causes two types of error:
 
 if Trainer(ngpus=2, distributed_backend='dp')
 
 <denchmark-code>Exception has occurred: AttributeError
 'NoneType' object has no attribute 'detach'
   File ""/home/user/gan.py"", line 146, in training_step
     self.discriminator(self.generated_imgs.detach()), fake)
 </denchmark-code>
 
 
 if Trainer(ngpus=2, distributed_backend='ddp')
 
 
 in ./lightling_logs one run creates two folders: version_0 and version_1
 Exception caused:
 File ""/opt/miniconda3/envs/ctln-gan/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 122, in _del_model
 os.remove(filepath)
 FileNotFoundError: [Errno 2] No such file or directory: '/home/user/pyproj/DCGAN/lightning_logs/version_1/checkpoints/epoch=0.ckpt'
 
 it seems that each subprocess tries to create its own checkpoints and delete not ctrated one.
 <denchmark-h:h4>Environment version:</denchmark-h>
 
 python 3.7.5
 pytorch 1.4.0
 pytorch-lightning 0.7.1
 	",55fdfe384537e4d43e7397306ba001ffc3474322,Artem Lobantsev,2020-05-31 08:31:21-04:00,MODIFY,4,pl_examples\domain_templates\generative_adversarial_net.py,pl_examples\domain_templates\generative_adversarial_net.py,1.0,"186,190,195,196","188,192",1.0,lobantseff,2020-03-24T14:24:41Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,lobantseff,2020-03-24T16:49:10Z,"
 		The problem is that gan.py example suppose to use buffered values self.generated_images and self.last_img, however during replicating and gathering in 
 
 
 pytorch-lightning/pytorch_lightning/overrides/data_parallel.py
 
 
          Line 64
       in
       22a7264
 
 
 
 
 
 
  replicas = self.replicate(self.module, self.device_ids[:len(inputs)]) 
 
 
 
 
 
 buffered values are not replicated and not gathered to main LightningModule model
 		",3.0,lobantseff,2020-03-27T12:01:55Z,"
 		@armavox good catch, mind draft a PR? 🤖
 		",4.0,lobantseff,2020-03-29T22:14:14Z,"
 		yep, I'll try
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,lobantseff,2020-04-14T17:29:01Z,"
 		@armavox how is it going?
 		",6.0,lobantseff,2020-04-22T09:43:20Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
   I assume to fix it by May
 		",7.0,lobantseff,2020-05-28T14:00:50Z,"
 		@armavox Any updates on this? Having the same issue...
 		",8.0,lobantseff,2020-05-30T15:50:34Z,"
 		Made some updates. Sorry for waiting.
 There is an official l warning about the use of local (buffered here) variables during the distributed training: <denchmark-link:https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel>https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel</denchmark-link>
 
 So I didn't try to create detours in the Lightning code and fixed only the example to work with dp and ddp.
 		",9.0,lobantseff,2020-05-30T15:55:50Z,"
 		The problem from point 2 in the heading post seems to be fixed by someone. But the unused folder for parallel experiment still created during ddp training. The problem is in 
 
 
 pytorch-lightning/pytorch_lightning/trainer/callback_config.py
 
 
          Line 66
       in
       fdbbe96
 
 
 
 
 
 
  os.makedirs(ckpt_path, exist_ok=True) 
 
 
 
 
 , which doesn't use rank_zero_only decorator or something else.
 I would propose some good fixes, but don't know how to do this elegant.
 Thanks for your work!
 Best regards, Artem.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,main,args,186,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,178,"179,180",on_epoch_end,self,177,183,1.0,"190,195,196","188,192",main,hparams,188,202,1.0,"104,105,106,107,116,117,118,126,147,150","105,109,110,111,117,118,119,127,148,151",training_step,"self,batch,batch_idx,optimizer_idx",102,160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1236,dumitrescustefan,2020-03-25T19:02:05Z,2020-03-29T18:56:37Z,AdvancedProfiler error,"
 Hi, as others have pointed out, the Profiler doesn't seem to work (it prints nothing), and trying out the AdvancedProfiler as in <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/profiler.html>https://pytorch-lightning.readthedocs.io/en/latest/profiler.html</denchmark-link>
  like:
 <denchmark-code>from pytorch_lightning.profiler import AdvancedProfiler
     profiler = AdvancedProfiler(output_filename=""prof.txt"")
     trainer = Trainer(profiler=profiler, (other params here)
 </denchmark-code>
 
 gives me the following error:
 <denchmark-code>Validation sanity check: 50it [00:00, 212.11it/s]             Traceback (most recent call last):
   File ""/Users/sdumitre/work/style/training.py"", line 177, in <module>
     main(hparams)
   File ""/Users/sdumitre/work/style/training.py"", line 77, in main
     trainer.fit(model)
   File ""/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 630, in fit
     self.run_pretrain_routine(model)
   File ""/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 810, in run_pretrain_routine
     _, _, _, callback_metrics, _ = self.process_output(eval_results)
   File ""/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py"", line 117, in process_output
     callback_metrics[k] = v.item()
 ValueError: only one element tensors can be converted to Python scalars
                                                  
 Process finished with exit code 1
 </denchmark-code>
 
 Any pointers?
 My env: torch 1.4 installed with pip, Python 3.7, no GPU, on, MacOS.
 Thanks for the great lib you're developing!
 	",54507f417eaf3317a798b3303c398152a4e35a18,Jeremy Jordan,2020-03-29 14:56:36-04:00,MODIFY,0,pytorch_lightning\__init__.py,pytorch_lightning\__init__.py,0.0,"13,15,16","13,15",1.0,dumitrescustefan,2020-03-25T19:46:36Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
 
 		",2.0,dumitrescustefan,2020-03-26T00:17:17Z,"
 		Hi <denchmark-link:https://github.com/dumitrescustefan>@dumitrescustefan</denchmark-link>
  thanks for submitting this issue.
 
 as others have pointed out, the Profiler doesn't seem to work
 
 If there are other Github Issues please reference them here
 
 the Profiler doesn't seem to work (it prints nothing)
 
 Did you configure logging? I usually do this in the root __init__.py of my projects.
 eg.
 <denchmark-code>import logging
 logging.basicConfig(level=logging.INFO)
 </denchmark-code>
 
 Finally, this seems unrelated to the AdvancedProfiler.
 <denchmark-code> File ""/Users/sdumitre/virtual/p3/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py"", line 117, in process_output
     callback_metrics[k] = v.item()
 ValueError: only one element tensors can be converted to Python scalars
 </denchmark-code>
 
 Can you show what your training_step and validation_step code looks like?
 		",3.0,dumitrescustefan,2020-03-26T04:48:18Z,"
 		btw I have value error but not in profiler. I was using loguru instead of logging.
 		",4.0,dumitrescustefan,2020-03-26T08:32:45Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  thanks for the tips! I put logging into .py, and tried with the ""basic"" profiler again, now I get the same error.
 Here are the train/val_steps:
 <denchmark-code>    def training_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:
         x_tensor, x_lengths, y_tensor = batch
 
         model_out = self.forward(x_tensor, x_lengths)
         loss_val = self.loss(model_out, y_tensor)
 
         tqdm_dict = {""train_loss"": loss_val}
         output = OrderedDict(
             {""loss"": loss_val, ""progress_bar"": tqdm_dict, ""log"": tqdm_dict}
         )
         return output
 
     def validation_step(self, batch: tuple, batch_nb: int, *args, **kwargs) -> dict:
         x_tensor, x_lengths, y_tensor = batch
 
         model_out = self.forward(x_tensor, x_lengths)
         loss_val = self.loss(model_out, y_tensor)
 
         output = OrderedDict({""val_loss"": loss_val})
 
         return output
 </denchmark-code>
 
 All the code (except the forward and the model params) is copy-pasted from a lightning tutorial. Without the profiler everything seems to work okay. The trainer is initialised with:
 <denchmark-code>trainer = Trainer(
         logger=setup_testube_logger(),
         checkpoint_callback=True,
         early_stop_callback=early_stop_callback,
         default_save_path=""experiments/"",
         gpus=hparams.gpus,
         distributed_backend=hparams.distributed_backend,
         use_amp=hparams.use_16bit,
         max_epochs=hparams.max_epochs,
         min_epochs=hparams.min_epochs,
         accumulate_grad_batches=hparams.accumulate_grad_batches,
         log_gpu_memory=hparams.log_gpu_memory,
         val_percent_check=hparams.val_percent_check,
         profiler=True <-- this is what I added
     )
 </denchmark-code>
 
 IMHO, shouldn't the profiler be agnostic to what I do in the code? Actually the in-built profiler is one of the main features that made me try out Lightning. I would be most grateful to have it work :) Please tell me what piece of code I could provide. The model itself aims at predicting a set of n values (floats) based on a number of sentences (embedded with BPE as ints). There is a sentence-level RNN that encodes each sentence, and then a ""document"" level RNN that runs over each sentence. This gets into a hidden->n linear layer and the error is MSELoss(). This is a baseline I created and I'd like to build from here, but I need to get past these initial errors. I don't know if this info is useful for you, I can provide all the code if required.
 Thanks!
 		",MODIFY,2.0,tests\test_profiler.py,tests\test_profiler.py,1.0,"77,81",77,test_simple_profiler_describe,"caplog,simple_profiler",77,81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,dumitrescustefan,2020-03-26T09:58:08Z,"
 		My bad. I did 2 things, I added the profiler and then I also added reduction='none' (in MSELoss) and in the loss function I forgot to do the reduction myself; thus the loss function returned a 2D tensor instead of a scalar, which broke the pytorch_lightning/trainer/logging.py, which led me to believe it was a logging error. I'm usually careful with changing more than a single item per run, but hey, blame the new lib I'm learning to use :)
 I'm not closing this issue because even though now the AdvancedProfiler works (dumps to a file), the basic one still doesn't want to print anything onscreen, even after adding level=DEBUG.
 If logging is required, maybe the docs could be updated ( <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/profiler.html>https://pytorch-lightning.readthedocs.io/en/latest/profiler.html</denchmark-link>
  ).
 Should I do anything more besides adding logging to init?
 <denchmark-code>import logging
 logging.basicConfig(level=logging.DEBUG)
 </denchmark-code>
 
 and in the Trainer object:
 <denchmark-code>profiler=True
 </denchmark-code>
 
 Thanks!
 Thanks and sorry for the time taken on my mistake!
 		",6.0,dumitrescustefan,2020-03-27T03:14:05Z,"
 		
 IMHO, shouldn't the profiler be agnostic to what I do in the code?
 
 yes! that's why i was confused about your error :)
 
 then I also added reduction='none' (in MSELoss) and in the loss function I forgot to do the reduction myself; thus the loss function returned a 2D tensor instead of a scalar, which broke the pytorch_lightning/trainer/logging.py
 
 but this makes perfect sense
 <denchmark-h:hr></denchmark-h>
 
 
 Actually the in-built profiler is one of the main features that made me try out Lightning. I would be most grateful to have it work :)
 
 that's great to hear! definitely want to help you get this figured out.
 i tried reproducing your error but it's working for me - check out this colab notebook
 <denchmark-link:https://colab.research.google.com/drive/1wSwMd5xGb36zdNS-5yk6ptHNEXqa1IMi>https://colab.research.google.com/drive/1wSwMd5xGb36zdNS-5yk6ptHNEXqa1IMi</denchmark-link>
 
 could you perhaps share a colab notebook where this is failing?
 btw i agree we should add a note to the documentation about enabling the logger, i believe we used to configure logging within the library but that was removed at one point
 		",7.0,dumitrescustefan,2020-03-27T09:36:22Z,"
 		similar question about missing logging table was raised also by <denchmark-link:https://github.com/dumitrescustefan>@dumitrescustefan</denchmark-link>
 
 I would suggest adding also a method  returning string of the stats
 so we avoid this confusion with logging init and a user can simply use:
 <denchmark-code>print(trainer.profiler.summary())
 </denchmark-code>
 
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  ^^
 		",8.0,dumitrescustefan,2020-03-27T11:11:34Z,"
 		
 
 if logging is required, let’s just auto configure it for the user? not doing so goes against our principles.
 
 
 i think a problem with the logger is that i think it prints when training completes. if you stop training early (early stopping), or just ctrl c, i suspect you won’t see it?
 
 
 adding that print suggestion i think is also yet another thing the user has to remember which goes against our principles haha.
 
 
 If (2) is true, then let’s just make a limit of 2 epochs when profiler is enabled?
 Another option is to always run the basic profiler for the sanity check. Then profiler=True would run it for training as well. but i think the sanity check profiler won’t reflect the true speed bc it doesn’t backprop?
 <denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  can we prioritize making these fixes as this is a key feature?
 		",9.0,dumitrescustefan,2020-03-28T00:55:48Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 
 if logging is required, let’s just auto configure it for the user? not doing so goes against our principles.
 
 at the time when the feature was merged, we were configuring logging. i went back to <denchmark-link:https://github.com/jeremyjordan/pytorch-lightning/blob/feature/profiling/pytorch_lightning/__init__.py#L29>the branch</denchmark-link>
  and verified that it was working out of the box. there was a later PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/767>#767</denchmark-link>
 ) which removed this.
 
 i think a problem with the logger is that i think it prints when training completes. if you stop training early (early stopping), or just ctrl c, i suspect you won’t see it?
 
 actually, we still do show it :) in both cases (early stopping + keyboard interrupt)
 
 can we prioritize making these fixes as this is a key feature?
 
 yeah for sure. as i understand, this should just involve adding the logging config back in
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,77,77,test_simple_profiler_describe,simple_profiler,77,79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1262,philip-bl,2020-03-27T12:38:00Z,2020-04-03T13:25:33Z,incorrect run on the test set with overwritten validation_end and test_epoch_end,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 If I override validation_end and test_epoch_end, TrainerEvaluationLoopMixin.evaluate works incorrectly on the test set
 Suppose we override  and , but not  and . (I actually did this since I am a newbie and haven't yet figured out how everything works; also it seems  is the same as , and  seems to be the same as ). Suppose I run . Consider lines 300-312 in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/6a0b171be4c08982a7a8b45ddffe04a726265323/pytorch_lightning/trainer/evaluation_loop.py#L300>evaluation_loop.py</denchmark-link>
 . Then we have , so the first  block is executed, that is . But look at the second  and its . We have , hence the  of the second  will also be executed, that is . And we will have validation results recorder as test results, which is a mistake.
 This problem is present in the commit <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/60b8246bc3d4e22f806780a816878c668ac647c4>60b8246</denchmark-link>
 . And the inverse problem (which happens if we override only  and  is present in 0.7.1.
 	",ebd9fc9530242e1c9b5f3093dc62ceb4185735b0,Adrian Wälchli,2020-04-03 09:25:32-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,64,,1.0,philip-bl,2020-03-27T13:00:04Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,philip-bl,2020-04-02T21:49:23Z,"
 		I can look into this.
 To summarize, there is a problem when we mix *_epoch_end (new version) and *_end (old naming)  for validation and test. There is no problem if we would consequently use only one of the two, either the deprecated or the new way. Is this correct?
 I propose this fix:
 <denchmark-code>if test_mode:
     if is_overriden(""test_end""):
         # get results and warn user
     elif is_overriden(""test_epoch_end""):
         # get results
 else: 
     # same as above but with ""validation_end"", ""validation_epoch_end""
 </denchmark-code>
 
 		",3.0,philip-bl,2020-04-02T21:53:57Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  yes
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316","298,299,300,301,302,303,304,305,306,307,308,309,310,311",_evaluate,"self,LightningModule,dataloaders,int,bool",219,324,MODIFY,3.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,"537,538,539","537,538,539",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_disabled_validation.validation_epoch_end,"self,args,kwargs",537,539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"537,538,539","537,538,539",test_disabled_validation.validation_end,"self,args,kwargs",537,539,1.0,"531,537,538,539,558,559,560,561,571,572,573,574","531,537,538,539,558,559,569,570",test_disabled_validation,,524,574,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1264,leemengtaiwan,2020-03-27T14:14:42Z,2020-03-30T22:37:03Z,Multiple undesired checkpoints created during single epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Thanks for the great project! When I sent custom ModelCheckpoint to Trainer and hoping to get one checkpoint each epoch, the Trainer eventually produced a lot of versioned checkpoints within a single epoch, wasting lots of disk space and were causing confusion. An example is shown as below:
 <denchmark-code>checkpoints/
 └── gan
     ├── _ckpt_epoch_0.ckpt
     ├── _ckpt_epoch_0_v0.ckpt
     ├── _ckpt_epoch_0_v10.ckpt
     ├── _ckpt_epoch_0_v11.ckpt
     ├── _ckpt_epoch_0_v12.ckpt
     ├── _ckpt_epoch_0_v13.ckpt
     ├── _ckpt_epoch_0_v14.ckpt
     ├── _ckpt_epoch_0_v15.ckpt
     ├── _ckpt_epoch_0_v16.ckpt
     ├── _ckpt_epoch_0_v17.ckpt
     ├── _ckpt_epoch_0_v18.ckpt
     ├── _ckpt_epoch_0_v19.ckpt
     ├── _ckpt_epoch_0_v1.ckpt
     ├── _ckpt_epoch_0_v20.ckpt
     ├── _ckpt_epoch_0_v21.ckpt
 </denchmark-code>
 
 The minimal code I used for training (below I have showed how to reproduce this exactly):
 checkpoint_callback = ModelCheckpoint(
         filepath=os.path.join(
             ""gan"",
             experiment_name
         ),
         save_top_k=-1,
         period=10
     )
     
     trainer = Trainer(
         checkpoint_callback=checkpoint_callback,
 		...
     )
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Clone the repo
 
 git clone https://github.com/leemengtaiwan/learnable_ai.git
 git checkout 66b4cc9db5d1b9f1e2cf6c190d1172cdf6adbe92
 
 Run the script to train a GAN with MNIST
 
 cd learnable_ai/
 python applications/image_generation/train_gan.py\
     --dataset mnist\
     --latent_dim 128\
     --dim 32\
     --channels 1\
     --batch_size 256 \
     --max_epochs 3
 
 See the generated check points under checkpoints:
 
 tree checkpoints/
 
 There should be multiple checkpoints within a single epoch
 
 checkpoints/
 └── gan
     ├── _ckpt_epoch_0.ckpt
     ├── _ckpt_epoch_0_v0.ckpt
     ├── _ckpt_epoch_0_v10.ckpt
     ├── _ckpt_epoch_0_v11.ckpt
     ├── _ckpt_epoch_0_v12.ckpt
     ├── _ckpt_epoch_0_v13.ckpt
     ├── _ckpt_epoch_0_v14.ckpt
     ├── _ckpt_epoch_0_v15.ckpt
     ├── _ckpt_epoch_0_v16.ckpt
     ├── _ckpt_epoch_0_v17.ckpt
     ├── _ckpt_epoch_0_v18.ckpt
     ├── _ckpt_epoch_0_v19.ckpt
     ├── _ckpt_epoch_0_v1.ckpt
     ├── _ckpt_epoch_0_v20.ckpt
     ├── _ckpt_epoch_0_v21.ckpt
 ...
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 One checkpoint for each epoch when using custom ModelCheckpoint for pl.LightningModule which don't simply use val_loss for evaulation metric (thus the default checkpoint functionality on Trainer will not work)
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
         GPU:
                 Tesla V100-SXM2-16GB
         available:           True
         version:             10.1
 packages:
         numpy:               1.18.1
         pyTorch_debug:       False
         pyTorch_version:     1.4.0
         pytorch-lightning:   0.7.1
         tensorboard:         2.2.0
         tqdm:                4.43.0
 system:
         OS:                  Linux
         architecture:
                 64bit
                 ELF
         processor:           x86_64
         python:              3.6.10
         version:             #1 SMP Tue Dec 24 03:25:32 UTC 2019
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 
 Related Slack discussion
 
 	",09167efdb59e1be8ffe9ff7010393bff084390be,Jirka Borovec,2020-03-30 18:37:02-04:00,MODIFY,1,pl_examples\multi_node_examples\multi_node_ddp_demo.py,pl_examples\multi_node_examples\multi_node_ddp_demo.py,1.0,19,"19,20,21,22,23",1.0,leemengtaiwan,2020-03-27T14:26:25Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,leemengtaiwan,2020-03-28T00:08:49Z,"
 		it seems that it triggered with step interval instead of epoch interval, continue debug
 		",3.0,leemengtaiwan,2020-03-28T18:24:26Z,"
 		the issue with checkpoint is that it just specifies the interval of being called assuming ha it is called just once per epoch... but in your case, you turned to fast run mode and the checkpoint is called every batch so the period is correct just it every 10 steps instead of every 10 epochs...
 I am going to change the checkpointing to cunt epochs explicitly...
 		",,,,,MODIFY,0.0,pytorch_lightning\callbacks\base.py,pytorch_lightning\callbacks\base.py,0.0,3,3,,,,,MODIFY,2.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212","184,185,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212",MODIFY,11.0,pytorch_lightning\profiler\profiler.py,pytorch_lightning\profiler\profiler.py,1.0,24,24,stop,"self,action_name",24,25,MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"202,203,204","202,203,204,205",set_distributed_mode,"self,distributed_backend,num_gpu_nodes",173,217,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",176,212,main,hparams,18,37,,,,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"584,585,586,587","587,588",sanitize_gpu_ids,gpus,574,588,1.0,"514,515,516,517","515,516,517,518,519,520",dp_train,"self,model",502,528,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,46,46,profile_iterable,"self,iterable,str",46,56,1.0,"325,326,327","325,326,327",run_evaluation,"self,bool",322,404,,,,,,,,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"739,740,746,747,753,754","742,743,749,750,756",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"142,143,144,145,146,147,148,149,150,151,152,153,154,155","142,143,144,145,146,147,148,149,150,151,152,153,154,155,156",format_checkpoint_name,"self,epoch,metrics,ver",139,174,,,,,,,,1.0,"129,131,132,133,134,135,136","129,131,132,133,134,135",__init__,"self,output_filename,line_count_restriction",129,139,1.0,20,20,start,"self,action_name",20,21,1.0,24,24,stop,"self,str",24,25,1.0,58,58,describe,self,58,60,1.0,"129,131,132,133,134,135,136","129,131,132,133,134,135",__init__,"self,str,float",129,140,1.0,28,28,profile,"self,action_name",28,44,1.0,20,20,start,"self,str",20,21,1.0,46,46,profile_iterable,"self,iterable,action_name",46,56,1.0,28,28,profile,"self,str",28,44,1.0,"494,495","494,495,496",tpu_train,"self,tpu_core_idx,model",471,500,__attach_dataloaders,"self,model,train_dataloader,val_dataloaders,test_dataloaders",734,756,1.0,855,"858,859",run_pretrain_routine,"self,LightningModule",814,916,MODIFY,8.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,,"344,345,346,347,348,349",restore_hpc_weights_if_needed,"self,model",344,362,1.0,146,"146,152,153,154",restore_weights,"self,LightningModule",146,180,1.0,,"263,271,272,273,274",restore,"self,checkpoint_path,on_gpu",263,292,1.0,"230,237,240","233,240,243",_atomic_save,"self,checkpoint,str",230,245,1.0,"237,240","233,240,243",_atomic_save,"self,checkpoint,filepath",233,248,1.0,260,"263,271,272,273,274",restore,"self,str,bool",260,285,1.0,"337,338","344,345,346,347,348,349",restore_hpc_weights_if_needed,"self,LightningModule",337,351,1.0,146,"146,152,153,154",restore_weights,"self,model",146,183,MODIFY,4.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"464,485,486,487,488,489,490,491","464,465",run_training_epoch,self,385,498,1.0,"726,727,728,730,731,747","721,722,724,725,741,742,743,744",update_learning_rates,"self,interval",721,747,1.0,"718,719","712,713,714,721,722,724",training_forward,"self,batch,batch_idx,opt_idx,hiddens",654,724,1.0,"726,727,728,730,731,747,748,749,750,751","741,742,743,744",update_learning_rates,"self,str",726,754,MODIFY,0.0,tests\base\__init__.py,tests\base\__init__.py,0.0,"43,54","43,54",MODIFY,2.0,tests\base\mixins.py,tests\base\mixins.py,1.0,"65,66,67","69,70",validation_epoch_end,"self,outputs",62,94,1.0,17,"17,18,19,20,21",validation_step,"self,batch,batch_idx,args,kwargs",16,53,MODIFY,5.0,tests\base\models.py,tests\base\models.py,1.0,68,"50,51,52,53,54",__init__,"self,hparams,force_remove_distributed_sampler",50,68,1.0,68,"74,75",__build_model,self,67,75,1.0,81,90,forward,"self,x",80,90,1.0,"47,48","47,48,50,51,52,53,54",__init__,"self,hparams,bool",47,62,1.0,97,"110,111,112,113,114",training_step,"self,batch,batch_idx,optimizer_idx",96,121,MODIFY,0.0,tests\test_profiler.py,tests\test_profiler.py,0.0,"38,39,40,41,42,57,58,59,60,61,107,108,109,110,111,129,130,131,132,133","38,53,99,117",MODIFY,2.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,"251,252,253,254,255,256,257,258,259,260,261,262,263,275,285,287,288,291","251,261,262,265,266,267,277,279,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397",test_model_checkpoint_options,tmpdir,251,398,1.0,"263,275,285,287,288,291","265,266,267,277,279,282,283,284,285,286,287,288,289,290,291,292",test_model_checkpoint_options,"tmpdir,save_top_k,file_prefix,expected_files",263,292,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1290,AmitMY,2020-03-30T11:08:50Z,2020-04-24T14:29:25Z,bug(logger): wandb fails on sweep,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using wandb sweeps for hyperparameters search, I get this error:
 
 wandb: ERROR Attempted to change value of key ""dropout_std"" from 0.030424838979365657 to 0.030424838979365654
 
 The reason is I ran:
 wandb_logger.log_hyperparams(params)
 Which I guess has some problem with floating-point numbers in high accuracy?
 	",f3d139e90f9212813c4f5e6de777bdef9dfe7635,Boris Dayma,2020-04-24 10:29:24-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"69,70",,1.0,AmitMY,2020-03-30T18:04:40Z,"
 		+1, I faced the same issue when using pytorch lightning with wandb sweeps. To summarize, wandb automatically logs hyperparams when we run the wandb sweep agent on a machine. Later, pytorch lightning again tries to log same hyperparams but due to precision error between lightning and wandb already logged hyperparams, wandb throws this error. Just a guess: wandb sweep agent might be using double format to generate new hyperparams and when lightning receives those args from command line, it converts them to float and tries to log it. I haven't digged in detail where these hyperparams get altered, it could be on wandb side or lightning side.
 I reported this issue to wandb and got the following response:
 
 It is preferred to either pass your config parameters all at once to: wandb.init(config=config_dict_that_could_have_params_set_by_sweep)
 or:
 experiment = wandb.init()
 experiment.config.setdefaults(config_dict_that_could_have_params_set_by_sweep)
 The advantage of doing this is that it will ignore setting any key that has already been set by the sweep.
 We will look into the torch lightning integration and see if we can make this safer for those using sweeps.
 
 		",2.0,AmitMY,2020-03-30T18:05:52Z,"
 		cc: <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
 , <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",3.0,AmitMY,2020-03-30T18:24:10Z,"
 		<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>
  <denchmark-link:https://github.com/calclavia>@calclavia</denchmark-link>
  pls ^^
 		",4.0,AmitMY,2020-03-30T18:37:27Z,"
 		The problem is that it tries to log this value twice and is probably called before automatically by pytorch-lightning.
 The callback will automatically log every parameter which is in pl.LightningModule.params (where you probably already have the dropout).
 See an example of using pytorch-lightning with wandb (including sweeps) here: <denchmark-link:https://github.com/borisdayma/lightning-kitti>https://github.com/borisdayma/lightning-kitti</denchmark-link>
 
 I'll be adding it to the pytorch-lightning repo later but still need to push a PR related to the watch method for it to work properly.
 		",MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,117,117,log_hyperparams,"self,str",115,117,MODIFY,1.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,26,26,,,,,,,,,,,,,,,,,,,,,,,5.0,AmitMY,2020-04-16T22:26:11Z,"
 		<denchmark-link:https://github.com/borisdayma>@borisdayma</denchmark-link>
  is it fixed now?
 		",6.0,AmitMY,2020-04-16T23:45:41Z,"
 		I added a fix.
 <denchmark-link:https://github.com/AmitMY>@AmitMY</denchmark-link>
  <denchmark-link:https://github.com/amoudgl>@amoudgl</denchmark-link>
  Feel free to test it with your sweeps and let me know if there's still an error.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_wandb_logger,wandb,11,32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1306,rzepinskip,2020-03-30T17:08:20Z,2020-04-07T00:29:56Z,RuntimeError: Unimplemented backend XLA on TPU,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
  raised for  line in  file when running MNIST on TPU. I think it was introduced in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/31b71483c47fa4aa688912b432726cdac0025a9b>31b7148</denchmark-link>
 .
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Go to MNIST on TPUs
 Run all
 Scroll down to trainer
 See error
 
 <denchmark-code>Traceback (most recent call last):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 119, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 119, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 119, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
 Exception in device=TPU:5: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
 RuntimeError: Unimplemented backend XLA
 Traceback (most recent call last):
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 119, in _start_fn
     fn(gindex, *args)
 RuntimeError: Unimplemented backend XLA
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 499, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 920, in run_pretrain_routine
     self.train()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 356, in train
     self.run_training_epoch()
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 425, in run_training_epoch
     output = self.run_training_batch(batch, batch_idx)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py"", line 582, in run_training_batch
     self.batch_loss_value.append(loss)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/supporting_classes.py"", line 23, in append
     if self.memory.type() != x.type():
 RuntimeError: Unimplemented backend XLA
 ---------------------------------------------------------------------------
 Exception                                 Traceback (most recent call last)
 <ipython-input-2-12f6e300d51d> in <module>()
       6 # most basic trainer, uses good defaults
       7 trainer = Trainer(num_tpu_cores=8)
 ----> 8 trainer.fit(model)
 
 3 frames
 /usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)
     111                 raise Exception(
     112                     ""process %d terminated with exit code %d"" %
 --> 113                     (error_index, exitcode)
     114                 )
     115 
 
 Exception: process 4 terminated with exit code 17
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.6
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source): -
 Python version: 3.6
 CUDA/cuDNN version: -
 GPU models and configuration: -
 Any other relevant information: TPU backend
 
 	",b8ff9bc1d242a18f5e7147f34d63f43fcdd0e50a,Paweł Rzepiński,2020-04-06 20:29:55-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"24,73,74,81,86","24,73,74,81",1.0,rzepinskip,2020-04-05T14:37:40Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 The issue is caused by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f1e11d8b3874067016693c50ae253ec79eecda09/pytorch_lightning/trainer/supporters.py#L40>this line</denchmark-link>
 :
 <denchmark-code>        if self.memory.type() != x.type():
             self.memory.type_as(x)
 </denchmark-code>
 
 For TPU x is a XLA tensor and x.type() results in Unimplemented backend XLA (for GPU type is torch.cuda.FloatTensor).
 Something like x = torch.Tensor([x]) before condition checking fixes the problem. Or we can just send one of the tensors to common device.
 Notebook for debugging on <denchmark-link:https://colab.research.google.com/drive/16Ug8IYPkqCu_NhK1FV7W-vszDsdgXfPD>Google Colab</denchmark-link>
 .
 		",,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\supporters.py,pytorch_lightning\trainer\supporters.py,1.0,"39,40,41","39,40,41",append,"self,x",38,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1322,VitorGuizilini-TRI,2020-03-31T17:35:57Z,2020-04-05T15:07:17Z,Training loop temporarily hangs after every 4 steps,"
 I am porting some of my code to pytorch lightning, and everything seems to work fine. However, for some reason after every 4 training steps I see some temporary hanging (~1 second), which is severely slowing down my overall training time. Am I missing some obvious configuration?  This is my Trainer configuration:
 <denchmark-code>    trainer = pl.Trainer(
         gpus=8
         num_nodes=1,
         distributed_backend='ddp',
         checkpoint_callback=False,
         max_epochs=50,
         max_steps=None,
         progress_bar_refresh_rate=1,
         check_val_every_n_epoch=1,
         val_check_interval=1.0,
         gradient_clip_val=0.0,
         log_save_interval=0,
         num_sanity_val_steps=0,
         amp_level='O0',
     )
 </denchmark-code>
 
 	",b18accc64ccd24095c11fdbd64cc924456134592,Ethan Harris,2020-04-05 11:07:16-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,29,,1.0,VitorGuizilini-TRI,2020-03-31T17:36:45Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,VitorGuizilini-TRI,2020-04-04T12:34:00Z,"
 		<denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
 
 		",3.0,VitorGuizilini-TRI,2020-04-04T12:39:03Z,"
 		Thanks for the issue! Would it be possible to post the code that reproduces this error? I've only seen this sort of behaviour before when the number of data loading workers is low - are you working with large data here (e.g. big images)?
 		",4.0,VitorGuizilini-TRI,2020-04-04T16:24:16Z,"
 		I increased the number of workers and it works perfectly now, thank you very much! You can close this issue.
 		",MODIFY,2.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"77,78,79,80,81",,_worker_check,"self,DataLoader,str",77,81,MODIFY,1.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527",,,,,,,,,,,,,,,,,,,,,,,,5.0,VitorGuizilini-TRI,2020-04-04T16:27:14Z,"
 		should we throw a warning when users use few workers?
 		",6.0,VitorGuizilini-TRI,2020-04-04T16:34:04Z,"
 		If possible, sure! Seems like an obvious solution now, but it could save a couple of hours for other people. :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_warning_with_few_workers,tmpdir,489,527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"122,128",,reset_train_dataloader,"self,LightningModule",114,162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1335,areshytko,2020-04-01T23:31:03Z,2020-04-06T14:17:17Z,Trainer DDP should invoke load_spawn_weights() only in proc_rank == 0,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Trainer DDP load_spawn_weights should happen only in proc_rank == 0 since only in this process (node) save_spawn_weights actually saves checkpoint
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 setup two-node cluster.
 set SLURM_NODEID on each node: '0' on node 0 and '1' on node 1.
 run the script python app.py on each node.
 see stdout on the node 1:
 
 <denchmark-code>Traceback (most recent call last):
   File ""app.py"", line 166, in <module>
     main_()  # pylint: disable=no-value-for-parameter
   File ""app.py"", line 162, in main_
     trainer.fit(model)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 593, in fit
     self.load_spawn_weights(model)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 368, in load_spawn_weights
     loaded_model = original_model.__class__.load_from_checkpoint(path)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py"", line 1353, in load_from_checkpoint
     checkpoint = torch.load(checkpoint_path, map_location=lambda storage, loc: storage)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py"", line 525, in load
     with _open_file_like(f, 'rb') as opened_file:
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py"", line 212, in _open_file_like
     return _open_file(name_or_buffer, mode)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.7/site-packages/torch/serialization.py"", line 193, in __init__
     super(_open_file, self).__init__(open(name, mode))
 FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/pytorch-lightning-intro-guide/__temp_weight_ddp_end.ckpt'
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 app.py:
 <denchmark-code>import pathlib
 
 import pytorch_lightning as pl
 import torch
 from torch.nn import functional as F
 from torch.optim import Adam
 from torch.utils.data import DataLoader, random_split
 from torchvision import datasets, transforms
 
 
 class LitMNIST(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 256)
         self.layer_3 = torch.nn.Linear(256, 10)
 
         self.train_dataset = None
         self.val_dataset = None
         self.test_dataset = None
 
     def forward(self, x):
         batch_size, channels, width, height = x.size()
         x = x.view(batch_size, -1)
         x = self.layer_1(x)
         x = F.relu(x)
         x = self.layer_2(x)
         x = F.relu(x)
         x = self.layer_3(x)
         x = F.log_softmax(x, dim=1)
         return x
 
     def prepare_data(self):
         # transform
         transform = transforms.Compose(
             [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
 
         # download
         data_dir = pathlib.Path.home() / 'data'
         mnist_train = datasets.MNIST(data_dir, train=True,
                                      download=True, transform=transform)
         mnist_test = datasets.MNIST(data_dir, train=False,
                                     download=True, transform=transform)
 
         # train/val split
         mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])
 
         # assign to use in dataloaders
         self.train_dataset = mnist_train
         self.val_dataset = mnist_val
         self.test_dataset = mnist_test
 
     def train_dataloader(self):
         return DataLoader(self.train_dataset, batch_size=64)
 
     def val_dataloader(self):
         return DataLoader(self.val_dataset, batch_size=64)
 
     def test_dataloader(self):
         return DataLoader(self.test_dataset, batch_size=64)
 
     def configure_optimizers(self):
         return Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
 
         # add logging
         logs = {'loss': loss}
         return {'loss': loss, 'log': logs}
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
         return {'val_loss': loss}
 
     def test_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
         return {'val_loss': loss}
 
     def test_epoch_end(self, outputs):
         avg_loss = torch.stack(  # pylint: disable=no-member
             [x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def init_ddp_connection(self, proc_rank: int, world_size: int) -> None:
         torch.distributed.init_process_group(
             'nccl', rank=proc_rank, world_size=world_size)
 
 def main():
     model = LitMNIST()
 
     gpus = 1
     num_nodes = 2
 
     trainer = pl.Trainer(gpus=gpus,
                          num_nodes=num_nodes,
                          distributed_backend='ddp',
                          max_epochs=3)
     trainer.fit(model)
 
 
 if __name__ == '__main__':
     main()
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 All workers on all nodes should finish without errors.
 <denchmark-h:h3>Environment</denchmark-h>
 
 On each node:
 <denchmark-code>cuda:
 	GPU:
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 	available:           True
 	version:             10.1
 packages:
 	numpy:               1.16.6
 	pyTorch_debug:       False
 	pyTorch_version:     1.4.0
 	pytorch-lightning:   0.7.1
 	tensorboard:         2.2.0
 	tqdm:                4.44.1
 system:
 	OS:                  Linux
 	architecture:
 		64bit
 
 	processor:           x86_64
 	python:              3.7.7
 	version:             #113-Ubuntu SMP Wed Jan 29 14:54:54 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",9754c5da55059dd89cf0a4fd582fe5df9449bbe5,areshytko,2020-04-06 10:17:16-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"367,369,370,371,372,373,374,375,376,377,378","366,367,368,370,371,373,374",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,load_spawn_weights,"self,original_model",359,380,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1366,belskikh,2020-04-03T16:01:47Z,2020-04-24T21:21:01Z,ModelCheckpoint tries to remove already removed checkpoint in DDP mode,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training in DDP mode with ModelCheckpoint callback, the train process fails, when ModelCheckpoint callback tries to remove previous checkpoint. I assume that it was already deleted by another process.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run training with ""ddp"" backend and ModelCheckpoint callback with save_top_k={some_number}
 <denchmark-code>  File ""/home/myuser/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap                                                                                                 
     fn(i, *args)                                                                                
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 342, in ddp_train
     self.run_pretrain_routine(model)                                                                                                        
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 830, in run_pretrain_routine
     self.train()                                                                                                                                                                                                   
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 343, in train                                                      
     self.run_training_epoch()                            
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 452, in run_training_epoch                                                                       
     self.call_checkpoint_callback()                                                                                                                                                   
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 737, in call_checkpoint_callback
     self.checkpoint_callback.on_validation_end(self, self.get_model())                                                                                                                                             
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 204, in on_validation_end                                      
     self._do_check_save(filepath, current, epoch)    
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 221, in _do_check_save                                                                      
     self._del_model(delpath)                                                                                                                                                           
   File ""/home/myuser/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 121, in _del_model
     os.remove(filepath)                                                                                                                                                                                            
 FileNotFoundError: [Errno 2] No such file or directory: {PREVIOUS_CHECKPOINT_NAME}
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I expect that ModelCheckpoint callbacks from different DDP processes will not concurrent with each other in saving/deleteng files.
 I fixed by rewriting _del_model method of ModelCheckpoint callback:
 <denchmark-code>class DDPModelCheckpoint(ModelCheckpoint):
     def _del_model(self, filepath):
         try:
             os.remove(filepath)
         except Exception:
             pass
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version: 1.4
 OS: Ubuntu 18.04
 How you installed PyTorch - conda
 Python version: 3.7
 CUDA/cuDNN version: 10.2
 GPU models and configuration: 2x2080Ti
 pytorch-lightning version: 0.7.1
 
 	",58a467dd68b157fdba8824a437dbaf698ad88569,Jirka Borovec,2020-04-24 17:21:00-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"25,26,27,28,40,45,46,84,85,102",,1.0,belskikh,2020-04-03T16:02:40Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,belskikh,2020-04-04T08:16:38Z,"
 		Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply
 <denchmark-code>if os.path.isfile(filepath):
     os.remove(filepath)
 </denchmark-code>
 
 		",3.0,belskikh,2020-04-04T08:18:29Z,"
 		Is there also an issue with saving? Does it save/overwrite the file in multiple processes?
 		",4.0,belskikh,2020-04-06T07:34:57Z,"
 		I encountered that one too. From my perspective, model updates should be happening within main worker only (master worker). However, I guess each workers lightning created is trying to delete their own checkpoints. However, slave worker never created one (and it shouldn't be). I solved the problem in a similar way with <denchmark-link:https://github.com/belskikh>@belskikh</denchmark-link>
  's workaround but it did not feel right and downgraded to 0.6.0.
 		",MODIFY,1.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"133,134",132,_del_model,"self,filepath",132,134,MODIFY,0.0,pytorch_lightning\loggers\__init__.py,pytorch_lightning\loggers\__init__.py,0.0,"33,34,84","33,83",MODIFY,4.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,,"255,256,257",rank,self,255,257,MODIFY,0.0,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,0.0,"27,29",27,,,,,5.0,belskikh,2020-04-06T09:47:20Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  of course it is not the best (it may be the worse, actually) solution
 It is just a fast workaround, waiting for solid fix
 I agree with logic, when only one ModelCheckpoint callback should save/delete weights, because all weights are the same on all nodes at the end of the training step.
 It can be done somehow like this:
 <denchmark-code>class ModelCheckpoint(..., main_worker_rank: int = 0):
 ....
     def _del_model(self, filepath):
         if self.main_worker_rank == dist.get_rank():
              # do delete
 </denchmark-code>
 
 And the same for the saving code.
 		",6.0,belskikh,2020-04-06T10:00:02Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  should Lightning do checkpoints only on rank 0? It could be a problem if writing to a shared filesystem between nodes. AFAIK the loggers already do that by only logging in process 0.
 		",7.0,belskikh,2020-04-06T11:20:09Z,"
 		I do not think, that it should do it only on specific rank, I think user should have ability to specify rank (node), where checkpoints will be saved
 		",8.0,belskikh,2020-04-07T19:23:25Z,"
 		
 Your suggestion to pass on an Exception is not the best, at least you should make it the specific error, i.e., FileNotFoundError. But in this case, I suggest to do simply
 if os.path.isfile(filepath):
     os.remove(filepath)
 
 
 this does not work in async, I have observed many times that the if pass for both but then when you try really delete it, it is missing for one of them...
 		",9.0,belskikh,2020-04-07T19:28:48Z,"
 		should checkpointing be done in only one process then, like loggers?
 		",10.0,belskikh,2020-04-07T20:11:15Z,"
 		yup. checkpoint should only happen from world_rank = 0 gpu 0
 		",11.0,belskikh,2020-04-07T21:01:26Z,"
 		well, ModelCeckpoint doesn't have a rank...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\loggers\mlflow.py,pytorch_lightning\loggers\mlflow.py,0.0,"18,19",18,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\loggers\neptune.py,pytorch_lightning\loggers\neptune.py,1.0,,"13,14,15,16,17,18,19,20,21,22,23,24,25",rank_zero_only,Callable,13,25,0.0,"21,22",21,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,0.0,"17,18",16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"21,22,23",rank_zero_only.wrapped_fn,"self,args,kwargs",21,23,1.0,,"260,261,262",rank,"self,int",260,262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\loggers\test_tube.py,pytorch_lightning\loggers\test_tube.py,1.0,,"138,139",rank,self,138,139,1.0,,"142,143,144,145",rank,"self,int",142,145,1.0,96,95,experiment,self,75,98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\loggers\trains.py,pytorch_lightning\loggers\trains.py,0.0,"22,23",22,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,0.0,"18,19",18,MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"328,329","325,326,328,329,330",ddp_train,"self,process_idx,model",298,373,,,,,,,,MODIFY,2.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,509,509,tpu_train,"self,tpu_core_idx,model",494,525,1.0,612,"612,613,614,615,616",horovod_train,"self,model",567,619,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,,"36,44",MODIFY,0.0,pytorch_lightning\utilities\__init__.py,pytorch_lightning\utilities\__init__.py,0.0,3,3,,,,,,,,,,,,configure_logger,"self,logger",28,44,ADD,0.0,None,pytorch_lightning\utilities\distributed.py,DELETE,0.0,pytorch_lightning\utilities\warnings.py,None,MODIFY,0.0,tests\loggers\test_base.py,tests\loggers\test_base.py,0.0,"8,9",8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1375,areshytko,2020-04-04T16:20:54Z,2020-04-07T10:39:55Z,Tensorboard logger error: lightning_logs directory not exists in multi-node DDP on nodes with rank != 0,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 In multi-node DDP train mode on all nodes except rank 0 errors appears at the start of the training caused by accessing lightning_logs directory in tensorboard logger which is not exist at the moment.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 setup multi-node cluster (without SLURM)
 set environment variables on each node:
 
 <denchmark-code>export MASTER_ADDR=<rank 0 node IP>
 export MASTER_PORT=23456
 export RANK=<node id>
 export SLURM_NODEID=<node id>
 export WORLD_SIZE=<world-size>
 </denchmark-code>
 
 
 install dependencies:
 
 <denchmark-code>pip install torch torchvision hydra-core pytorch-lightning
 </denchmark-code>
 
 
 copy app.y and conf.yaml to each node
 run script on each node
 
 <denchmark-code>python app.py
 </denchmark-code>
 
 
 see the error:
 
 <denchmark-code>Exception:
 
 -- Process 0 terminated with the following error:
 Traceback (most recent call last):
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
     fn(i, *args)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 342, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 777, in run_pretrain_routine
     self.configure_checkpoint_callback()
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_config.py"", line 45, in configure_checkpoint_callback
     f'version_{self.logger.version}',
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py"", line 161, in version
     self._version = self._get_next_version()
   File ""/home/ubuntu/anaconda3/envs/nightly_pt/lib/python3.6/site-packages/pytorch_lightning/loggers/tensorboard.py"", line 167, in _get_next_version
     for d in os.listdir(root_dir):
 FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/pytorch-lightning-intro-guide/outputs/2020-04-04/15-53-26/lightning_logs'
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 app.py:
 <denchmark-code>import pathlib
 
 import hydra
 import pytorch_lightning as pl
 import torch
 from omegaconf import OmegaConf
 from torch.nn import functional as F
 from torch.optim import Adam
 from torch.utils.data import DataLoader, random_split
 from torchvision import datasets, transforms
 
 
 class LitMNIST(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 256)
         self.layer_3 = torch.nn.Linear(256, 10)
 
         self.train_dataset = None
         self.val_dataset = None
         self.test_dataset = None
 
     def forward(self, x):
         batch_size, channels, width, height = x.size()
         x = x.view(batch_size, -1)
         x = self.layer_1(x)
         x = F.relu(x)
         x = self.layer_2(x)
         x = F.relu(x)
         x = self.layer_3(x)
         x = F.log_softmax(x, dim=1)
         return x
 
     def prepare_data(self):
         # transform
         transform = transforms.Compose(
             [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
 
         # download
         data_dir = pathlib.Path.home() / 'data'
         mnist_train = datasets.MNIST(data_dir, train=True,
                                      download=True, transform=transform)
         mnist_test = datasets.MNIST(data_dir, train=False,
                                     download=True, transform=transform)
 
         # train/val split
         mnist_train, mnist_val = random_split(mnist_train, [55000, 5000])
 
         # assign to use in dataloaders
         self.train_dataset = mnist_train
         self.val_dataset = mnist_val
         self.test_dataset = mnist_test
 
     def train_dataloader(self):
         return DataLoader(self.train_dataset, batch_size=64)
 
     def val_dataloader(self):
         return DataLoader(self.val_dataset, batch_size=64)
 
     def test_dataloader(self):
         return DataLoader(self.test_dataset, batch_size=64)
 
     def configure_optimizers(self):
         return Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
 
         # add logging
         logs = {'loss': loss}
         return {'loss': loss, 'log': logs}
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
         return {'val_loss': loss}
 
     def validation_epoch_end(self, outputs):
         avg_loss = torch.stack(  # pylint: disable=no-member
             [x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def test_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
         return {'val_loss': loss}
 
     def test_epoch_end(self, outputs):
         avg_loss = torch.stack(  # pylint: disable=no-member
             [x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def init_ddp_connection(self, proc_rank: int, world_size: int) -> None:
         torch.distributed.init_process_group(
             'nccl', rank=proc_rank, world_size=world_size)
 
 
 @hydra.main(config_path='conf.yaml')
 def main(conf: OmegaConf):
     model = LitMNIST()
 
     trainer = pl.Trainer(gpus=conf.gpus,
                          num_nodes=conf.num_nodes,
                          distributed_backend=conf.distributed_backend,
                          max_epochs=3)
     trainer.fit(model)
 
 
 if __name__ == '__main__':
     main()  # pylint: disable=no-value-for-parameter
 </denchmark-code>
 
 conf.yaml:
 <denchmark-code>gpus: 1
 num_nodes: 2
 distributed_backend: ddp
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Train should go without error
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
 	GPU:
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 		Tesla K80
 	available:           True
 	version:             10.1
 packages:
 	numpy:               1.18.1
 	pyTorch_debug:       False
 	pyTorch_version:     1.4.0
 	pytorch-lightning:   0.7.1
 	tensorboard:         2.2.0
 	tqdm:                4.45.0
 system:
 	OS:                  Linux
 	architecture:
 		64bit
 
 	processor:           x86_64
 	python:              3.6.10
 	version:             #113-Ubuntu SMP Wed Jan 29 14:54:54 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",495ffbd028ae860528c719544cf0409b41d5ef5a,areshytko,2020-04-07 06:39:54-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,86,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,1.0,"167,168,169,170,171",,_get_next_version,self,165,180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
138,ananyahjha93,2019-08-18T19:51:39Z,2019-08-19T14:42:25Z,val_dataloader is not optional in distributed_backend='ddp',"
 Describe the bug
 val_dataloader function is kept optional but a line in the code does not check for 'if self.val_dataloader is not None'. Which leads to the following error:
 File ""/misc/vlgscratch4/FergusGroup/ananya/pyenv/py3.6/lib/python3.6/site-packages/pytorch_lightning/models/trainer.py"", line 500, in get_dataloaders
 for dataloader in self.val_dataloader):
 TypeError: 'NoneType' object is not iterable
 File: models/trainer.py
 line 500
 To Reproduce
 Steps to reproduce the behavior:
 
 
 Not write the optional function: val_dataloader
 
 
 Use the following trainer configuration.
 trainer = Trainer(
 experiment=exp,
 checkpoint_callback=checkpoint_callback,
 distributed_backend='ddp',
 gpus=args.gpu_id,
 amp_level='O2',
 use_amp=True,
 max_nb_epochs=args.epochs,
 progress_bar=True
 )
 
 
 Expected behavior
 Code should ignore ""all(isinstance(dataloader, DistributedSampler) for dataloader in self.val_dataloader)"" check if self.val_dataloader is None.
 Environment:
 
 PyTorch 1.1.0
 Cuda 10.0
 test-tube 0.6.7.6
 pytorch-lightning 0.4.6
 
 	",5b694c7e0ec5f83bf4ec93860d91a48757351265,Ananya Harsh Jha,2019-08-19 15:03:04-04:00,MODIFY,1,pytorch_lightning\models\trainer.py,pytorch_lightning\models\trainer.py,1.0,"498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519","498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518",1.0,ananyahjha93,2019-08-18T22:20:01Z,"
 		good catch
 		",2.0,ananyahjha93,2019-08-18T23:59:54Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  The bug has not been resolved in your latest commits. Do you want me to add the fix? Or am I missing something here?
 		",3.0,ananyahjha93,2019-08-19T00:17:45Z,"
 		pressed wrong button. reopened
 		",4.0,ananyahjha93,2019-08-19T02:21:24Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I have created a pull request for this issue. I did see the commit from 'sidhanthholalkere' but could not find a pull request for it.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get_dataloaders,"self,model",463,519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1388,tasercake,2020-04-06T04:36:54Z,2020-04-17T22:18:30Z,Use isinstance() instead of type() in trainer.distrib_parts.check_gpus_data_type,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When instantiating a Trainer object, it makes sense to be able to pass a subclass of list.
 Ideally, this would be something even more general like collections.abc.Sequence, but I'm not too familiar with Lightning's codebase and that change would have a greater likelihood of breaking things.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Instantiate a Trainer with the gpus parameter being a subclass of list.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 >>> from pytorch_lightning import Trainer
 >>> class MyList(list):
 ...     pass
 ... 
 >>> gpus = MyList([0])
 >>> t = Trainer(gpus=gpus)
 This produces
 <denchmark-code>Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 366, in __init__
     self.data_parallel_device_ids = parse_gpu_ids(self.gpus)
   File ""/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 613, in parse_gpu_ids
     check_gpus_data_type(gpus)
   File ""/opt/anaconda/miniconda3/envs/ai/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 561, in check_gpus_data_type
     raise MisconfigurationException(""GPUs must be int, string or list of ints or None."")
 pytorch_lightning.utilities.debugging.MisconfigurationException: GPUs must be int, string or list of ints or None.
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Trainer is instantiated normally as it would had a list been passed.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version: 1.4.0
 PyTorch Lightning Version: 0.7.1
 OS: Ubuntu 19.10
 How you installed PyTorch: pip
 Python version: 3.7
 
 <denchmark-h:h3>Potential Fix</denchmark-h>
 
 In pytorch_lightning/trainer/distrib_parts.py check types using isinstance() instead of type():
 def check_gpus_data_type(gpus):
     # if gpus is not None and type(gpus) not in (int, str, list):
     if gpus is not None and not isinstance(gpus, (int, str, list)):
         raise MisconfigurationException(""GPUs must be int, string or list of ints or None."")
 I'll put in a PR if this change sounds good
 	",a22a8142ac65668781a6e6f76d3c4e55ea7c249a,Krishna Penukonda,2020-04-17 18:18:29-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,569,569,1.0,tasercake,2020-04-06T04:37:36Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,tasercake,2020-04-08T11:55:13Z,"
 		I do like this shift from  to an  which extend accepted types also to child...
 as always a good PR is always welcome
 cc: <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
  <denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,check_gpus_data_type,gpus,561,570,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
142,sebftw,2019-08-18T23:07:54Z,2019-08-23T12:22:12Z,AttributeError: 'xxx' object has no attribute 'tng_dataloader' continued...,"
 This is the same issue as mentioned in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/125>#125</denchmark-link>
 .
 Describe the bug
 Whenever a @property raises an AttributeError, it will be looked up in __getattr__ (if defined), and then the original error message is lost. And LightingModule inherits __getattr__ from torch.nn.Module.
 This makes debugging difficult as you lose line number and you get an error message that is misleading - the attribute exists! Look at the minimal example in the next section to see what I mean.
 To Reproduce
 <denchmark-code>import torch
 
 class Foo(torch.nn.Module):
     @property
     def bar(self):
         return torch.does_not_exist  # Raises an AttributeError.
 
 Foo().bar
 </denchmark-code>
 
 The output produced:
 <denchmark-code>Traceback (most recent call last):
   File ""C:/minimal.py"", line 8, in <module>
     Foo().bar
   File ""C:\Program Files\Python37\lib\site-packages\torch\nn\modules\module.py"", line 591, in __getattr__
     type(self).__name__, name))
 AttributeError: 'Foo' object has no attribute 'bar'
 </denchmark-code>
 
 The trace says the  attribute does not exist, but it does. It is the  attribute that is nonexistent. Now if you look in the stack trace in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/125>#125</denchmark-link>
  it is even worse.
 Expected behavior
 One would expect the stack trace to be
 <denchmark-code>Traceback (most recent call last):
   File ""C:/minimal.py"", line 7, in bar
     return torch.does_not_exist  # Raises an AttributeError.
 AttributeError: module 'torch' has no attribute 'does_not_exist'
 </denchmark-code>
 
 I think it could be very useful to add try...catch to the pl.data_loader.
 One could also make a pl.attribute decorator that users (and pl.data_loader) can use, and document its use case in the docs. It should simply extend the usual @attribute with a try...catch.
 An example of a more desirable stack trace
 <denchmark-code>Traceback (most recent call last):
   File ""C:/minimal.py"", line 7, in bar
     return torch.does_not_exist  # Raises an AttributeError.
 AttributeError: module 'torch' has no attribute 'does_not_exist'
 
 The above exception was the direct cause of the following exception:
 
 Traceback (most recent call last):
   File ""C:/minimal.py"", line 11, in <module>
     Foo().bar
   File ""C:/minimal.py"", line 9, in bar
     raise RuntimeError('An AttributeError was encountered in Foo.bar.') from e
 RuntimeError: An AttributeError was encountered in Foo.bar.
 </denchmark-code>
 
 In this case the error is correctly shown to be at line 7.
 This stack trace is achieved by replacing line 7 with:
 <denchmark-code>try:
     return torch.does_not_exist  # Raises an AttributeError.
 except AttributeError as e:
     raise RuntimeError('An AttributeError was encountered in Foo.bar.') from e
 </denchmark-code>
 
 
 See also the discussion in <denchmark-link:https://github.com/pytorch/pytorch/issues/13981>pytorch/pytorch#13981</denchmark-link>
  of the problem. It seems to be a <denchmark-link:https://bugs.python.org/issue24983>troublesome</denchmark-link>
  design decision of the language that will be hard to get around.
 Do you think @pl.attribute is a good idea?
 There seem to be no other way around the issue if we must use nn.Module and @attribute.
 	",b31539f62e60e1ad370214e05003c214298d2431,Sebastian Præsius,2019-08-23 08:21:39-04:00,MODIFY,2,pytorch_lightning\root_module\decorators.py,pytorch_lightning\root_module\decorators.py,1.0,"13,14,15,16,17,18,19,20,21,22","13,14,15",1.0,sebftw,2019-08-18T23:10:59Z,"
 		<denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  read my mind haha. was about to add this bug! want to give it a shot?
 		",2.0,sebftw,2019-08-18T23:12:15Z,"
 		<denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  it probably has to catch all exceptions though. the attribute doesn't exist is hiding whatever issue the dataloader code has (ie: user error), so we want to forward that to the user via 
 		",3.0,sebftw,2019-08-18T23:13:58Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Yeah. I can try and look into it tomorrow. I think we only have to catch AttributeError as other exceptions will produce a normal stack trace.
 		",4.0,sebftw,2019-08-19T15:20:51Z,"
 		I have made a sketch of the implementation and how to test it.
 <denchmark-code>import torch.nn as nn
 import pytest
 
 # ------------------------------------------------------------------------
 # IMPLEMENTATION
 # ------------------------------------------------------------------------
 
 errormessage = 'An AttributeError was encountered: '
 def safe_property(fn):
     @property
     def wrapper(*args, **kwargs):
         try:
             return fn(*args, **kwargs)
         except AttributeError as e:
             raise RuntimeError(errormessage + str(e)) from e
     return wrapper
 
 
 # ------------------------------------------------------------------------
 # TESTS
 # ------------------------------------------------------------------------
 
 returnexample = 1
 exceptionexample = ZeroDivisionError
 informative_error_message = ""module 'torch.nn' has no attribute 'does_not_exist'""
 
 class PropertyTest(nn.Module): # Note it is an nn.Module.
     @property
     def test_property(self):
         return returnexample
 
     @safe_property
     def test_safe_property(self):
         return returnexample
 
     @property
     def test_property_attributeerror(self):
         raise AttributeError(informative_error_message)
         # ^ Equivalent to: return nn.does_not_exist, which will also raise AttributeError.
 
     @safe_property
     def test_safe_property_attributeerror(self):
         raise AttributeError(informative_error_message)
 
     @property
     def test_property_error(self):
         raise exceptionexample(informative_error_message)
 
     @safe_property
     def test_safe_property_error(self):
         raise exceptionexample(informative_error_message)
 
 
 def test_same_return():
     testobj = PropertyTest()
     # Test that they both return the same.
     assert testobj.test_property == testobj.test_safe_property
 
 def test_same_interface():
     # This is a design decision we haven't discussed.
     # Only include if we find it important that the safe property doesn't do anything more
     #  than a normal property, i.e. has the same interface and all.
     assert dir(PropertyTest.test_property) == dir(PropertyTest.test_property)
     assert isinstance(PropertyTest.test_safe_property, type(PropertyTest.test_property))  # <class 'property'>
 
 def test_error():
     # In this we test that normal exceptions are raised normally.
     testobj = PropertyTest()
     # We expect an exception to be raised and propagated if it is of any type other than AttributeError.
     with pytest.raises(exceptionexample) as exc_info:
         testobj.test_property_error
 
     # This checks that in this case the exception and message matches that of a normal property.
     with pytest.raises(exc_info.type, match=exc_info.value.args[0]):
         testobj.test_safe_property_error
 
 def test_attributeerror():
     # In this we test that AttributeErrors are better handled by the safe_property.
     testobj = PropertyTest()
     # In the case of an AttributeError, we expect an uninformative error message from nn.Module.
     with pytest.raises(AttributeError) as exc_info:
         testobj.test_property_attributeerror
 
     # Check that PyTorch has indeed removed the informative message completely.
     # This is the reason we are making a safe_property, so if this test fails we may not even
     #  need the decorator we are testing here.
     assert informative_error_message not in exc_info.value.args[0]
 
     # This tests that safe_attribute raises an exception still containing the
     #  informative error message.
     with pytest.raises(BaseException, match=informative_error_message):
         testobj.test_safe_property_attributeerror
 
 if __name__ == '__main__':
     pytest.main([__file__])
 </denchmark-code>
 
 With this code, accessing test_safe_property_attributeerror will raise a RuntimeError with the message An AttributeError was encountered: module 'torch.nn' has no attribute 'does_not_exist'.
 I am not sure how the tests are structured - I am new to using pytest - but I'll make a pull request for the safe property now.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,sebftw,2019-08-19T19:05:38Z,"
 		<denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  let's do this as a simple try catch in the dataloader decorator. A principle of lightning is to give users less to remember.
 		",6.0,sebftw,2019-08-19T22:12:36Z,"
 		Good idea
 		",7.0,sebftw,2019-08-21T14:37:20Z,"
 		<denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  any updates on this?
 		",8.0,sebftw,2019-08-22T12:53:19Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I made a PR. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/161>#161</denchmark-link>
 
 		",9.0,sebftw,2019-08-23T12:22:12Z,"
 		<denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  thanks for the PR! merged
 		",10.0,sebftw,2019-08-30T09:06:28Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/sebftw>@sebftw</denchmark-link>
  Does it only fix the suppresion of  inside the dataloader and not the other user errors?
 		",11.0,sebftw,2019-08-30T09:44:01Z,"
 		<denchmark-link:https://github.com/metrofun>@metrofun</denchmark-link>
  Yes
 		",12.0,sebftw,2019-08-30T10:12:58Z,"
 		Unless I misunderstood the context, wouldn't it be more reasonable to surface any kind of errors from the user defined code, not just the AttributeError?
 		",13.0,sebftw,2019-08-31T12:51:23Z,"
 		<denchmark-link:https://github.com/metrofun>@metrofun</denchmark-link>
  Other exceptions are not supressed, so they will surface as normal. It is just  which is an odd case, so we catch it and re-raise it as a  as our fix.
 		",14.0,sebftw,2019-08-31T14:32:55Z,"
 		@sebtfw Oh, I see, thanks for the explanation
 		",,,,,,,,,data_loader,fn,2,24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"13,14,15,16,17,18,19,20,21,22","13,14,15",data_loader._data_loader,self,12,22,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1421,david-alexander-white,2020-04-09T03:35:45Z,2020-04-09T19:01:09Z,run_training_batch breaks on None batch or -1 response from on_batch_start (in new 0.7.2 release),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 run_training_batch now is supposed to return a 4-tuple in 0.7.2
 however, there are two places where it still returns a 3-tuple, which will cause the program to crash, saying ""ValueError: not enough values to unpack (expected 4, got 3)""
 (training_loop.py:533)
 <denchmark-code>if batch is None:
     return 0, grad_norm_dic, {}
 </denchmark-code>
 
 (training_loop.py:543)
 <denchmark-code>if response == -1:
     return -1, grad_norm_dic, {}
 </denchmark-code>
 
 vs. the standard return
 return 0, grad_norm_dic, all_log_metrics, batch_output
 <denchmark-h:h3>To reproduce</denchmark-h>
 
 just return -1 from on_batch_start
 	",b2707c9b2ebeac03f19a3939df9432ac8859d894,Jirka Borovec,2020-04-09 15:01:08-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,17,,1.0,david-alexander-white,2020-04-09T12:44:13Z,"
 		good catch. mind submitting a PR?
 		",2.0,david-alexander-white,2020-04-09T19:35:36Z,"
 		Nice, beat me to it
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,325,325,dump_checkpoint,self,287,331,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"535,545","535,545",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_training_batch,"self,batch,batch_idx",524,658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1422,bobofzhang,2020-04-09T09:22:50Z,2020-04-09T12:52:16Z,Not auto add DistributedSampler for DDP training,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 in 0.72, even if we don't set sampler, pytorch_lightning will not add  DistributedSampler for us.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 the reason is in pytorch, if we don't set sampler,  pytorch will add a sampler for us.
 in pytorch's dataloader.py:
 <denchmark-code>         if sampler is None:  # give default samplers
             if self._dataset_kind == _DatasetKind.Iterable:
                 # See NOTE [ Custom Samplers and IterableDataset ]
                 sampler = _InfiniteConstantSampler()
             else:  # map-style
                 if shuffle:
                     sampler = RandomSampler(dataset)
                 else:
                     sampler = SequentialSampler(dataset)
 </denchmark-code>
 
 but in pytorch_lightning we check whether sampler is None to  decide to add sampler
 in  data_loading.py funciton auto_add_sampler:
 <denchmark-code>        no_sampler_added = dataloader.sampler is None
 </denchmark-code>
 
 because pytorch have default sampler for us, which is not None, pytorch_lighting will not automatically add sampler.
 	",21a1972921809ea04ab1e4c657e326dfecd5e352,William Falcon,2020-04-09 08:52:15-04:00,MODIFY,1,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,91,"90,92",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,auto_add_sampler,"self,DataLoader,bool",83,111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1435,yukw777,2020-04-09T20:06:11Z,2020-04-10T12:43:23Z,Test metrics is not being reported to TensorBoard since 0.7.2,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 <denchmark-link:https://colab.research.google.com/drive/1fM6xL140u9pU0vcmJf6qKzHwczjcMpcF>https://colab.research.google.com/drive/1fM6xL140u9pU0vcmJf6qKzHwczjcMpcF</denchmark-link>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Please see the colab above.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The test metrics should be reported.
 <denchmark-h:h3>Environment</denchmark-h>
 
 The Colab environment:
 <denchmark-code>cuda:
 	GPU:
 	available:           False
 	version:             10.1
 packages:
 	numpy:               1.18.2
 	pyTorch_debug:       False
 	pyTorch_version:     1.4.0
 	pytorch-lightning:   0.7.2
 	tensorboard:         2.2.0
 	tqdm:                4.38.0
 system:
 	OS:                  Linux
 	architecture:
 		64bit
 		
 	processor:           x86_64
 	python:              3.6.9
 	version:             #1 SMP Wed Feb 19 05:26:34 PST 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Regression from 0.7.1
 	",1f685c2882d2bb0755a7ab0ed6819b008780948e,William Falcon,2020-04-10 08:43:22-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"18,19",,1.0,yukw777,2020-04-09T20:06:56Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,yukw777,2020-04-09T20:41:23Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  ^^ 0.7.3?
 		",3.0,yukw777,2020-04-14T00:44:08Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1441>#1441</denchmark-link>
  didn't fix the issue with Tensorboard (the issue with the console output has been fixed). I've updated the colab to test the same code with 0.7.3 (<denchmark-link:https://colab.research.google.com/drive/1fM6xL140u9pU0vcmJf6qKzHwczjcMpcF>https://colab.research.google.com/drive/1fM6xL140u9pU0vcmJf6qKzHwczjcMpcF</denchmark-link>
 ), and you'd notice that the  metrics is not rendered in Tensorboard.
 		",4.0,yukw777,2020-04-14T05:45:46Z,"
 		it seems like this issue is now being tracked here: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1447>#1447</denchmark-link>
 
 		",MODIFY,0.0,pytorch_lightning\__init__.py,pytorch_lightning\__init__.py,0.0,3,3,,,,,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"380,383","380,383",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_evaluation,"self,bool",326,416,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1442,alexeykarnachev,2020-04-10T12:52:39Z,2020-04-10T15:43:07Z,Failed to configure_optimizers from dictionary without lr_scheduler field presented,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Optimizer is failed to be configured from the dictionary without lr_sheduler field.
 Consider an example of the Module configure_optimizers method:
         def configure_optimizers(self):
             config = {
                 'optimizer': torch.optim.SGD(params=self.parameters(), lr=1e-03)
             }
             return config
 Then, we run a simple trainer:
     trainer_options = dict(default_save_path=tmpdir, max_epochs=1)
     trainer = Trainer(**trainer_options)
     _ = trainer.fit(model)
 And we fail with an error:
 <denchmark-code>UnboundLocalError: local variable 'lr_schedulers' referenced before assignment
 </denchmark-code>
 
 I believe, that the reason is that lr_schedulers local variable is not determined here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/optimizers.py
 
 
         Lines 36 to 42
       in
       8dd9b80
 
 
 
 
 
 
  # single dictionary 
 
 
 
  elif isinstance(optim_conf, dict): 
 
 
 
  optimizer = optim_conf[""optimizer""] 
 
 
 
  lr_scheduler = optim_conf.get(""lr_scheduler"", []) 
 
 
 
  if lr_scheduler: 
 
 
 
  lr_schedulers = self.configure_schedulers([lr_scheduler]) 
 
 
 
  return [optimizer], lr_schedulers, [] 
 
 
 
 
 
 I think, it could be fixed like this:
         # single dictionary
         elif isinstance(optim_conf, dict):
             optimizer = optim_conf[""optimizer""]
             lr_scheduler = optim_conf.get(""lr_scheduler"", [])
             if lr_scheduler:
                 lr_schedulers = self.configure_schedulers([lr_scheduler])
             else:
                 lr_schedulers = []
             return [optimizer], lr_schedulers, []
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Create a simple Module with configure_optimizers which looks like above.
 Run the fit Trainer method with the model.
 See error
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-link:https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc>https://gist.github.com/alexeykarnachev/c61a5b1ca3bf876e19b4547eeb9f42dc</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I suppose, that such the configuration: {""optimizer"": ...}, without ""lr_scheduler"" must be a valid one, and this error must not be occurred.
 <denchmark-h:h3>Environment</denchmark-h>
 
 OS: Linux
 architecture: 64bit
 processor: x86_64
 python: 3.7.6
 version: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/97>#97</denchmark-link>
 ~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020
 pytorch-lightning: 0.7.3rc1
 	",4c34d16a349bc96a717be5674606c2577fab8946,Alexey Karnachev,2020-04-10 11:43:06-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,15,,1.0,alexeykarnachev,2020-04-10T13:21:59Z,"
 		yeah, agreed that dict should work without the scheduler. mind submitting a PR?
 		",,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\optimizers.py,pytorch_lightning\trainer\optimizers.py,0.0,"42,43",,,,,,MODIFY,0.0,pytorch_lightning\trainer\supporters.py,pytorch_lightning\trainer\supporters.py,0.0,23,,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"557,558",557,add_argparse_args.allowed_type,x,557,558,MODIFY,2.0,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,"280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300",,test_configure_optimizer_from_dict,tmpdir,280,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"286,287,288,289,290",,test_configure_optimizer_from_dict.configure_optimizers,self,286,290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"557,558",557,add_argparse_args,"cls,ArgumentParser",536,568,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1447,WSzP,2020-04-10T16:16:14Z,2020-04-15T00:32:34Z,"Test results not logged to tensorboard, since 0.7.3, this worked in 0.7.1","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Test results are not logged to TensorBoard. With the exact same code, version 0.7.1 logged them flawlessly. Also, with the exact same code, validation and train results are logged. So I assumed the issue is with the test.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run test() step with a model that has TensorBoard logging.
 logger = TensorBoardLogger(LOG_DIR, name=NAME)
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>def validation_step(self, val_batch, batch_idx):
     [...]
     return {'val_loss': loss}
 
 def validation_epoch_end(self, outputs):
     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
     tensorboard_logs = {'val_loss': avg_loss}
     return {'avg_val_loss': avg_loss, 'log': tensorboard_logs} #this works!
 
 def test_step(self, test_batch, batch_idx):
     [...]           
     return {'test_loss': loss}
 
 def test_epoch_end(self, outputs):
     avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
     tensorboard_logs = {'MSE': avg_loss}
     print(f""Test Mean Squared Error (MSE): {avg_loss}"")  #this works!                         
     return {'avg_test_loss': avg_loss, 'log': tensorboard_logs} #the issue might be here
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The expected behavior is for tensorboard_logs to contain the MSE, but when I open them in TensorBoard they don't contain MSE, only the val_loss and train_loss. The exact same code used to work in 0.7.1. So I believe some changes in 0.7.3 produced this bug.
 The print works, so the correct value is printed, but I assume there is some issue when you return 'log': tensorboard_logs.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.4.0
 OS (e.g., Linux): Windows 10 x64
 How you installed PyTorch: pip
 Build command you used (if compiling from source): n/a
 Python version: 3.7.7
 Any other relevant information: The full code can be found here: https://github.com/WSzP/uxml-ecommerce/blob/master/train-uxml-basic-matrix-factorization.ipynb
 
 	",b3fe17ddeb00fb66db08e5fc7414591662ebd440,Jirka Borovec,2020-04-14 20:32:33-04:00,MODIFY,0,.github\workflows\ci-testing.yml,.github\workflows\ci-testing.yml,0.0,31,31,1.0,WSzP,2020-04-10T16:22:13Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,WSzP,2020-04-11T14:48:20Z,"
 		ummm. i thought we fixed this in 0.7.3. can you post a colab to reproduce?
 		",3.0,WSzP,2020-04-11T15:36:03Z,"
 		
 ummm. i thought we fixed this in 0.7.3. can you post a colab to reproduce?
 
 Thank you so much for the quick reply.
 <denchmark-link:https://colab.research.google.com/drive/1bexbN61LpWVZ106glFhAVF7Vz1jXQr1L>https://colab.research.google.com/drive/1bexbN61LpWVZ106glFhAVF7Vz1jXQr1L</denchmark-link>
 
 Hopefully, this works. (I'm using Google Colab for the first time, I'm more of a localhost  first -> deploy to AWS/Azure kind of guy.)
 		",4.0,WSzP,2020-04-11T18:58:31Z,"
 		<denchmark-link:https://github.com/WSzP>@WSzP</denchmark-link>
  we probably need also the dataset...
 <denchmark-code>FileNotFoundError                         Traceback (most recent call last)
 <ipython-input-9-0af11722af78> in <module>()
      20                      callbacks=[TestingCallbacks()]
      21                      )                
 ---> 22 trainer.fit(model)
 
 3 frames
 /usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py in load(file, mmap_mode, allow_pickle, fix_imports, encoding)
     426         own_fid = False
     427     else:
 --> 428         fid = open(os_fspath(file), ""rb"")
     429         own_fid = True
     430 
 
 FileNotFoundError: [Errno 2] No such file or directory: '/content/uxm_train.npz'
 </denchmark-code>
 
 		",MODIFY,0.0,CHANGELOG.md,CHANGELOG.md,0.0,37,37,,,,,MODIFY,6.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,242,,MODIFY,2.0,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,1.0,"39,40,41,42,43,44,45,46,47","39,40,41,42",__init__,"self,None,None,None,None,None,None,None,kwargs",39,47,MODIFY,5.0,pytorch_lightning\loggers\mlflow.py,pytorch_lightning\loggers\mlflow.py,1.0,43,"42,43",__init__,"self,str,None,str,None",42,43,5.0,WSzP,2020-04-11T19:33:20Z,"
 		you can just use fake data generators with the right dimensions. this is just about logging anyhow
 		",6.0,WSzP,2020-04-11T19:46:30Z,"
 		
 you can just use fake data generators with the right dimensions. this is just about logging anyhow
 
 Ok, I just changed the code to generate a random sparse matrix. Thanks for the idea.
 		",7.0,WSzP,2020-04-11T20:46:38Z,"
 		
 When I run it, I see the test score on the board...
 
 I only see train_loss and val_loss, but not the test score.
 <denchmark-link:https://user-images.githubusercontent.com/150962/79054564-a83bb500-7c4e-11ea-8013-29a6c41937ff.png></denchmark-link>
 
 		",8.0,WSzP,2020-04-11T21:33:18Z,"
 		I think that I see the problem, it comes with introduces agg_and_log_metrics for the logger...
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/logging.py
 
 
          Line 74
       in
       3f1e4b9
 
 
 
 
 
 
  self.logger.agg_and_log_metrics(scalar_metrics, step=step) 
 
 
 
 
 
 in this case, it is called and saved to accumulator till it receives another step or the logger is terminated which activated the flush results...
 
 
 
 pytorch-lightning/pytorch_lightning/loggers/base.py
 
 
         Lines 232 to 237
       in
       3f1e4b9
 
 
 
 
 
 
  def close(self) -> None: 
 
 
 
  """"""Do any cleanup that is necessary to close an experiment."""""" 
 
 
 
  agg_step, metrics_to_log = self._finalize_agg_metrics() 
 
 
 
  
 
 
 
  if metrics_to_log is not None: 
 
 
 
  self.log_metrics(metrics=metrics_to_log, step=agg_step) 
 
 
 
 
 
 the solution is to replicate the same action to the logger.save()
 		",9.0,WSzP,2020-04-11T22:13:33Z,"
 		<denchmark-link:https://github.com/WSzP>@WSzP</denchmark-link>
  pls try this fix
 <denchmark-code>! pip install https://github.com/PyTorchLightning/pytorch-lightning/archive/bugfix/flush-logger.zip -U
 </denchmark-code>
 
 		",10.0,WSzP,2020-04-11T22:38:59Z,"
 		It works like a charm. Thank you so much <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 . Cheers!
 		",11.0,WSzP,2020-04-12T05:53:14Z,"
 		Let's keep it open till the fix is merged to master...
 		",,,,,,,,,,,,,,,,,close,self,240,242,,,,,1.0,113,,finalize,"self,str",112,116,1.0,,"105,106",save,self,105,106,MODIFY,3.0,pytorch_lightning\loggers\neptune.py,pytorch_lightning\loggers\neptune.py,1.0,"32,33,34,35,36,38,39,40,41,42","32,33,35,36,38",__init__,"self,None,None,True,bool,None,None,str,None,str,None,None,kwargs",32,42,1.0,251,,finalize,"self,str",250,253,MODIFY,4.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,1.0,206,,name,self,205,206,1.0,"43,44,45","43,44,45",__init__,"self,str,int,None,kwargs",43,45,1.0,130,,save,self,129,150,MODIFY,5.0,pytorch_lightning\loggers\test_tube.py,pytorch_lightning\loggers\test_tube.py,1.0,"21,22,23,24,25,26,27","21,22,23,24",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"108,109","108,109",_reduce_agg_metrics,self,108,117,1.0,238,"234,235,236,237",finalize,"self,str",232,238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"32,33,34,35,36","32,33,35,36",__init__,"self,None,None,True,bool,None,None,str,None,str,None,None,kwargs",32,36,__init__,"self,str,str,None,bool,None,bool",21,27,1.0,"21,22,23","21,22,23",__init__,"self,str,str,None,bool,None,bool",21,23,MODIFY,2.0,pytorch_lightning\loggers\trains.py,pytorch_lightning\loggers\trains.py,1.0,300,"299,300",finalize,"self,str",299,305,1.0,,"298,299",save,self,298,299,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,"49,50,51,52,53,54,55,56,57,58,59,60","49,50,51,52,53",__init__,"self,None,None,bool,None,bool,None,None,None,bool,experiment,entity",49,60,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"373,379","373,374,380",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,296,"296,297",,,,,,,,,,,,MODIFY,2.0,tests\base\mixins.py,tests\base\mixins.py,1.0,"358,359","358,359",test_epoch_end,"self,outputs",329,360,1.0,"92,93","92,93",validation_epoch_end,"self,outputs",62,94,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tests\loggers\test_all.py,,,,MODIFY,5.0,tests\loggers\test_base.py,tests\loggers\test_base.py,1.0,"172,173,174,175",,test_with_accumulate_grad_batches.log_metrics,"self,metrics,step",172,175,1.0,"167,168,169",,test_with_accumulate_grad_batches.__init__,self,167,169,,,,,MODIFY,2.0,tests\loggers\test_comet.py,tests\loggers\test_comet.py,MODIFY,3.0,tests\loggers\test_mlflow.py,tests\loggers\test_mlflow.py,MODIFY,4.0,tests\loggers\test_neptune.py,tests\loggers\test_neptune.py,1.0,85,"83,84,85,86",1.0,230,230,save,self,228,230,1.0,,295,rank,"self,int",294,297,1.0,"119,120,121,122,123,124,125",,_finalize_agg_metrics,self,119,125,1.0,"45,46,47,48,49",,__init__,"self,str,None,str,None,None",45,49,1.0,60,62,experiment,self,60,70,1.0,95,95,experiment,self,81,96,1.0,"42,43,44,45,46","43,44,45,46",__init__,"self,str,int,None,kwargs",42,46,1.0,118,,finalize,"self,str",117,122,1.0,111,,save,self,110,114,1.0,126,,close,self,125,131,run_evaluation,"self,bool",326,415,1.0,,"68,69,70,71",log_metrics,"self,metrics,step",68,71,1.0,,"63,64,65",__init__,self,63,65,1.0,"165,166,167,168,169,170,171,172,173,174,175,176",,test_with_accumulate_grad_batches,,163,185,1.0,,"125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156",test_comet_pickle,"tmpdir,monkeypatch",125,156,1.0,,"15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48",test_comet_logger,"tmpdir,monkeypatch",15,48,1.0,,"39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54",test_mlflow_pickle,tmpdir,39,54,1.0,,"10,12,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36",test_mlflow_logger,tmpdir,10,36,1.0,"4,6,8,9","4,5,7",test_mlflow_logger_exists,tmpdir,4,9,test_neptune_leave_open_experiment_after_fit,tmpdir,62,86,1.0,13,"12,13,14,15,16,17",test_neptune_online,neptune,12,17,1.0,85,"83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98",test_neptune_pickle,tmpdir,83,98,1.0,13,"12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29",test_neptune_logger,tmpdir,12,29,MODIFY,4.0,tests\loggers\test_tensorboard.py,tests\loggers\test_tensorboard.py,1.0,"49,51",,test_tensorboard_no_name,"tmpdir,name",49,52,1.0,,"82,84,85,86,87,88",test_tensorboard_no_name,tmpdir,82,89,1.0,,"13,14,15,16,17,18,19,20,21,22,23,24,25,26,27",test_tensorboard_logger,tmpdir,13,27,1.0,,"30,31,32,33,34,35,36,37,38,39,40",test_tensorboard_pickle,tmpdir,30,40,DELETE,0.0,tests\loggers\test_test_tube.py,None,MODIFY,1.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,"38,39,40",40,test_wandb_pickle,wandb,36,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1468,siddk,2020-04-12T22:32:36Z,2020-04-19T11:03:41Z,Mixing hparams and arguments in LightningModule.__init__() crashes load_from_checkpoint(),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Right now, if you initialize a Lightning Module with a mixture of a Namespace (hparams) as well as additional arguments (say to a Dataset), load_from_checkpoint can't recover.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Create a LightningModule as follows:
 class Model(pl.LightningModule):
       def __init__(self, hparams, train_dataset, val_dataset):
               self.hparams = hparams
               self.train_dset, self.val_dset = train_dataset, val_dataset
               ...
 Run training, then try to restore from checkpoint, via:
 nn = Model.restore_from_checkpoint(<PATH>, train_dataset=None, val_dataset=None)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Ideally, you'd just be able to pass in the additional arguments (as above) and everything would work.
 	",3c6f856f232ccd124ca90621cdda8094bae6e332,Hengjian (Henry) Jia,2020-04-19 07:03:40-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"42,43",,1.0,siddk,2020-04-12T22:33:18Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,siddk,2020-04-13T09:08:22Z,"
 		Very interesting issue. I think the whole restore_from_checkpoint could use a bit better documentation. I think you are trying to use checkpoints for something that was not the intention.
 I think what you want is run the model without training (this is why the train and val datasets are empty). In this case I would try falling back to plain torch solution, such as saving and reusing the state_dict:
 <denchmark-code>torch.save(nn.state_dict(), SAVE_PATH)
 loaded_model = nn(hparams)
 loaded_model.load_state_dict(torch.load(SAVE_PATH))
 loaded_model.eval()
 print(""Model's state_dict:"")
 for param_tensor in loaded_model.state_dict():
   print(param_tensor, ""\t"", loaded_model.state_dict()[param_tensor].size())
 </denchmark-code>
 
 		",3.0,siddk,2020-04-13T15:59:15Z,"
 		That makes sense for this usecase --> but say I want to pause/resume training later on (the intended usecase of restore from checkpoint). Here, I really want to be able to leverage the fact that Lightning remembers my hyperparameters, and I want to be able to just pass in the additional arguments (like the datasets I've constructed).
 		",4.0,siddk,2020-04-13T16:04:49Z,"
 		This is the behavior in 0.7.3
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class>https://pytorch-lightning.readthedocs.io/en/0.7.3/lightning-module.html#lightningmodule-class</denchmark-link>
 
 # or load passing whatever args the model takes to load
 MyLightningModule.load_from_checkpoint(
     'path/to/checkpoint.ckpt',
     learning_rate=0.1,
     layers=2,
     pretrained_model=some_model
 )
 		",MODIFY,1.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,1548,"1547,1548,1549,1550",_load_model_state,"cls,str,args,kwargs",1522,1554,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,siddk,2020-04-13T16:32:15Z,"
 		This doesn't seem to be true if I explicitly pass a hparams argument... only if I break out each of the arguments and pass them to the init() method.
 		",6.0,siddk,2020-04-13T16:35:00Z,"
 		share a colab? this should be true on 0.7.3.
 For now you can do Trainer(PATH, **hparams)
 		",7.0,siddk,2020-04-13T20:19:19Z,"
 		<denchmark-link:https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST>https://colab.research.google.com/drive/1WmuZfOQxyi4nF_YvjFcJrcpVhNvuCXST</denchmark-link>
 
 		",8.0,siddk,2020-04-16T04:46:35Z,"
 		I can reproduce this with that script locally as well on <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/e3001a092913514d65547d4e912382525cfedad2>e3001a0</denchmark-link>
 
 Gives me the following traceback
 <denchmark-code>Traceback (most recent call last):
   File ""pl_loadfromcheckpoint.py"", line 40, in <module>
     new_model = Model.load_from_checkpoint('save.ckpt', c='this is', d='a test')
   File ""/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py"", line 1509, in load_from_checkpoint
     model = cls._load_model_state(checkpoint, *args, **kwargs)
   File ""/home/henry/Coding/pytorch-lightning/pytorch_lightning/core/lightning.py"", line 1541, in _load_model_state
     model = cls(*model_args)
 TypeError: __init__() missing 2 required positional arguments: 'c' and 'd'
 </denchmark-code>
 
 It seems to be due to this if statement here which stops it taking into account anymore arguments if there's a hparam in the checkpoint
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/lightning.py#L1540</denchmark-link>
 
 EDIT: PRed <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1505>#1505</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1476,rmrao,2020-04-13T17:57:32Z,2020-04-20T12:03:53Z,Learning rate scheduler should step after each optimizer step,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I'm not sure that this is a bug or if it is a deliberate design decision, but right now the learning rate schedule gets updated at every ""step"" which actually corresponds to every forward pass. I think a more standard implementation would have the learning rate scheduler ""step"" interval correspond to being updated every backwards pass. This has caused me a lot of problems with instability as I did not realize that using standard learning rate warmups of say 16000 steps would actually only warm up for 1000 steps if I set accumulate_grad_batches=16.
 	",0203938af8f69a19b7e0264f18e03d543d86e0e9,Roshan Rao,2020-04-20 08:03:52-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"27,28,34,39,41","29,33,38,40,55",1.0,rmrao,2020-04-13T18:00:27Z,"
 		good point. it should be every backward pass as you mention.
 		",2.0,rmrao,2020-04-13T18:00:37Z,"
 		mind submitting  PR?
 		",3.0,rmrao,2020-04-13T18:06:12Z,"
 		Sure, will do.
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"457,458,459,460,461","457,458",run_training_epoch,self,405,530,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1485,olineumann,2020-04-14T09:02:22Z,2020-05-02T12:50:48Z,wandb logger 'global_step' affects other logger,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The wandb logger adds a 'global_step' to the metric dict which appears in all other loggers (e.g. Tensorboard). Only the wandb logger is adding 'global_step' to metric and I think it is not necessary. Another side effect of that is, that 'global_step' is also added to empty dicts which then are logged and resulting to strange graphs like this:
 <denchmark-link:https://user-images.githubusercontent.com/12863612/79206766-1dbaa780-7e40-11ea-9c1c-c05e92c94641.png></denchmark-link>
 
 or this
 <denchmark-link:https://user-images.githubusercontent.com/12863612/79206838-34f99500-7e40-11ea-80bd-87462acf81fc.png></denchmark-link>
 
 I also wrote a simple logger class to print out metrics. I got this output:
 <denchmark-code>Step  0
 {'global_step': 0}
 Step  10
 {'global_step': 10}
 [...]
 Step  190
 {'global_step': 190}
 Step  200
 {'global_step': 200}
 Step  0
 {'val/mse': 0.01713273860514164, 'train/mse': 0.04259789362549782, 'global_step': 0}
 Step  207
 {'global_step': 207}
 Step  217
 {'global_step': 217}
 [...]
 Step  397
 {'global_step': 397}
 Step  407
 {'global_step': 407}
 Step  1
 {'val/mse': 0.013123581185936928, 'train/mse': 0.01449404377490282, 'global_step': 1}
 Step  414
 {'global_step': 414}
 Step  424
 {'global_step': 424}
 ...
 Step  604
 {'global_step': 604}
 Step  614
 {'global_step': 614}
 Step  2
 {'val/mse': 0.012394818477332592, 'train/mse': 0.012575697153806686, 'global_step': 2}
 
 [...]
 
 Step  5
 {'val/mse': 0.012411396019160748, 'train/mse': 0.011899641714990139, 'global_step': 5}
 Step  1242
 {'global_step': 1242}
 Step  1252
 {'global_step': 1252}
 [...]
 Step  1432
 {'global_step': 1432}
 Step  1442
 {'global_step': 1442}
 Step  6
 {'val/mse': 0.01244258601218462, 'train/mse': 0.011944737285375595, 'global_step': 6}
 Step  1449
 {'global_step': 1449}
 Step  1459
 {'global_step': 1459}
 [...]
 Step  1639
 {'global_step': 1639}
 Step  1649
 {'global_step': 1649}
 Step  7
 {'val/mse': 0.01261985208839178, 'train/mse': 0.011924241669476032, 'global_step': 7}
 Step  1656
 {'global_step': 1656}
 Step  1666
 {'global_step': 1666}
 [...]
 Step  1846
 {'global_step': 1846}
 Step  1856
 {'global_step': 1856}
 Step  8
 {'val/mse': 0.012863481417298317, 'train/mse': 0.011850016191601753, 'global_step': 8}
 Step  1863
 {'global_step': 1863}
 Step  1873
 [...]
 Step  2053
 {'global_step': 2053}
 Step  2063
 {'global_step': 2063}
 </denchmark-code>
 
 Also notice: I set max_epochs to 10 so expected to be 10 measurements. The last one is missing. But this could be handled in an other issue.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Use training_epoch_end and validation_epoch_end to log metric like {'log': {'loss': loss}} (see code bellow)
 Run training with wandb logger and one more logger of your choice.
 See global_step graphs.
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Important LightningModule Methods:
 def training_step(self, batch, batch_idx):
         # calculate actual model prediction given batch
         # and calculate loss
         x, y = batch
         y_hat = self(x)
         
         # print out current loss on training every n-th iteration
         loss = F.mse_loss(y_hat, y)
         return {
             ""loss"": loss
         }
     
 def training_epoch_end(self, outputs):
     loss_mean = torch.stack([x[""loss""] for x in outputs]).mean().item()
     return {
         ""log"": {
             ""train/mse"": loss_mean,
             ""step"": self.current_epoch
         }
     }
 
 def validation_step(self, batch, batch_idx):
     x, y = batch
     y_hat = self(x)        
     return {'val_loss': F.mse_loss(y_hat, y)}
 
 def validation_epoch_end(self, outputs):
     val_loss_mean = torch.stack([x[""val_loss""] for x in outputs]).mean().item()
     return {
         ""val_loss"": val_loss_mean,
         ""log"": {
             ""val/mse"": val_loss_mean,
             ""step"": self.current_epoch
         }
     }
 Training:
 clbk_terminal = TerminalCallback()
 checkpoint = ModelCheckpoint(filepath=""ckpts/"" + name + ""{_val_loss:.5f}_{epoch:03d}"", prefix=""BasicNN_"", monitor=""val_loss"", verbose=False, save_top_k=3, save_weights_only=True)
 earlystopping = EarlyStopping(monitor=""val_loss"", patience=25, verbose=True)
 loggers = [
     WandbLogger(project=""nwp-energy-load"", name=name, log_model=True),
     TensorBoardLogger(save_dir=""tb_logs"", name=name, version=0),
     MyLogger() # only prints metric; can also be ignored
 ]
 
 trainer = Trainer(gpus=-1, max_epochs=10, progress_bar_refresh_rate=0, logger=loggers, log_save_interval=1, row_log_interval=10,
                   callbacks=[], early_stop_callback=earlystopping, checkpoint_callback=checkpoint)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Is 'global_step' needed in wandb logger? If so, it should not affect other loggers. Also if there is nothing to log (e.g. in training_step) the logger should log nothing.
 <denchmark-h:h3>Environment</denchmark-h>
 
 Linux Arch
 Python 3.8.2
 Pytorch 1.4.0
 Pytorch_Lightning 0.7.3
 	",152a2eb30ce82deefdb738b81fda66a9c218ed76,Oliver Neumann,2020-05-02 08:50:47-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,27,,1.0,olineumann,2020-04-14T09:03:13Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,olineumann,2020-04-14T12:28:11Z,"
 		I looked over the code and found the issue. Maybe the wandb python API didn't accepted step as parameter in the log function in a version before. So the step was added to the metric dict (and not copied so affected other loggers).
 Also I think that empty metric dicts could be skipped in the base logger. You can see my fix in the following commit:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/compare/master...olineumann:issue/wandb_global_step>master...olineumann:issue/wandb_global_step</denchmark-link>
 
 		",3.0,olineumann,2020-04-14T17:25:18Z,"
 		<denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>
  nice, mind send a PR, seems that is could be one-click only :]
 		",4.0,olineumann,2020-05-27T21:43:27Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  Now  is giving warnings  and not logging when I try to use the WandbLogger with k-fold cross-validation because there I am using the same instance of wandb_logger but using  multiple times for different train_dl and valid_dl. Since the step gets repeated in each case, it's not logging anything after the 1st fold is complete even though the log keys are completely different. For now, I have to create a different logger separately for each fold, but is there any other way around to make it work with the single instance.
 		",MODIFY,1.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,128,128,agg_and_log_metrics,"self,str,None",116,129,MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,122,122,MODIFY,1.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,"17,21","17,21",test_wandb_logger,wandb,11,30,,,,,,,,,,,,5.0,olineumann,2020-05-27T22:54:29Z,"
 		I also noticed that 'epoch' appearing now after upgrading to the current version in the metric dict without me logging any 'epoch'. It comes from trainer.logging:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/logging.py#L69>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/logging.py#L69</denchmark-link>
 
 Because of this the step=global_step and when I log something in my pytorch_lightning module with step=epoch it will crash because of the same reason. The only way to solve it is to pass a log dict in train_step and validation_step only containing 'step': epoch.
 What would be the best solution? I think:
 
 First case: User passes no step in log dict, so nothing is added to the dict and wandb.log(..., step=None).
 Second case: If the user passes step, 'step' should be added to the dict (without affecting other loggers!) and also, in this case, wandb.log(..., step=None).
 
 Sounds that useful/logical? I think this should work but I'm also tired. I can create a PR if wanted in the next days (Fr/Sa).
 		",6.0,olineumann,2020-05-28T09:17:23Z,"
 		<denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>
  I would prefer the second as we do not want to affect the other loggers
 		",7.0,olineumann,2020-06-05T20:37:54Z,"
 		I just wanted to fix it and pulled the current master from GitHub. It seems to be fixed already.
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/wandb.py#L131>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/wandb.py#L131</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log_metrics,"self,str,None",121,122,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1503,rmrao,2020-04-15T17:12:50Z,2020-05-12T04:14:36Z,Do not configure python logging,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 pytorch-lightning right now configures the python logging module (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8322f1b039c890b8ccdbfe29bf42056e5273d74f/pytorch_lightning/__init__.py#L16>here</denchmark-link>
 ). This is generally not recommended when writing a library as it makes it difficult for users to modify logging format (see <denchmark-link:https://docs.python.org/3/howto/logging.html#configuring-logging-for-a-library>python docs</denchmark-link>
 , Stack Overflow <denchmark-link:https://stackoverflow.com/questions/27016870/how-should-logging-be-used-in-a-python-package>post</denchmark-link>
 ). I would suggest deleting the configuration line.
 	",1df0d2dc97e20b9646cbe0f42060a57f99f397fc,Jeremy Jordan,2020-05-12 00:14:35-04:00,MODIFY,0,pytorch_lightning\__init__.py,pytorch_lightning\__init__.py,0.0,"37,38",37,1.0,rmrao,2020-04-15T18:05:05Z,"
 		I agree. Logging configuration should be disabled by default. We could still set up the logging configuration manually just after importing PL.
 		",2.0,rmrao,2020-04-15T21:21:46Z,"
 		we had the discussion here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1267#discussion_r399640416>#1267 (comment)</denchmark-link>
  <denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  ^^
 		",3.0,rmrao,2020-04-15T21:29:41Z,"
 		Hmm... ok. In that case, could I potentially suggest that the call to basicConfig be placed in Trainer's __init__? Otherwise you require the user to configure logging before importing pytorch lightning. There are a couple problems with this:
 
 imports should generally be placed at the top of a file. Configuring logging before importing pytorch-lightning means users have to violate PEP8 at some point.
 It's even more difficult when using DDP, since it's not possible to configure the logging for a DDP process before importing pytorch lightning.
 
 		",4.0,rmrao,2020-04-15T23:56:00Z,"
 		
 We could still set up the logging configuration manually just after importing PL.
 
 <denchmark-link:https://github.com/hadim>@hadim</denchmark-link>
  isn't that what we do now? we configure logging when pytorch-lightning is imported
 
 could I potentially suggest that the call to basicConfig be placed in Trainer's init?
 
 <denchmark-link:https://github.com/rmrao>@rmrao</denchmark-link>
  i'm not sure this would help you since we import  in the project's root 
 however, if you configure logging as you wish in your project's root __init__.py, by the time lightning is imported the root handler will already be configured and basicConfig will not run.
 tagging <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  since we had this discussion <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1236#issuecomment-604943375>here</denchmark-link>
  already. would prefer not to keep going back and forth on how we configure logging for the project.
 		",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,rmrao,2020-04-16T00:05:57Z,"
 		
 
 We could still set up the logging configuration manually just after importing PL.
 
 @hadim isn't that what we do now? we configure logging when pytorch-lightning is imported
 
 now it sets the level for all logging not just for this lightning logger, that is what I was talking before...
 		",6.0,rmrao,2020-04-16T00:19:31Z,"
 		just doing something like
 <denchmark-code>logger = logging.getLogger(__name__)
 logger.setLevel(logging.INFO)
 </denchmark-code>
 
 doesn't actually set up a handler to send the log messages.
 however, we could just simply add a line to configure a stream handler
 <denchmark-code>logger = logging.getLogger(__name__)
 logger.addHandler(logging.StreamHandler())
 logger.setLevel(logging.INFO)
 </denchmark-code>
 
 		",7.0,rmrao,2020-04-28T15:39:19Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  my suggestion is actually to put  inside of the Trainer's  function. This would only call basicConfig when an instance of  is created (not when it's imported). You could even include the log level as an argument to .
 		",8.0,rmrao,2020-04-30T00:26:26Z,"
 		<denchmark-link:https://github.com/rmrao>@rmrao</denchmark-link>
  ah i see, i misunderstood your suggestion. i think that's a fine solution
 		",9.0,rmrao,2020-05-01T14:34:26Z,"
 		I was having problems because of this. It would duplicate my log messages.
 I understand this will be solved soon, but in case anyone has the same problem, this is how I circumvented it. When first defining the root logger, I do:
 <denchmark-code>    logger = logging.getLogger()
     # if there is already a handler, remove it.
     if logger.handlers:
         logger.handlers.pop()
 </denchmark-code>
 
 (logging is the python logging package, not pl's)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1506,s-rog,2020-04-16T05:13:36Z,2020-04-19T20:58:59Z,0.7.3 breaks reusable dataloaders in DDP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 0.7.3 breaks reusable dataloaders in DDP
 <denchmark-code>Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
     fn(i, *args)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 345, in ddp_train
     self.run_pretrain_routine(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 864, in run_pretrain_routine
     self.train()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 296, in train
     self.reset_train_dataloader(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py"", line 128, in reset_train_dataloader
     self.train_dataloader = self.auto_add_sampler(self.train_dataloader, train=True)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/data_loading.py"", line 112, in auto_add_sampler
     dataloader = type(dataloader)(**dl_args)
   File ""../main/dataset.py"", line 15, in __init__
     super().__init__(*args, **kwargs)
 TypeError: __init__() got an unexpected keyword argument 'iterator'
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>class _RepeatSampler(object):
     def __init__(self, sampler):
         self.sampler = sampler
 
     def __iter__(self):
         while True:
             yield from iter(self.sampler)
 
 class FastDataLoader(torch.utils.data.dataloader.DataLoader):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))
         self.iterator = super().__iter__()
 
     def __len__(self):
         return len(self.batch_sampler.sampler)
 
     def __iter__(self):
         for i in range(len(self)):
             yield next(self.iterator)
 </denchmark-code>
 
 replace Dataloader with FastDataLoader in lightning
 (this snippet is from <denchmark-link:https://github.com/pytorch/pytorch/issues/15849>pytorch/pytorch#15849</denchmark-link>
 )
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Dataloaders initialize correctly and are reused between train/val/epochs (works as expected in 0.7.1)
 <denchmark-h:h3>Probable Cause</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425>#1425</denchmark-link>
 
 	",c71bd73acb5a89bb2a8ff44beab37fd2ceba352b,Justus Schock,2020-04-19 16:58:57-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,11,,1.0,s-rog,2020-04-16T12:22:08Z,"
 		ummm yeah. we should change the dataloader swap with swapping a dataloader init from the class or not swipe the dataloder at all but set the correct sampler.
 <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  any ideas?
 		",2.0,s-rog,2020-04-17T07:43:44Z,"
 		This is a mixture of  <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1425>#1425</denchmark-link>
  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1346>#1346</denchmark-link>
 
 And I don't think we can prevent this when we want to set correct samplers also in subclasses of DataLoader. We use all public attributes for reinitialization.
 The probably easiest fix for you, would be to change self.iterator to self._iterator to avoid passing this argument in reinit.
 If we just change the sampler, this might yield unexpected behaviour.
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"92,93","91,92,93,94",auto_add_sampler,"self,DataLoader,bool",87,113,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,str,int,None,None,None,bool,bool,bool,False,bool,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,bool,kwargs",85,140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1507,fellnerse,2020-04-16T08:01:01Z,2020-04-23T21:32:37Z,After update from 0.5.x to 0.7.3 merge_dicts #1278 sometimes breaks training,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 After I updated from a quite old lightning version to the newest one, I sometimes get a TypeError from merge_dicts. I guess it's related to this MR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1278>#1278</denchmark-link>
  . This Type error is deterministic, meaning it always occurs at the same global step during training. It somehow seems to be related to val_check_interval as well. For some data changing this value leads to no Error. But for other datasets this does not work. Also this only happens during training step, I suspect the training step after validating.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 I have no Idea.
 <denchmark-code>File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 363, in train
     self.run_training_epoch()
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 470, in run_training_epoch
     self.log_metrics(batch_step_metrics, grad_norm_dic)
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py"", line 74, in log_metrics
     self.logger.agg_and_log_metrics(scalar_metrics, step=step)
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 128, in agg_and_log_metrics
     agg_step, metrics_to_log = self._aggregate_metrics(metrics=metrics, step=step)
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 101, in _aggregate_metrics
     agg_step, agg_mets = self._finalize_agg_metrics()
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 116, in _finalize_agg_metrics
     agg_mets = merge_dicts(self._metrics_to_agg, self._agg_key_funcs, self._agg_default_func)
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 347, in merge_dicts
     agg_val = fn([v for v in [d_in.get(k) for d_in in dicts] if v is not None])
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/numpy/core/fromnumeric.py"", line 3118, in mean
     out=out, **kwargs)
   File ""/home/sebastian/.cache/pypoetry/virtualenvs/forgerydetection-iC5ox0X1-py3.7/lib/python3.7/site-packages/numpy/core/_methods.py"", line 75, in _mean
     ret = umr_sum(arr, axis, dtype, out, keepdims)
 TypeError: unsupported operand type(s) for +: 'dict' and 'dict'
 </denchmark-code>
 
 Sometimes its also 'dict' and 'int'
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 At least should not break training, but maybe a more verbose message what is wrong. Its quite hard for me to debug, as the structure of the logs I'm returning to lightning does not change.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
         GPU:
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
                 GeForce RTX 2080 Ti
         available:           True
         version:             10.1.243
 packages:
         numpy:               1.16.4
         pyTorch_debug:       False
         pyTorch_version:     1.3.0
         pytorch-lightning:   0.7.3
         tensorboard:         2.2.0
         tqdm:                4.45.0
 system:
         OS:                  Linux
         architecture:
                 64bit
                 ELF
         processor:           x86_64
         python:              3.7.7
         version:             #97~16.04.1-Ubuntu SMP Wed Apr 1 03:03:31 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Also for some reason some runs have an issue with multiprocessing, but it does not break the training:
 <denchmark-code>Traceback (most recent call last):████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00,  8.76it/s]
   File ""/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py"", line 277, in _run_finalizers
     finalizer()
   File ""/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py"", line 201, in __call__
     res = self._callback(*self._args, **self._kwargs)
   File ""/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/multiprocessing/util.py"", line 110, in _remove_temp_dir
     rmtree(tempdir)
   File ""/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/shutil.py"", line 498, in rmtree
     onerror(os.rmdir, path, sys.exc_info())
   File ""/home/sebastian/.pyenv/versions/3.7.7/lib/python3.7/shutil.py"", line 496, in rmtree
     os.rmdir(path)
 OSError: [Errno 39] Directory not empty: '/tmp/pymp-jcqai2xr'
 </denchmark-code>
 
 	",edb8d7a23cac91d607ab97c0adcbb815780936ac,Alexey Karnachev,2020-04-23 17:32:36-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"53,54",,1.0,fellnerse,2020-04-16T15:45:31Z,"
 		Did you passed any 'agg_key_funcs' to the logger class? If I understand the code correctly, by default np.mean is used to aggregate the dict values returned during training. Maybe numpy tries in the mean function to add (+ func) values which can't be summed up?
 Can you maybe post the code snippets where you return the metrics to log in the lightning module and the initialization of the logger if you use one? If you don't use a logger, you can disable it by passing logger=False to the trainer (don't know if your previous version had logger on by default).
 Hope I can help :)
 		",2.0,fellnerse,2020-04-16T16:43:42Z,"
 		Thanks for the quick reply!
 No I'm not using any 'agg_key_funcs' that I know of.
 
 If I understand the code correctly, by default np.mean is used to aggregate the dict values returned during training.
 
 This only happens when there is a step in time where two times stuff is logged, right? So my guess is that at some point that is the case that two logs have to be ""unified""  but this fails, because I'm using ""dict in dicts"". I need this tho, because I want to have i.e. loss train and val in the same graph.
 I'm using the TestTubeLogger:
    logger = TestTubeLogger(save_dir=log_dir, name=name, description=description)
 and just pass this to the Trainer.
 The metric logging to lightning is a bit scattered:
 
 train_step in model:
 
 <denchmark-code>       x, target = batch
        pred = self.forward(x)
        loss = self.loss(pred, target)
        lightning_log = {""loss"": loss}
 
        with torch.no_grad():
            train_acc = self.calculate_accuracy(pred, target)
            tensorboard_log = {""loss"": loss, ""acc"": train_acc}
 
        return tensorboard_log, lightning_log
 </denchmark-code>
 
 
 this is passed to a function that lets me add train and val to same graph:
 
 <denchmark-code>    def _construct_lightning_log(
         self,
         tensorboard_log: dict,
         lightning_log: dict = None,
         suffix: str = ""train"",
         prefix: str = ""metrics"",
     ):
         lightning_log = lightning_log or {}
         fixed_log = {}
 
         for metric, value in tensorboard_log.items():
             if isinstance(value, dict):
                 fixed_log[f""{prefix}/{metric}""] = value
             else:
                 fixed_log[f""{prefix}/{metric}""] = {suffix: value}
         return {""log"": fixed_log, **lightning_log}
 </denchmark-code>
 
 		",3.0,fellnerse,2020-04-16T20:39:29Z,"
 		Do you pass it after training_step or training_epoch_end? I think lightning collects your logs and tries to aggregate it to one value. I can't test it now. Maybe tomorrow.
 But when I quickly type this into python interpreter:
 <denchmark-code>>>> d={}
 >>> np.mean([d,d])
 Traceback (most recent call last):
   File ""<stdin>"", line 1, in <module>
   File ""<__array_function__ internals>"", line 5, in mean
   File ""/usr/lib/python3.8/site-packages/numpy/core/fromnumeric.py"", line 3334, in mean
     return _methods._mean(a, axis=axis, dtype=dtype,
   File ""/usr/lib/python3.8/site-packages/numpy/core/_methods.py"", line 151, in _mean
     ret = umr_sum(arr, axis, dtype, out, keepdims)
 TypeError: unsupported operand type(s) for +: 'dict' and 'dict'
 </denchmark-code>
 
 Seems like getting your error.
 Maybe print what you exactly return and when it crashes. When I have time tomorrow, I will also make some tests.
 		",4.0,fellnerse,2020-04-17T11:01:44Z,"
 		After training_step. I not have a training_epoch_end or training_end method defined.
 
 I think lightning collects your logs and tries to aggregate it to one value.
 
 Yes I think so as well.
 Ok I return something like this:
 {'metrics/aud_std': {'test': tensor(1.6337, device='cuda:0')}, 'metrics/class_loss_diff': {'test': tensor(nan)}, 'metrics/class_loss_val': {'0': tensor(nan), '1': tensor(91.5485)}, 'metrics/loss': {'test': tensor(45.7742, device='cuda:0')}, 'metrics/vid_std': {'test': tensor(1.6506, device='cuda:0')}}
 What do you mean by when it crashes exactly? I think when it crashes it's always the train step after an validation step (keep in mind I'm validation several times during one epoch). If I change the val_check_interval the error either disappears or happens at a different batch number.
 		",MODIFY,0.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,0.0,"283,351,352,353,355,357,358,359,360,361,363,367,368,369,370,371,372,373","350,351,352,354,356,358,362,363,364",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,fellnerse,2020-04-17T19:48:25Z,"
 		Hello.
 I think the problem is in your metrics type. Metrics must have the Dict[str, float] type. But in your case, the metrics is a nested dict. So, that's why values are failed to be aggregated.
 Is it possible for you to flatten the dictionary?
 		",6.0,fellnerse,2020-04-20T06:56:20Z,"
 		<denchmark-link:https://github.com/alexeykarnachev>@alexeykarnachev</denchmark-link>
  Hey! Ah yes that's what I thought. Do you know why the metrics dict is enforced to be of this type? In 0.5.x this was not an issue as far as I know.
 I mean, yes I can flatten it but I want to have i.e. val/loss and train/loss in the same graph. It's basically this: <denchmark-link:https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars>https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_scalars</denchmark-link>
 
 I know that here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1144#issuecomment-599089378>#1144 (comment)</denchmark-link>
  It was said that this should not be done, but for me this is essential.
 Is there a way that I can overwrite the merge_dicts function? If so how would I do that?
 		",7.0,fellnerse,2020-04-20T12:12:03Z,"
 		<denchmark-link:https://github.com/fellnerse>@fellnerse</denchmark-link>
  Okay, I got your point, let's ask Borda's advice)
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , what do you think? Is it possible to combine nested metrics dictionaries with metrics aggregation logic? At first sight, it doesn't look like a big problem. Maybe you can see any side effects of tracking aggregated metrics with nested dictionaries? If no, I can try to fix this issue
 		",8.0,fellnerse,2020-04-20T14:37:49Z,"
 		I ques it can be used, just need to care about the depth and the aggregation will be a bit complicated...
 		",9.0,fellnerse,2020-04-24T15:26:45Z,"
 		Cool, thanks for implementing this so fast!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1510,alexeykarnachev,2020-04-16T16:25:24Z,2020-04-19T20:41:55Z,Memory (CPU and GPU) leaks during the 1st epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Hello.
 This memory leak occurs during the first epoch. If one has a large epoch time (I had > 10 days), the OOM error will come. It's interesting, that in precision=16 mode, it leaks out on the GPU and the CPU both. If we switch amp optimization off (precision=32), the leak goes only on the CPU.
 Also, I checked the number of tensors, which are tracked by the garbage collector. And it appeared to be linearly increasing during the first epoch, and then (on the 2nd epoch starts), it falls to the initial value and begins increasing again.
 Let me provide the plots:
 <denchmark-h:hr></denchmark-h>
 
 Experiment 1: amp_level='O2', precision=16
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478408-0cc28f80-8014-11ea-861b-1b9443de5351.png></denchmark-link>
 
 
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478523-324f9900-8014-11ea-8e05-c9ef20b1c4d8.png></denchmark-link>
 
 
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478667-5ad79300-8014-11ea-8727-545230c81649.png></denchmark-link>
 
 
 <denchmark-h:hr></denchmark-h>
 
 Experiment 2: amp_level=None, precision=None
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478906-a4c07900-8014-11ea-80f6-9284bf86eafe.png></denchmark-link>
 
 
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478952-b4d85880-8014-11ea-849c-f38c3e1a88ce.png></denchmark-link>
 
 
 <denchmark-link:https://user-images.githubusercontent.com/7495098/79478999-c588ce80-8014-11ea-8755-4d2a2d7451d5.png></denchmark-link>
 
 
 <denchmark-h:hr></denchmark-h>
 
 As you can see, both cases have a CPU leak. The ""amp""-case also has a GPU leak.
 Also, it's clear, that such leaky behavior stops when the 2nd epoch starts.
 On these plots, the 2nd epoch starts on the 2nd ""saw claw"" of the ""Num-of-tensors"" plot.
 Also, there is another observation: the speed of tensors number increasing is 1001. And this is my forward pass method:
     def training_step(self, batch, batch_idx):
         losses = self.forward(batch)
         num_of_tensors = get_num_of_tensors()
         log = {'Num-of-tensors': num_of_tensors, 'Cpu-mem-usg': get_cpu_mem()}
 
         for i, loss in enumerate(losses):
             log[f'loss{i}'] = loss
 
         print(num_of_tensors)
         return {'loss': losses[0], 'log': log}
 Here I return exactly 1001 tensor: one for loss and 1000 for log.
 In my real experiments I had only 3 tensors. It took ~2-3 days to get OOM. But in the current example (see To Reproduce) it will crash much faster.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Execute Code sample (this script has no arguments, so change needed values manually in script).
 Go to the tensorboard to check plots.
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-link:https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a>https://gist.github.com/alexeykarnachev/47de06b93a717ab0664eded42ed2826a</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The number of tensors, GPU and CPU memory does not increase during the training.
 <denchmark-h:h3>Environment</denchmark-h>
 
 PyTorch version: 1.4.0
 OS: Ubuntu 16.04.6 LTS
 Python version: 3.7
 Versions of relevant libraries:
 [pip] numpy==1.18.1
 [pip] pytorch-lightning==0.7.3
 [pip] torch==1.4.0
 [pip] torchvision==0.5.0
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Sorry for so messy flow of the information, but I don't know, how to structure it more clearly.
 	",ae2e14e3ed45e23dbe2868017b630fa7be9e5604,William Falcon,2020-04-19 16:41:54-04:00,MODIFY,1,pl_examples\basic_examples\cpu_template.py,pl_examples\basic_examples\cpu_template.py,1.0,31,31,1.0,alexeykarnachev,2020-04-16T16:35:20Z,"
 		by leak you mean tensors build up during epoch 1? but after that the memory stays constant? ie: there is no more ""leak"" for epochs >= 2?
 		",2.0,alexeykarnachev,2020-04-16T16:41:11Z,"
 		Yes, the memory stays constant after 1st epoch ends (although, the number of tensors begins increasing again)
 		",3.0,alexeykarnachev,2020-04-16T20:06:55Z,"
 		The whole output of a training step is stored.
 In your code with every training step, there are new tensors created.
 With z log[f'loss{i}'] = loss.item()there is no leak.
 I think there is a mistake in optimizer_closure() in the training loop which, returns whole batch output dict. It should be enough to return only callback_metrics instead of the whole batch output.
 		",4.0,alexeykarnachev,2020-04-16T20:47:48Z,"
 		Yes, I agreed, that with .item() there is no leak because all tensors ""disappear in place"" (I did not check it, but I believe that it so). But, I suppose, that .item() will slow my code.
 On the other hand, .item() is performed anyway by the Trainer itself (before logging), so maybe it's not a big deal to call .item() beforehand. At least as a hotfix solution
 		",MODIFY,6.0,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,"73,74,75,76,77,78,79,80",72,check_metrics,"self,logs",72,86,MODIFY,2.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"213,214,215",206,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"416,417",,run_evaluation,"self,bool",326,417,MODIFY,1.0,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,"177,178,179","176,177,178,179",process_output,"self,output,train",98,181,5.0,alexeykarnachev,2020-04-16T22:47:18Z,"
 		Oh, no sorry, just checked: it will be a leak even if we perform log[f'loss{i}'] = loss.item()
 Because we still have 'loss': losses[0] part (the actual loss tensor, which needs to be minimized).
 So, it will be a leak with speed 1 tensor per step. It's very slow, but the OOM will occur anyway in 6-9 days
 		",6.0,alexeykarnachev,2020-04-17T00:33:33Z,"
 		can you submit a PR? i thought we took care of all the metrics.
 we should also use detach instead of item no? to not slow code down
 		",7.0,alexeykarnachev,2020-04-17T06:20:31Z,"
 		We take care of it in process_output() but then in optimizer_closure() we return original output_dict again.
 We pass then a list of original outputs to the training_epoch_end().
 I think w should not do that bc loss, log and progress_bar is handling by us in a proper way so we should return to training_epoch_end only other keys from output_dict and let a user manage it.
 		",8.0,alexeykarnachev,2020-04-17T08:41:19Z,"
 		What about fp32-mode? There is no leak on the GPU in such a case. What could be the reason?
 		",9.0,alexeykarnachev,2020-04-17T11:36:23Z,"
 		@AratorField , do you mean this?
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 427 to 428
       in
       9b31272
 
 
 
 
 
 
  # bookkeeping 
 
 
 
  outputs = [] 
 
 
 
 
 
 Here is a list that stores all train step outputs during the epoch.
 		",10.0,alexeykarnachev,2020-04-17T11:47:21Z,"
 		but we detach everything.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 448
       in
       9b31272
 
 
 
 
 
 
  outputs.append(_recursive_detach(batch_output)) 
 
 
 
 
 
 how could it leak?
 		",11.0,alexeykarnachev,2020-04-17T11:55:28Z,"
 		Yes, but they (tensors) are still on the GPU after detach. So, in case of long epochs or huge outputs from the training step, the GPU memory will blow after some time.
 		",12.0,alexeykarnachev,2020-04-17T12:39:52Z,"
 		We can create something like _recursive_item() or remove keys loss, log, progress_bar from batch_output before appending to outputs.
 		",13.0,alexeykarnachev,2020-04-17T13:45:17Z,"
 		Is it in general a good practice to store values during the epoch? The size of such a bookkeeping list is undetermined in the general case. I mean, that one could have almost an infinite epoch and sooner or later he'll be faced with OOM (GPU or CPU, it does not matter).
 		",14.0,alexeykarnachev,2020-04-17T13:59:01Z,"
 		the thing is that .item() slows things down.
 so we want to detach but not .item().
 The tradeoff is that we plug the memory leak but slow things down.
 		",15.0,alexeykarnachev,2020-04-17T14:03:27Z,"
 		There is no reason to store loss, log and progress_bar for the whole epoch.
 Any other key in output_dict could be valuable and has to be stored i.e. for metrics calculating.
 		",on_validation_end,"self,trainer,pl_module",186,224,main,hparams,18,36,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"894,896",895,run_pretrain_routine,"self,LightningModule",806,909,,,,,,,,MODIFY,6.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,,,,,,,,1.0,"800,801",803,call_checkpoint_callback,self,800,803,1.0,"799,800,801",,call_early_stop_callback,self,799,801,ADD,0.0,None,pytorch_lightning\utilities\memory_utils.py,,,,1.0,"75,76,77,78,79,80",92,_validate_condition_metric,"self,logs",75,94,1.0,100,97,on_train_start,"self,trainer,pl_module",96,100,,,,,,,,,,,,,,,,,,,,,,1.0,"142,143,144,145",,check_monitor_top_k,"self,current",138,146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tests\base\utils.py,tests\base\utils.py,1.0,26,26,assert_speed_parity,"pl_times,pt_times,num_epochs",23,33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"448,449,450,451,452,470,471,472,476,477,507,516,519","447,448,482,483,484,485,486,487,488,489,490,491,508,509,510,519,522,523,524",run_training_epoch,self,405,527,1.0,606,611,run_training_batch,"self,batch,batch_idx",529,664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,alexeykarnachev,2020-04-17T14:11:32Z,"
 		Maybe it's possible to introduce a flag, which shows, should we store tensors in this list during an epoch or not.
 Or, maybe you can advise me some hot-fix, that I can apply locally. Because now, I can not train even 1 epoch :)
 		",17.0,alexeykarnachev,2020-04-17T14:16:33Z,"
 		I even have no training_epoch_end method. Maybe, we can check if this method is not determined by the user, we can skip batch results bookkeeping?
 		",18.0,alexeykarnachev,2020-04-17T14:28:42Z,"
 		
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 611
       in
       8544b33
 
 
 
 
 
 
  return closure_loss, output_dict 
 
 
 
 
 
 change it to return closure_loss , callback_metrics
 		",19.0,alexeykarnachev,2020-04-17T14:34:48Z,"
 		Thank you, I'll patch it locally for now.
 		",1.0,,46,__init__,"self,str,float,int,bool,str,bool",46,47,1.0,49,,__init__,"self,str,float,int,bool,str,bool",49,50,1.0,"105,109,110,111",,on_epoch_end,"self,trainer,pl_module",102,122,1.0,,"819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842",_recursive_detach,in_dict,819,842,1.0,606,,run_training_batch.optimizer_closure,,573,606,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1520,guydav,2020-04-17T18:47:13Z,2020-07-18T14:28:00Z,"Bug and question about logging -- missing epoch, validation before train?","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 First, the clear bug: in TrainerLoggingMixin.log_metrics() the epoch is added to the metrics variable (line 70) which is never accessed again. That should be to scalar_metrics, shouldn't it?
 Second, a question: I implemented a very primitive logger (to stdout) and logging to it. I don't get training results when the first epoch ends until after the first epoch validation step, and consequently don't get training metrics from the last epochs. See code and sample output below. Does this make sense?
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Add the following code to a Lightning Module and run a trainer with the following logger:
 use_gpu = int(torch.cuda.is_available())
 print_logger = PrintLogger()
 trainer = Trainer(gpus=use_gpu, max_epochs=5, logger=print_logger)
 trainer.fit(model)
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Minimal logging in the LightningModule:
     def training_epoch_end(self, outputs):
         avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
         avg_acc = torch.stack([x['acc'] for x in outputs]).mean()
         logs = dict(train_loss=avg_loss, train_acc=avg_acc)
         return dict(log=logs)
 
     def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
         avg_acc = torch.stack([x['acc'] for x in outputs]).mean()
         logs = dict(val_loss=avg_loss, val_acc=avg_acc)
         return dict(log=logs)
 A minimal logger:
 from pytorch_lightning.loggers import LightningLoggerBase, rank_zero_only
 
 class PrintLogger(LightningLoggerBase):
     
     def __init__(self):
         super(PrintLogger, self).__init__()
     
     @property
     def name(self):
         return 'Test'
     
     @property
     def experiment(self):
         return self.name()
     
     @property
     def version(self):
         return '0.0.1'
     
     @rank_zero_only
     def log_hyperparams(self, params):
         print(f'Hyperparameters:\n{params}')
 
     @rank_zero_only
     def log_metrics(self, metrics, step):
         if metrics is not None and len(metrics.keys()) > 0:
             print(f'{step}: {metrics}')
 
     def save(self):
         # Optional. Any code necessary to save logger data goes here
         pass
 
     @rank_zero_only
     def finalize(self, status):
         # Optional. Any code that needs to be run after training
         # finishes goes here
         pass
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I would expect to see the training output for each epoch followed by the validation output for each epoch, for each of the five epochs. Instead, I see the following -- four train outputs and five validation ones, and seeing the validation first:
 <denchmark-h:h4>Observed behavior:</denchmark-h>
 
 <denchmark-code>63: {'val_loss': 0.6922042369842529, 'val_acc': 0.51458740234375}
 64: {'train_acc': 0.503265380859375, 'train_loss': 1.0884952545166016}
 127: {'val_loss': 0.6919643878936768, 'val_acc': 0.51861572265625}
 128: {'train_acc': 0.51318359375, 'train_loss': 0.6927268505096436}
 191: {'val_loss': 0.6915570497512817, 'val_acc': 0.526611328125}
 192: {'train_acc': 0.5161285400390625, 'train_loss': 0.6924755573272705}
 255: {'val_loss': 0.6915992498397827, 'val_acc': 0.52325439453125}
 256: {'train_acc': 0.5159149169921875, 'train_loss': 0.6921626329421997}
 319: {'val_loss': 0.6915264129638672, 'val_acc': 0.521240234375}
 </denchmark-code>
 
 <denchmark-h:h4>Expected behavior:</denchmark-h>
 
 Where n is the number of steps/batches per epoch:
 <denchmark-code>n-1: {'train_acc': ..., 'train_loss': ...}
 n-1: {'val_loss': ..., 'val_acc': ...}
 2n-1 {'train_acc': ..., 'train_loss': ...}
 2n-1: {'val_loss': ..., 'val_acc': ...}
 3n-1 {'train_acc': ..., 'train_loss': ...}
 3n-1: {'val_loss': ..., 'val_acc': ...}
 ...
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
 	GPU:
 	available:           False
 	version:             10.0.130
 packages:
 	numpy:               1.18.2
 	pyTorch_debug:       False
 	pyTorch_version:     1.3.1
 	pytorch-lightning:   0.7.3
 	tensorboard:         2.2.0
 	tqdm:                4.45.0
 system:
 	OS:                  Linux
 	architecture: 64bit
 	processor:           x86_64
 	python:              3.7.4
 	version:             #1 SMP Tue Feb 4 23:02:59 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",fe2b6666e0c3a47992860a2200ab40ae1c2ea6c7,Guy Davidson,2020-04-23 17:52:41-04:00,MODIFY,1,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,71,71,1.0,guydav,2020-04-17T18:47:54Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,guydav,2020-04-17T20:38:00Z,"
 		Just to clarify your bug, you are missing some metrics? If I get it correctly, this shall be your fix <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1459>#1459</denchmark-link>
 
 		",3.0,guydav,2020-04-17T20:48:45Z,"
 		Hi <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  , I don't think that captures it. I'm reporting one clear bug (the epoch field doesn't get to the logger, since it's added to the  dict, rather than the  dict, in  .
 The second bug I'm reporting may not be a bug, but at the very least is confusing behavior. As a user, I would expect my logger to first get training results for an epoch, and then validation results. The PR you're referencing might solve the problem of the last training results not being returned at all, but won't fix the fact that something in the way steps are used in the logger causes validation results to come before test results.
 		",4.0,guydav,2020-04-17T20:51:30Z,"
 		I updated under  'Expected Behavior' to clarify the discrepancy I found.
 		",MODIFY,1.0,tests\loggers\test_all.py,tests\loggers\test_all.py,1.0,"65,66,67","65,66,67",test_loggers_fit_test,"tmpdir,monkeypatch,logger_class",30,67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,guydav,2020-04-17T21:04:48Z,"
 		<denchmark-link:https://github.com/guydav>@guydav</denchmark-link>
  mind send a PR? :]
 		",6.0,guydav,2020-04-17T21:09:13Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  for the first issue, sure, it's a one-line fix, but I'd need to set up to be able to run the tests.
 For the second issue, I don't quite know where it originates from. I was hoping someone with a better understanding of how logging is structured chimes in before I start to try and learn my way around that entire codebase.
 		",7.0,guydav,2020-06-16T21:53:49Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",8.0,guydav,2020-06-17T08:51:55Z,"
 		Hi <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  -- I think there's still another underlying issue with how metrics are reported unless someone interacted with this code over the last while.
 If you look at the observed behavior and expected behavior steps above, do you agree there's an issue? That it makes no sense for the validation metrics for a particular epoch to report before the training metrics for that epoch, and with a different step at that?
 		",9.0,guydav,2020-07-14T15:42:34Z,"
 		This behaviour seems to have changed.
 I copied your logger to the pl_examples/gpu_template.py and launched it with
     trainer = Trainer(
         max_epochs=2, 
         gpus=1,
         logger=PrintLogger(),
         limit_train_batches=10,
         limit_val_batches=10,
         row_log_interval=1,
         progress_bar_refresh_rate=0
     )
 output is
 0: {'train_loss': 2.503892660140991, 'epoch': 0}
 1: {'train_loss': 2.096820831298828, 'epoch': 0}
 2: {'train_loss': 8.215052604675293, 'epoch': 0}
 3: {'train_loss': 5.370606422424316, 'epoch': 0}
 4: {'train_loss': 5.988080978393555, 'epoch': 0}
 5: {'train_loss': 2.3805108070373535, 'epoch': 0}
 6: {'train_loss': 4.3501176834106445, 'epoch': 0}
 7: {'train_loss': 9.668755531311035, 'epoch': 0}
 8: {'train_loss': 6.58243465423584, 'epoch': 0}
 
 # this is the last step of the epoch, metrics get combined and logged together
 9: {'epoch': 0.0, 'val_loss': 4.287566661834717, 'train_loss': 12.217967987060547, 'val_acc': 0.515625}
 
 10: {'train_loss': 1.7836229801177979, 'epoch': 1}
 11: {'train_loss': 1.7488218545913696, 'epoch': 1}
 12: {'train_loss': 2.221280097961426, 'epoch': 1}
 13: {'train_loss': 3.4499270915985107, 'epoch': 1}
 14: {'train_loss': 3.5983619689941406, 'epoch': 1}
 15: {'train_loss': 2.813007116317749, 'epoch': 1}
 16: {'train_loss': 3.2659897804260254, 'epoch': 1}
 17: {'train_loss': 4.156956672668457, 'epoch': 1}
 18: {'train_loss': 2.931321859359741, 'epoch': 1}
 # no val logs here :( we expect a dict as in step 9
 The original problem you describe seems to be gone, but I notice two other issues:
 
 at step 9, the epoch is a float 0.0
 the validation metrics of epoch 2 (last one) do not get logged
 
 		",10.0,guydav,2020-07-15T10:24:13Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 , thank you for looking into it again! I agree this does look better. It's been a while since I dug through this code, but I think I have a hunch for at least one of these issues. Note that in the second issue you point out, we're missing the last set of train metrics, that should arrive with the validation metrics.
 Reading through <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/base.py>LightningLoggerBase</denchmark-link>
 , its API seems to be through the function , which is called from the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/7b4db3045dcc9e6bb0b66e409b25bb2c7fa378f0/pytorch_lightning/trainer/logging.py#L73>TrainerLoggingMixin</denchmark-link>
 .  calls , which only omits metrics to log if the current step is different from the previous step. Since there's never a call to this function after the last validation epoch, it doesn't see a new step, and therefore never omits the last output. It looks like a call to any of , , or  should result in a call to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/7b4db3045dcc9e6bb0b66e409b25bb2c7fa378f0/pytorch_lightning/loggers/base.py#L108>finalize_agg_metrics</denchmark-link>
 , which should do the trick. So either it's not getting called, or for some reason, it doesn't do what it should.
 The float epoch thing is probably a smaller bit. I'll try to debug both of these later today if I have time.
 		",11.0,guydav,2020-07-15T11:01:38Z,"
 		<denchmark-link:https://github.com/guydav>@guydav</denchmark-link>
  I checked again, the step 8 was missing from my post by accident because I had to copy paste around some warnings that were printed to the console and it seems I missed one line, but the step 8 is there and I edited my post.
 		",12.0,guydav,2020-07-15T11:03:59Z,"
 		Oh, I mean that we're missing step 19, which contains both the 10th training batch from the second epoch and the validation metrics for that epoch.
 		",13.0,guydav,2020-07-15T11:08:10Z,"
 		Yes I agree, that's the big one :) It should definitely log a dict like step 9
 		",14.0,guydav,2020-07-18T14:28:00Z,"
 		Update: it appears that I am the problem. I don't know why, but I overrode save and finalize above to do nothing. Omitting that (commenting them out, or a super call) makes everything work out. I honestly have no idea how that happened, but now everything looks fine. I don't see the floating point epoch either:
 Here's a printout:
 <denchmark-code>0: {'train_loss': 2.517963171005249, 'epoch': 0}
 1: {'train_loss': 2.1298298835754395, 'epoch': 0}
 2: {'train_loss': 8.561811447143555, 'epoch': 0}
 3: {'train_loss': 5.23430871963501, 'epoch': 0}
 4: {'train_loss': 6.442159175872803, 'epoch': 0}
 5: {'train_loss': 2.1811487674713135, 'epoch': 0}
 6: {'train_loss': 4.158588409423828, 'epoch': 0}
 7: {'train_loss': 10.028255462646484, 'epoch': 0}
 8: {'train_loss': 6.593491077423096, 'epoch': 0}
 9: {'val_loss': 4.531818389892578, 'val_acc': 0.453125, 'epoch': 0}
 9: {'train_loss': 10.541756629943848, 'epoch': 0}
 10: {'train_loss': 1.6655378341674805, 'epoch': 1}
 11: {'train_loss': 2.284700393676758, 'epoch': 1}
 12: {'train_loss': 2.4957871437072754, 'epoch': 1}
 13: {'train_loss': 4.456875324249268, 'epoch': 1}
 14: {'train_loss': 4.337017059326172, 'epoch': 1}
 15: {'train_loss': 3.4667391777038574, 'epoch': 1}
 16: {'train_loss': 3.3742592334747314, 'epoch': 1}
 17: {'train_loss': 3.353729248046875, 'epoch': 1}
 18: {'train_loss': 2.8706002235412598, 'epoch': 1}
 19: {'val_loss': 4.367581844329834, 'val_acc': 0.571875, 'epoch': 1}
 19: {'train_loss': 4.163558483123779, 'epoch': 1}
 </denchmark-code>
 
 		",15.0,guydav,2020-07-18T16:58:06Z,"
 		Oh great you found this. Last time I tried to debug it I was stuck because we actually have tests for these things and I was very confused why it would not work :)
 Thanks <denchmark-link:https://github.com/guydav>@guydav</denchmark-link>
 
 		",,,,,log_metrics,"self,metrics,grad_norm_dic,step",46,76,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1522,Jonas-Jaeger,2020-04-17T23:15:19Z,2020-04-19T03:07:16Z,Performance drop when activating gradient clipping,"
 Hello all,
 I experienced a substantial drop in computation time when activating gradient clipping (by passing a non-zero value to the keyword argument gradient_clip_val when initializing the Trainer).
 I noticed that in the current implementation of the clipping_gradient method in pytorch-lightning/trainer/training_tricks.py redundant computations are made by first computing the 2-norm and second squaring this result, which could be shortened by computing the sum of squares directly. This saves one square root and squaring operation per parameter set.
 Best,
 Jonas
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
 	GPU:
 	available:           False
 	version:             None
 packages:
 	numpy:               1.18.1
 	pyTorch_debug:       False
 	pyTorch_version:     1.4.0
 	pytorch-lightning:   0.7.4-dev
 	tensorboard:         2.2.1
 	tqdm:                4.45.0
 system:
 	OS:                  Darwin
 	architecture:
 		64bit
 		
 	processor:           i386
 	python:              3.8.2
 	version:             Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 I trained a relatively small (two-layered) MLP on MNIST; perhaps this performance drop does not become that apparent when training on larger network architectures.
 	",e02146943d3373020b7fa6e8acc31dc18b4201e4,Jonas-Jaeger,2020-04-18 23:07:15-04:00,MODIFY,1,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,1.0,43,43,1.0,Jonas-Jaeger,2020-04-17T23:15:56Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Jonas-Jaeger,2020-04-17T23:20:13Z,"
 		Unfortunately, it seems that I do not have push access to this repository to push the proposed fix and create a pull request for this issue.
 		",3.0,Jonas-Jaeger,2020-04-17T23:42:13Z,"
 		fork the repo
 make the fix
 submit a PR
 :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,clip_gradients,self,26,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1538,lezwon,2020-04-20T18:04:20Z,2020-04-23T11:12:55Z,`num_tpu_cores=8` does not work on kaggle,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When I try to train a model on Kaggle TPU's with num_tpu_cores set to 8, I receive an error Exception: process 2 terminated with exit code 1 . Would be great if this worked on kaggle.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run this notebook:
 https://www.kaggle.com/lezwon/pytorch-on-tpu-with-pytorch-lightning
 
 <denchmark-code>---------------------------------------------------------------------------
 Exception                                 Traceback (most recent call last)
 <ipython-input-9-9251330963d1> in <module>
       3 # most basic trainer, uses good defaults (1 TPU)
       4 trainer = pl.Trainer(num_tpu_cores=8)
 ----> 5 trainer.fit(mnist_model)
 
 /opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, test_dataloaders)
     714 
     715             # train
 --> 716             xmp.spawn(self.tpu_train, args=(model,), nprocs=self.num_tpu_cores, start_method=start_method)
     717 
     718             # load weights if not interrupted
 
 /opt/conda/lib/python3.6/site-packages/torch_xla/distributed/xla_multiprocessing.py in spawn(fn, args, nprocs, join, daemon, start_method)
     180         join=join,
     181         daemon=daemon,
 --> 182         start_method=start_method)
 
 /opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in start_processes(fn, args, nprocs, join, daemon, start_method)
     156 
     157     # Loop on join until it returns True or raises an exception.
 --> 158     while not context.join():
     159         pass
     160 
 
 /opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py in join(self, timeout)
     111                 raise Exception(
     112                     ""process %d terminated with exit code %d"" %
 --> 113                     (error_index, exitcode)
     114                 )
     115 
 
 Exception: process 3 terminated with exit code 1
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 trainer = pl.Trainer(num_tpu_cores=8, precision=16) 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Run the model utilizing all 8 TPU cores.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>cuda:
 	GPU:
 	available:           False
 	version:             None
 packages:
 	numpy:               1.18.2
 	pyTorch_debug:       False
 	pyTorch_version:     1.6.0a0+30e7055
 	pytorch-lightning:   0.7.3
 	tensorboard:         2.1.1
 	tqdm:                4.42.0
 system:
 	OS:                  Linux
 	architecture:
 		64bit
 		
 	processor:           
 	python:              3.6.6
 	version:             #1 SMP Sat Apr 4 00:12:45 PDT 2020
 </denchmark-code>
 
 	",831842972f7e2d25ae3a376d5584748c3054f899,Lezwon Castelino,2020-04-23 07:12:54-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"29,30",,1.0,lezwon,2020-04-20T22:54:06Z,"
 		I think this is a kaggle problem?
 <denchmark-link:https://github.com/dlibenzi>@dlibenzi</denchmark-link>
  any ideas?
 		",2.0,lezwon,2020-04-20T22:58:07Z,"
 		It prolly needs this on top:
 <denchmark-code>!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py
 !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev
 </denchmark-code>
 
 		",3.0,lezwon,2020-04-20T23:34:16Z,"
 		those lines are already at the top:
 <denchmark-link:https://www.kaggle.com/pytorchlightning/pytorch-on-tpu-with-pytorch-lightning>https://www.kaggle.com/pytorchlightning/pytorch-on-tpu-with-pytorch-lightning</denchmark-link>
 
 <denchmark-link:https://user-images.githubusercontent.com/3640001/79809189-eb2c1580-833d-11ea-80d2-4954e5ffca0d.png></denchmark-link>
 
 		",4.0,lezwon,2020-04-21T00:55:23Z,"
 		I bet the issue is here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 762
       in
       bd16881
 
 
 
 
 
 
  start_method = 'fork' if os.getenv('COLAB_GPU') else 'spawn' 
 
 
 
 
 
 		",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,758,758,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,lezwon,2020-04-21T01:03:08Z,"
 		ah... yes. good catch.
 know of something more general that we can check? i assume the only two options are kaggle and colab?
 		",6.0,lezwon,2020-04-21T01:04:22Z,"
 		<denchmark-link:https://github.com/lezwon>@lezwon</denchmark-link>
  want to find an environment variable we can check to know if on kaggle and submit a PR?
 		",7.0,lezwon,2020-04-21T01:06:17Z,"
 		Honestly, pytorch does not like fork because of CUDA, but I would make that the default, with ability to change via some environment variable in cases someone have issues.
 		",8.0,lezwon,2020-04-21T01:11:15Z,"
 		on GCP it would still be fork?
 when would it not be fork with TPUs?
 		",9.0,lezwon,2020-04-21T01:24:59Z,"
 		Fork is an issue with pytorch/CUDA mostly.
 But for safety, I would just add a Kaggle check as well in your code, and leave spawn as default.
 Fork also helps Colab and Kaggle because, being them low memory VMs, one can reduce the memory consumption by creating the model (on default pytorch/cpu) at global scope, and then doing to(xla_device) from within the xmp.spawn() target functions.
 This avoids creating pytorch/cpu models in each of the processes (one per core).
 You can see a few tricks to fit models on Colab here:
 <denchmark-link:https://colab.research.google.com/drive/1IvCxIg-Q_DlI7UNJuajpl4UZXNiW5jMg>https://colab.research.google.com/drive/1IvCxIg-Q_DlI7UNJuajpl4UZXNiW5jMg</denchmark-link>
 
 Like create model at global scope, and serialize the to(xla_device) calls to avoid all 8 processes rushing into allocation host memory at the same time.
 		",10.0,lezwon,2020-06-12T13:07:48Z,"
 		I also have this issue. if I use GPU, the model is training normally, but when I try to TPU, this happens.
 EDIT: Having analyzed the issue is about the RAM crashing.
 
 I believe this has to do with XLA using up RAM. I constantly use up all my RAM, which causes the
 SIGKILL error. If you take a look at this: pytorch/xla#1280  --- reference Kaggle discussions
 
 <denchmark-code>INIT TPU local core: 0, global rank: 0
 INIT TPU local core: 4, global rank: 4
 INIT TPU local core: 6, global rank: 6
 INIT TPU local core: 3, global rank: 3
 INIT TPU local core: 7, global rank: 7
 INIT TPU local core: 5, global rank: 5
 INIT TPU local core: 2, global rank: 2
 INIT TPU local core: 1, global rank: 1
 
  
 Validation sanity check:
 0/? [00:00<?, ?it/s]
 Exception in device=TPU:6: Invalid argument: From /job:tpu_worker/replica:0/task:0:
 2 root error(s) found.
   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
 	 [[XRTCompile_G3]]
 0 successful operations.
 0 derived errors ignored.
 Traceback (most recent call last):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 535, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 1001, in run_pretrain_routine
     False)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 256, in _evaluate
     for batch_idx, batch in enumerate(dataloader):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py"", line 31, in __next__
     return self.next()
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py"", line 37, in next
     xm.mark_step()
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py"", line 536, in mark_step
     wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))
 RuntimeError: Invalid argument: From /job:tpu_worker/replica:0/task:0:
 2 root error(s) found.
   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
 	 [[XRTCompile_G3]]
 0 successful operations.
 0 derived errors ignored.
 Exception in device=TPU:1: Invalid argument: From /job:tpu_worker/replica:0/task:0:
 2 root error(s) found.
   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
 	 [[XRTCompile_G3]]
 0 successful operations.
 0 derived errors ignored.
 Traceback (most recent call last):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 231, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 535, in tpu_train
     self.run_pretrain_routine(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 1001, in run_pretrain_routine
     False)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 256, in _evaluate
     for batch_idx, batch in enumerate(dataloader):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py"", line 31, in __next__
     return self.next()
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/parallel_loader.py"", line 37, in next
     xm.mark_step()
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/core/xla_model.py"", line 536, in mark_step
     wait=xu.getenv_as('XLA_SYNC_WAIT', bool, False))
 RuntimeError: Invalid argument: From /job:tpu_worker/replica:0/task:0:
 2 root error(s) found.
   (0) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
   (1) Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 	 [[{{node XRTCompile}}]]
 	 [[XRTCompile_G3]]
 0 successful operations.
 0 derived errors ignored.
 ---------------------------------------------------------------------------
 Exception                                 Traceback (most recent call last)
 <ipython-input-29-f6eba0e942ef> in <module>()
       1 model = hatefull_memesCL()
       2 if __name__ == '__main__':
 ----> 3     trainer.fit(model)
 
 3 frames
 /usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py in join(self, timeout)
     111                 raise Exception(
     112                     ""process %d terminated with exit code %d"" %
 --> 113                     (error_index, exitcode)
     114                 )
     115 
 
 Exception: process 6 terminated with exit code 17 
 </denchmark-code>
 
 		",11.0,lezwon,2020-06-12T13:45:40Z,"
 		Hmm, this is something different:
 <denchmark-code>Invalid argument: Computation requires more parameters (732) than supported (limit 236).
 </denchmark-code>
 
 We have seen that a few time but I keep forgetting what the root cause was.
 It's a misconfiguration of the TPU service, but I do not remember how it can get in that state.
 		",12.0,lezwon,2020-06-12T13:47:52Z,"
 		<denchmark-link:https://github.com/dlibenzi>@dlibenzi</denchmark-link>
  it is interesting issue, i will let you know if i find the bug
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
154,williamFalcon,2019-08-20T16:57:39Z,2019-08-20T20:59:27Z,transfer_to_batch_gpu returns null when input has primitives,"
 Describe the bug
 when passing a batch such as:
 batch = list(tensor, tensor, [0,1,2])  
 the list of ints won't be returned correctly
 Additional context
 Fix should add a return of the item it no condition matches
 	",55a804b7cfb9b2376ccaa1253a966dcaa9b6ab07,William Falcon,2019-08-20 16:59:26-04:00,MODIFY,1,pytorch_lightning\models\trainer.py,pytorch_lightning\models\trainer.py,1.0,"924,925,937,938","924,925",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,transfer_batch_to_gpu,"self,batch,gpu_id",919,938,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1540,kevinc13,2020-04-20T22:25:22Z,2020-04-21T18:29:16Z,DDP on GPUs invalid ordinal,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 On latest version (master), training with DDP backend on GPUs (let's say 6,7) results in a CUDA error ""Invalid device ordinal.""
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run any Lightning Module with DDP backend on more than 1 GPU with GPU indexes that do not start from 0
 See error
 
 <denchmark-code>INFO:lightning:GPU available: True, used: True
 INFO:lightning:VISIBLE GPUS: 6,7
 WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.
 WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost
 WARNING:lightning:SLURM_NODEID or NODE_RANK environment variable is not defined. Set as 0.
 WARNING:lightning:MASTER_ADDR environment variable is not defined. Set as localhost
 THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal
 THCudaCheck FAIL file=/pytorch/torch/csrc/cuda/Module.cpp line=59 error=101 : invalid device ordinal
 Traceback (most recent call last):
   File ""bin/run.py"", line 110, in <module>
     run(config, args.mode)
   File ""bin/run.py"", line 86, in run
     trainer.fit(system)
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 744, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 171, in spawn
     while not spawn_context.join():
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 118, in join
     raise Exception(msg)
 Exception:
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
     fn(i, *args)
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 332, in ddp_train
     torch.cuda.set_device(self.root_gpu)
   File ""/home/kevin/miniconda3/lib/python3.7/site-packages/torch/cuda/__init__.py"", line 292, in set_device
     torch._C._cuda_setDevice(device)
 RuntimeError: cuda runtime error (101) : invalid device ordinal at /pytorch/torch/csrc/cuda/Module.cpp:59
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I expect this error to not occur.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - TITAN RTX
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.17.4
 - pyTorch_debug:     False
 - pyTorch_version:   1.4.0
 - pytorch-lightning: 0.7.4rc1
 - tensorboard:       2.0.0
 - tqdm:              4.45.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.4
 - version:           #97-Ubuntu SMP Wed Apr 1 03:25:46 UTC 2020
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 My best guess is that due to the new refactoring for ddp cpu backend, setting the CUDA device to root_gpu instead of using the provided gpu_idx parameter in ddp_train() is causing the error. Specifically, for 2 gpu, it's using index 6 instead of index 0 and index 7 instead of index 1 since we're setting CUDA_VISIBLE_DEVICES=6,7.
 	",bafdeca42f746aac59b4f0c1103264d7bff556db,Kevin Chen,2020-04-21 14:29:15-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,330,330,1.0,kevinc13,2020-04-20T22:25:56Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,kevinc13,2020-04-20T22:51:53Z,"
 		good catch. mind submitting a PR?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ddp_train,"self,process_idx,model",280,359,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1546,festeh,2020-04-21T12:02:31Z,2020-05-02T12:41:38Z,LightningTemplateModel is broken,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
  has no implemented  method. See <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8035c10f379dd0c00dcf00bd3fdee3f66fb074a2/pl_examples/models/lightning_template.py#L245>here</denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Just try to run test evaluation on MNIST example
 <denchmark-h:h3>Expected behavoir</denchmark-h>
 
 It should not crash
 	",210cd657dd0f83069b8c6abc7402508f354668b3,Dmitry Lipin,2020-05-02 08:41:37-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,72,,1.0,festeh,2020-04-21T19:05:56Z,"
 		<denchmark-link:https://github.com/festeh>@festeh</denchmark-link>
  mind draft a PR to fix it? 
 		",2.0,festeh,2020-04-22T09:25:34Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  yes, will make it soon
 		",,,,,,,,,MODIFY,14.0,pl_examples\models\lightning_template.py,pl_examples\models\lightning_template.py,1.0,"91,93","91,92,93",loss,"self,labels,logits",91,93,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"111,112,113,114,115","111,112,113,114,115",test_epoch_end,"self,outputs",111,115,1.0,145,"143,144",test_dataloader,self,143,145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"106,107,108,109","102,103,105,106,107,108,109",validation_epoch_end,"self,outputs",101,109,1.0,"93,94,95,96,97,98,99",93,test_step,"self,batch,batch_idx",93,99,1.0,"62,67","57,58,59,60,61,62,63,64",forward,"self,x",57,67,,,,,,,,,,,,,,,1.0,"132,133","129,130,132,133",prepare_data,self,129,133,1.0,141,"139,140,141",val_dataloader,self,139,141,1.0,,"190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208",__dataloader,"self,train",190,208,1.0,,"49,50,51,52,53,54,55,56",__init__,"self,hparams",42,56,1.0,137,"135,136,137",train_dataloader,self,135,137,1.0,"62,67","61,62,63,64",__build_model,self,61,71,1.0,"88,90,91","81,85,87,88,89,90,91",validation_step,"self,batch,batch_idx",81,91,1.0,"77,78,79","73,74,75",training_step,"self,batch,batch_idx",69,79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1547,festeh,2020-04-21T12:25:52Z,2020-07-29T21:53:03Z,Metric aggragation is broken for LoggerCollection,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 After changes in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1278>#1278</denchmark-link>
  it is now not possible to log testing metrics after traning while using several loggers.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Say we want to run a <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/cpu_template.py>MINST example</denchmark-link>
  and also want to add a change - log testing metrics after training. For that we define a Callback
 <denchmark-code>class TestCallback(Callback):
     def on_train_end(self, trainer, pl_module):
         # note that it would crash if you don't pass the `pl_module`
         trainer.test(pl_module)
 </denchmark-code>
 
 and pass it to trainer callbacks argument.
 We would also like to use several loggers to track all metrics, say MLFlowLogger and TensorBoardLogger. For this we create instances of these loggers and pass them into Trainer in a list.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Testing metrics should be logged - but they don't as there's no final aggregation when our logger is a LoggerCollection
 <denchmark-h:h3>Additional context</denchmark-h>
 
 In my opinion, the logic in agg_and_log_metrics  and _finalize_agg_metrics is hard to follow, so I'd be happy if user could choose plain old log_metrics which worked nicely.
 	",458d3e210e2da10482d97a996708731b8b0fabae,Ethan Harris,2020-07-29 23:53:02+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"44,45",,1.0,festeh,2020-04-21T19:06:56Z,"
 		are you using the last master?
 		",2.0,festeh,2020-04-22T09:08:28Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  thanks! It now works for a single logger. But it seems that problem persists when several loggers are used, so I updated the issue. This happens because metrics to aggregate are not propagated to child loggeres
 		",3.0,festeh,2020-04-25T00:57:55Z,"
 		I'm also having this issue with multiple loggers on master. Seems that the test results are printed at the end, but the loggers don't log the test metrics.
 		",4.0,festeh,2020-05-25T23:26:02Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  any update on this issue? I'm still encountering this on the latest release, and it seems related to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1859>#1859</denchmark-link>
 
 		",MODIFY,8.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,"275,276,277,278",,update_agg_funcs,"self,str,None,mean",275,278,MODIFY,1.0,tests\loggers\test_base.py,tests\loggers\test_base.py,1.0,"25,26,27,28,29,30,31,32,33,34",,,,,,,,,,,,,,,,,,,,,,,,5.0,festeh,2020-07-25T01:38:03Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_logger_collection,,13,37,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"296,297",,log_hyperparams,"self,str",295,297,1.0,"312,313,314",,save_dir,self,312,314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"287,288,289",289,agg_and_log_metrics,"self,str,None",287,289,1.0,"308,309",,close,self,307,309,1.0,"304,305",,finalize,"self,str",303,305,,,,,,,,,,,,,,,1.0,"300,301",,save,self,299,301,1.0,"292,293",292,log_metrics,"self,str,None",291,293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1566,karlinjf,2020-04-22T21:32:23Z,2020-04-23T18:28:21Z,Batch being moved to gpu repeatedly with multiple optimizers and single gpu training,"
 If you have multiple optimizers, then transfer_batch_to_gpu winds up getting called once per opt_idx, and the batch is copied each time via copy.copy(batch) in training_forward. Why copy the batch when there is only a single gpu? By removing the copy.copy() my GAN model moves from 8.53it/s to 9.25it/s. Pretty significant speedup.
 	",41b6cbb3ca8a3a43e091d7f0de4d0184a8870d19,karlinjf,2020-04-23 14:28:20-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"757,758,759,760,761",757,1.0,karlinjf,2020-04-22T22:15:11Z,"
 		amazing find! mind submitting a PR?
 		",2.0,karlinjf,2020-04-22T22:44:28Z,"
 		Sure, will do.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,training_forward,"self,batch,batch_idx,opt_idx,hiddens",715,792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
157,eqs,2019-08-21T04:32:22Z,2019-08-21T14:22:59Z,Recursive device conversion of tuple,"
 This bug report related to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/116>#116</denchmark-link>
  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/154>#154</denchmark-link>
 .
 Describe the bug
 When passing a batch such as:
 batch = ([tensor, tensor], [{'a': tensor, 'b': tensor}, {'a': tensor, 'b': tensor}])
 transfer_batch_to_gpu raises the error: TypeError: 'tuple' object does not support item assignment.
 I found that above error caused by concatenation of two conditions: isinstance(batch, list) or isinstance(batch, tuple).
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/55a804b7cfb9b2376ccaa1253a966dcaa9b6ab07/pytorch_lightning/models/trainer.py#L925-L928>https://github.com/williamFalcon/pytorch-lightning/blob/55a804b7cfb9b2376ccaa1253a966dcaa9b6ab07/pytorch_lightning/models/trainer.py#L925-L928</denchmark-link>
 
 Expected behavior
 A batch
 batch = ([tensor, tensor], [{'a': tensor, 'b': tensor}, {'a': tensor, 'b': tensor}])
 should be
 batch = ([cuda_tensor, cuda_tensor], [{'a': cuda_tensor, 'b': cuda_tensor}, {'a': cuda_tensor, 'b': cuda_tensor}])
 Additional context
 I already fixed the bug and will submit PR soon.
 	",4a0b56755c7239e6a85b32a3d3e97cf7f9e39044,eqs,2019-08-21 10:22:51-04:00,MODIFY,1,pytorch_lightning\models\trainer.py,pytorch_lightning\models\trainer.py,1.0,"924,925,930,931,932,933,934,935,936","924,925",1.0,eqs,2019-08-21T10:23:14Z,"
 		awesome! good catch
 		",,,,,,,,,,,,,MODIFY,1.0,tests\test_models.py,tests\test_models.py,1.0,"95,96,97,98,99,100,101,102,103,104,105",,test_single_gpu_batch_parse,,60,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,transfer_batch_to_gpu,"self,batch,gpu_id",919,945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1570,fschlatt,2020-04-23T12:46:12Z,2020-04-23T15:44:19Z,Trainer.add_argparse_args bool type,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The boolean arguments added using Trainer.add_argparse_args always evaluate to True. This is caused by the following lines of the add_argparse_args fucntion:
 if isinstance(allowed_type, bool):
     def allowed_type(x):
          return bool(distutils.util.strtobool(x))
 Because allowed_type is the actual data type and not an instance of bool. isinstance(bool, bool) is equal to False. bool is bool is equal True
 	",545b38ec5f1b1de7aaabec7a6cf4f2f4d7893b71,Ferdinand Schlatt,2020-04-23 11:44:18-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,65,,1.0,fschlatt,2020-04-23T14:41:25Z,"
 		I just found the Trainer.add_argparse_args clashes with the auto_lr_find argument as auto_lr_find can be either bool or str. I would propose removing the option to provide a string to auto_lr_find. If I understand correctly then the str option is only there to specify a separate learning rate using argparse anyway. On top of being confusing you could just use hparams.lr and set auto_lr_find to False which is more straightforward
 		",,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,606,606,add_argparse_args,"cls,ArgumentParser",586,618,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1588,nathanbreitsch,2020-04-24T03:46:31Z,2020-04-30T12:04:51Z,Named converted to regular tuples when sent to the gpu.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Named tuples returned from Dataset get converted to regular tuples when sent to the gpu.
 This happens because isinstance(instance_of_a_named_tuple, tuple) evaluates to True in distrib_parts.py
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/distrib_parts.py
 
 
          Line 463
       in
       67d5f4d
 
 
 
 
 
 
  if isinstance(batch, tuple): 
 
 
 
 
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 import pytorch_lightning as pl
 from collections import namedtuple
 import torch
 import numpy
 
 NamedTupleDemoInput = namedtuple('DemoInput', ['x1', 'x2', 'y'])
 
 class NamedTupleDemoDataset:
     def __len__(self):
         return 30000
 
     def __getitem__(self, index):
         x1 = numpy.random.uniform(0, 100)
         x2 = numpy.random.uniform(0, 100)
         y = 2*x1 + 3*x2 + numpy.random.normal(0, 0.05)
         return NamedTupleDemoInput(x1, x2, y)
 
 class WeightedSum(torch.nn.Module):
     def __init__(self):
         super(WeightedSum, self).__init__()
         self.a = torch.nn.Parameter(torch.zeros(1))
         self.b = torch.nn.Parameter(torch.zeros(1))
 
     def forward(self, x1, x2):
         return self.a * x1 + self.b * x2
 
 class NamedTupleDemo(pl.LightningModule):
 
     def __init__(self):
         super(NamedTupleDemo, self).__init__()
         self.model = WeightedSum()
 
     def forward(self, x1, x2):
         return self.model(x1, x2)
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(NamedTupleDemoDataset(), batch_size=128)
 
     def training_step(self, batch, batch_index):
         yhat = self.forward(batch.x1, batch.x2)
         return {'loss': torch.nn.functional.mse_loss(batch.y, yhat)}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-2)
 
 if __name__ == '__main__':
     module = NamedTupleDemo()
     pl.Trainer(max_epochs=20, gpus=1).fit(module)
     print(f'a={float(module.model.a)} b={float(module.model.b)}')
 <denchmark-code>Traceback (most recent call last):
   File ""demo.py"", line 48, in <module>
     pl.Trainer(max_epochs=20, gpus=1).fit(module)
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 749, in fit
     self.single_gpu_train(model)
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/distrib_parts.py"", line 491, in single_gpu_train
     self.run_pretrain_routine(model)
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 910, in run_pretrain_routine
     self.train()
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py"", line 384, in train
     self.run_training_epoch()
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py"", line 456, in run_training_epoch
     _outputs = self.run_training_batch(batch, batch_idx)
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py"", line 633, in run_training_batch
     loss, batch_output = optimizer_closure()
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py"", line 597, in optimizer_closure
     output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)
   File ""/home/n/repos/pytorch-lightning/pytorch_lightning/trainer/training_loop.py"", line 770, in training_forward
     output = self.model.training_step(*args)
   File ""demo.py"", line 40, in training_step
     yhat = self.forward(batch.x1, batch.x2)
 AttributeError: 'tuple' object has no attribute 'x1'
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Namedtuples returned from the dataset should be keep their original fields.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - GeForce RTX 2080 Ti
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.18.3
 - pyTorch_debug:     False
 - pyTorch_version:   1.5.0
 - pytorch-lightning: 0.7.4rc5
 - tensorboard:       2.2.1
 - tqdm:              4.45.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 - ELF
 - processor:
 - python:            3.8.2
 - version:           #1 SMP PREEMPT Sun, 05 Apr 2020 05:13:14 +0000
 
 	",3eac6cfd4fbbc4d13f4e93f6d90f8ee5302c421e,Nathan Breitsch,2020-04-30 08:04:50-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"464,465,466,467,468,469,470,471,472","464,465,466,467",1.0,nathanbreitsch,2020-04-24T03:47:10Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,MODIFY,1.0,tests\models\test_cpu.py,tests\models\test_cpu.py,1.0,"225,226,227,228,229,230",,test_single_gpu_batch_parse,,185,230,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__transfer_data_to_device,"self,batch,device,gpu_id",442,482,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1620,yukw777,2020-04-26T17:06:09Z,2020-04-27T11:46:36Z,horovod cicd tests are failing on ubuntu 18.04 python 3.6 latest,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The failed job: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/runs/620109522>https://github.com/PyTorchLightning/pytorch-lightning/runs/620109522</denchmark-link>
 
 We see two errors:
 
 RuntimeError: Failed to determine if NCCL support has been built. Run again with --verbose for more details.
 ImportError: /opt/hostedtoolcache/Python/3.6.10/x64/lib/python3.6/site-packages/horovod/torch/mpi_lib_v2.cpython-36m-x86_64-linux-gnu.so: undefined symbol: _ZTIN3c1021AutogradMetaInterfaceE
 
 My hunch is that both are caused by the same horovod compilation issue.
 Another thing to note is that the same tests are passing on ubuntu 18.04 python 3.6 minimal.
 <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  maybe you have an idea?
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run the cicd test suite.
 	",813e37916d9b17224be3d6c4d1672876bfb88a54,J. Borovec,2020-04-27 15:48:58+02:00,MODIFY,0,.github\workflows\ci-testing.yml,.github\workflows\ci-testing.yml,0.0,"63,64,65,66,67,68,69,73,74,76,89,90,91,92,93,94,95,129","66,67,68,70,71,72,73,74,75,76,77,78,114,125",1.0,yukw777,2020-04-26T17:28:06Z,"
 		Hey <denchmark-link:https://github.com/yukw777>@yukw777</denchmark-link>
 , I have seen this issue before.  If I remember correctly, it is related to having a corrupted pip cache.  I would suggest commenting out the step ""Cache pip"" and see if that fixes the issue.
 My suspicion is that the CI system used a cached version of Horovod, but reinstalled a new version of PyTorch.  Because Horovod is built against a specific version of PyTorch, that can lead to these incompatibilities and runtime if the PyTorch version is swapped out.
 If I remember correctly, the pip cache didn't save that much time, so it may be worth removing it.
 		",2.0,yukw777,2020-04-26T17:45:53Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  shall I send you guys a PR that removes the pip caching step for now?
 		",3.0,yukw777,2020-04-26T19:23:40Z,"
 		check <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1558>#1558</denchmark-link>
 
 		",4.0,yukw777,2020-04-26T19:26:00Z,"
 		
 My suspicion is that the CI system used a cached version of Horovod, but reinstalled a new version of PyTorch. Because Horovod is built against a specific version of PyTorch, that can lead to these incompatibilities and runtime if the PyTorch version is swapped out.
 
 that would perfectly fit our case as Horovod has been adding fro about two weeks and just last week new PyTorch was released so other cache stuck with Horovod and reinstalled only PyTorch as Horovod version hold...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,yukw777,2020-04-26T19:46:42Z,"
 		<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1558>#1558</denchmark-link>
  should fix it for now. Currently there's no way to manually invalidate the cache, so whenever this happens again, we can add a version number to the cache key and bump that up to invalidate the old cache.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1628,ternaus,2020-04-27T00:24:51Z,2020-04-27T11:24:33Z,"Bug in DDP, but not DP modes.","
 Pytorch 1.5
 <denchmark-code>In [3]: pytorch_lightning.__version__                                                                                                                                                                                                                                                                
 Out[3]: '0.7.5rc1'
 
 </denchmark-code>
 
 In DP everything works.
 In DDP fails with:
 <denchmark-code> File ""/home/vladimir/anaconda3/envs/solaris/lib/python3.7/multiprocessing/popen_fork.py"", line 20, in __init__
     self._launch(process_obj)
   File ""/home/vladimir/anaconda3/envs/solaris/lib/python3.7/multiprocessing/popen_spawn_posix.py"", line 47, in _launch
     reduction.dump(process_obj, fp)
   File ""/home/vladimir/anaconda3/envs/solaris/lib/python3.7/multiprocessing/reduction.py"", line 60, in dump
     ForkingPickler(file, protocol).dump(obj)
 _pickle.PicklingError: Can't pickle <class 'torch._C._VariableFunctions'>: it's not the same object as torch._C._VariableFunctions
 </denchmark-code>
 
 	",9604d7bf8994615431af9d86c8de154677237b75,Wojciech Jabłoński,2020-04-27 15:56:20+02:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"154,155,156,157,158,159",154,1.0,ternaus,2020-04-27T00:30:05Z,"
 		can you post a colab? does 0.7.4 work ok?
 		",2.0,ternaus,2020-04-27T00:31:25Z,"
 		<denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#make-model-picklable>https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#make-model-picklable</denchmark-link>
 
 		",3.0,ternaus,2020-04-27T08:29:05Z,"
 		I am getting the same issue.
 
 The model is picklable
 It worked fine on 0.7.2
 It breaks after the update to 0.7.4
 
 My stacktrace:
 <denchmark-code>Traceback (most recent call last):
   File ""./train_stage3.py"", line 254, in <module>
     trainer.fit(model)
   File ""/net/people/plgquinor/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 744, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
   File ""/net/people/plgquinor/venv/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 200, in spawn
     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
   File ""/net/people/plgquinor/venv/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 149, in start_processes
     process.start()
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/process.py"", line 105, in start
     self._popen = self._Popen(self)
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/context.py"", line 284, in _Popen
     return Popen(process_obj)
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py"", line 32, in __init__
     super().__init__(process_obj)
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_fork.py"", line 19, in __init__
     self._launch(process_obj)
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/popen_spawn_posix.py"", line 47, in _launch
     reduction.dump(process_obj, fp)
   File ""/net/software/local/python/3.6.5/lib/python3.6/multiprocessing/reduction.py"", line 60, in dump
     ForkingPickler(file, protocol).dump(obj)
 _pickle.PicklingError: Can't pickle <class 'torch._C._VariableFunctions'>: it's not the same object as torch._C._VariableFunctions
 </denchmark-code>
 
 		",4.0,ternaus,2020-04-27T08:44:11Z,"
 		The model is picklable - the error appears when I try to pickle the Trainer instance.
 [Update] I had found the culprit. ModelCheckpoint is non-picklable due to:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L118>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L118</denchmark-link>
 
 torch.lt, torch.gt etc. being non-picklable. It worked previously when those were np.* ops.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,ternaus,2020-04-27T13:37:11Z,"
 		<denchmark-link:https://github.com/quinor>@quinor</denchmark-link>
  <denchmark-link:https://github.com/ternaus>@ternaus</denchmark-link>
  thanks for bringing this up! turned out to be a bigger deal haha.
 Released 0.7.5 to fix - please skip 0.7.4
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,check_monitor_top_k,"self,current",146,159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1665,tshrjn,2020-04-29T15:59:49Z,2020-05-12T12:53:27Z,Trainer add args doesn't add default root dir,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 
 When using parser = Trainer.add_argparse_args(parser), it's supposed to put all Trainer's arguments in the argparse with default values. Though currently it doesn't add default_root_dir and you get the error:
 
 <denchmark-code>'Namespace' object has no attribute 'default_root_dir'
 </denchmark-code>
 
 It does add default_save_path which is deprecated.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code Sample</denchmark-h>
 
 import argparse
 from pytorch_lightning import Trainer
 
 parser = argparse.ArgumentParser(description='demo')
 parser = Trainer.add_argparse_args(parser)
 args = parser.parse_args()
 
 print(args.default_root_dir)
 A similar unit test could also be made, if not there already.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
         - available:         True
         - version:           10.1
 * Packages:
         - numpy:             1.18.1
         - pyTorch_debug:     False
         - pyTorch_version:   1.4.0
         - pytorch-lightning: 0.7.3
         - tensorboard:       2.2.0
         - tqdm:              4.45.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 -
         - processor:         x86_64
         - python:            3.6.7
         - version:           #75-Ubuntu SMP Tue Oct 1 05:24:09 UTC 2019
 </denchmark-code>
 
 	",9059d21042a5f18fcb18a1792a901e8e62a3b61a,Oliver Neumann,2020-05-12 08:53:26-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"65,66",,1.0,tshrjn,2020-04-30T11:46:49Z,"
 		Did you tried to update to 0.7.5. Maybe it is already solved.
 		",2.0,tshrjn,2020-05-04T07:15:14Z,"
 		Hi <denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>
 , yes updating did resolve this. However, the  arg is now broken. The same demo code above with  gives the same error .
 		",3.0,tshrjn,2020-05-04T08:57:26Z,"
 		What do you mean 'with profiler'? Do you mean Trainer(..., profiler=True)? But you don't initialize a Trainer.
 Running your code or this below didn't crash with any error on my machine.
 import argparse
 from pytorch_lightning import Trainer
 
 parser = argparse.ArgumentParser(description='demo')
 trainer = Trainer(profiler=True)
 parser = trainer.add_argparse_args(parser)
 args = parser.parse_args()
 
 print(args.default_root_dir)
 Maybe you could post the complete error message from the python interpreter.
 		",4.0,tshrjn,2020-05-06T22:12:44Z,"
 		add_argparse_args  is supposed to add the args from trainer to parser. But it doesn't do that for a few args. In this case profiler, previously the issue was for default_root_dir.
 Try the following code by running:
 python demo.py --profiler True or  other possibly accepted way python demo.py --profiler  with the following code:
 import argparse
 from pytorch_lightning import Trainer
 
 trainer = Trainer()
 parser = argparse.ArgumentParser(description='demo')
 parser = trainer.add_argparse_args(parser)
 args = parser.parse_args()
 
 print(args.profiler)
 		",MODIFY,4.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,587,,get_init_arguments_and_types,cls,559,604,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,tshrjn,2020-05-12T03:12:23Z,"
 		Any update?
 		",6.0,tshrjn,2020-05-12T10:43:03Z,"
 		I just created a PR. After looking at the code I found out that add_argparse_args is checking the argument types and is only adding attributes of type str, float, int or bool. The profiler attribute could be of type bool so it should be a bug.
 I saw that get_init_arguments_and_types() is returning profiler as argument but only of type BaseProfiler. After updating typing annotation of profiler argument it worked. Should be available in the next version.
 See PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1794>#1794</denchmark-link>
 
 		",7.0,tshrjn,2020-05-12T21:06:28Z,"
 		A similar issue is with the pickling of the profiler when it's a Profile object & the trainer tries to save the hparams.
 TypeError: can't pickle Profile objects
 Example code:
 import argparse
 from pytorch_lightning import Trainer
 from pytorch_lightning import profiler
 from pl_bolts.models.gans import BasicGAN
 
 trainer = Trainer()
 parser = argparse.ArgumentParser(description='demo')
 parser = trainer.add_argparse_args(parser)
 args = parser.parse_args()
 model = BasicGAN()
 
 trainer = Trainer.from_argparse_args(
         args, profiler=profiler.AdvancedProfiler())
 trainer.fit(model)
 		",8.0,tshrjn,2020-05-13T08:02:45Z,"
 		Can't reproduce your issue with pl version 0.7.6rc1. On my machine your code runs and saves checkpoints without crashing. Also this wouldn't belong to the topic of this issue imo. This would be a bug in the saving routine.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,127,127,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,int,None,None,BaseProfiler,None,bool,bool,bool,bool,False,bool,ProgressBarBase,True,bool,None,str,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,kwargs",87,145,1.0,127,127,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,int,None,None,None,bool,bool,bool,bool,False,bool,ProgressBarBase,True,bool,None,str,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,kwargs",87,145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651",,add_argparse_args,"cls,ArgumentParser",616,682,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1683,hirune924,2020-05-01T03:39:30Z,2020-05-10T17:19:19Z,NeptuneLogger doesn't work with distributed_backend='ddp',"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using NeptuneLogger with distributed_backend='ddp' and running it on a single node with two GPUs, I find an error like this.
 <denchmark-code>Traceback (most recent call last):
   File ""pl.py"", line 146, in <module>
     main()
   File ""pl.py"", line 103, in main
     trainer.fit(model)
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 751, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_processes, args=(model,))
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 200, in spawn
     return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 158, in start_processes
     while not context.join():
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 119, in join
     raise Exception(msg)
 Exception:
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap
     fn(i, *args)
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 370, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 871, in run_pretrain_routine
     self.configure_checkpoint_callback()
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_config.py"", line 54, in configure_checkpoint_callback
     self.logger.name,
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/loggers/neptune.py"", line 267, in name
     return self.experiment.name
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/pytorch_lightning/loggers/neptune.py"", line 230, in experiment
     **self._kwargs)
   File ""/home/hirune/anaconda3/envs/PANDA/lib/python3.7/site-packages/neptune/__init__.py"", line 222, in create_experiment
     raise Uninitialized()
 neptune.exceptions.Uninitialized: You must initialize neptune-client first. For more information, please visit: https://github.com/neptune-ai/neptune-client#initialize-neptune
 </denchmark-code>
 
 And I found a similar error with CommetLogger
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run the following code on a machine with two GPUs.
 This code is a slightly modified version of what was on this page.
 <denchmark-link:https://docs.neptune.ai/integrations/pytorch_lightning.html>https://docs.neptune.ai/integrations/pytorch_lightning.html</denchmark-link>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 from torchvision import transforms
 
 import pytorch_lightning as pl
 
 MAX_EPOCHS=20
 LR=0.01
 BATCHSIZE=32
 CHECKPOINTS_DIR = 'my_models/checkpoints/7'
 
 class CoolSystem(pl.LightningModule):
 
     def __init__(self):
         super(CoolSystem, self).__init__()
         # not the best model...
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_idx):
         # REQUIRED
         x, y = batch
         y_hat = self.forward(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def validation_step(self, batch, batch_idx):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def test_step(self, batch, batch_idx):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'test_loss': F.cross_entropy(y_hat, y)}
 
     def test_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
         tensorboard_logs = {'test_loss': avg_loss}
         return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         # REQUIRED
         # can return multiple optimizers and learning_rate schedulers
         # (LBFGS it is automatically supported, no need for closure function)
         return torch.optim.Adam(self.parameters(), lr=LR)
 
     @pl.data_loader
     def train_dataloader(self):
         # REQUIRED
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
     @pl.data_loader
     def val_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
     @pl.data_loader
     def test_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=BATCHSIZE)
 
 
 from pytorch_lightning.loggers.neptune import NeptuneLogger
 def main():
     neptune_logger = NeptuneLogger(
         api_key=""ANONYMOUS"",
         project_name=""shared/pytorch-lightning-integration"",
         close_after_fit=False,
         experiment_name=""default"",  # Optional,
         params={""max_epochs"": MAX_EPOCHS,
                 ""batch_size"": BATCHSIZE,
                 ""lr"": LR}, # Optional,
         tags=[""pytorch-lightning"", ""mlp""]  # Optional,
     )
     model_checkpoint = pl.callbacks.ModelCheckpoint(filepath=CHECKPOINTS_DIR)
     
     from pytorch_lightning import Trainer
     
     model = CoolSystem()
     trainer = Trainer(max_epochs=MAX_EPOCHS,
                       logger=neptune_logger,
                       checkpoint_callback=model_checkpoint,
                       gpus=-1,
                       distributed_backend='ddp',
                       )
     trainer.fit(model)
     trainer.test(model)
     
     # Get predictions on external test
     import numpy as np
     
     model.freeze()
     test_loader = DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=256)
     
     y_true, y_pred = [],[]
     for i, (x, y) in enumerate(test_loader):
         y_hat = model.forward(x).argmax(axis=1).cpu().detach().numpy()
         y = y.cpu().detach().numpy()
     
         y_true.append(y)
         y_pred.append(y_hat)
     
         if i == len(test_loader):
             break
     y_true = np.hstack(y_true)
     y_pred = np.hstack(y_pred)
     
     # Log additional metrics
     from sklearn.metrics import accuracy_score
     
     accuracy = accuracy_score(y_true, y_pred)
     neptune_logger.experiment.log_metric('test_accuracy', accuracy)
     
     # Log charts
     from scikitplot.metrics import plot_confusion_matrix
     import matplotlib.pyplot as plt
     
     fig, ax = plt.subplots(figsize=(16, 12))
     plot_confusion_matrix(y_true, y_pred, ax=ax)
     neptune_logger.experiment.log_image('confusion_matrix', fig)
     
     # Save checkpoints folder
     neptune_logger.experiment.log_artifact(CHECKPOINTS_DIR)
     
     # You can stop the experiment
     neptune_logger.experiment.stop()
 
 if __name__ == ""__main__"":
         main()
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - GeForce GTX TITAN X
 - GeForce GTX TITAN X
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.18.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.5.0
 - pytorch-lightning: 0.7.5
 - tensorboard:       2.2.1
 - tqdm:              4.42.1
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.6
 - version:           #86-Ubuntu SMP Fri Jan 17 17:24:28 UTC 2020
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",0cb676746568b6ca3c1ef9d9d2879b913f183179,Piotr Łusakowski,2020-05-10 13:19:18-04:00,MODIFY,5,pytorch_lightning\loggers\neptune.py,pytorch_lightning\loggers\neptune.py,1.0,"200,201,203,204,205","198,199,200,201,202,203",1.0,hirune924,2020-05-01T03:40:09Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,hirune924,2020-05-01T07:38:15Z,"
 		<denchmark-link:https://github.com/jakubczakon>@jakubczakon</denchmark-link>
  pls ^^
 		",3.0,hirune924,2020-05-01T07:54:03Z,"
 		Hi <denchmark-link:https://github.com/hirune924>@hirune924</denchmark-link>
  and thanks for raising it!
 I've already notified the dev team and <denchmark-link:https://github.com/pitercl>@pitercl</denchmark-link>
  will get back to you once we have a solution.
 		",4.0,hirune924,2020-05-03T22:14:44Z,"
 		<denchmark-link:https://github.com/jakubczakon>@jakubczakon</denchmark-link>
  I assume that this is also Neptune issue only, not related to PL, right?
 		",MODIFY,3.0,tests\loggers\test_neptune.py,tests\loggers\test_neptune.py,1.0,"34,35,36,37,38,40,41,44,45,48,49,52,53,56,57,60,61,64,67,70,71,74","34,37,38,41,42,45,46,49,52,55,56,59",test_neptune_additional_methods,neptune,34,74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,hirune924,2020-05-04T06:02:40Z,"
 		<denchmark-link:https://github.com/hirune924>@hirune924</denchmark-link>
  mentioned:
 
 And I found a similar error with CommetLogger
 
 So I think it may be a more general problem <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 ,
 		",6.0,hirune924,2020-05-04T16:15:43Z,"
 		Hi <denchmark-link:https://github.com/hirune924>@hirune924</denchmark-link>
 , <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , just a quick update on this: from my initial tests, our Neptune logger and multiprocessing don't mix well. Tomorrow I'll dig a bit deeper into that and see if/how it can be remedied.
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , I don't know yet if this is more on Neptune side or PL side.
 		",7.0,hirune924,2020-05-06T13:15:46Z,"
 		Another quick update: I have a solution to this and will create a PR with a fix today or tomorrow.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__getstate__,self,198,206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"26,29,30","26,29,30",test_neptune_offline,neptune,26,30,1.0,"13,15,16,17,18,19,20,21,22","13,14,16,17,21",test_neptune_online,neptune,12,22,1.0,258,,name,self,257,261,1.0,"361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380",,_create_or_get_experiment,self,361,380,1.0,265,264,version,self,264,268,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"220,221,223,224","224,225",experiment,self,209,225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1687,drozzy,2020-05-01T09:48:04Z,2020-06-30T20:59:36Z,name 'IProgress' is not defined,"
 name 'IProgress' is not defined when running from Jupyter notebook.
 	",1a54ed6ad9f1faf8ac58bbded4b71e4dd18246d6,Oliver Neumann,2020-06-30 16:59:35-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"9,10,11,12",23,1.0,drozzy,2020-05-02T08:28:09Z,"
 		Did you try to install ipywidgets?
 <denchmark-link:https://ipywidgets.readthedocs.io/en/latest/user_install.html>https://ipywidgets.readthedocs.io/en/latest/user_install.html</denchmark-link>
 
 		",2.0,drozzy,2020-05-02T10:46:20Z,"
 		should we try catch this? and change usage + warn user when this happens?
 		",3.0,drozzy,2020-05-02T15:23:49Z,"
 		Isn't it only tqdm.auto which uses ipywidgets to display a progress bar? That we only use here:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/search?q=tqdm.auto&unscoped_q=tqdm.auto>https://github.com/PyTorchLightning/pytorch-lightning/search?q=tqdm.auto&unscoped_q=tqdm.auto</denchmark-link>
 
 Maybe it should be changed in tqdm.auto to not take IProgess if ipywidgets is not installed. There are already some checks:
 <denchmark-link:https://github.com/tqdm/tqdm/blob/master/tqdm/autonotebook.py>https://github.com/tqdm/tqdm/blob/master/tqdm/autonotebook.py</denchmark-link>
 
 But I had a similar problem when using jupyter lab and I had to install nodejs for it (which I wanted to avoid) but found no way of disable tqdm.auto taking IProgress. In my case it didn't crashed but no progress bar were displayed. If someone knows a way of overwriting the tqdm.auto selection let me know. Otherwise I think the checks should be better or there should be a way of overwriting the tqdm.auto selection. (In my opinion)
 		",4.0,drozzy,2020-05-28T09:48:35Z,"
 		I am doing:
 from tqdm import tqdm
 and it seems to work.
 In addition I had to install ipywidgets.
 Perhaps pytorch-lightning should depend on ipywidgets? It seems like jupyter is a pretty common use case for people.
 		",MODIFY,0.0,pytorch_lightning\callbacks\progress.py,pytorch_lightning\callbacks\progress.py,0.0,"8,11,12,13,14,15,16,17",10,,,,,MODIFY,0.0,pytorch_lightning\trainer\lr_finder.py,pytorch_lightning\trainer\lr_finder.py,0.0,"4,5,13,14,15,16,17,18,19,20","11,12",,,,,,,,,,,,,,,,,,,,,,,5.0,drozzy,2020-06-26T14:02:38Z,"
 		I do not think we want to add  as dependency bu we can do similar check on our side too
 <denchmark-link:https://github.com/olineumann>@olineumann</denchmark-link>
  mind send PR?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1697,MrinalJain17,2020-05-01T23:14:20Z,2020-05-14T06:36:46Z,[Examples] The UNet model has some bugs,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The UNet model definition has some bugs pertaining to bilinear interpolation.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 
 
 
 
 
 
 pytorch-lightning/pl_examples/models/unet.py
 
 
         Lines 35 to 37
       in
       2950f66
 
 
 
 
 
 
  for _ in range(num_layers - 1): 
 
 
 
  layers.append(Up(feats, feats // 2), bilinear) 
 
 
 
  feats //= 2 
 
 
 
 
 
 In the code above, there seems to be a typo. The bilinear flag should be passed to the function Up(). It has instead been passed to the .append() method of the list.
 
 
 
 
 
 
 pytorch-lightning/pl_examples/models/unet.py
 
 
         Lines 101 to 104
       in
       2950f66
 
 
 
 
 
 
  if bilinear: 
 
 
 
  self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) 
 
 
 
  else: 
 
 
 
  self.upsample = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2) 
 
 
 
 
 
 The number of channels once the input passes through either one of these layers is different. For ""bilinear"", the number of channels remains the same, whereas they decrease to half if a ConvTranspose2d is used. This gives an error in the network's .forward() method.
 I wanted to directly use the model for some other application, but not sure how issue 2 should be solved. Maybe use a 1x1 convolution to reduce the channels to half?
 	",cf2d32d0a6c757aad39c36b621a646ed3a24619a,Nand Dalal,2020-05-14 02:36:45-04:00,MODIFY,1,pl_examples\domain_templates\semantic_segmentation.py,pl_examples\domain_templates\semantic_segmentation.py,1.0,168,168,1.0,MrinalJain17,2020-05-01T23:14:59Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,MrinalJain17,2020-05-03T21:42:31Z,"
 		<denchmark-link:https://github.com/MrinalJain17>@MrinalJain17</denchmark-link>
  mind draft a PR?
 		",,,,,,,,,MODIFY,0.0,pl_examples\models\unet.py,pl_examples\models\unet.py,0.0,36,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,validation_epoch_end,"self,outputs",167,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1721,Borda,2020-05-03T21:50:02Z,2020-05-04T11:13:13Z,instable GitHub action cache,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 there is some issue with GH action and caching as it is randomly failing with using Horovod
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1709#issuecomment-623178716>#1709 (comment)</denchmark-link>
 
 	",281a73ccf7a22cdf004755f1f7b4aead40b12d84,Jirka Borovec,2020-05-04 07:13:11-04:00,MODIFY,0,.github\workflows\ci-testing.yml,.github\workflows\ci-testing.yml,0.0,"74,76,102,108,109","74,76,102,108,109",1.0,Borda,2020-05-03T21:57:09Z,"
 		If the goal is to keep the cache to speed things up, my suggestion would be to add a couple checks in the ""Install dependencies"" step in the workflow:
 
 Check the version of torch before installing requirements.txt, save in Bash variable.
 Install requirements.txt
 Check the version of torch after, save in Bash variable.
 If torch version before != version after, then uninstall Horovod.
 Install requirements-extra.txt.
 
 This way, we should be able to leverage the cache to speed things up when nothing changes, without running into incompatibilities when the torch version is upgraded.
 I can put together a PR for this.
 		",2.0,Borda,2020-05-03T22:01:44Z,"
 		<denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  PR would be great!
 I am a bit suspicions as you mentioned that the cache is not loaded properly so thinking about opening an issue with <denchmark-link:https://github.com/actions/cache>https://github.com/actions/cache</denchmark-link>
 
 		",3.0,Borda,2020-05-04T06:35:31Z,"
 		I see the issue now, the problem is that the min requirement of  is the actual one so the loaded cache even with  is satisfied
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1709/checks?check_run_id=641571597>https://github.com/PyTorchLightning/pytorch-lightning/pull/1709/checks?check_run_id=641571597</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1751,marcopodda,2020-05-07T06:12:51Z,2020-05-26T23:06:06Z,Early Stopping behavior,"
 Hi there,
 thanks for the great library (I am using 0.7.5.). I am not following the bug report template as I'm not sure this is indeed a bug, or simply I cannot understand how early stopping is implemented. My code looks as follows:
 <denchmark-code>    early_stop_callback = EarlyStopping(
         monitor='val_acc',
         min_delta=0.0,
         patience=80,
         verbose=True,
         mode=self.mode
     )
 
     trainer = Trainer(
         early_stop_callback=early_stop_callback,
         auto_select_gpus=True,
         max_epochs=200,
         terminate_on_nan=True,
         show_progress_bar=True,
         fast_dev_run=False,
         gpus=1
     )
 </denchmark-code>
 
 As I understand it, the model should perform early stopping after AT LEAST 80 epochs have passed without improvement on the validation accuracy. However, in my case, early stopping happened at epoch 75. Is this how it should be?
 As I said, I am not sure this is actually a bug or a choice (perhaps early stopping is implemented at the batch level?). If it is indeed a bug, I will work a reproducible example. Thank you!
 	",3af4994d5a84bc80738b50983b4b42c3eb946433,Mateusz Pieniak,2020-05-26 19:06:06-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"41,42",,1.0,marcopodda,2020-05-07T06:13:44Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,marcopodda,2020-05-07T08:07:26Z,"
 		I would expect that it should iterate for at least 80 epochs, too. So to me, it looks like a bug or some kind of unexpected behavior. Would be great to figure it out!
 		",3.0,marcopodda,2020-05-07T09:54:37Z,"
 		Ok then, I'll work out some notebook to see if I can reproduce.
 		",4.0,marcopodda,2020-05-07T14:16:58Z,"
 		Thanks <denchmark-link:https://github.com/mateuszpieniak>@mateuszpieniak</denchmark-link>
 
 Here is a working example. As you can see, it stops at epoch 41 even though patience is set to 80.
 <denchmark-link:https://github.com/marcopodda/pl-es-example/blob/master/ES%20example.ipynb>https://github.com/marcopodda/pl-es-example/blob/master/ES%20example.ipynb</denchmark-link>
 
 		",MODIFY,2.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,,"794,795,796",call_early_stop_callback,self,794,796,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,marcopodda,2020-05-07T16:00:54Z,"
 		It is definitely a bug. I discovered that EarlyStopping.on_epoch_end is executed twice within one epoch, meaning that patience=160 should solve your issue temporarily.
 In the file training_loop.py:
 First call:
 <denchmark-code>            if self.fast_dev_run or should_check_val:
                 self.run_evaluation(test_mode=self.testing)
                 self.call_checkpoint_callback()
                 self.call_early_stop_callback()
 </denchmark-code>
 
 Second call:
 <denchmark-code>                # TODO wrap this logic into the callback
                 if self.enable_early_stop:
                     if (met_min_epochs and met_min_steps) or self.fast_dev_run:
                         should_stop = self.early_stop_callback.on_epoch_end(self, self.get_model())
                         # stop training
                         stop = should_stop and met_min_epochs
                         if stop:
                             self.run_training_teardown()
                             return
 </denchmark-code>
 
 		",6.0,marcopodda,2020-05-08T09:45:12Z,"
 		I upgraded to the bleeding edge version yesterday and can confirm that this started happening to me too. I didn't have an issue before I upgraded (I think I was on 0.7.3 before?)
 		",7.0,marcopodda,2020-05-10T23:57:11Z,"
 		Yep we ran into this as well. It is called once in trainer and once in the on epoch end callback.
 		",8.0,marcopodda,2020-05-11T20:45:55Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  <denchmark-link:https://github.com/ricpruss>@ricpruss</denchmark-link>
  mind send a fix, PR?
 		",9.0,marcopodda,2020-05-11T21:44:26Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  Well, I would love to make my first PL's PR if that's okay? 
 		",10.0,marcopodda,2020-05-11T21:48:58Z,"
 		<denchmark-link:https://github.com/mateuszpieniak>@mateuszpieniak</denchmark-link>
  sure go ahead! 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"455,501",run_training_epoch,self,380,509,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1829,williamFalcon,2020-05-14T01:52:04Z,2020-05-14T21:56:12Z,Allow boolean flags to work without passing True,"
 We tried to fix this but it's still broken
 This fails when adding args to argparse automatically...
 <denchmark-code>--auto_lr_find
 </denchmark-code>
 
 Instead we have to do:
 <denchmark-code>--auto_lr_find True
 </denchmark-code>
 
 which is not great
 	",bee0392c372936567b2bbe6e7ed5828cb3078354,Jirka Borovec,2020-05-14 17:56:11-04:00,MODIFY,12,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,135,135,1.0,williamFalcon,2020-05-14T14:14:42Z,"
 		This only seems to be a problem with the auto_lr_find flag, since it is the only argument that is a union between the 4 allowed types (str, float, int, bool) in the add_argparse_args. So either this needs to be handled as a special case in the argparser or remove one of its allowed value (str or bool).
 		",2.0,williamFalcon,2020-05-14T15:10:24Z,"
 		it's a problem for many flags haha. The pattern is bool or the callback or string.
 So, early stopping, checkpoint, etc... all have this problem.
 And for the batch size and lr finder stuff we have a few options
 auto_lr_find=True
 auto_lr_find='some.path'
 But True is getting parsed as a string which breaks everything.
 		",3.0,williamFalcon,2020-05-14T15:11:59Z,"
 		I think we need to solve this before 0.7.6 release as this is causing a lot of unexpected behaviors <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 .
 Basically i think we need to:
 
 allow the flag to be passed in with set_true --my_flag (this becomes a bool with True)
 but if there is something else, then treat that thing as a string --my_flag this_is_a_string
 
 		",4.0,williamFalcon,2020-05-14T15:25:54Z,"
 		I think that the complication comes with
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 131
       in
       236c137
 
 
 
 
 
 
  auto_lr_find: Union[bool, str] = False, 
 
 
 
 
 
 so you cannot in argparse define to be store_true  and optional string at the same time
 		",MODIFY,1.0,tests\trainer\test_trainer_cli.py,tests\trainer\test_trainer_cli.py,1.0,"102,103,104,105,106,107,108,109,110,111,112",,test_argparse_args_parsing,"cli_args,expected",102,112,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,williamFalcon,2020-05-14T15:58:32Z,"
 		well this is the minimal solution
 import argparse
 p = argparse.ArgumentParser()
 p.add_argument(""--a"", type=str, default=False, nargs=""?"")
 v = vars(p.parse_args())
 v = {k: True if v is None else v for k, v in v.items()}
 print(v)
 gives:
 
 python sample.py --a  >> {'a': True}
 python sample.py  >> {'a': False}
 python sample.py --a park  >> {'a': 'park'}
 
 		",6.0,williamFalcon,2020-05-14T17:29:45Z,"
 		let's merge this asap for 0.7.6
 		",7.0,williamFalcon,2020-05-14T17:59:48Z,"
 		
 let's merge this asap for 0.7.6
 
 ready to review... ^^ <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,int,None,None,BaseProfiler,None,bool,bool,bool,bool,False,bool,ProgressBarBase,True,bool,None,str,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,kwargs",87,145,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"669,670",669,add_argparse_args.allowed_type,x,669,670,1.0,699,703,_allowed_type,x,699,703,1.0,705,703,arg_default,x,703,707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,705,,_arg_default,x,705,709,1.0,135,135,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,None,float,int,int,int,str,None,bool,None,None,int,float,int,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,add_row_log_interval,None,int,bool,None,int,None,None,BaseProfiler,None,bool,bool,bool,bool,False,bool,ProgressBarBase,True,bool,str,False,str,default_save_path,gradient_clip,nb_gpu_nodes,max_nb_epochs,min_nb_epochs,use_amp,show_progress_bar,nb_sanity_val_steps,kwargs",87,145,1.0,"712,713,714,715,716",,parse_argparser,ArgumentParser,712,716,1.0,"711,712,713,714,715",710,from_argparse_args,"cls,args,kwargs",710,715,1.0,676,"676,677",add_argparse_args.use_type,x,676,677,1.0,"719,720,722,723,724,725,726,727,728,729",,from_argparse_args,"cls,Namespace,kwargs",719,733,1.0,699,697,allowed_type,x,697,701,1.0,"666,667,668,669,670,671,672,673,674,675,676,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695","666,667,668,669,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,697",add_argparse_args,"cls,ArgumentParser",618,697,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1850,binshengliu,2020-05-16T05:36:55Z,2020-05-19T06:39:20Z,lr_find doesn't return the correct suggestion if some losses are nan,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 lr_finder doesn't return the correct suggestion if some losses are nan. The returned loss is the one corresponding to the nan value, which is very big in my case.
 <denchmark-link:https://user-images.githubusercontent.com/441707/82111685-3513ea00-978a-11ea-9de2-f436d2fb1750.png></denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 This depends on the dataset. Please see the code sample.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 I believe this is caused by numpy. The related code should be 
 
 
 pytorch-lightning/pytorch_lightning/trainer/lr_finder.py
 
 
          Line 325
       in
       b84b024
 
 
 
 
 
 
  min_grad = (np.gradient(np.array(loss))).argmin() 
 
 
 
 
 
 example_losses = [0.90, 0.89, 0.87, 0.86, 0.85, 0.84]
 print(np.gradient(example_losses).argmin())
 example_losses = [0.90, 0.89, 0.87, 0.86, 0.85, 0.84, float('nan')]
 print(np.gradient(example_losses).argmin())
 Output:
 <denchmark-code>1
 5
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Return the correct suggested loss.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           10.2
 
 
 Packages:
 
 numpy:             1.18.4
 pyTorch_debug:     False
 pyTorch_version:   1.5.0
 pytorch-lightning: 0.7.6
 tensorboard:       2.2.0
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:
 python:            3.7.6
 version:           #1 SMP Debian 4.19.118-2 (2020-04-29)
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 NA
 	",ac76dfcf62a672c84f843f2e3158e4c6262776da,Rohit Gupta,2020-05-19 08:39:19+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"17,18",,1.0,binshengliu,2020-05-16T05:37:40Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,binshengliu,2020-05-16T08:04:29Z,"
 		Good catch. I guess that we can not do much about how  behave but we can filter  values before doing the calculation. <denchmark-link:https://github.com/binshengliu>@binshengliu</denchmark-link>
  are you up for doing a PR?
 		",3.0,binshengliu,2020-05-16T10:53:58Z,"
 		Filtering out nan would be a reasonable approach. Locally I just reset nan to inf also to avoid dealing with indexes.
 Sorry I'm quite inundated with my own projects recently and may not have enough time to shape a proper PR.
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\lr_finder.py,pytorch_lightning\trainer\lr_finder.py,1.0,"324,325,326","324,325",suggestion,"self,int,int",313,331,MODIFY,3.0,tests\trainer\test_lr_finder.py,tests\trainer\test_lr_finder.py,1.0,127,127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_call_to_trainer_method,tmpdir,117,136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202",,test_suggestion_with_non_finite_values,tmpdir,184,202,1.0,173,173,test_suggestion_parameters_work,tmpdir,164,181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1857,Varal7,2020-05-16T23:37:00Z,2020-05-17T13:17:29Z,Can not use Trainer.test() if train and val dataloaders are not defined,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When the model does not define train_dataloader and no val_dataloader, we can not use trainer.test(model, test_dataloaders=test_dl).
 The configuration checks fail with a MisconfigurationException.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>model = ...  # a model with no `train_dataloader`, `val_dataloader` defined
 test_dl = ... # a dataloader
 trainer = pl.Trainer()
 trainer.test(model, test_dataloaders=test_dl)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 We expect the testing loop to execute.
 	",1a797bdad5df6d4e7ccc586ddeb93dccb2a9648a,Victor Quach,2020-05-17 16:30:20-04:00,MODIFY,1,tests\trainer\test_checks.py,tests\trainer\test_checks.py,1.0,"124,125,126,127,128,129,130,131,132,133,134",,1.0,Varal7,2020-05-16T23:37:37Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Varal7,2020-05-17T00:07:23Z,"
 		Whoops, sorry I tried searching for the issue / PR but couldn't find the relevant ones. Looks like there are already good ideas in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1804>#1804</denchmark-link>
 , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1720>#1720</denchmark-link>
 , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1754>#1754</denchmark-link>
 , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1804>#1804</denchmark-link>
 .
 		",3.0,Varal7,2020-05-17T13:17:29Z,"
 		Fixed in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1804>#1804</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_wrong_test_settigs,tmpdir,73,134,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1878,tullie,2020-05-18T23:00:34Z,2020-06-13T16:00:15Z,prepare_data called multiple times per node for slurm and elastic training,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Slurm and elastic training create the training processes per node outside of the lightning context. This means that when the fit function calls prepare_data, the assumption that it's only being called on proc 0 is broken and it gets called for each process.
 This is an issue computational reasons (e.g. downloading a whole dataset) and for training stability if the data preparation process isn't deterministic.
 See calling code here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 825
       in
       7c7e50c
 
 
 
 
 
 
  model.prepare_data() 
 
 
 
 
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Add print statements to prepare_data
 Train a lightning model with either slurm or elastic training
 See that it's being called multiple times.
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Expected prepare_data to only be called once per node.
 	",5fd01b0e68a6087908ac0bcefd4edaeddfb0e248,William Falcon,2020-06-13 12:00:14-04:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,232,232,1.0,tullie,2020-06-08T11:01:00Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  any idea how to fix?
 		",,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,102,102,print,"self,args,kwargs",86,103,MODIFY,0.0,pytorch_lightning\trainer\__init__.py,pytorch_lightning\trainer\__init__.py,0.0,"588,589,590,591,592,593,594,595,596,597,598,599,600",,MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,163,163,_get_distributed_sampler,"self,dataloader",150,165,MODIFY,5.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"169,170",,is_global_zero,self,169,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",230,278,1.0,510,505,save_spawn_weights,"self,model",504,512,1.0,524,,load_spawn_weights,"self,original_model",514,535,MODIFY,2.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"186,187","186,187",tpu_train,"self,tpu_core_idx,model",173,205,1.0,"292,293","292,293",horovod_train,"self,model",248,303,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,,,,,,,,1.0,378,382,run_evaluation,"self,bool",337,406,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,72,72,1.0,925,925,init_ddp_connection,"self,int,int,bool",923,927,1.0,925,925,init_ddp_connection,"self,int,int,bool",923,927,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log_metrics,"self,metrics,grad_norm_dic,step",44,74,,,,,,,,MODIFY,5.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"491,492",,is_global_zero,self,491,492,1.0,949,931,run_pretrain_routine,"self,LightningModule",905,1005,1.0,545,,get_init_arguments_and_types,cls,521,567,1.0,"886,887,888,889,890,891",,can_prepare_data,self,886,891,1.0,126,,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,False,None,None,float,int,int,int,str,None,bool,None,None,int,float,int,float,1,int,bool,int,int,1,int,int,None,None,float,float,float,float,int,int,None,int,bool,None,int,None,None,BaseProfiler,None,bool,bool,bool,bool,False,bool,bool,str,False,bool,str,None,use_amp,show_progress_bar",79,130,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,265,265,save_checkpoint,"self,filepath,bool",262,274,1.0,216,216,sig_handler,"self,signum,frame",215,236,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,483,484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"307,308,309,310,311,312",,determine_local_rank,self,307,312,1.0,"437,438,442,443,447,453,454,455,456,457,458,459,460,473","426,430,434,440,453,454,491",ddp_train,"self,process_idx,model,is_master,proc_offset",420,502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_training_epoch,self,404,531,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1889,HansBambel,2020-05-19T10:28:37Z,2020-05-19T17:16:27Z,trainer.scale_batch_size() throws exception due to LRScheduler,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I tried finding the biggest possible batch_size for my training, but PL raises a MisconfigurationException saying that my LRScheduler (ReduceLROnPlateau) is conditioned on a metric that is only available after validation_epoch_end. The available metrics are: loss, val_loss.
 I assume the LRScheduler requires a metric from the training loop for this to work? Why is this neccessary?
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Have a model with a metric that only exists in validation_epoch_end
 Have a LRScheduler which monitors that metric
 Use trainer.scale_batch_size
 See error
 
 <denchmark-code>File ""C:\ProgramData\Anaconda3\envs\ml\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 779, in update_learning_rates
     raise MisconfigurationException(
 pytorch_lightning.utilities.exceptions.MisconfigurationException: ReduceLROnPlateau conditioned on metric meanIoU which is not available. Available metrics are: loss,train_loss. Condition can be set using `monitor` key in lr scheduler dict
 
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>trainer = pl.Trainer(gpus=hparams.gpus)
 new_batch_size = trainer.scale_batch_size(net, mode='binsearch', init_val=8)
 </denchmark-code>
 
 and in my model:
 <denchmark-code>    def configure_optimizers(self):
         opt = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
         scheduler = {
          'scheduler': optim.lr_scheduler.ReduceLROnPlateau(opt, mode=""max"", factor=0.5, patience=5),
          'monitor': 'meanIoU',  # Default: val_loss
         }
         return [opt], [scheduler]
 
     def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x[""val_loss""] for x in outputs]).mean()
         iou_class, mean_iou = self.iou_metric.value()
         mean_iou = torch.tensor(mean_iou)
         self.iou_metric.reset()
         logs = {""val_loss"": avg_loss, ""meanIoU"": mean_iou}
         return {""meanIoU"": mean_iou, ""log"": logs,
                 ""progress_bar"": {""val_loss"": avg_loss, ""meanIoU"": mean_iou}}
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 No Exception and the maximum batch_size for my model.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 GeForce RTX 2070 SUPER
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.1
 pyTorch_debug:     False
 pyTorch_version:   1.4.0
 pytorch-lightning: 0.7.6
 tensorboard:       2.1.0
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Windows
 architecture:
 
 64bit
 WindowsPE
 
 
 processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
 python:            3.8.2
 version:           10.0.18362
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",3459a546672303204a4ae6efcc2613a90f003903,Kevin Trebing,2020-05-19 13:16:26-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"354,355,356","350,351,352",1.0,HansBambel,2020-05-19T11:52:22Z,"
 		Just to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?
 		",2.0,HansBambel,2020-05-19T13:05:33Z,"
 		
 Just to be sure, this error only happens when you run the .scale_batch_size(...) method and not when you run .fit(...)?
 
 Correct
 		",3.0,HansBambel,2020-05-19T13:14:53Z,"
 		The problem here is that learning rates are updated right after the training_epoch has returned, even when we have reached max_steps and should just run the training_teardown
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 348 to 355
       in
       ac76dfc
 
 
 
 
 
 
  self.run_training_epoch() 
 
 
 
  
 
 
 
  # update LR schedulers 
 
 
 
  self.update_learning_rates(interval='epoch') 
 
 
 
  
 
 
 
  if self.max_steps and self.max_steps == self.global_step: 
 
 
 
  self.run_training_teardown() 
 
 
 
  return 
 
 
 
 
 
 We could probably just switch order of the two statements and this will be solved.
 		",4.0,HansBambel,2020-05-19T13:18:44Z,"
 		Great that you found the reason for that behavior already!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,HansBambel,2020-05-19T13:19:50Z,"
 		<denchmark-link:https://github.com/HansBambel>@HansBambel</denchmark-link>
  can you locally try if this works for you, and if it does, would you be up for doing a PR?
 		",6.0,HansBambel,2020-05-19T13:46:52Z,"
 		
 @HansBambel can you locally try if this works for you, and if it does, would you be up for doing a PR?
 
 I'll try it out!
 		",7.0,HansBambel,2020-05-19T15:10:37Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  it worked! I created a PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1891>#1891</denchmark-link>
  for it.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,train,self,302,377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
189,ExpectationMax,2019-09-03T15:48:23Z,2019-09-05T02:31:13Z,Logging of GPU memory utilization can significantly slow down training,"
 When training using GPUs pytorch_lightning automatically logs the GPU memory utilization during training. This is a useful feature, but can severely impact performance dependent on the speed of the nvidia-smi call.
 On our particular cluster (University-scale hpc cluster based on IBM LSF), this leads to a performance decrease of almost 10 fold when training on GPU vs. CPU.
 Describe the bug
 Logging of GPU memory can have a severe impact on training performance.
 
 Remove gpu memory logging by commenting out the lines 937 to 939 in pytorch_lightning/models/trainer.py
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/models/trainer.py#L937-L939>see here</denchmark-link>
 
 Expected behavior
 Logging of GPU memory utilization should not impede performance (by running the nvidia-smi call in the background) or it should at least be possible to deactivate it in case performance issues arise.
 Desktop (please complete the following information):
 
 OS: Ubuntu Linux 16.04, NVIDIA Geforce GTX 1080
 
 	",dac41030d48acbfecdf7c083b8e7b00f3fd9be06,Max Horn,2019-09-04 10:43:46-04:00,MODIFY,0,docs\Trainer\Logging.md,docs\Trainer\Logging.md,0.0,"17,18,19,20,21,22,23,24",,1.0,ExpectationMax,2019-09-05T02:31:12Z,"
 		merged
 		",,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\models\trainer.py,pytorch_lightning\models\trainer.py,1.0,66,,__init__,"self,experiment,early_stop_callback,checkpoint_callback,gradient_clip,cluster,process_position,current_gpu_name,nb_gpu_nodes,gpus,log_gpu_memory,show_progress_bar,overfit_pct,track_grad_norm,check_val_every_n_epoch,fast_dev_run,accumulate_grad_batches,max_nb_epochs,min_nb_epochs,train_percent_check,val_percent_check,test_percent_check,val_check_interval,log_save_interval,add_log_row_interval,distributed_backend,use_amp,print_nan_grads,print_weights_summary,amp_level,nb_sanity_val_steps",56,86,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,942,937,run_tng_epoch,self,892,969,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1898,williamFalcon,2020-05-20T04:02:38Z,2020-05-25T11:43:57Z,batch size finder does not recognize flag and seems to download often,"
 the --auto_scale_batch_size flag requires a string. But it should also just work with set_true
 
 ie: support this case:
 
 <denchmark-code>--auto_scale_batch_size
 </denchmark-code>
 
 
 it seems to trigger data downloads more frequently?
 
 <denchmark-code>Files already downloaded and verified
 Files already downloaded and verified
 Files already downloaded and verified
 Files already downloaded and verified
 Files already downloaded and verified
 Files already downloaded and verified
 Batch size 2 succeeded, trying batch size 4
 Files already downloaded and verified
 </denchmark-code>
 
 	",a34eb9e169622fe91fdf4d98560b65b2f2b5c8d0,Nicki Skafte,2020-05-25 07:43:56-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"35,36",,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"404,827,828,829,836,841",826,,,,,MODIFY,1.0,tests\trainer\test_lr_finder.py,tests\trainer\test_lr_finder.py,1.0,"204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224",,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153",,test_logger_reset_correctly,tmpdir,133,153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_logger_reset_correctly,tmpdir,204,224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1899,binshengliu,2020-05-20T06:25:58Z,2020-06-23T15:21:25Z,Incorrect number of batches when multiple test loaders are used and test_percent_check is specified,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When there are multiple test dataloaders and test_percent_check is specified. The estimated total batches are incorrect and progress bar doesn't show properly.
 For example, when I specify two dataloaders each of which has 100 batches and test_percent_check=0.1. The expected total batches are 200*0.1=20. But actually, 40 batches are run.
 At this line, num_batches is the global number of batches and will be assigned to self.num_test_batches. 
 
 
 pytorch-lightning/pytorch_lightning/trainer/data_loading.py
 
 
          Line 243
       in
       3459a54
 
 
 
 
 
 
  num_batches = int(num_batches * percent_check) 
 
 
 
 
 
 while in the evaluation loop, max_batches is regarded as the number of batches for one data loader.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
          Line 262
       in
       3459a54
 
 
 
 
 
 
  if batch_idx >= max_batches: 
 
 
 
 
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Return multiple dataloaders from test_dataloaders()
 Specify test_percent_check.
 Run trainer.test()
 Observe expected_batches * num_loaders be run. The progress bar also fails to show progress after expected_batches as it exceeds its specified total steps.
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Run correct number of batches.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           10.2
 
 
 Packages:
 
 numpy:             1.18.4
 pyTorch_debug:     False
 pyTorch_version:   1.5.0
 pytorch-lightning: 0.7.6
 tensorboard:       2.2.0
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:
 python:            3.7.6
 version:           #1 SMP Debian 4.19.118-2 (2020-04-29)
 
 
 
 	",e085e93dd303e80af2e9a5fe4aa392055c831114,Adrian Wälchli,2020-06-23 11:21:24-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"21,22",,1.0,binshengliu,2020-05-20T12:33:46Z,"
 		Just had a look at this. The problem is in the trainer as you say, not the progress bar.
 There are two loops, the outer runs through the number of dataloaders and the inner loop runs through each.
 so the max batches should be the number of batches to run in each dataloader, not totally.
 We can easily fix this.
 There should really be a test. There seems to be no test that checks that *_percent_check works with the correct amount of data. we should definitely have these tests.
 		",2.0,binshengliu,2020-05-20T22:19:23Z,"
 		Same case might be hapenning with val_dataloaders. max_batches should be a list I suggest.
 		",3.0,binshengliu,2020-05-21T04:25:02Z,"
 		That's true yes, I agree, because they could have different length.
 		",4.0,binshengliu,2020-05-21T17:53:56Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Anyone working on this or should I submit a PR? Need this to be fixed for a personal project debugging and testing.
 		",MODIFY,0.0,pytorch_lightning\trainer\__init__.py,pytorch_lightning\trainer\__init__.py,0.0,"459,460,478,479",,,,,,MODIFY,0.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,0.0,,"290,291",MODIFY,2.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"225,226,227,228,229,230","225,229,230",_evaluate,"self,LightningModule,int,bool",225,230,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,226,226,,,,,5.0,binshengliu,2020-05-21T17:59:23Z,"
 		if you like, that would help us a lot .)
 I could help with the tests if you need help :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,0.0,,211,,,,,,,,,,,,MODIFY,3.0,tests\base\model_test_dataloaders.py,tests\base\model_test_dataloaders.py,1.0,"225,226,227,228,229,230,231,235,236,237,238,254,255,256,257","225,229,230,231",_evaluate,"self,LightningModule,dataloaders,bool",225,338,1.0,"22,23,24,25",,test_dataloader__multiple_mixed_length,self,22,25,1.0,10,10,dataloader,"self,args,kwargs",10,11,MODIFY,2.0,tests\base\model_test_epoch_ends.py,tests\base\model_test_epoch_ends.py,1.0,10,10,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_epoch_end,"self,outputs",8,39,1.0,43,43,test_epoch_end__multiple_dataloaders,"self,outputs",41,75,MODIFY,2.0,tests\base\model_utilities.py,tests\base\model_utilities.py,1.0,"9,10","9,10",dataloader,"self,bool,int",9,18,1.0,"9,10","9,10",dataloader,"self,train",9,18,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,tests\base\model_valid_dataloaders.py,tests\base\model_valid_dataloaders.py,1.0,10,10,dataloader,"self,args,kwargs",10,11,1.0,"16,17,18,19",,val_dataloader__multiple_mixed_length,self,16,19,1.0,10,10,dataloader,"self,bool",10,11,,,,,,,,MODIFY,2.0,tests\base\model_valid_epoch_ends.py,tests\base\model_valid_epoch_ends.py,1.0,31,31,MODIFY,1.0,tests\test_deprecated.py,tests\test_deprecated.py,1.0,"143,156","143,156",test_tbd_remove_in_v1_0_0_model_hooks,,130,157,,,,,,,,MODIFY,4.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,92,92,test_multiple_val_dataloader,tmpdir,86,112,1.0,"263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293",,test_dataloaders_with_limit_percent_batches,"tmpdir,limit_train_batches,limit_val_batches,limit_test_batches",263,293,1.0,"304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326",,test_dataloaders_with_limit_num_batches,"tmpdir,limit_train_batches,limit_val_batches,limit_test_batches",304,326,1.0,227,227,test_multiple_dataloaders_passed_to_fit,"tmpdir,ckpt_path",222,251,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,10,10,dataloader,"self,bool",10,11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,validation_epoch_end_multiple_dataloaders,"self,outputs",31,59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,31,31,validation_epoch_end__multiple_dataloaders,"self,outputs",31,59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1916,jeremyjordan,2020-05-21T03:19:41Z,2020-07-09T11:10:31Z,Trainer.parse_argparser does not yield sensible default for default_root_dir,"
 <denchmark-h:h3>🐛 Bug</denchmark-h>
 
 Using Trainer.parse_argparser returns True for default_root_dir, however, a string is expected.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 <denchmark-code>>>> from pytorch_lightning import Trainer
 >>> from argparse import ArgumentParser, Namespace
 >>> parser = ArgumentParser(add_help=False)
 >>> parser = Trainer.add_argparse_args(parent_parser=parser)
 >>> args = Trainer.parse_argparser(parser)
 >>> args
 Namespace(accumulate_grad_batches=1, amp_level='O1', auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=True, default_root_dir=True, deterministic=False, distributed_backend=True, early_stop_callback=False, fast_dev_run=False, gpus=<function Trainer._arg_default at 0x1219efdd0>, gradient_clip_val=0, log_gpu_memory=True, log_save_interval=100, logger=True, max_epochs=1000, max_steps=True, min_epochs=1, min_steps=True, num_nodes=1, num_processes=1, num_sanity_val_steps=2, overfit_pct=0.0, precision=32, print_nan_grads=False, process_position=0, profiler=True, progress_bar_callback=True, progress_bar_refresh_rate=1, reload_dataloaders_every_epoch=False, replace_sampler_ddp=True, resume_from_checkpoint=True, row_log_interval=10, terminate_on_nan=False, test_percent_check=1.0, tpu_cores=True, track_grad_norm=-1, train_percent_check=1.0, truncated_bptt_steps=True, val_check_interval=1.0, val_percent_check=1.0, weights_save_path=True, weights_summary='full')
 </denchmark-code>
 
 	",b3ebfec863df8513f42e7211a29f857139e8ede4,Espen Haugsdal,2020-07-09 07:10:30-04:00,MODIFY,2,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"797,800,801","797,800,801",1.0,jeremyjordan,2020-05-22T00:55:04Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  any ideas?
 		",2.0,jeremyjordan,2020-05-24T03:24:20Z,"
 		Same unexpected behavior for  which is causing some tests to fail in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1504>#1504</denchmark-link>
 , I can temporarily resolve this by treating  the same as  (eg. select default value for the user) but this might not be ideal.
 		",3.0,jeremyjordan,2020-06-23T19:27:12Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  is there an update on this? I was facing this issue while writing CLI tests for <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2094>#2094</denchmark-link>
 . I can continue with those tests once this issue is fixed.
 		",4.0,jeremyjordan,2020-07-06T07:30:36Z,"
 		This issue actually affects all paramters that default to None in __init__, for instance: min_steps, max_steps, log_gpu_memory, distributed_backend, weights_save_path, truncated_bptt_steps, and resume_from_checkpoint are all set to True.
 I have a branch with a test for this here: <denchmark-link:https://github.com/EspenHa/pytorch-lightning/tree/add_argparse_test>https://github.com/EspenHa/pytorch-lightning/tree/add_argparse_test</denchmark-link>
 
 I also implemented a fix here: <denchmark-link:https://github.com/EspenHa/pytorch-lightning/tree/fix_argparse_bug>https://github.com/EspenHa/pytorch-lightning/tree/fix_argparse_bug</denchmark-link>
 
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  If you would like, I can submit these as a PR?
 		",MODIFY,0.0,tests\trainer\test_trainer_cli.py,tests\trainer\test_trainer_cli.py,0.0,"105,106,107,108,109,110,111,112,113,114,115,116,117,118,119",105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,jeremyjordan,2020-07-06T08:45:58Z,"
 		<denchmark-link:https://github.com/EspenHa>@EspenHa</denchmark-link>
  great, pls send a PR 
 		",6.0,jeremyjordan,2020-07-06T09:59:21Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  PR submitted here: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2526>#2526</denchmark-link>
  
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,parse_argparser,ArgumentParser,797,801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"797,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821","797,800,801",parse_argparser,"cls,ArgumentParser",797,821,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1937,awaelchli,2020-05-24T23:28:26Z,2020-08-08T03:26:06Z,"TODO list for ""replace Hparams by init args"" PR","
 <denchmark-h:h2>🚀 TODO: Follow up work on module arguments rework in #1896</denchmark-h>
 
 
 
  1. (docs) Make clear the multiple ways args can and cannot be passed in.
 Example:
  class LitModel(LightningModule):
     def __init__(self, arg1, arg2):
      ...
  Trainer.add_argparse_args(parser)
  LitModel.add_model_specific_args(parser)
  LitModel(parser.parse_args())  # this will fail
 This won't work since the list of arguments in constructor is a fixed size.
 We can fix it in two ways:
 
 Add **kwargs to the init signature to catch any unnecessary args (not good design but works)
 Split the parsers to separate model args from Trainer args
 
 
 
  2. (docs) make it clear which types we save to the checkpoints and which not (nn.Module for example). The name ""module_arguments"" maybe misleading to believe all args are saved.
 
 
  3. Some old code was left commented, including tests, as mentioned by @yukw777
 
 
  4.  (tests) The model checkpointing has changed, we should thoroughly test that the correct args are loaded.
 
 
  5. (tests) Test case for positional args
 
 
  6. (bugfix) Fix for when super() is not called or called after other local vars were added, e.g.,
  class LitModel(LightningModule):
     def __init__(self, arg1, arg2):
         my_local_var = 2
         super().__init__()
         # module_arguments now contains ""my_local_var""
  
  LitModel.load_from_checkpoint(...)  # this fails
  # TypeError: __init__ got an unexpected argument ""my_local_var""
 We obviously don't want any local vars other than the arguments in the checkpoint.
 
 
  7. (bugfix) In Python we are not forced to call the instance ""self"", this is currently hardcoded and leads to:
  class LitModel(LightningModule):
     def __init__(obj, arg1, arg2):
         obj.arg1 = arg1
         super().__init__()
         # module_arguments will contain LitModel() itself
 same applies to the conventional naming of ""*args"" and ""**kwargs""
 
 
  8. (tests) make sure the LRfinder still works as expected by passing in the suggested learning rate as argument (fixed in #2821 )
 
 
  9. (enhancement) @festeh wants to add support for dataclasses
 
 
  10. (bugfix) some of the examples are broken because of the problem mentioned in 1.
 
 
  11. (test) multiple inheritance
 
 
  12. Should error or warn when self.auto_collect_arguments() is called somewhere other than in init. A specific use case that is currently not working is #1976
 
 
 Feel free to add additional bullet points I missed :)
 	",4234992302608e1999c00b4faffac591fb537a34,Adrian Wälchli,2020-06-04 08:35:50-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"79,80,81,82",,1.0,awaelchli,2020-05-25T05:46:22Z,"
 		We should also make sure, that the current hparams will always be supported. There are definitely usecases where hparams are not suitable.
 		",2.0,awaelchli,2020-05-25T06:50:47Z,"
 		
 We should also make sure, that the current hparams will always be supported. There are definitely usecases where hparams are not suitable.
 
 they are as Namespace and dict are in allowed primitives
 		",3.0,awaelchli,2020-05-25T06:55:28Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  yes, but to make sure, I'd prefer to have an explicit test for this :)
 Since we should really take care of backwards compatibility.
 		",4.0,awaelchli,2020-05-25T07:23:15Z,"
 		
 @Borda yes, but to make sure, I'd prefer to have an explicit test for this :)
 Since we should really take care of backwards compatibility.
 
 Sure, agree, mind draw the test in PR and I will finish it / ensure the compatibility =)
 		",MODIFY,4.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1722,1723,1724,1725,1726,1727,1728","1727,1728,1729,1730,1731,1732,1733,1734,1735",module_arguments,self,1721,1735,MODIFY,4.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,"73,74,75,76,77,78",,,,,,,,,,,,,,,,,,,,,,,,5.0,awaelchli,2020-05-25T11:01:32Z,"
 		
  class LitModel(LightningModule):
     def __init__(self, arg1, arg2):
      ...
  Trainer.add_argparse_args(parser)
  LitModel(parser.pase_args())  # this will fail
 
 <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Just for clarification: this will not fail because you have a typo in , but because the call is not supported, right?
 		",6.0,awaelchli,2020-05-25T11:04:59Z,"
 		yes exactly, it will fail because the argparser has many more args than just arg1, arg2.
 I will fix the typo.
 		",7.0,awaelchli,2020-06-16T14:16:46Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  let's update the list with respect to what has been done...
 <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  mind help?
 		",8.0,awaelchli,2020-08-03T18:45:20Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  whats left here?
 		",9.0,awaelchli,2020-08-03T18:58:07Z,"
 		I think most of the points are outdated, much has changed. I think we can close it and track any remaining issues via reported bugs.  Although I think testing of the ""save_hyperparameters"" feature could be more thorough in general (bullet points 5., 8., 11)
 		",,,,,,,,,,,,,,,,,,,,,,,,,__init__,"obj,more_args,other_arg,more_kwargs",73,78,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1744,1745,1746,1747,1748,1749","1744,1745,1746,1747,1748,1749",_get_latest_child,"frame,object",1744,1749,1.0,"1701,1702,1703,1704,1705,1706,1710,1713,1714,1715,1716,1718","1701,1702,1706,1709,1710,1712,1716",auto_collect_arguments,self,1701,1718,,,,,,,,,,,,,,,,,,,,,,1.0,"141,142,143,144,145",,__init__,"self,arg1,arg2,args,kwargs",141,145,1.0,"163,164,165,166,167,168",,test_collect_init_arguments_with_local_vars,cls,163,168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,47,"48,49,50,51,52,53,54",test_omegaconf,tmpdir,47,64,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767","1742,1743,1744,1745,1746,1747,1748,1749",_collect_init_args,"frame,list",1738,1772,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
195,falceeffect,2019-09-04T13:13:13Z,2019-09-05T11:13:07Z,Non plain-tensor batches,"
 Hello!
 I have been trying to use Lightning to train a graph neural network built with <denchmark-link:https://github.com/rusty1s/pytorch_geometric/>torch_geometric</denchmark-link>
  package using a GPU.
 This is the error I get when I try to fit the model:
 ~/miniconda3/envs/pyg/lib/python3.7/site-packages/torch_geometric/nn/conv/gcn_conv.py in forward(self, x, edge_index, edge_weight)
      83     def forward(self, x, edge_index, edge_weight=None):
      84         """"""""""""
 ---> 85         x = torch.matmul(x, self.weight)
      86 
      87         if self.cached and self.cached_result is not None:
 
 RuntimeError: Expected object of backend CPU but got backend CUDA for argument #2 'mat2'
 I investigated the code of Lightning for a probable cause and found that transfer_batch_to_gpu causes this error.
 The current behavior of this function considers a batch to be either a 'plain'  or some simple collection of such objects (a list, a dict or a tuple). The problem is that torch_geometric uses a custom aggregate type  (<denchmark-link:https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data>docs</denchmark-link>
 ) which itself implements  method (and does not implement , I believe).
 It would be nice if you could make the code more flexible to process this case (and similar cases in general) correctly. I believe, the best solution is to replace isinstance(batch, torch.Tensor) by a condition of batch object having either to or cuda method implemented. This should solve the problem in general.
 	",34b824a9d3d0fdd377da675e0398c66ab5e16e7b,Anton Konstantinov,2019-09-05 07:13:06-04:00,MODIFY,1,pytorch_lightning\models\trainer.py,pytorch_lightning\models\trainer.py,1.0,"998,999,1002,1003,1004","998,999",1.0,falceeffect,2019-09-04T13:37:03Z,"
 		good suggestion. want to make the change and submit a PR?
 		",2.0,falceeffect,2019-09-04T13:55:20Z,"
 		Okay, I will do that.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,transfer_batch_to_gpu,"self,batch,gpu_id",997,1026,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2027,williamFalcon,2020-05-31T12:33:44Z,2020-06-08T11:19:35Z,Support DictConfig,"
 We need to add DictConfig support for Omegaconf <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  to the auto hparam save
 	",d2967d9305b42c9260f821f2b7fb43fbf19ca1aa,Jirka Borovec,2020-06-08 07:19:34-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,79,"79,80,81",1.0,williamFalcon,2020-05-31T19:28:03Z,"
 		Yes, I agree. Users should have other options besides argument parser to set up their configuration. Personally speaking, I don't like typing too much on the command line b/c that is error-prone. A dictionary-like configuration system would be great. One example would be Ross's <denchmark-link:https://github.com/rbgirshick/yacs>yacs</denchmark-link>
  which works pretty well.
 		",2.0,williamFalcon,2020-06-01T19:02:26Z,"
 		OmegaConf is along the same lines as Yacs, but with more features (might as well support YACS too, but just saying).
 		",3.0,williamFalcon,2020-06-01T22:00:59Z,"
 		<denchmark-link:https://github.com/DKandrew>@DKandrew</denchmark-link>
  <denchmark-link:https://github.com/Darktex>@Darktex</denchmark-link>
  you would keep passing one eg  argument which is used internally? Kind of pseudocode...
 
 
  conf = OmegaConf(...)
  model = MyModel(conf)
 
 
 
  conf = OmegaConf(...)
  model = MyModel(**vars(conf))
 
 
 
 assuming that the conf can be also loaded from a file...
 		",4.0,williamFalcon,2020-06-02T20:22:53Z,"
 		Hi <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 I am not sure if I understand your question correctly, are you asking which approach you listed above is better?
 		",MODIFY,0.0,docs\source\hyperparameters.rst,docs\source\hyperparameters.rst,0.0,"105,106,108,109,111,112,114,115,116,117,118,119,120,121,122,123,124,125,126,127,129,130,131,132,133,137,140,142,143,145,146,149,167,171,174,176,178,179,181,182,183,185,186,188,189","106,108,109,111,112,114,118,121,122,123,125,126,127,129,130,132,133,135,136,137,138,139,140,141,142,143,144,145,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,184,186,187,189,191,192,195",,,,,MODIFY,11.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,,"1717,1723,1725,1729,1730,1734",MODIFY,3.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,"29,30,31,32,33,34,35,36,37,38,39",,load_from_metrics,"cls,weights_path,tags_csv,map_location",29,39,MODIFY,3.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,689,,add_argparse_args.use_type,x,688,689,5.0,williamFalcon,2020-06-02T22:19:50Z,"
 		
 I am not sure if I understand your question correctly, are you asking which approach you listed above is better?
 
 I am trying to understand your use-case, mind draw your use case?
 		",6.0,williamFalcon,2020-06-02T23:42:38Z,"
 		Sure. My use-case is the first one
 <denchmark-code>conf = OmegaConf(...)
 model = MyModel(conf)
 </denchmark-code>
 
 I don't want to use the second case because conf contains too many entries: entries for network layers, dataloader, training, optimizer/scheduler, etc. I don't want to expand all of them into my __init__() because that would be too long. After all, MyModel is just an nn.Module with additional features, so internally I have a self.conf to store the entire config and use it whenever I need it.
 <denchmark-code>class MyModel(LightningModule)
     def __init__(self, conf):
         self.conf = conf
         # Normal network parameters like in_features, out_features
 </denchmark-code>
 
 		",7.0,williamFalcon,2020-06-02T23:49:42Z,"
 		let’s do this:
 <denchmark-h:h2>case 1</denchmark-h>
 
 User explicitly says what they want to save.
 class LitModel(...):
 
     def __init__(self, conf):
         self.save_hyperparameters(conf)
 <denchmark-h:h2>Case 2:</denchmark-h>
 
 User wants to save all the init stuff.
 They can do it all manually or ask us to do it automatically
 class LitModel(...):
 
     def __init__(self, arg1, arg2, arg3):
         # manually
         self.save_hyperparameters(arg_name=arg1, arg_name=arg2, arg_name=arg3)
         
         # equivalent automatic
         self.save_hyperparameters()
 <denchmark-h:h2>Case 3:</denchmark-h>
 
 They want to save ONLY some of the init stuff
 class LitModel(...):
 
     def __init__(self, arg1, arg2, arg3):
         # manually
         self.save_hyperparameters(arg_name=arg2)
 <denchmark-h:h2>Special cases:</denchmark-h>
 
 
 namespace
 
     def __init__(self, hparams):
         # manually
         self.save_hyperparameters(hparams)
 
 dict
 
     def __init__(self, some_dict):
         # manually
         self.save_hyperparameters(some_dict)
 
 omniconf
 
     def __init__(self, conf):
         # manually
         self.save_hyperparameters(conf)
 
 anything they want
 
     def __init__(self, some_random_alternative_to_config):
         # manually
         self.save_hyperparameters(some_random_alternative_to_config)
 <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
 
 		",8.0,williamFalcon,2020-06-04T03:10:30Z,"
 		
 let’s do this:
 
 Sounds good to me. Looking forward to it!
 		",9.0,williamFalcon,2020-06-04T12:12:14Z,"
 		
 case 1
 User explicitly says what they want to save.
 class LitModel(...):
 
     def __init__(self, conf):
         self.save_hyperparameters(conf)
 
 is very tricky as we would need to do some pairing from init frame and hope none of the conf1, conf2, ... has the same value so I would skip this case...
 
 Special cases:
 
 namespace
 dict
 
     def __init__(self, conf):
         # manually
         self.save_hyperparameters(conf)
 
 here you want to unroll all elements?
 assume you have some_dict=dict(a=1, b=3) then you would in fact do something similar like self.save_hyperparameters(**some_dict)
 if fact
 <denchmark-code>def save_hyperparameters(**kwargs):
     ...
     for elms in kwargs:
         if isisntance(conf, (dict, OmegaConf)):
             self.save_hyperparameters(**conf)
         if isisntance(conf, Namespace):
             self.save_hyperparameters(**vars(conf))
        ...
 </denchmark-code>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,auto_collect_arguments,self,1717,1734,,,,,1.0,689,690,add_argparse_args,"cls,ArgumentParser",631,709,1.0,966,967,run_pretrain_routine,"self,LightningModule",943,1043,MODIFY,3.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"476,477","480,481",hpc_save,"self,str,logger",452,482,1.0,"270,271",,save_checkpoint,"self,filepath,bool",262,274,MODIFY,0.0,pytorch_lightning\utilities\__init__.py,pytorch_lightning\utilities\__init__.py,1.0,"164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188",,_load_model_state,"cls,str,args,kwargs",164,188,0.0,5,,,,,,,,,,,,,MODIFY,8.0,pytorch_lightning\utilities\parsing.py,pytorch_lightning\utilities\parsing.py,1.0,28,"27,28,29,30,31",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460",load_from_metrics,"cls,weights_path,tags_csv,map_location",1450,1460,1.0,,"1737,1738,1739,1741,1742,1743,1745,1746,1747,1748,1749,1750,1751",module_arguments,self,1737,1751,1.0,"42,43,44,45,46,47,48,49",,load_from_checkpoint,"cls,str,args,str,str,device,int,None,None,None,kwargs",42,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"357,358,359,361,362","363,365,366",dump_checkpoint,"self,bool",311,367,clean_namespace,hparams,27,46,1.0,"70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93",,collect_init_args,"frame,list,bool",70,93,MODIFY,0.0,tests\base\model_template.py,tests\base\model_template.py,0.0,54,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tests\loggers\test_trains.py,tests\loggers\test_trains.py,1.0,25,25,test_trains_logger,tmpdir,8,27,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tests\models\test_cpu.py,tests\models\test_cpu.py,1.0,,115,MODIFY,30.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,"95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113","95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113",test_explicit_args_hparams,tmpdir,95,113,1.0,"101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136","101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,123,124,125,126,127,128,129,130,131,132,133,134,135,136",test_collect_init_arguments,"tmpdir,cls",101,136,MODIFY,1.0,tests\models\test_restore.py,tests\models\test_restore.py,1.0,,"273,274",test_model_pickle,tmpdir,272,276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\requirements-devel.txt,tests\requirements-devel.txt,0.0,"5,6,7",5,MODIFY,1.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,45,46,test_no_val_module,tmpdir,21,54,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1681,1682",,hparams,"self,dict,Namespace",1681,1682,1.0,"1564,1569,1570,1571,1572,1573,1574,1575,1577,1578,1580,1584,1585,1589,1590","1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590",_auto_collect_arguments,"cls,frame",1564,1590,1.0,"1585,1589,1590,1592,1593,1595,1596,1597,1598,1599","1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599",_load_model_state,"cls,str,args,kwargs",1585,1599,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67",,get_init_args,frame,49,67,1.0,"121,122,123,124,125,126,127,128",,__repr__,self,121,128,1.0,"118,119",,__setattr__,"self,key,val",118,119,test_multi_cpu_model_ddp,tmpdir,114,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1592,1593,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657","1592,1593,1594,1595,1596,1597,1598,1599,1600",save_hyperparameters,"self,args,frame",1592,1657,1.0,,"1463,1464,1465,1466,1467,1468,1469,1470",load_from_checkpoint,"cls,str,args,str,str,device,int,None,None,None,kwargs",1463,1470,1.0,,"1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1784,1785,1786,1787,1788",_collect_init_args,"frame,list",1754,1788,1.0,"1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672",,_set_hparams,"self,dict,Namespace",1659,1672,1.0,"1675,1676,1677,1678",,hparams,self,1675,1678,1.0,"5,13,15","4,12,14",strtobool,val,4,23,1.0,"112,113,114,115,116",,__getattr__,"self,key",112,116,1.0,"5,13,15","12,14",str_to_bool,val,5,24,1.0,"123,124,125","123,124,125",test_implicit_args_hparams.__init__,"self,test_arg,test_arg2",123,125,1.0,"23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45","21,25,33,38",test_class_nesting,tmpdir,21,45,1.0,"33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59","33,38,48",_run_standard_hparams_test,"tmpdir,model,cls",33,59,1.0,"65,66,67,68","65,66,67,68",__init__,"self,args,subclass_arg,kwargs",65,68,1.0,"18,20",18,__init__,"self,hparams",18,20,1.0,"204,205,206,207,208,209",,test_omegaconf,tmpdir,203,219,1.0,"72,73,74,75,76,77","72,73,74,75,76,77",test_dict_hparams,"tmpdir,cls",72,77,1.0,"205,206,207,208",,test_omegaconf.__init__,"self,ogc",205,208,1.0,"175,179,187,192",,test_class_nesting,,175,199,1.0,"63,64,65,66,67,68","63,64,65,66,67,68",test_namespace_hparams,"tmpdir,cls",63,68,1.0,"116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134","116,117,118,119,120,123,124,125,126,127,128,129,130,131,132,133,134",test_implicit_args_hparams,tmpdir,116,134,1.0,335,,__init__,"self,arg1,arg2,args,kwargs",330,335,1.0,"74,75,76,77,78,79","74,75,76,77,78,79",__init__,"obj,more_args,other_arg,more_kwargs",74,79,1.0,"382,383,384",,__init__,"self,arg1,arg2",382,384,1.0,"81,82,83,84,85,86,87,88,89,90,91,92","81,82,83,84,85,86,87,88,89,90,91,92",test_omega_conf_hparams,"tmpdir,cls",81,92,1.0,"144,145,146",,test_explicit_missing_args_hparams.__init__,"self,test_arg,test_arg2",144,146,1.0,"391,392,393,394",,test_single_config_models_fail,"tmpdir,cls,config",391,394,1.0,"308,309,310,311,312,314",,_raw_checkpoint_path,trainer,308,314,1.0,"345,346,347",,test_collect_init_arguments_with_local_vars,cls,342,347,1.0,"102,103,104","102,103,104",test_explicit_args_hparams.__init__,"self,test_arg,test_arg2",102,104,1.0,192,,test_class_nesting.test,self,190,192,1.0,179,,test_class_nesting.forward,self,178,179,1.0,"16,17,18","15,17,18",__init__,"self,ogc",15,18,1.0,"88,89,90,91","88,89,90,91",__init__,"self,args,my_loss",88,91,1.0,187,,test_class_nesting.test_outside,,185,187,1.0,"137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168","138,139,149,150,157,161,162,167,168",test_explicit_missing_args_hparams,tmpdir,137,168,1.0,"376,377,378",,__init__,"self,arg1",376,378,1.0,"397,398,399,400,401,402",,test_hparams_pickle,tmpdir,397,402,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2058,ssakhavi,2020-06-03T08:16:37Z,2020-06-23T15:20:45Z,Hydra MLFlow Clash,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using the MLFlow logger with Hydra, because the parameters passed to the LightningModule is a DictConfig, the condition in the logger/base.py is not met.
 
 
 
 pytorch-lightning/pytorch_lightning/loggers/base.py
 
 
          Line 177
       in
       8211256
 
 
 
 
 
 
  if isinstance(input_dict, dict): 
 
 
 
 
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Use Hydra and MLFlow together.
 Traceback (most recent call last):
   File ""/home/siavash/KroniKare/kwae2/kwae_ma/models/pl_train_segmentation_model.py"", line 115, in <module>
     main()
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/hydra/main.py"", line 24, in decorated_main
     strict=strict,
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/hydra/_internal/utils.py"", line 174, in run_hydra
     overrides=args.overrides,
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/hydra/_internal/hydra.py"", line 86, in run
     job_subdir_key=None,
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/hydra/plugins/common/utils.py"", line 109, in run_job
     ret.return_value = task_function(task_cfg)
   File ""/home/siavash/KroniKare/kwae2/kwae_ma/models/pl_train_segmentation_model.py"", line 111, in main
     trainer.fit(wound_seg_pl)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 765, in fit
     self.single_gpu_train(model)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 492, in single_gpu_train
     self.run_pretrain_routine(model)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 843, in run_pretrain_routine
     self.logger.log_hyperparams(ref_model.hparams)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 275, in log_hyperparams
     [logger.log_hyperparams(params) for logger in self._logger_iterable]
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/loggers/base.py"", line 275, in <listcomp>
     [logger.log_hyperparams(params) for logger in self._logger_iterable]
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py"", line 10, in wrapped_fn
     return fn(*args, **kwargs)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/pytorch_lightning/loggers/mlflow.py"", line 105, in log_hyperparams
     self.experiment.log_param(self.run_id, k, v)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/mlflow/tracking/client.py"", line 206, in log_param
     self._tracking_client.log_param(run_id, key, value)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/mlflow/tracking/_tracking_service/client.py"", line 177, in log_param
     _validate_param_name(key)
   File ""/home/siavash/anaconda3/envs/kwae-ma/lib/python3.7/site-packages/mlflow/utils/validation.py"", line 120, in _validate_param_name
     INVALID_PARAMETER_VALUE)
 mlflow.exceptions.MlflowException: Invalid parameter name: ''. Names may be treated as files in certain cases, and must not resolve to other names when treated as such. This name would resolve to '.'
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Check whether the instance if dict or DictConfig in the given line.
 	",44385bb582467acaa35cd4da553b2343a7860598,Siavash Sakhavi,2020-06-23 17:20:44+02:00,MODIFY,2,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,"175,177","177,179",1.0,ssakhavi,2020-06-03T08:17:21Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,ssakhavi,2020-06-03T09:35:01Z,"
 		
 Check whether the instance if dict or DictConfig in the given line.
 
 <denchmark-link:https://github.com/ssakhavi>@ssakhavi</denchmark-link>
  that sounds reasonable solution, mind sending a PR - fix and its test?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_flatten_dict,"str,str",155,186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"175,177","177,179",_flatten_dict._dict_generator,"input_dict,prefixes",173,184,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2092,edenlightning,2020-06-05T20:33:09Z,2020-06-08T21:55:26Z,[ddp] New ddp implementation doesn't work in notebooks / using scripts,"
 The using .spawn() to spin off subprocesses ddp in had a few problems:
 
 Everything needs to be picklable.
 It doesn’t work well with num_workers on dataloaders because of spawn
 fit(model) trains the model in a subprocess, so the original model is not updated.
 Those are not limitations of lightning, but of pytorch and python.
 
 As a result, we removed .spawn and instead call the script under the hood.
 This approach solves all problems above, but it assumes you can call your model like
 python train.py ... and does not support other ways of calling the script.
 We should decide how to support DDP on Jupyter notebooks.
 	",3260e59b2723a0f5d666c6779486717aa3a9373d,William Falcon,2020-06-08 17:55:25-04:00,MODIFY,0,docs\source\multi_gpu.rst,docs\source\multi_gpu.rst,0.0,"203,204,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628",203,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,142,,_get_distributed_sampler,"self,dataloader",134,148,MODIFY,4.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"398,406,407,408,461","397,457",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"249,879,883,884,885,886,887,888",249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ddp_train,"self,process_idx,model,is_master",397,468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,224,224,set_distributed_mode,"self,distributed_backend",194,256,1.0,"398,406,407,408,461",457,ddp_train,"self,process_idx,model,is_master,proc_offset",398,472,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,381,,spawn_ddp_children,"self,model",342,396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2131,sshleifer,2020-06-09T15:01:43Z,2020-06-30T22:09:17Z,wandLogger().name is None in DDP mode,"
 If you remove <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8ca8336ce52ee7379f4d399520636143eb31018b/pytorch_lightning/trainer/callback_config.py#L54>line 54</denchmark-link>
 , everything works.
 When you're not in ddp mode, this is not an issue.
 	",145670f893f43ff70866668cf087d82fe51a22a6,Adrian Wälchli,2020-06-30 18:09:16-04:00,MODIFY,2,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"319,320",,1.0,sshleifer,2020-06-09T16:35:41Z,"
 		<denchmark-link:https://github.com/vanpelt>@vanpelt</denchmark-link>
  any thoughts on this?
 		",2.0,sshleifer,2020-06-17T11:30:00Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  it is caused by missing 
 
 
 which comes from the wrong  init
 
 
 Do you have a minimal example which we can add among tests?
 		",3.0,sshleifer,2020-06-19T01:34:24Z,"
 		Unfortunately I do not, the problem was encountered with
 Trainer(distributed_backend=""ddp"",  logger=WandbLogger(name='hello'))
 So if you put that wherever you guys test ddp, it should suffice.
 		",4.0,sshleifer,2020-06-29T02:08:36Z,"
 		I will try to write a test that (ideally for all loggers) makes sure the logger is only used in rank 0 process, therefore not creating any files on rank > 0 and then I can also make sure these NoneTypeErrors don't happen on rank > 0.
 		",MODIFY,3.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,"384,385,386,387,388,389,390,391,392",,rank_zero_experiment,Callable,384,392,MODIFY,0.0,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,0.0,"34,143,196,197",34,MODIFY,1.0,pytorch_lightning\loggers\mlflow.py,pytorch_lightning\loggers\mlflow.py,1.0,"117,118",,log_metrics,"self,str,None",116,124,MODIFY,0.0,pytorch_lightning\loggers\neptune.py,pytorch_lightning\loggers\neptune.py,0.0,"23,214,253",23,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",269,321,,,,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,1.0,"140,141",,log_metrics,"self,str,None",139,145,1.0,100,,experiment,self,87,103,MODIFY,0.0,pytorch_lightning\loggers\test_tube.py,pytorch_lightning\loggers\test_tube.py,,,,,,,,0.0,"15,80",15,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,"132,133",,1.0,"389,390",,rank_zero_experiment.rank_zero_experiment.experiment.get_experiment,,389,390,1.0,"387,388,389,390,391",,rank_zero_experiment.experiment,self,387,391,1.0,"240,261,262,263",239,on_train_start,"self,trainer,pl_module",230,266,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log_metrics,"self,str,None",131,134,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,110,,copy_trainer_model_properties,"self,model",100,119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"36,37,38,39,40,506","362,363,505,846",,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,tests\loggers\test_all.py,tests\loggers\test_all.py,1.0,"27,28",,MODIFY,0.0,tests\models\test_horovod.py,tests\models\test_horovod.py,0.0,"56,74",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_get_logger_args,"logger_class,save_dir",21,29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"156,157,158,159,160,161,162,163,164,165,166,167,168,169,170",,test_logger_created_on_rank_zero_only,"tmpdir,logger_class",156,170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"138,139,140,141,142,143,144",,on_batch_start,"self,trainer,pl_module",138,144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2143,ZhaofengWu,2020-06-11T02:44:58Z,2020-10-05T02:18:50Z,Fix checkpoint warning for floats,"
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1819>#1819</denchmark-link>
  added this warning when saving checkpoints. 
 
 
 Is there a reason why native Python floats aren't supported? I think it's actually quite common especially when libraries e.g. scipy are involved when computing the metrics.
 It says checkpoint not saved, but actually checkpoints are still saved. This check doesn't actually change the saving logic besides giving that warning.
 
 	",97e62b38cfa0c9ce14050b603ec3e735ba760a71,William Falcon,2020-10-04 22:18:49-04:00,MODIFY,1,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"897,898",,1.0,ZhaofengWu,2020-06-11T08:32:28Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  mind have a look :]
 		",2.0,ZhaofengWu,2020-06-11T08:58:32Z,"
 		The reason it is still working is due to the fact that it is being converted to a torch.tensor later on:
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/model_checkpoint.py
 
 
         Lines 176 to 181
       in
       bd49b07
 
 
 
 
 
 
  if not isinstance(current, torch.Tensor): 
 
 
 
  rank_zero_warn( 
 
 
 
  f'{current} is supposed to be a torch.Tensor. Saving checkpoint may not work correctly. ' 
 
 
 
  f'HINT: check the value of {self.monitor} in your validation loop', RuntimeWarning 
 
 
 
      ) 
 
 
 
  current = torch.tensor(current) 
 
 
 
 
 
 so in that sense they are supported. I guess the warning is there due to:
 
 If you are returning something other than a native float, like a dict, you will get a non-informative error (RuntimeError: Could not infer dtype of dict). The warning could hopefully help you figure out what is going on. So the only way we can guarantee that this will work, is if you return a torch.Tensor.
 The PR that implemented this, tried to fix some ddp bugs. I cannot completely figure out how this is related to ddp right now, but I guess that again torch.Tensor is the only type that guarantee that this works in ddp mode.
 
 		",3.0,ZhaofengWu,2020-06-11T18:15:12Z,"
 		If types such as floats are supported, should they be excluded from the check and don't show warnings? And IMHO the warning message saying checkpoints won't be saved while they actually are is not desirable.
 		",4.0,ZhaofengWu,2020-06-11T18:18:13Z,"
 		sounds reasonable, mind draft a PR? 🐰
 		",MODIFY,2.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,"207,208,209,210,211",,test_eval_float_logging.validation_step,"self,batch,batch_idx",207,211,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,ZhaofengWu,2020-07-28T15:59:10Z,"
 		<denchmark-link:https://github.com/ZhaofengWu>@ZhaofengWu</denchmark-link>
  Checking in :)
 		",6.0,ZhaofengWu,2020-07-29T05:21:30Z,"
 		Ah sorry I overlooked this. I might not have the time recently to do this. Sorry :(
 		",7.0,ZhaofengWu,2020-09-28T08:20:03Z,"
 		You can assign this to me :).
 		",8.0,ZhaofengWu,2020-09-28T10:03:59Z,"
 		Logging floats actually doesn't work with the current version.
 I tried logging a metric with different dtypes (tensor, float, int, dict, list) and it only works with tensors. See this <denchmark-link:https://colab.research.google.com/drive/1XpeQQuLSXOaaM_RxmPZtHpBLXB0U6nNT#scrollTo=PnBye6lrwH-P>colab</denchmark-link>
 .
 For the other types it fails even before reaching the  code because of reduction of the per validation step results at the end of a validation epoch in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/step_result.py#L874>step_result.py</denchmark-link>
 .
 <denchmark-code>    873 def weighted_mean(result, weights):
 --> 874     weights = weights.to(result.device)
     875     numerator = torch.dot(result.float(), weights.transpose(-1, 0).float())
     876     result = numerator / weights.sum().float()
 
 AttributeError: 'list' object has no attribute 'device'
 </denchmark-code>
 
 For example if in 2 validation steps the logged metric is 42.0 and 43.0 (floats) the collected metric values to be reduced are [42.0, 43.0] and they do not have the device method.
 I think this issue is connected with <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3678>#3678</denchmark-link>
   and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2636>#2636</denchmark-link>
 .
 Basically, shouldn't we ensure that the metrics that user logs fulfill some basic properties? Then later in the program we can assume that they are valid and avoid later extra checks like above in this issue:
 
 tensor (clone and detach the tensor if possible to deal with #3678) or convertible to tensor (in which case convert straight away)
 non nan/inf (to deal with #2636)
 scalar (perhaps only for a monitored metric since logging 2D tensors is a desired functionality #3697 )
 
 		",9.0,ZhaofengWu,2020-10-02T17:29:43Z,"
 		
 For example if in 2 validation steps the logged metric is 42.0 and 43.0 (floats) the collected metric values to be reduced are [42.0, 43.0] and they do not have the device method.
 
 then you need to make a tensor which has those attributes... :]
 		",10.0,ZhaofengWu,2020-10-03T10:16:21Z,"
 		Yeah :) Just found out <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3540>this PR</denchmark-link>
  which could fix float logging when merged.
 		",,,,,,,,,,,,,,,,,,,,,,,,,weighted_mean,"result,weights",896,902,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230",,test_eval_float_logging,tmpdir,199,230,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2167,Shreeyak,2020-06-13T00:54:14Z,2020-06-27T01:46:09Z,"The docker image tagged with Pytorch 1.5 and Python 3.8, has Pytorch 1.4 installed and is running Python 3.7","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The docker image <denchmark-link:https://hub.docker.com/layers/pytorchlightning/pytorch_lightning/0.8.0rc1-py3.8-torch1.5/images/sha256-9f34736c8ebc3ee3f07eea1079d7bd02b066773b208ae3feb3c4ed7752119c12?context=explore>tagged</denchmark-link>
  with Pytorch 1.5, eg , has torch 1.4 installed in it, as seen via pip list. Also, it is running Python 3.7 instead of Python 3.8, as the tag indicates.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Pull docker image: docker pull pytorchlightning/pytorch_lightning:0.8.0rc1-py3.8-torch1.5
 Run the container: docker run --rm -it --init pytorchlightning/pytorch_lightning:0.8.0rc1-py3.8-torch1.5
 Check version of python and pytorch:
 
 containeruser@c5c87a61b71c:~$ python --version
 Python 3.7.7
 
 containeruser@71d3c9f95de7:~$ pip list | grep torch
 pytorch-lightning         0.8.0rc1           
 torch                     1.4.0
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Since the tag of the docker image is 0.8.0rc1-py3.8-torch1.5, I expect to see it running Python 3.8 and Pytorch 1.5.
 <denchmark-h:h3>Environment</denchmark-h>
 
 N/A
 	",2f739f5977640bb8580b82a11f322f81f1b90d09,Jirka Borovec,2020-06-26 21:46:08-04:00,MODIFY,0,.github\workflows\docker-builds.yml,.github\workflows\docker-builds.yml,0.0,"3,35,53","34,52",1.0,Shreeyak,2020-06-13T00:54:56Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Shreeyak,2020-06-13T16:52:00Z,"
 		Which OS are you running the project?
 		",3.0,Shreeyak,2020-06-26T14:11:24Z,"
 		seems like this args are not executed correctly
 
 
 
 pytorch-lightning/.github/workflows/docker-builds.yml
 
 
          Line 30
       in
       7c0a3f4
 
 
 
 
 
 
  buildargs: PYTHON_VERSION=${{ matrix.python_version }},PYTORCH_VERSION=${{ matrix.pytorch_version }} 
 
 
 
 
 
 		",4.0,Shreeyak,2020-06-26T15:48:07Z,"
 		it seems we had a typo and the image was build always with the default args, see <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2374>#2374</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2180,Nilanshrajput,2020-06-14T09:57:52Z,2020-06-17T14:52:59Z,Global Gradient calculation is turned off during validation step.,"
 If an error occurs during the validation step, the tradition calculation is turned off for the runtime, you have to either specifically enable it or restart runtime!
 	",25c7465591371f8fe4b4244ccc996706f4136cea,Nilansh Rajput,2020-06-17 10:52:58-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"322,323,324,325,326,327",,1.0,Nilanshrajput,2020-06-15T17:28:49Z,"
 		<denchmark-link:https://github.com/Nilanshrajput>@Nilanshrajput</denchmark-link>
  is it a question?
 		",2.0,Nilanshrajput,2020-06-15T17:31:58Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , no it's a bug!
 		",3.0,Nilanshrajput,2020-06-15T19:45:09Z,"
 		could you please describe more what is happening or give an example?
 		",4.0,Nilanshrajput,2020-06-15T20:09:08Z,"
 		I used the  MNIST example for demonstrating the error. <denchmark-link:https://colab.research.google.com/drive/1LWlUurpIu_pD4fP_cUgwhF5f0nS6fvb4?usp=sharing>https://colab.research.google.com/drive/1LWlUurpIu_pD4fP_cUgwhF5f0nS6fvb4?usp=sharing</denchmark-link>
 
 Here in validation step I added  which will cause the error during execution, to remove the error you can comment that line, but this error will pop-up  . This error is generated because gradient calculation was turned off during the validation step globally, and due to error while executing and never reverted.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,Nilanshrajput,2020-06-15T20:24:30Z,"
 		# disable gradients to save memory torch.set_grad_enabled(False) 
 This line in trainer.evaluation_loop.py sets global grad calculation False, this is reverted at the end of evaluation loop but that's is never executed if there is an error raised in between, and the model remain in eval mode with gradient calculation disabled.
 		",6.0,Nilanshrajput,2020-06-15T20:30:20Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  This could be resolved in two ways i think:
 
 use local context for disabling grad  with torch.no_grad():. but here also if error occurs the model will remain in eval mode, also for a long piece of code it might not look good.
 At the start of training_loop add following lines model.train() torch.set_grad_enabled(True). This is better method I think.
 I can add a PR if you agree.
 
 		",7.0,Nilanshrajput,2020-06-15T20:58:38Z,"
 		
 
 At the start of training_loop add following lines model.train() torch.set_grad_enabled(True). This is better method I think.
 I can add a PR if you agree.
 
 
 Yeah, sending a PR would be great...
 cc: <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 		",8.0,Nilanshrajput,2020-06-16T11:50:42Z,"
 		If an error is raised, the program should crash no?
 In what scenario do you want to keep training with an error in your validation loop?
 		",9.0,Nilanshrajput,2020-06-16T11:57:49Z,"
 		No program should crash, but when you are working in notebooks, after resolving the error, the grad calculation does not take place. as in validation step it was turned off and never turned back on.
 Can you try the collab link i provided, after resolving error you have start the runtime.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,train,self,309,411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2188,xiadingZ,2020-06-15T03:10:15Z,2020-08-03T18:44:12Z,[hparams] save_hyperparameters doesn't save kwargs,"
 <denchmark-h:h2>❓ Questions and Help</denchmark-h>
 
 when I use hyperparemeters like docs:
 <denchmark-code>class LitMNIST(LightningModule):
 
     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
         super().__init__()
         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
         self.save_hyperparameters()
 
 </denchmark-code>
 
 model checkpoint doesn't save args in kwargs. But kwargs is important. Args such as num_frames, img_size, img_std ... must be used in creating dataloader, but it will be tedious if writes them in __init__ explicitly .  it can make code clean if hides them in kwargs.
 Before I use hparams, it's ok. But now it's not recommended to use hparams,  is there any good idea to  deal with this problem?
 	",6ae9a97b09fd8e3239219c2882c6f3cc31a2ccf8,William Falcon,2020-06-18 23:08:25-04:00,MODIFY,2,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1696,1697,1698",1695,1.0,xiadingZ,2020-06-15T03:24:08Z,"
 		if don't use hparams, it will put all args of model, dataset, dataloader... in a LightnModule' s __init__ method, and save_hyperparameters  doesn't save args in kwargs. It this really a good idea?
 		",2.0,xiadingZ,2020-06-15T17:21:33Z,"
 		Could you please share your model example?
 		",3.0,xiadingZ,2020-06-16T02:50:51Z,"
 		
 Could you please share your model example?
 
 <denchmark-code>class LitMNIST(LightningModule):
 
     def __init__(self, layer_1_dim=128, learning_rate=1e-2, **kwargs):
         super().__init__()
         # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint
         self.save_hyperparameters()
         self.kwargs = kwargs
         ...
     
     def train_dataloader(self):
         img_size = self.kwargs['img_size']
         ...
 </denchmark-code>
 
 I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size
 		",4.0,xiadingZ,2020-06-16T14:34:35Z,"
 		
 I can train this model, but when I load from checkpoint, it says kwargs hasn't img_size
 
 I see, we need to ignore  from the model hparams saving...
 <denchmark-link:https://github.com/xiadingZ>@xiadingZ</denchmark-link>
  mind adding PR with a test for this case and I ll finish it with a patch?
 		",MODIFY,1.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,"179,184,187,190",,_load_model_state,"cls,str,args,kwargs",175,207,MODIFY,1.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,42,42,,,,,,,,,,,,,,,,,,,,,,,5.0,xiadingZ,2020-06-19T00:48:13Z,"
 		<denchmark-code>Traceback (most recent call last):
   File ""./training.py"", line 101, in <module>
     main(hparam_trial)
   File ""./training.py"", line 86, in main
     model = module(hparams, fold_train, fold_val, data_dir+img_dir)
   File ""../main/module.py"", line 18, in __init__
     self.hparams = hparams
   File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 638, in __setattr__
     object.__setattr__(self, name, value)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py"", line 1695, in hparams
     self.save_hyperparameters(hp, frame=inspect.currentframe().f_back.f_back)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py"", line 1662, in save_hyperparameters
     cand_names = [k for k, v in init_args.items() if v == hp]
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/lightning.py"", line 1662, in <listcomp>
     cand_names = [k for k, v in init_args.items() if v == hp]
   File ""/opt/conda/lib/python3.6/site-packages/pandas/core/generic.py"", line 1479, in __nonzero__
     f""The truth value of a {type(self).__name__} is ambiguous. ""
 ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
 </denchmark-code>
 
 I'm guessing this is why 0.8.0 causes this error, it's trying to save all args (including dataframes in my case) outside of hparams?
 Edit: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2250>#2250</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_standard_hparams_test,"tmpdir,model,cls,try_overwrite",33,62,hparams,"self,dict,Namespace",1695,1698,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712",,__get_hparams_assignment_variable,self,1700,1712,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2205,xiadingZ,2020-06-16T02:29:09Z,2020-09-16T18:31:56Z,[metrics] Accuracy Metric: Tensors must be CUDA and dense,"
 I try the new Accuracy Metric, but it throws error:
 <denchmark-code>Traceback (most recent call last):
   File ""main.py"", line 139, in <module>
     main(hparams)
   File ""main.py"", line 69, in main
     trainer.fit(model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 820, in fit
     self.ddp_train(task, model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 502, in ddp_train
     self.run_pretrain_routine(model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 990, in run_pretrain_routine
     False)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 278, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 418, in evaluation_forward
     output = model(*args)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 558, in __call__
     result = self.forward(*input, **kwargs)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py"", line 96, in forward
     output = self.module.validation_step(*inputs[0], **kwargs[0])
   File ""/mnt/lustre/maxiao1/PVM/models/baseline.py"", line 374, in validation_step
     acc = self.accuracy(labels_hat, labels)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/metric.py"", line 147, in __call__
     return apply_to_collection(self._orig_call(*args, **kwargs), torch.Tensor,
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py"", line 59, in new_func
     return func_to_apply(result, *dec_args, **dec_kwargs)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 26, in apply_to_collection
     return function(data, *args, **kwargs)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/metrics/converters.py"", line 244, in _sync_ddp_if_available
     async_op=False)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/torch/distributed/distributed_c10d.py"", line 898, in all_reduce
     work = _default_pg.allreduce([tensor], opts)
 RuntimeError: Tensors must be CUDA and dense
 </denchmark-code>
 
 This is my code:
 <denchmark-code>            pred = pred.view(-1, pred.shape[-1])
             labels = labels.view(-1)
             valid_index = torch.where(labels != -1)
             # select valid part to calculate
             pred = pred[valid_index].contiguous()
             labels = labels[valid_index].contiguous()
             loss = self.loss_fn(pred, labels)
             labels_hat = torch.argmax(pred, dim=1).type_as(labels)
             acc = self.accuracy(labels_hat, labels)
 </denchmark-code>
 
 Also have a question, TensorMetric's default reduce_op is SUM, does it automatically calculate average acc?
 	",17d87731062691f4510c75f12f2ce63b5dde0a43,Nicki Skafte,2020-08-26 13:01:29+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,12,,1.0,xiadingZ,2020-06-16T11:37:36Z,"
 		1.) What are your devices for labels_hat and labels? Are you running in a DDP environment?
 2.) No it doesn't. It does what it says (calculates the sum) unfortunately there is no DDP reduction  op that calculates the average. For averaging, you still need to divide by the size of your process group
 		",2.0,xiadingZ,2020-06-16T11:47:34Z,"
 		This is my code:
 <denchmark-code>            imgs = batch['imgs']
             labels = batch['labels']
             result = self(imgs)
 
             pred = result['total']
             pred = pred.view(-1, pred.shape[-1])
             labels = labels.view(-1)
             valid_index = torch.where(labels != -1)
             # select valid part to calculate
             pred = pred[valid_index]
             labels = labels[valid_index]
             loss = self.loss_fn(pred, labels)
             labels_hat = torch.argmax(pred, dim=1).type_as(labels)
             acc = self.accuracy(labels_hat, labels)
 </denchmark-code>
 
 I'm running in DDP environment, I think labels be automatically transfered to one gpu device, and I use type_as to ensure labels_hat on same device as labels
 		",3.0,xiadingZ,2020-06-16T11:51:39Z,"
 		can you try to call .contiguous() on the tensors before?
 		",4.0,xiadingZ,2020-06-16T11:53:22Z,"
 		I tried on labels and labels_hat, but it doesn't work
 		",MODIFY,0.0,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,0.0,"23,127","23,127",,,,,MODIFY,12.0,pytorch_lightning\metrics\converters.py,pytorch_lightning\metrics\converters.py,1.0,112,110,MODIFY,0.0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,0.0,307,307,,,,,MODIFY,9.0,pytorch_lightning\metrics\metric.py,pytorch_lightning\metrics\metric.py,1.0,"140,141,142,143,144,145,146,147,148,149,150,151,152","150,151,152",compute,"self,Any,Any",140,152,5.0,xiadingZ,2020-06-16T11:53:53Z,"
 		do you use sparse tensors?
 		",6.0,xiadingZ,2020-06-16T11:54:42Z,"
 		No
 		",7.0,xiadingZ,2020-06-16T11:59:43Z,"
 		And I think 2) should add some example in docs. Now code example  in docs is
 <denchmark-code># PyTorch Lightning
 class MyModule(LightningModule):
     def __init__(self):
         super().__init__()
         self.metric = Accuracy()
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = ...
         acc = self.metric(y_hat, y)
 </denchmark-code>
 
 and it says can run in ddp mode, but it doesn't say we should divide by the size of process group by hand if using ddp
 		",8.0,xiadingZ,2020-06-16T12:00:53Z,"
 		But it also does not state, that it calculates the mean. I will have a look how much work it is, to integrate this.
 		",9.0,xiadingZ,2020-09-01T18:02:47Z,"
 		<denchmark-link:https://github.com/xiadingZ>@xiadingZ</denchmark-link>
  are you still facing the  error?
 Your second point, about dividing by result by process group can be achieved by setting the  argument to either  or  (solved by PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2568>#2568</denchmark-link>
 )
 		",10.0,xiadingZ,2020-09-16T18:31:56Z,"
 		closing this. please comment if this needs to be reopened.
 		",11.0,xiadingZ,2020-10-03T21:23:33Z,"
 		
 @xiadingZ are you still facing the RuntimeError: Tensors must be CUDA and dense error?
 
 I am running into this issue, using R2Score metric. Same traceback.
 		",12.0,xiadingZ,2020-10-05T13:11:47Z,"
 		<denchmark-link:https://github.com/wconnell>@wconnell</denchmark-link>
  is am not able to reproduce on master using . Do you have an code example that can reproduce the error?
 		",,,,,,,,,,,,,_convert_to_numpy,"Tensor,ndarray",110,126,,,,,1.0,"72,73,74,75,76,77,78,79,80,81,82","77,78,80,81,82",input_convert,"self,Any",72,82,1.0,"61,62,63,64,65,66,67,68,69",,__init__,"self,str",53,69,MODIFY,1.0,tests\base\model_train_steps.py,tests\base\model_train_steps.py,1.0,"179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194",,training_step__using_metrics,"self,batch,batch_idx,optimizer_idx",179,194,,,,,,,,MODIFY,1.0,tests\metrics\test_classification.py,tests\metrics\test_classification.py,,,,,,,,1.0,,48,test_confusion_matrix,normalize,42,50,,,,,,,,MODIFY,4.0,tests\metrics\test_converters.py,tests\metrics\test_converters.py,1.0,66,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"89,95,96,102,105,107","89,100,103,105",convert_to_tensor,"Any,dtype,device",89,109,1.0,"89,95,96,102,105,107","89,100,103,105",_convert_to_tensor,Any,89,107,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_convert_to_tensor,,64,68,1.0,"126,131","126,131",_ddp_test_fn,"rank,worldsize,bool,reduction_mean",119,134,MODIFY,3.0,tests\metrics\test_metrics.py,tests\metrics\test_metrics.py,1.0,"174,175,176,177,178,179,180,181,182,183,184,185,186,187,188",,test_saving_pickable,"tmpdir,Metric",174,188,1.0,"150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170",,test_model_pickable,"tmpdir,Metric",150,170,1.0,19,,forward,"self,input1,input2",16,19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tests\metrics\test_sklearn.py,tests\metrics\test_sklearn.py,1.0,"166,173,176","166,173,176",test_sklearn_metric,"metric_class,sklearn_func,inputs",165,179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,112,,convert_to_numpy,"Tensor,ndarray",112,128,1.0,331,,decorator_fn,func_to_decorate,329,332,1.0,"243,244,245,246","243,244",sync_ddp_if_available,"None,None",243,246,1.0,"110,111,112,113,114,115,116,117,118,119,120,121,122","118,119,121,122",ddp_sync,"self,Any,Any",110,122,1.0,"85,90",85,forward,"self,args,kwargs",85,93,,,,,,,,,,,,,,,1.0,73,73,test_convert_to_numpy,,71,75,1.0,161,161,test_sync_reduce_simple,,157,164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"283,284",,gather_all_tensors_if_available,None,283,284,1.0,155,153,_tensor_metric_output_conversion,Callable,145,155,1.0,203,201,_tensor_collection_metric_output_conversion,Callable,192,203,1.0,142,140,_numpy_metric_input_conversion,Callable,131,142,1.0,"243,244","241,242,243,244",_sync_ddp_if_available,"None,None",241,244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,189,187,_tensor_metric_input_conversion,Callable,178,189,1.0,"96,97,98,99,100,101,102,103,104,105,106,107",,output_convert,"self,Any,Any",96,107,1.0,"81,82","81,82",__call__._to_device_dtype,Tensor,81,82,1.0,"80,81,82,85","80,81,82,84,85",__call__,"self,args,kwargs",80,85,1.0,"125,126,127,128,129,130,131,132,133,134,135,136,137","125,126",aggregate,"self,Any,Any",125,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
224,ananyahjha93,2019-09-15T19:54:42Z,2019-09-16T16:35:30Z,set_epoch for DistributedSampler,"
 Describe the bug
 PyTorch example suggests the use set_epoch function for DistributedSampler class before each epoch start. I could not find this function call in lightning's trainer module.
 <denchmark-link:https://github.com/pytorch/examples/blob/master/imagenet/main.py>https://github.com/pytorch/examples/blob/master/imagenet/main.py</denchmark-link>
 
 Line 232-234
 As can be seen from the DistributedSampler class code (<denchmark-link:https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py>https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py</denchmark-link>
 ), the set_epoch function is required to set the seed for each  function call.
 Can you confirm if this function has been called on DistributedSampler (for training dataset) at some point in lightning's trainer module?
 	",c0f3b6b035f955fc371dec412d3816712f3fc1dd,Ananya Harsh Jha,2019-09-16 10:21:00-04:00,MODIFY,1,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"932,933,934,935",,1.0,ananyahjha93,2019-09-15T22:28:13Z,"
 		it's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling
 		",2.0,ananyahjha93,2019-09-15T22:28:21Z,"
 		do you want to submit that PR?
 		",3.0,ananyahjha93,2019-09-16T07:21:25Z,"
 		Added a PR for this, do you want me to update any docs in my commit related to this issue? Also, I have successfully tested the library with this fix included in my experiments.
 		",4.0,ananyahjha93,2020-11-02T19:26:27Z,"
 		
 it's not called which means it is equivalent to shuffle=False (ie: shuffled once but not again every epoch). We can add it though to enable shuffling
 
 A short note on this for future readers:
 No setting epochs in DistributedSampler does not equivalent to shuffle=False (for both dataloader and distributed sampler).
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__train,self,929,970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2254,s-rog,2020-06-19T02:37:22Z,2020-07-10T01:20:18Z,"Single node DDP: ""Default process group is not initialized""","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Unable to start single node ddp training on 0.8.0
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 was going to run the gpu_template but... #2235
 both methods of running the template result in the same error
 <denchmark-code>$ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp_spawn
 $ python -m pl_examples.basic_examples.gpu_template --gpus 4 --distributed_backend ddp
 </denchmark-code>
 
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0,1,2,3]
 Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
     ""__main__"", mod_spec)
   File ""/opt/conda/lib/python3.6/runpy.py"", line 85, in _run_code
     exec(code, run_globals)
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 80, in <module>
     main(hyperparams)
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 41, in main
     trainer.fit(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 860, in fit
     self.barrier('fit_prepare_data')
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1261, in barrier
     torch_distrib.barrier()
   File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 1484, in barrier
     _check_default_pg()
   File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 187, in _check_default_pg
     ""Default process group is not initialized""
 AssertionError: Default process group is not initialized
 </denchmark-code>
 
 	",57d5f6e74a3bcd8f5c73211ba3a4e2480fcc1114,William Falcon,2020-06-19 00:42:20-04:00,MODIFY,0,docs\source\trainer.rst,docs\source\trainer.rst,0.0,12,12,1.0,s-rog,2020-06-19T02:47:50Z,"
 		can you post code to reproduce? just a minimal example that breaks
 BTW, the GPU template is fixed...
 		",2.0,s-rog,2020-06-19T02:50:00Z,"
 		done, let me post my env as well
 		",3.0,s-rog,2020-06-19T02:50:36Z,"
 		ok wait... i think i see it. one sec
 		",4.0,s-rog,2020-06-19T04:50:07Z,"
 		I just tested the merged changes with both ddp and ddp_spawn again got this:
 <denchmark-code>Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 80, in <module>
     main(hyperparams)
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 41, in main
     trainer.fit(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 891, in fit
 Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
     ""__main__"", mod_spec)
   File ""/opt/conda/lib/python3.6/runpy.py"", line 85, in _run_code
     self.ddp_train(task, model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 479, in ddp_train
     exec(code, run_globals)
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 80, in <module>
     main(hyperparams)
   File ""/opt/conda/lib/python3.6/site-packages/pl_examples/basic_examples/gpu_template.py"", line 41, in main
     trainer.fit(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 907, in fit
     self.setup()
 TypeError: setup() missing 1 required positional argument: 'stage'
     self.spawn_ddp_children(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 441, in spawn_ddp_children
     self.ddp_train(local_rank, model, is_master=True)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 479, in ddp_train
     self.setup()
 TypeError: setup() missing 1 required positional argument: 'stage'
 </denchmark-code>
 
 		",MODIFY,1.0,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,33,33,teardown,"self,str",28,34,MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"478,479,480,481,482",,MODIFY,4.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"179,180,181,182,183",,tpu_train,"self,tpu_core_idx,model",178,215,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"894,899,906,929,930,931,948,949,950,951,952","860,861,862,863,864,865,900,905,912",,,,,5.0,s-rog,2020-06-19T05:14:30Z,"
 		try again. that was a typo
 		",6.0,s-rog,2020-06-19T05:47:52Z,"
 		cheers, works now!
 		",7.0,s-rog,2020-06-23T05:35:19Z,"
 		Still having the Default process group is not initialized issue when using trainer.test
 		",8.0,s-rog,2020-06-23T06:30:56Z,"
 		
 Still having the Default process group is not initialized issue when using trainer.test
 
 I still have this bug as well. One temporary solution is creating a new single GPU trainer to do the test.
 Like
 <denchmark-code>trainer = Trainer(gpus=1, deterministic=True, logger=logger)
 trainer.model = model
 trainer.test()
 </denchmark-code>
 
 		",9.0,s-rog,2020-06-23T19:57:28Z,"
 		Right, I know it works on single gpu. I have a large test set and ideally want faster inference using multiple gpus.
 		",10.0,s-rog,2020-07-02T15:11:23Z,"
 		Can we re-open this issue? I am still having the Default process group is not initialized issue when I hit trainer.test() with ddp (with any number of gpus, even 1). I'm using the latest release from yesterday.
 		",11.0,s-rog,2020-07-02T15:33:13Z,"
 		+1, doesn't look like the issue is resolved yet.
 		",12.0,s-rog,2020-07-04T05:32:04Z,"
 		having the same problem..... I also tried to downgrade pl to an older version, like 0.7.5, and try to using the older version to do the inference. But, the model trained and saved using the 0.8.x seems to not directly be compatible with older version.
 		",13.0,s-rog,2020-07-09T12:11:00Z,"
 		version: 0.8.4  train with ddp,  Got ""Default process group is not initialized"" when run trainer.test()
 		",14.0,s-rog,2020-07-09T12:18:32Z,"
 		could you try master? this is fixed there
 		",15.0,s-rog,2020-07-09T19:06:49Z,"
 		Just tried it, it works fine now! Thank you!
 		",ddp_train,"self,process_idx,model,is_master,proc_offset",443,530,,,,,,,,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,262,261,is_function_implemented,"self,args",261,262,1.0,"705,706,707,708,709",,run_training_teardown,self,702,723,,,,,1.0,"263,264,265,266,267",,horovod_train,"self,model",262,322,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"218,219,220,221",,dp_train,"self,model",217,260,1.0,"158,159,160,161,162",,single_gpu_train,"self,model",157,176,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,262,,is_function_implemented,"self,args,kwargs",262,263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,s-rog,2020-08-17T19:13:27Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Trying 0.8.5
 Trained with ddp, and testing with ddp, but got the following error message:
 <denchmark-code>AssertionError: DistributedDataParallel is not needed when a module doesn't have any parameter that requires a gradient.
 </denchmark-code>
 
 Any idea?
 Thanks!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2281,Kshitij09,2020-06-19T20:22:20Z,2020-06-20T11:38:48Z,RuntimeError: OrderedDict mutated during iteration,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I was getting RuntimeError: OrderedDict mutated during iteration.
 It seems like using the same LightningModule object with ModelSummary and Trainer causes this error.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 from pytorch_lightning.core.memory import ModelSummary
 model = CifarNet() # any pl module would work here
 ModelSummary(model,mode='full')
 trainer = Trainer(fast_dev_run=True,gpus=1)
 trainer.fit(model)
 Steps to reproduce the behavior:
 
 View model summary using ModelSummary class
 Call trainer.fit with same object.
 
 <denchmark-h:h3>Stacktrace</denchmark-h>
 
 RuntimeError                              Traceback (most recent call last)
 <ipython-input-20-8badc092c0ba> in <module>()
       1 # Checking for errors
       2 trainer = Trainer(fast_dev_run=True,gpus=1)
 ----> 3 trainer.fit(model)
 11 frames
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
     916 
     917         elif self.single_gpu:
 --> 918             self.single_gpu_train(model)
     919 
     920         elif self.use_tpu:  # pragma: no-cover
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py in single_gpu_train(self, model)
     174             self.reinit_scheduler_properties(self.optimizers, self.lr_schedulers)
     175 
 --> 176         self.run_pretrain_routine(model)
     177 
     178     def tpu_train(self, tpu_core_idx, model):
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    1091 
    1092         # CORE TRAINING LOOP
 -> 1093         self.train()
    1094 
    1095     def test(
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in train(self)
     373                 # RUN TNG EPOCH
     374                 # -----------------
 --> 375                 self.run_training_epoch()
     376 
     377                 if self.max_steps and self.max_steps == self.global_step:
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_epoch(self)
     456             # RUN TRAIN STEP
     457             # ---------------
 --> 458             _outputs = self.run_training_batch(batch, batch_idx)
     459             batch_result, grad_norm_dic, batch_step_metrics, batch_output = _outputs
     460 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in run_training_batch(self, batch, batch_idx)
     632 
     633                 # calculate loss
 --> 634                 loss, batch_output = optimizer_closure()
     635 
     636                 # check if loss or model weights are nan
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in optimizer_closure()
     596                                                                     opt_idx, self.hiddens)
     597                         else:
 --> 598                             output_dict = self.training_forward(split_batch, batch_idx, opt_idx, self.hiddens)
     599 
     600                         # format and reduce outputs accordingly
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py in training_forward(self, batch, batch_idx, opt_idx, hiddens)
     771             batch = self.transfer_batch_to_gpu(batch, gpu_id)
     772             args[0] = batch
 --> 773             output = self.model.training_step(*args)
     774 
     775         # TPU support
 <ipython-input-11-2482ebcf9d12> in training_step(self, batch, batch_idx)
      55   def training_step(self,batch,batch_idx):
      56     x, y = batch
 ---> 57     y_hat = self(x)
      58 
      59     return {'loss': F.cross_entropy(y_hat, y)}
 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     548             result = self._slow_forward(*input, **kwargs)
     549         else:
 --> 550             result = self.forward(*input, **kwargs)
     551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
 <ipython-input-11-2482ebcf9d12> in forward(self, x)
      11 
      12   def forward(self,x):
 ---> 13     return self.model(x)
      14 
      15   def prepare_data(self):
 /usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     549         else:
     550             result = self.forward(*input, **kwargs)
 --> 551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
     553             if hook_result is not None:
 RuntimeError: OrderedDict mutated during iteration
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 We should be able to use same object with both the classes
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 		- Tesla T4
 	- available:         True
 	- version:           10.1
 * Packages:
 	- numpy:             1.18.5
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.5.0+cu101
 	- pytorch-lightning: 0.8.1
 	- tensorboard:       2.2.2
 	- tqdm:              4.41.1
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.6.9
 	- version:           #1 SMP Wed Feb 19 05:26:34 PST 2020
 </denchmark-code>
 
 	",f972ab3a828eae1847a793da0b2c25c6074647a4,Adrian Wälchli,2020-06-20 07:38:47-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"21,22",,1.0,Kshitij09,2020-06-19T20:23:01Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Kshitij09,2020-06-19T21:31:22Z,"
 		
 model = CifarNet() # any pl module would work here
 
 Could you paste the minimal code for CifarNet? I cannot reproduce it with PL examples, sorry.
 		",3.0,Kshitij09,2020-06-19T22:59:56Z,"
 		Okay ! I'm not sure which part is pertaining to this issue, so here is the link to my <denchmark-link:https://colab.research.google.com/drive/13ER3opHF3IacEfAyEWojuplBUHn2MBKU?usp=sharing>colab notebook</denchmark-link>
 
 		",4.0,Kshitij09,2020-06-20T08:48:54Z,"
 		Thanks, your notebook was very helpful. I fixed the bug here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2298>#2298</denchmark-link>
 
 You can verify that it works by installing from
 !pip install --upgrade git+https://github.com/awaelchli/pytorch-lightning@bugfix/summary_hook_handles timm wandb
 in the first cell of your notebook.
 		",MODIFY,7.0,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,"198,199",,summarize,self,194,200,MODIFY,1.0,tests\core\test_memory.py,tests\core\test_memory.py,1.0,"96,97,98,99,100,101,102,103",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_hooks_removed_after_summarize,mode,96,103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"61,63,64,66,67,68,76","69,74,78",_register_hook,self,61,78,1.0,93,,out_size,self,93,94,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,76,74,_register_hook.hook,"module,inp,out",71,76,1.0,"80,81,82,83,84,85,86",,detach_hook,self,80,86,1.0,"58,59",59,__del__,self,58,59,,,,,,,,,,,,,,,1.0,89,,in_size,self,89,90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2286,hjalmarlucius,2020-06-19T21:38:11Z,2020-07-05T11:17:23Z,example_input_array dtype,"
 Currently assumed that example_input_array dtype to be equal to model dtype. This is not necessarily correct - e.g. if input is a vector of INT.
 
 
 
 pytorch-lightning/pytorch_lightning/core/memory.py
 
 
          Line 192
       in
       7dc58bd
 
 
 
 
 
 
  input_ = apply_to_collection(input_, torch.Tensor, lambda x: x.type(model.dtype)) 
 
 
 
 
 
 	",6bfcfa8671c4bf54b34290171f191db65fa27d8c,Adrian Wälchli,2020-07-05 07:17:22-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,26,,1.0,hjalmarlucius,2020-06-23T17:10:20Z,"
 		Hi, I don't understand. Does it throw an error or does it display nothing? Could you clarify?
 I don't think we can very accurately define the ""input shape"" for anything other than tensors.
 For this reason we exclude things like dicts from the overview, because it is not very practical to visualize this in a table.
 		",2.0,hjalmarlucius,2020-06-24T02:11:09Z,"
 		Hi, currently the model is run with input_ as an input. If the model expects a tensor of INTs then it will crash if floats come. I encountered this issue when pretraining an ALBERT-like model. This receives word embeddings as inputs, which have to be integers as they're going into a nn.Embedding
 		",3.0,hjalmarlucius,2020-06-24T13:38:31Z,"
 		okay I see, so we should not change the dtype as given by example_input_array.
 I can't recall why I added this conversion, maybe  it was because of amp and the half conversions. I'll have a closer look, thanks for bringing it up.
 		",4.0,hjalmarlucius,2020-06-24T13:39:20Z,"
 		as a workaround until it is fixed, cast your input to int before feeding to the embedding layer, or don't use the example_input_array.
 		",MODIFY,1.0,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,,211,_forward_example_input,self,204,227,MODIFY,5.0,tests\core\test_memory.py,tests\core\test_memory.py,1.0,"104,105,106,107,108,109,110,111,112,113,114,115",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_mixed_dtype_model_summary,,104,115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"81,83",88,test_linear_model_summary_shapes,"device,mode",81,101,1.0,"54,55",,forward,"self,x",54,55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"75,76,77,78,81,83","68,70,88",test_linear_model_summary_shapes,"device,dtype,mode",68,89,1.0,"48,49,50,51,52",,__init__,self,48,52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2299,Laksh1997,2020-06-20T14:15:24Z,2020-06-27T19:08:23Z,DDP Bug with Model Checkpoint parsing,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 My script works with CPU, single-GPU and dp.
 I need ddp to do 16 bit training. Also even on a single machine ddp is faster.
 Here is my ModelCheckpoint code:
 <denchmark-code>def setup_model_checkpoint(config):
     kwargs = config[""model_checkpoint_kwargs""]
     metrics = kwargs.pop(""metrics"", [""val_loss""])
     if isinstance(metrics, str):
         metrics = [metrics]
 
     fp = ""checkpoints/{epoch}""
     for metric in metrics:
         fp += ""-{""
         fp += str(metric)
         fp += "":.2f}""
 
     return ModelCheckpoint(filepath=fp, **kwargs)
 </denchmark-code>
 
 In my case it would generate the checkpoint: checkpoints/epoch=4_val_loss=0.6_auc=0.85 for example.
 Although I even tried it with just checkpoints and it's the same issue.
 The issue is the following:
 <denchmark-code>2020-06-20T14:50:19.704+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 891, in fit
 2020-06-20T14:50:19.704+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 891, in fit
 2020-06-20T14:50:19.704+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 891, in fit
 2020-06-20T14:50:19.705+01:00
 self.ddp_train(task, model)
 2020-06-20T14:50:19.705+01:00
 self.ddp_train(task, model)
 2020-06-20T14:50:19.705+01:00
 self.ddp_train(task, model)
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 530, in ddp_train
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 530, in ddp_train
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 530, in ddp_train
 2020-06-20T14:50:19.705+01:00
 self.run_pretrain_routine(model)
 2020-06-20T14:50:19.705+01:00
 self.run_pretrain_routine(model)
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1046, in run_pretrain_routine
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1046, in run_pretrain_routine
 2020-06-20T14:50:19.705+01:00
 self.run_pretrain_routine(model)
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1046, in run_pretrain_routine
 2020-06-20T14:50:19.705+01:00
 self.configure_checkpoint_callback()
 2020-06-20T14:50:19.705+01:00
 self.configure_checkpoint_callback()
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_config.py"", line 60, in configure_checkpoint_callback
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_config.py"", line 60, in configure_checkpoint_callback
 2020-06-20T14:50:19.705+01:00
 self.configure_checkpoint_callback()
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/site-packages/pytorch_lightning/trainer/callback_config.py"", line 60, in configure_checkpoint_callback
 2020-06-20T14:50:19.705+01:00
 ""checkpoints""
 2020-06-20T14:50:19.705+01:00
 ""checkpoints""
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/posixpath.py"", line 94, in join
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/posixpath.py"", line 94, in join
 2020-06-20T14:50:19.705+01:00
 ""checkpoints""
 2020-06-20T14:50:19.705+01:00
 genericpath._check_arg_types('join', a, *p)
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/genericpath.py"", line 149, in _check_arg_types
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/posixpath.py"", line 94, in join
 2020-06-20T14:50:19.705+01:00
 genericpath._check_arg_types('join', a, *p)
 2020-06-20T14:50:19.705+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/genericpath.py"", line 149, in _check_arg_types
 2020-06-20T14:50:19.705+01:00
 (funcname, s.__class__.__name__)) from None
 2020-06-20T14:50:19.705+01:00
 genericpath._check_arg_types('join', a, *p)
 2020-06-20T14:50:19.705+01:00
 TypeError: join() argument must be str or bytes, not 'NoneType'
 2020-06-20T14:50:19.706+01:00
 File ""/home/user/miniconda/envs/py36/lib/python3.6/genericpath.py"", line 149, in _check_arg_types
 2020-06-20T14:50:19.706+01:00
 (funcname, s.__class__.__name__)) from None
 2020-06-20T14:50:19.706+01:00
 TypeError: join() argument must be str or bytes, not 'NoneType'
 2020-06-20T14:50:19.706+01:00
 (funcname, s.__class__.__name__)) from None
 2020-06-20T14:50:19.706+01:00
 TypeError: join() argument must be str or bytes, not 'NoneType'
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.4
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): Conda
 Build command you used (if compiling from source):
 Python version: 3.6.5
 CUDA/cuDNN version: 10.1
 GPU models and configuration: 4 x V100
 Any other relevant information: Pytorch lightning 0.8.0
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",90f641af0d509645ecd679d00f1213f68d4a44ad,William Falcon,2020-06-27 15:08:22-04:00,MODIFY,3,pl_examples\models\lightning_template.py,pl_examples\models\lightning_template.py,1.0,,161,1.0,Laksh1997,2020-06-20T14:32:05Z,"
 		Ahh, it may be because I do kwargs.pop(""metrics"") which then means for the other processes its a NoneType.
 I've copy.deepcopy'd kwargs. Let's see if that fixes it!
 		",2.0,Laksh1997,2020-06-20T14:46:40Z,"
 		Turns out the above didn't help.
 		",3.0,Laksh1997,2020-06-20T14:50:09Z,"
 		The error seems to be here (trainer/callback_config.py line 60)
 <denchmark-code>    def configure_checkpoint_callback(self):
         """"""
         Weight path set in this priority:
         Checkpoint_callback's path (if passed in).
         User provided weights_saved_path
         Otherwise use os.getcwd()
         """"""
         ckpt_path = self.default_root_dir
         if self.checkpoint_callback:
             # init a default one
             if self.logger is not None:
                 save_dir = (getattr(self.logger, 'save_dir', None) or
                             getattr(self.logger, '_save_dir', None) or
                             self.default_root_dir)
 
                 # weights_save_path overrides anything
                 if self.weights_save_path is not None:
                     save_dir = self.weights_save_path
 
                 version = self.logger.version if isinstance(
                     self.logger.version, str) else f'version_{self.logger.version}'
                 ckpt_path = os.path.join(
                     save_dir,
                     self.logger.name,
                     version,
                     ""checkpoints""
                 )
 </denchmark-code>
 
 		",4.0,Laksh1997,2020-06-20T14:52:13Z,"
 		One of the arguments in the last os.path.join is a NoneType. But it only happens on ddp2. Confusing...
 More confusing- exactly 3 (not 4) processes log this error. So one process seems to be fine but not the other 3!
 		",MODIFY,1.0,pytorch_lightning\trainer\callback_config.py,pytorch_lightning\trainer\callback_config.py,1.0,"45,56","45,56,57,58,59,60,61",configure_checkpoint_callback,self,35,90,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,Laksh1997,2020-06-20T14:57:52Z,"
 		I have a suspicion self.default_root_dir is None.
 I am now explicitly passing in a default root dir.
 		",6.0,Laksh1997,2020-06-20T15:14:09Z,"
 		The above didn't work. I now think that it's something to do with the logger, specifically logger.name
 		",7.0,Laksh1997,2020-06-20T19:03:40Z,"
 		Here is my logger code:
 <denchmark-code>def setup_wandb_logging(total_cfg: Dict):
     """"""
     Helper function to set-up WandB logging.
     Parameters
     ----------
     total_cfg: A dictionary containing all possible config for a training run! (Model + Data + Training config)
 
     Returns
     -------
     a WandbLogger ready to log in the training run.
     """"""
     wandb_logger = WandbLogger(
         name=total_cfg[""name""],
         version=total_cfg[""name""],
         save_dir=""checkpoints"",
         offline=False,
         anonymous=False,
         project=total_cfg[""project""],
         tags=None,
         experiment=None,
     )
     wandb_logger.log_hyperparams(total_cfg)
     return wandb_logger
 </denchmark-code>
 
 which I then pass into trainer as Trainer(logger=wandb_logger, **kwargs)
 		",8.0,Laksh1997,2020-06-22T17:26:12Z,"
 		Response from wandb:
 <denchmark-code>Hi Laksh - Try using wandb.init (reinit=True) wandb.join at the end of each run 
 </denchmark-code>
 
 		",9.0,Laksh1997,2020-06-24T02:16:44Z,"
 		looking at this... so it looks to be specifically wb related?
 		",10.0,Laksh1997,2020-06-24T02:17:02Z,"
 		can you put up a colab that creates this issue?
 		",11.0,Laksh1997,2020-06-24T08:48:34Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Yeah it looks like it. I have tried to contact the wandb team but they've given me limited response so far. Sure, let me put together a basic colab.
 		",12.0,Laksh1997,2020-06-24T16:49:12Z,"
 		Hey guys, we can look into this.  <denchmark-link:https://github.com/Laksh1997>@Laksh1997</denchmark-link>
  can you share a basic colab to reproduce?
 		",13.0,Laksh1997,2020-06-24T18:22:22Z,"
 		Hi <denchmark-link:https://github.com/vanpelt>@vanpelt</denchmark-link>
 , thanks! I'm working on a colab right now. Will need to go on a multi GPU machine to confirm error.
 I have tried 4 x V100 (p3.8xlarge) and 16 x K80 (p2.16xlarge) and I get the same error on only ddp (but works fine on dp).
 		",14.0,Laksh1997,2020-06-24T18:48:55Z,"
 		<denchmark-link:https://github.com/vanpelt>@vanpelt</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
   Here's a working notebook.
 <denchmark-link:https://colab.research.google.com/drive/1dTgKDU4S8Oy8g9AvXZLEoE5_aDyzBLb_?usp=sharing>https://colab.research.google.com/drive/1dTgKDU4S8Oy8g9AvXZLEoE5_aDyzBLb_?usp=sharing</denchmark-link>
 
 To run on multi GPU you will need to copy the code (probs only 150 lines) input a script.
 The key things to note is in the Hparams, change your wandb name and project to whatever you want.
 Also, remember to turn on multi GPU (set gpus: 4 and distributed_backend=""ddp"" in trainer_kwargs in hparams).
 Let me know if any issues
 		",15.0,Laksh1997,2020-06-25T00:59:45Z,"
 		<denchmark-link:https://github.com/vanpelt>@vanpelt</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Update - I just tried this on pytorch 1.6 nightly, and the error persists.
 		",,,,,test_dataloader,self,160,162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,157,val_dataloader,self,156,158,1.0,,153,train_dataloader,self,152,154,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,Laksh1997,2020-06-27T13:12:19Z,"
 		<denchmark-link:https://github.com/vanpelt>@vanpelt</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Have you by any chance had any luck with the issue?
 		",17.0,Laksh1997,2020-06-27T13:41:19Z,"
 		looking in a few hours. want to get this fix into 0.8.2
 		",18.0,Laksh1997,2020-06-27T14:27:33Z,"
 		Thanks so much!
 		",19.0,Laksh1997,2020-06-27T17:13:52Z,"
 		ok... running your exact code on a single GPU:
 <denchmark-code>
   | Name  | Type                | Params
 ----------------------------------------------
 0 | model | EncoderDecoderModel | 245 M 
 Epoch 1:   0%|                                                                                                                                                       | 0/828 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:
         addcmul_(Number value, Tensor tensor1, Tensor tensor2)
 Consider using one of the following signatures instead:
         addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:761.)
   exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
 Epoch 1:  16%|██████████████████▏                                                                                              | 133/828 [00:50<04:23,  2.64it/s, loss=6.848, v_num=2nn4c31m]^Cwandb: Ctrl-c pressed.
 wandb: Program failed with code 255. Press ctrl-c to abort syncing.
 /opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
   warnings.warn(*args, **kwargs)
 Epoch 1:  16%|██████████████████▏                                                                                              | 133/828 [00:50<04:23,  2.63it/s, loss=6.848, v_num=2nn4c31m]
 
 wandb: Waiting for W&B process to finish, PID 29584
 wandb: Run summary:
 wandb:         _step 135
 wandb:    _timestamp 1593277979.7518523
 wandb:      _runtime 72.95207810401917
 wandb:          loss 6.491087913513184
 wandb:   global_step 100
 wandb:         epoch 0
 wandb: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
 wandb:                                                                                
 wandb: Synced MY-WANDB-NAME: https://app.wandb.ai/stk/MY-WANDB-PROJECT/runs/2nn4c31m
 </denchmark-code>
 
 Trying ddp now
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,Laksh1997,2020-06-27T17:49:00Z,"
 		ok fixed!
 <denchmark-code>/opt/conda/lib/python3.7/site-packages/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working
   assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
 initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
 name: MY-WANDB-NAME
 project: MY-WANDB-PROJECT
 train_bs: 4
 val_bs: 4
 num_workers: 4
 max_length: 160
 num_datapoints: 100000
 optimizer: Ranger
 optimizer_kwargs:
   lr: 0.0003
   alpha: 0.5
   betas:
   - 0.95
   - 0.999
   eps: 1.0e-05
   weight_decay: 0.001
 schedulers_kwargs:
   num_warmup_steps: 1000
 trainer_kwargs:
   gpus: 2
   gradient_clip_val: 0.5
   accumulate_grad_batches: 4
   min_epochs: 5
   max_epochs: 100
   precision: 32
   distributed_backend: ddp
 
 initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
 ----------------------------------------------------------------------------------------------------
 distributed_backend=ddp
 All DDP processes registered. Starting ddp with 2 processes
 ----------------------------------------------------------------------------------------------------
 wandb: Tracking run with wandb version 0.9.1
 wandb: Run data is saved locally in wandb/run-20200627_174833-2sdoruup
 wandb: Syncing run MY-WANDB-NAME
 wandb: ⭐️ View project at https://app.wandb.ai/stk/MY-WANDB-PROJECT
 wandb: 🚀 View run at https://app.wandb.ai/stk/MY-WANDB-PROJECT/runs/2sdoruup
 wandb: Run `wandb off` to turn off syncing.
 
 
   | Name  | Type                | Params
 ----------------------------------------------
 0 | model | EncoderDecoderModel | 245 M 
 wandb: Tracking run with wandb version 0.9.1
 wandb: Run data is saved locally in wandb/run-20200627_174834-3gkane0x
 wandb: Syncing run MY-WANDB-NAME
 wandb: ⭐️ View project at https://app.wandb.ai/stk/MY-WANDB-PROJECT
 wandb: 🚀 View run at https://app.wandb.ai/stk/MY-WANDB-PROJECT/runs/3gkane0x
 wandb: Run `wandb off` to turn off syncing.
 
 Epoch 1:   0%|                                                                                                                                                       | 0/414 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:
         addcmul_(Number value, Tensor tensor1, Tensor tensor2)
 Consider using one of the following signatures instead:
         addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:761.)
   exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
 /opt/conda/lib/python3.7/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:
         addcmul_(Number value, Tensor tensor1, Tensor tensor2)
 Consider using one of the following signatures instead:
         addcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:761.)
   exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)
 Epoch 1:   7%|███████▉                                                                                                          | 29/414 [00:11<02:36,  2.46it/s, loss=9.714, v_num=2sdoruup]
 </denchmark-code>
 
 		",21.0,Laksh1997,2020-06-27T17:53:24Z,"
 		2 things:
 
 we had a bug where something was trying to access a property not on rank zero which is now fixed.
 your prepare_data is not correct now.
 
 Prepare data is only ever called on the root GPU... this means assigning something self.something = a will only work on GPU 0. So, when you try to access that it will break on other GPUs.
 We fixed this by introducing
 <denchmark-code>def setup(self, step):
 </denchmark-code>
 
 in setup, you can assign whatever you want.
 Here are all the details on how to prepare data
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/stable/lightning-module.html#data-preparation>https://pytorch-lightning.readthedocs.io/en/stable/lightning-module.html#data-preparation</denchmark-link>
 
 <denchmark-link:https://user-images.githubusercontent.com/3640001/85928748-4cf88680-b87d-11ea-9820-d78aacedb255.png></denchmark-link>
 
 The fix is very simple in the HF example:
 <denchmark-code># old
 def prepare_data(self):
     self.x = train_split
 
 # new
 def setup(self, step):
     self.x = train_split
 </denchmark-code>
 
 And use prepare_data only for downloads
 <denchmark-code>def prepare_data(self):
     self.download()
     tokenize()
     etc()
 
    self.dont_assing = it_wont_work_on_other_gpus
 </denchmark-code>
 
 FYI <denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
 
 <denchmark-link:https://github.com/Laksh1997>@Laksh1997</denchmark-link>
  here's the fixed code
 <denchmark-link:https://gist.github.com/williamFalcon/645019619bdd897d135d232556bcf27d>https://gist.github.com/williamFalcon/645019619bdd897d135d232556bcf27d</denchmark-link>
 
 		",22.0,Laksh1997,2020-06-27T20:48:48Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Thank you so much! Amazing!
 So if the data is particularly light to download, we might as well just put all the code in setup?
 Also what is the step argument for?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2311,drozzy,2020-06-21T23:44:11Z,2020-09-19T23:00:59Z,overfit_batches doesn't work,"
 When I try to use overfit_batches:
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data>https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data</denchmark-link>
 
 <denchmark-code> trainer = Trainer(gpus=num_gpus, max_epochs=config.epochs, overfit_batches=0.01, logger=logger)
 </denchmark-code>
 
 my code fails with:
 <denchmark-code>   trainer.fit(module)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 918, in fit
     self.single_gpu_train(model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 176, in single_gpu_train
     self.run_pretrain_routine(model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1065, in run_pretrain_routine
     self.reset_val_dataloader(ref_model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py"", line 331, in reset_val_dataloader
     self._reset_eval_dataloader(model, 'val')
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py"", line 314, in _reset_eval_dataloader
     f'you requested to check {limit_eval_batches} of the {mode} dataloader but'
 pytorch_lightning.utilities.exceptions.MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091
 </denchmark-code>
 
 P.S.: I also tried setting limit_val_batches=0.09090909090909091. Same error.
 	",e6c7548b306055e41552e23d57f0057e7f441256,Adrian Wälchli,2020-09-19 19:00:58-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"69,70,71",,1.0,drozzy,2020-06-21T23:47:34Z,"
 		Did you check if the length of your dataloader (how many iteration it has) is different than 0?
 		",2.0,drozzy,2020-06-21T23:49:05Z,"
 		Not sure what you mean, but I am able to train the regular way... without the overfit_batches setting.
 		",3.0,drozzy,2020-06-21T23:50:29Z,"
 		Does the validation step of your model goes without any problem?
 		",4.0,drozzy,2020-06-21T23:56:00Z,"
 		Oh yeah.
 		",MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,158,158,_get_distributed_sampler,"self,dataloader,train",143,160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,drozzy,2020-06-22T20:04:24Z,"
 		
 Not sure what you mean, but I am able to train the regular way... without the overfit_batches setting.
 
 can you check the value of len(valid_dataloader)??
 		",6.0,drozzy,2020-06-22T21:52:27Z,"
 		I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.
 		",7.0,drozzy,2020-06-23T00:26:27Z,"
 		
 When I try to use overfit_batches:
 https://pytorch-lightning.readthedocs.io/en/latest/debugging.html#make-model-overfit-on-subset-of-data
  trainer = Trainer(gpus=num_gpus, max_epochs=config.epochs, overfit_batches=0.01, logger=logger)
 
 my code fails with:
    trainer.fit(module)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 918, in fit
     self.single_gpu_train(model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 176, in single_gpu_train
     self.run_pretrain_routine(model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1065, in run_pretrain_routine
     self.reset_val_dataloader(ref_model)
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py"", line 331, in reset_val_dataloader
     self._reset_eval_dataloader(model, 'val')
   File ""/home/andriy/miniconda3/envs/patchy_discs_model/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py"", line 314, in _reset_eval_dataloader
     f'you requested to check {limit_eval_batches} of the {mode} dataloader but'
 pytorch_lightning.utilities.exceptions.MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091
 
 P.S.: I also tried setting limit_val_batches=0.09090909090909091. Same error.
 
 try using a slightly bigger overfit_batches (e.g. overfit_batches=0.1) or number of batches (e.g. overfit_batches=10)
 
 I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.
 
 Probably the training get's stuck in local optima.
 		",8.0,drozzy,2020-06-23T07:38:17Z,"
 		
 I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.
 
 overfit_batches just reduces your num_batches so that it can overfit your model on a small batch to check whether the model can adapt your dataset or not. It will still run for n epochs even if you set overfit_batches to any value.
 		",9.0,drozzy,2020-06-24T03:56:11Z,"
 		<denchmark-link:https://github.com/Kshitij09>@Kshitij09</denchmark-link>
  you also need to adjust your learning rate... otherwise it might get stuck in a local min (likely lower your lr)
 		",10.0,drozzy,2020-06-24T13:34:20Z,"
 		
 
 I've also tried using overfit_batches on MNIST dataset and it didn't work. The training continues for 1000 epochs and stops (hitting max_epochs). I observed the loss fluctuating around 0.1 to 0.2 all the time whereas, in actual training, my model reached train_loss=0.02 in just 4 epochs.
 
 overfit_batches just reduces your num_batches so that it can overfit your model on a small batch to check whether the model can adapt your dataset or not. It will still run for n epochs even if you set overfit_batches to any value.
 
 <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  so do I need to couple it with  ?
 		",11.0,drozzy,2020-06-24T13:36:54Z,"
 		
 @Kshitij09 you also need to adjust your learning rate... otherwise it might get stuck in a local min (likely lower your lr)
 
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  yes I've also incorporated  with this which dropped lr upto  but didn't stop training
 		",12.0,drozzy,2020-08-25T15:31:29Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",13.0,drozzy,2020-09-02T09:31:51Z,"
 		import os
 import time
 
 import torch
 import torch.nn.functional as F
 from torchvision.datasets import MNIST
 from torch.utils.data import DataLoader, random_split
 from torchvision import transforms
 import pytorch_lightning as pl
 
 
 class LitClassifier(pl.LightningModule):
 
     def __init__(self):
         super().__init__()
         self.l1 = torch.nn.Linear(28 * 28, 10)
         self.i = 0
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_idx):
         x, (y, idxs) = batch
         print(f""training step {self.i}, batch_idx {batch_idx}, items: {idxs.cpu().numpy()}"")
         self.i += 1
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         result = pl.TrainResult(loss)
         result.log('train_loss', loss, on_epoch=True)
         return result
 
     def validation_step(self, batch, batch_idx):
         x, (y, idxs) = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         result = pl.EvalResult(checkpoint_on=loss)
         result.log('val_loss', loss)
         return result
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
 
 class MNISTDataset(MNIST):
     def __getitem__(self, item):
         x, y = super().__getitem__(item)
         return (x, (y, item))
 
 
 # train!
 dataset = MNISTDataset(os.getcwd(), download=True, transform=transforms.ToTensor())
 train, val = random_split(dataset, [55000, 5000])
 
 model = LitClassifier()
 trainer = pl.Trainer(overfit_batches=1, gpus=1, progress_bar_refresh_rate=0)
 trainer.fit(model, DataLoader(train, shuffle=False, batch_size=4, num_workers=0),
             DataLoader(val, batch_size=4, num_workers=0))
 Produces
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0]
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not l
 og computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
 warnings.warn(*args, **kwargs)
 
 | Name | Type   | Params
 --------------------------------
 0 | l1   | Linear | 7 K
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa
 der, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try
 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa
 der, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try
 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 training step 0, batch_idx 0, items: [13556 55560 49266  5079]
 training step 1, batch_idx 0, items: [13556 55560 49266  5079]
 training step 2, batch_idx 0, items: [13556 55560 49266  5079]
 training step 3, batch_idx 0, items: [13556 55560 49266  5079]
 training step 4, batch_idx 0, items: [13556 55560 49266  5079]
 training step 5, batch_idx 0, items: [13556 55560 49266  5079]
 training step 6, batch_idx 0, items: [13556 55560 49266  5079]
 training step 7, batch_idx 0, items: [13556 55560 49266  5079]
 training step 8, batch_idx 0, items: [13556 55560 49266  5079]
 training step 9, batch_idx 0, items: [13556 55560 49266  5079]
 training step 10, batch_idx 0, items: [13556 55560 49266  5079]
 training step 11, batch_idx 0, items: [13556 55560 49266  5079]
 training step 12, batch_idx 0, items: [13556 55560 49266  5079]
 training step 13, batch_idx 0, items: [13556 55560 49266  5079]
 training step 14, batch_idx 0, items: [13556 55560 49266  5079]
 training step 15, batch_idx 0, items: [13556 55560 49266  5079]
 training step 16, batch_idx 0, items: [13556 55560 49266  5079]
 training step 17, batch_idx 0, items: [13556 55560 49266  5079]
 training step 18, batch_idx 0, items: [13556 55560 49266  5079]
 training step 19, batch_idx 0, items: [13556 55560 49266  5079]
 training step 20, batch_idx 0, items: [13556 55560 49266  5079]
 training step 21, batch_idx 0, items: [13556 55560 49266  5079]
 training step 22, batch_idx 0, items: [13556 55560 49266  5079]
 training step 23, batch_idx 0, items: [13556 55560 49266  5079]
 training step 24, batch_idx 0, items: [13556 55560 49266  5079]
 </denchmark-code>
 
 Toggling shuffle=True results in
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0]
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not l
 og computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
 warnings.warn(*args, **kwargs)
 
 | Name | Type   | Params
 --------------------------------
 0 | l1   | Linear | 7 K
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You request
 ed to overfit but enabled training dataloader shuffling. We are turning it off for you.
 warnings.warn(*args, **kwargs)
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa
 der, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try
 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 /home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloa
 der, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try
 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 training step 0, batch_idx 0, items: [19838 40946 18620 21942]
 training step 1, batch_idx 0, items: [59940 37871 44153  6899]
 training step 2, batch_idx 0, items: [53096 12012 22479 27454]
 training step 3, batch_idx 0, items: [12640 55771 22517 27844]
 training step 4, batch_idx 0, items: [35820 56735 43191 58511]
 training step 5, batch_idx 0, items: [35818 38129  4901 46901]
 training step 6, batch_idx 0, items: [21038 14631 15166 15581]
 training step 7, batch_idx 0, items: [ 7095 15539  8672 39255]
 training step 8, batch_idx 0, items: [ 6397 24324 27822 53308]
 training step 9, batch_idx 0, items: [ 7261 45991 58502 38393]
 training step 10, batch_idx 0, items: [50646 43129  4348 32436]
 training step 11, batch_idx 0, items: [11271 13858 11991 43261]
 training step 12, batch_idx 0, items: [29346 42714 52281 36790]
 training step 13, batch_idx 0, items: [21324 32598 43017  8024]
 training step 14, batch_idx 0, items: [30809 50140  5554 36657]
 training step 15, batch_idx 0, items: [ 1462  4226 44369 40183]
 training step 16, batch_idx 0, items: [53579 10375 22340  4105]
 training step 17, batch_idx 0, items: [47785 10585 12661 35176]
 training step 18, batch_idx 0, items: [16489 26748 25997  8344]
 training step 19, batch_idx 0, items: [38492 45758 56593 37933]
 training step 20, batch_idx 0, items: [  527  4662 29285 26215]
 training step 21, batch_idx 0, items: [ 1838 42586  9805 13441]
 training step 22, batch_idx 0, items: [12649 11892  4140 56752]
 training step 23, batch_idx 0, items: [48902 57464 57910 54211]
 training step 24, batch_idx 0, items: [38774 16780 10018 49934]
 </denchmark-code>
 
 Note the warning
 <denchmark-code>/home/willprice/.conda/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You request
 ed to overfit but enabled training dataloader shuffling. We are turning it off for you.
 </denchmark-code>
 
 Is incorrect and very misleading to the user.
 		",14.0,drozzy,2020-09-02T12:12:34Z,"
 		Similar issue here.
 With DDP backend the trainer keeps drawing random samples even if shuffle is manually set to False (works fine with DP backend).
 Other configs/params:
 gpus=2
 batch_size=1
 overfit_batches=4
 accumulate_grad_batches=1
 num_workers=2 (larger values resulted in the trainer draw overfit_batchesnum_workersbatch_size random samples every epoch which is hard to follow)
 Minimal code for reproduction (I also tried with different number of batches, batch size, num workers, etc.)
 <denchmark-code>import torch
 from torch.nn import Conv2d
 from torch.optim import SGD
 from torch.utils.data import DataLoader, Dataset
 from pytorch_lightning.metrics.regression import MSE
 import pytorch_lightning as pl
 from pytorch_lightning import Trainer
 
 
 class MyDataset(Dataset):
     def __init__(self, size=100):
         super(MyDataset, self).__init__()
         self.data = torch.stack([idx * torch.ones(3,100,100) for idx in range(size)])
         self.idx_list = []
 
     def __getitem__(self, idx):
         return self.data[idx]
 
     def __len__(self):
         return self.data.shape[0]
 
 class MyModel(pl.LightningModule):
     def __init__(self):
         super(MyModel, self).__init__()
         self.conv_1 = Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1)
         self.loss = MSE()
         self.idx_list = []
     def forward(self, batch):
         return self.conv_1(batch)
     
     def training_step(self, batch, batch_idx):
         idx = batch[0,0,0,0].detach()
         pred = self.forward(batch)
         loss = self.loss(pred, batch)
         return {'loss': loss, 'idx': idx}
 
     def training_epoch_end(self, outputs):
         idx_list = torch.tensor([x['idx'] for x in outputs])
         print('Epoch: {}, device: {} samples: {}'.format(self.current_epoch, self.device, idx_list))
         return torch.stack([x['loss'] for x in outputs]).mean()
 
     def setup(self, stage):
         self.dataset = MyDataset()
 
     def train_dataloader(self):
         loader = DataLoader(self.dataset, batch_size=1, num_workers=20, pin_memory=True, shuffle=False)
         return loader
 
     def configure_optimizers(self):
         return SGD(self.parameters(), lr=0.001)
 
 
 def main():
 
     pl_model = MyModel()
     # trainer = Trainer(distributed_backend='ddp', num_nodes=1, gpus=2, overfit_batches=4)
     trainer = Trainer(distributed_backend='ddp', gpus=2, overfit_batches=5, max_epochs=4, check_val_every_n_epoch=100)
     trainer.fit(pl_model)
 
 
 if __name__ == '__main__':
     main()
 </denchmark-code>
 
 Output (ddp backend):
 
 Epoch: 0, device: cuda:0 samples: tensor([44., 93., 71., 37., 53.])
 Epoch: 0, device: cuda:1 samples: tensor([19., 90., 69., 95., 91.])
 Epoch: 1, device: cuda:0 samples: tensor([45., 90., 35., 17., 79.])
 Epoch: 1, device: cuda:1 samples: tensor([15., 32., 63., 72., 96.])
 Epoch: 2, device: cuda:0 samples: tensor([48.,  1., 90., 10.,  7.])
 Epoch: 2, device: cuda:1 samples: tensor([97., 81., 49.,  8., 20.])
 Epoch: 3, device: cuda:0 samples: tensor([86., 89.,  3., 22., 25.])
 Epoch: 3, device: cuda:1 samples: tensor([42., 92., 20., 48., 93.])
 
 Output (dp backend):
 
 Epoch: 0, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])
 Epoch: 1, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])
 Epoch: 2, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])
 Epoch: 3, device: cuda:0 samples: tensor([0., 1., 2., 3., 4.])
 
 		",15.0,drozzy,2020-09-17T22:04:54Z,"
 		I just looked at it. In summary:
 
 
 OP had this problem:
 MisconfigurationException: you requested to check 0.01 of the val dataloader but 0.01*0 = 0. Please increase the limit_val_batches. Try at least limit_val_batches=0.09090909090909091
 This message is correct, it is telling you that the percentage you have chosen corresponds to less than one batch.
 Solution:
 You need to increase the value. But what you probably want is overfit_batches=1, and this works with exactly one batch without error.
 
 
 The example code by @willprice works on master #3501
 
 
 The example by @itsikad shows an issue with DDP. As far as I can tell, this is the only remaining problem in this thread here. I can take a look
 
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2314,digitalillusions,2020-06-22T12:59:13Z,2020-06-24T03:41:03Z,Breaking compatibility with custom datatypes implementing `.to`,"
 <denchmark-h:h2>🚀 Feature</denchmark-h>
 
 Bring back compatibility for custom datatypes in collections implementing .to for transferring data.
 <denchmark-h:h3>Motivation</denchmark-h>
 
 I am using Pytorch Lightning together with Pytorch Geometric. Pytorch Geometric implements several custom datatypes and dataloaders which is really useful for geometric deep learning. Everything worked well with pytorch lightning 0.7.6, as the custom datatypes implement a .to method for transferring the data to different devices.
 However, with the recent 0.8.1 update, this is no longer possible and I had to scour the documentation to be able to implement a fix using transfer_batch_to_device(batch, device). This is in my opinion not very pretty, as my batch looks like this
 <denchmark-code>{""data"": pytorch geometric batch object, ""id"": tensor, ...}
 </denchmark-code>
 
 i.e. it is just a dictionary of types that all implement the .to method.
 <denchmark-h:h3>Pitch</denchmark-h>
 
 
 Make it possible for classes implementing the .to method to be transferred automatically
 If part of the batch could not be transferred automatically output a warning letting the user know, that a custom transfer function for the batch might be required, or to implement the .to method for custom datatypes in the batch
 Add a note to the introduction guide about custom datatypes and handling for custom datatypes
 
 <denchmark-h:h3>Alternatives</denchmark-h>
 
 If this change was intentional and the behavior of trying to call the .to method is not desired, I think there should definitely be some more documentation about this, in a more obvious place.
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",aab9e77d2d4ac601a08ca6365dd846a88b83517f,Adrian Wälchli,2020-06-23 23:41:02-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"31,32",,1.0,digitalillusions,2020-06-22T12:59:55Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,digitalillusions,2020-06-23T16:14:37Z,"
 		This should definitely work. It was not my intention to break it and I thought I had even a test for it. I will look into this and also converting the issue to a bug report.
 		",3.0,digitalillusions,2020-06-23T16:55:49Z,"
 		Have identified the problem and fix is in the works... Silly oversight on my part.
 Regarding the warning, not sure. Is it not enough to point out in the docs that anything not a tensor (or .to() movable) will be left untouched unless the hook is implemented?
 Otherwise, such a check would have to be done on every batch, because in theory the batch can (structurally) change every iteration, and that would cause a spam of warning messages.
 		",4.0,digitalillusions,2020-06-23T18:10:40Z,"
 		
 Have identified the problem and fix is in the works... Silly oversight on my part.
 
 Awesome, thank you!
 
 Regarding the warning, not sure. Is it not enough to point out in the docs that anything not a tensor (or .to() movable) will be left untouched unless the hook is implemented?
 Otherwise, such a check would have to be done on every batch, because in theory the batch can (structurally) change every iteration, and that would cause a spam of warning messages.
 
 That's a good point, in that case I think the docs should suffice. Though maybe the compatible data types can be mentioned in the Introduction Guide? Reading through the Introduction Guide I found no mention of this anywhere, and I think it may fit well into the Data Section. Possibly after the Models defined by data section, there could be a section which talks briefly about custom data loaders, custom batches and what kind of custom data lightning can handle automatically. Maybe also just refer to the appropriate place in the documentation.
 		",MODIFY,1.0,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,207,207,transfer_batch_to_device,"self,Any,device",200,243,MODIFY,4.0,pytorch_lightning\utilities\apply_func.py,pytorch_lightning\utilities\apply_func.py,1.0,"57,58","57,58",MODIFY,3.0,tests\models\test_gpu.py,tests\models\test_gpu.py,1.0,"289,290,291,292,293,294,295,296,297,298,299,300",,test_single_gpu_batch_parse,,245,300,,,,,,,,,,,,5.0,digitalillusions,2020-06-23T22:18:23Z,"
 		<denchmark-link:https://github.com/digitalillusions>@digitalillusions</denchmark-link>
  would you like to review/test the fix? <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2335>#2335</denchmark-link>
 
 Also, since the docs were your idea, would you like to make a separate PR with your docs suggestion to the intro guide?
 		",6.0,digitalillusions,2020-06-23T22:25:22Z,"
 		Sure thing, I think I'll have time to review and to test it tomorrow.
 I can also try my hand at a pull request for the docs, though I'm still fairly new to lightning.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,move_data_to_device.to,tensor,57,58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"295,296,297",,test_single_gpu_batch_parse.to,"self,args,kwargs",295,297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"87,88",,move_data_to_device.to,data,87,88,1.0,"63,64,65,66,67",,__subclasshook__,"cls,subclass",63,67,1.0,"292,293",,test_single_gpu_batch_parse.__init__,self,292,293,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"72,73,76,77,78,87,88,89",,move_data_to_device,"Any,device",70,89,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2315,elias-ramzi,2020-06-22T13:25:26Z,2020-06-26T13:33:36Z,Bug in average_precision Metric,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Hi everyone, I encountered a bug when using the average_precision metric (pytorch_lightning.metrics.functional.classification). It yields incorrect results (negative ones).
 There seems to be a missing parenthesis in the code here :
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/functional/classification.py#L847>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/functional/classification.py#L847</denchmark-link>
 
 It works when corrected as :
 return -torch.sum((recall[1:] - recall[:-1]) * precision[:-1])
 In order to reproduce negative results :
 <denchmark-code>import torch
 import pytorch_lightning.metrics.functional.classification as M
 
 torch.manual_seed(23)
 truth = (torch.rand(100) > .6)
 pred = torch.rand(100)
 
 M.average_precision(pred, truth)
 </denchmark-code>
 
 I did not find an issue on this topic yet. If needed I can submit a PR.
 Thanks ☺️
 	",92f122e0df7e233f3a8b7873c7294155afbbf852,elias-ramzi,2020-06-23 13:21:00+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"23,24",,1.0,elias-ramzi,2020-06-22T13:26:06Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,elias-ramzi,2020-06-22T14:58:38Z,"
 		I'd like to fix it
 		",3.0,elias-ramzi,2020-06-22T18:22:07Z,"
 		Hi, I have already created a branch for the PR !
 		",4.0,elias-ramzi,2020-06-22T20:10:01Z,"
 		Thanks for the issue and the fix! <denchmark-link:https://github.com/InCogNiTo124>@InCogNiTo124</denchmark-link>
  Just saying- there are many other open issues if you want to take a stab at :)
 		",MODIFY,0.0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,0.0,847,847,,,,,MODIFY,2.0,tests\metrics\functional\test_classification.py,tests\metrics\functional\test_classification.py,1.0,"345,352,353,354,355,356","345,348,350,351,353,356",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_average_precision_constant_values,,345,356,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"356,357",356,test_average_precision,"scores,target,expected_score",356,357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2330,stllfe,2020-06-23T14:44:48Z,2020-06-25T13:21:43Z,`use_amp` and multiple optimizers bug,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Faced this issue when tried to use mixed precision with my two-head model which has two pairs of optimizer & scheduler. Without use_amp everything works fine.
 With it enabled, I get:
 <denchmark-code>TypeError: 'CosineAnnealingLR' object is not subscriptable
 </denchmark-code>
 
 My investigation has ended up here:
     def reinit_scheduler_properties(self, optimizers: list, schedulers: list):
         # Reinitialize optimizer.step properties added by schedulers
         for scheduler in schedulers:
             for optimizer in optimizers:
                 scheduler = scheduler['scheduler']  # <===== this place
                 # ...
                 if scheduler.optimizer == optimizer:
                     # ...
 Obviously, next optimizer will get scheduler as an actual non-dict object as it was reassigned on the first iteration...
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Build any LightningModule, which configure_optimizers() method outputs lists of two optimizers and two schedulers. In my case it's something like:
 
 def configure_optimizers(self):
     opt_1 = Adam(params=self.head_1.params(), ...)
     opt_2 = Adam(params=self.head_2.params(), ...)
     
     sch_1 = CosineAnnealingLR(optimizer=opt_1, ...)
     sch_2 = CosineAnnealingLR(optimizer=opt_2, ...)
     return [opt_1, opt_2], [sch_1, sch_2]
 
 Build a Trainer object with  use_amp=True
 Call trainer.fit(model)
 See error
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
                 - GeForce GTX 1080 Ti
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.18.5
         - pyTorch_debug:     False
         - pyTorch_version:   1.5.0
         - pytorch-lightning: 0.8.1
         - tensorboard:       1.15.0
         - tqdm:              4.45.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 -
         - processor:         x86_64
         - python:            3.7.5
         - version:           #107-Ubuntu SMP Thu Jun 4 11:27:52 UTC 2020
 </denchmark-code>
 
 	",c275e1fc91df4d351799b633e9df08e010094bfe,William Falcon,2020-06-25 09:21:41-04:00,MODIFY,1,pytorch_lightning\trainer\optimizers.py,pytorch_lightning\trainer\optimizers.py,1.0,"114,115,121,122,123,124,126,127,128,129,130,131,132","115,120,122",1.0,stllfe,2020-06-23T14:45:28Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,stllfe,2020-06-24T10:09:24Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  this seems easy to fix (move the  part to the outer loop), and we could also fix issue <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2078>#2078</denchmark-link>
  while fixing this function
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,reinit_scheduler_properties,"self,list,list",111,132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2333,filaPro,2020-06-23T20:59:22Z,2020-07-10T01:20:02Z,AttributeError: 'LightningDataParallel' object has no attribute 'teardown',"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 <denchmark-code>trainer = pytorch_lightning.Trainer(
     gpus=2,
     distributed_backend='dp'
 )
 model = BaseModel.load_from_checkpoint(...)
 trainer.test(model)
 </denchmark-code>
 
 Traceback (most recent call last):
 File ""run_kitti.py"", line 351, in 
 trainer.test(model)
 File ""/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1198, in test
 self.model.teardown('test')
 File ""/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 594, in getattr
 type(self).name, name))
 AttributeError: 'LightningDataParallel' object has no attribute 'teardown'
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 GeForce GTX 1080 Ti
 GeForce GTX 1080 Ti
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.1
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.8.1
 tensorboard:       2.2.2
 tqdm:              4.46.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.7.7
 version:           #53~18.04.1-Ubuntu SMP Thu Jun 4 14:58:26 UTC 2020
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 If I'm not missing something, this AttributeError is a bug on your side.
 	",d22181714ac3f201ea7a35b7fd06d85db82c0465,William Falcon,2020-06-25 11:10:17-04:00,MODIFY,0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"1196,1197",1196,1.0,filaPro,2020-06-23T21:00:02Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,filaPro,2020-06-24T00:50:29Z,"
 		+1 on this issue.
 		",3.0,filaPro,2020-06-24T09:28:13Z,"
 		Also confirm this issue.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2334,dscarmo,2020-06-23T21:05:29Z,2020-06-28T21:20:34Z,LightningModule.load_from_checkpoint not working with .ckpt from 0.7.6,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Trying to use an old experiment .ckpt (generated with ModelCheckpoint(monitor=""val_loss"", mode=""min') in 0.7.6) results in an error when trying to load.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Train something with 0.7.6 and save checkpoint with the checkpoint callback
 Try to load checkpoint with 0.8.1
 
 <denchmark-code>Trying to load: ckpts/MNIST_0.7.6epoch=4.ckpt with PL version: 0.8.1
 ---------------------------------------------------------------------------
 TypeError                                 Traceback (most recent call last)
 <ipython-input-3-36a7290b43c6> in <module>()
      35 selected_ckpt = glob(os.path.join(""ckpts"", ""*.ckpt""))[0]
      36 print(f""Trying to load: {selected_ckpt} with PL version: {pl.__version__}"")
 ---> 37 LitClassifier.load_from_checkpoint(selected_ckpt)
 
 1 frames
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/saving.py in load_from_checkpoint(cls, checkpoint_path, map_location, hparams_file, tags_csv, *args, **kwargs)
     169         checkpoint[cls.CHECKPOINT_HYPER_PARAMS_KEY].update(kwargs)
     170 
 --> 171         model = cls._load_model_state(checkpoint, *args, **kwargs)
     172         return model
     173 
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/saving.py in _load_model_state(cls, checkpoint, *args, **kwargs)
     184 
     185             if cls.CHECKPOINT_HYPER_PARAMS_TYPE in checkpoint:
 --> 186                 model_args = checkpoint[cls.CHECKPOINT_HYPER_PARAMS_TYPE](model_args)
     187 
     188             args_name = checkpoint.get(cls.CHECKPOINT_HYPER_PARAMS_NAME)
 
 TypeError: 'str' object is not callable
 </denchmark-code>
 
 Here is a Colab notebook reproducing the issue: <denchmark-link:https://colab.research.google.com/drive/1s7SzG3EkLJZOmUOkb0mJ85zDcOxBcDLG?usp=sharing>https://colab.research.google.com/drive/1s7SzG3EkLJZOmUOkb0mJ85zDcOxBcDLG?usp=sharing</denchmark-link>
 
 Maybe this is related to the changes in hparams recently?
 It happened in my local enviroment and in Google Colab's enviroment.
 I was able to load the checkpoint after downgrading to 0.7.6 without problems.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Being able to use old .ckpts.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.5
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.6.9
 CUDA/cuDNN version: 10.2
 GPU models and configuration: 1060
 Any other relevant information:
 
 Also happened in the Colab enviroment.
 	",861a73be12ef17214bb0ed49aabc9f48a80fde16,Jirka Borovec,2020-06-28 17:20:33-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"45,46",,1.0,dscarmo,2020-06-25T12:51:43Z,"
 		Added a Google Colab notebook reproducing the issue.
 		",2.0,dscarmo,2020-06-29T01:55:26Z,"
 		<denchmark-link:https://user-images.githubusercontent.com/3640001/85965231-09d80980-b98a-11ea-9291-0510ebebf40e.png></denchmark-link>
 
 <denchmark-link:https://github.com/dscarmo>@dscarmo</denchmark-link>
  no more bug :) <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  fixed it!
 		",,,,,,,,,MODIFY,2.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,"250,251,252,253,254,255,256,257,258,259",,_convert_loaded_hparams,"dict,Callable,None",250,259,MODIFY,2.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,271,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_collect_init_arguments,"tmpdir,cls",249,291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,183,"183,184",_load_model_state,"cls,str,cls_args,cls_kwargs",173,208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,395,,test_load_past_checkpoint,"tmpdir,past_key",384,403,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2359,narain1,2020-06-25T11:45:38Z,2020-07-23T13:32:12Z,Problem with loading checkpoint of a model with embeddings,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Unable to load from checkpoint for model with embeddings
 <denchmark-h:h4>Code sample</denchmark-h>
 
 model arch
 <denchmark-code>class Model(pl.LightningModule):
       def __init__(self, emb_szs):
             super().__init__()
             m = get_base()
             self.enc =  nn.Sequential(*list(m.children())[:-1], nn.Flatten())    
             nc = list(m.children())[-1].in_features
             self.head = nn.Sequential(nn.Linear(2*nc+25,512),Mish(),
                                 nn.BatchNorm1d(512), nn.Dropout(0.5),nn.Linear(512,2))
             self.embs = nn.ModuleList([nn.Embedding(c, s) for c,s in emb_szs])
     
       def forward(self, xb, x_cat, x_cont):
              x1 = [e(x_cat[:,i]-1) for i,e in enumerate(self.embs)]
              x1 = torch.cat(x1, 1)
              x_img = self.enc(xb)
              x = torch.cat([x1, x_cont.unsqueeze(1)], 1)
              x = torch.cat([x, x_img], 1)
              return self.head(x)
 </denchmark-code>
 
 <denchmark-code>  checkpoint_callback = ModelCheckpoint(
              filepath=os.path.join(os.getcwd(), 'model_dir'),
              #     save_top_k=True,
              verbose=True,
              monitor='val_loss',
              mode='min',
              prefix=''
              )
 
    trainer = Trainer(max_epochs=15, 
               early_stop_callback = early_stopping,
               gpus=1,
               gradient_clip_val=1.0,
               weights_save_path=os.getcwd(),
               checkpoint_callback = checkpoint_callback,
               num_sanity_val_steps=0
              )
 </denchmark-code>
 
 <denchmark-h:h4>the training loop has no problem but when I call trainer.test() a runtime error arrises</denchmark-h>
 
 RuntimeError: Error(s) in loading state_dict for Model:
 Unexpected key(s) in state_dict: ""embs.0.weight"", ""embs.1.weight"", ""embs.2.weight"", ""embs.3.weight"".
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 As in the documentation It should have used the best checkpoint for test but loading checkpoint fails
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 Tesla P100-PCIE-16GB
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.1
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.8.1
 tensorboard:       2.2.2
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.7.6
 version:           #1 SMP Sat Jun 13 11:04:33 PDT 2020
 
 
 
 	",51711c265a9e234f2b4164f1a2fab73373707d61,Jirka Borovec,2020-06-27 16:38:03-04:00,MODIFY,0,.github\PULL_REQUEST_TEMPLATE.md,.github\PULL_REQUEST_TEMPLATE.md,0.0,14,14,1.0,narain1,2020-06-25T11:46:23Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,narain1,2020-06-28T19:35:09Z,"
 		<denchmark-link:https://github.com/narain1>@narain1</denchmark-link>
  mind sharing a minimal running example, in your case there are several functions that cannot be traced...
 		",3.0,narain1,2020-06-29T03:33:52Z,"
 		<denchmark-link:https://github.com/narain1/projects/blob/master/melanoma-lit-x2.ipynb>https://github.com/narain1/projects/blob/master/melanoma-lit-x2.ipynb</denchmark-link>
 
 Above is the link to the jupyter notebook along with the stack trace
 		",,,,,MODIFY,0.0,CHANGELOG.md,CHANGELOG.md,0.0,"43,44",,,,,,MODIFY,2.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,"173,187,188,189,192,193,194,195,197,198,200,203","173,187,190,191,193,194,196,199",MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,327,327,dump_checkpoint,"self,bool",315,374,MODIFY,2.0,tests\base\model_template.py,tests\base\model_template.py,1.0,"39,40,41,42,43,44,45,46,47,48,49,50,51","39,40,41,42,43,44,45,46,47,48,49,50,51",__init__,"self,args,float,int,int,float,str,str,int,int,float,float,kwargs",39,51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_load_model_state,"cls,str,args,kwargs",173,205,,,,,1.0,"39,40,41,42,43,44,45,46,47,48,49,50","39,40,41,42,43,44,45,46,47,48,49,50",__init__,"self,float,int,int,float,str,str,int,int,float,float",39,50,,,,,,,,MODIFY,1.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,"278,279,280,281,282,283,284,285,286","278,279,280,281,282,283,284,285,286,287",test_collect_init_arguments,"tmpdir,cls",249,290,,,,,,,,MODIFY,1.0,tests\test_deprecated.py,tests\test_deprecated.py,,,,,,,,1.0,"132,145","131,133,146",test_tbd_remove_in_v1_0_0_model_hooks,,130,156,,,,,,,,MODIFY,1.0,tests\trainer\test_lr_finder.py,tests\trainer\test_lr_finder.py,1.0,185,185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"173,187,188,189,192,193,194,195,197,198,200,203","173,187,190,191,193,194,196,199",_load_model_state,"cls,str,cls_args,cls_kwargs",173,209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_suggestion_with_non_finite_values,tmpdir,181,199,,,,,,,,MODIFY,1.0,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,234,234,test_configure_optimizer_from_dict,tmpdir,223,242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2371,pvnieo,2020-06-26T10:44:44Z,2020-08-03T21:50:08Z,hparams are not logged in tensorboard,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I'm using the latest version of PL. My problem is when I launch an experiment, everything is logged correctly to tensorboard, except the hparams:
 <denchmark-link:https://user-images.githubusercontent.com/22404728/85848735-f2422a80-b7a9-11ea-80f6-96ab9d6a4a61.png></denchmark-link>
 
 I tried to launch the cpu_template on a fresh virtual environement, but it didn't work. However, when I launch the pytorch example of logging hparams to tensorboard (<denchmark-link:https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams>https://pytorch.org/docs/stable/tensorboard.html#torch.utils.tensorboard.writer.SummaryWriter.add_hparams</denchmark-link>
 ), it worked!
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 I succeeded to reproduce this using the official MNIST collab (<denchmark-link:https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg>https://colab.research.google.com/drive/1F_RNcHzTfFuQf-LeKvSlud6x7jXYkG31#scrollTo=HOk9c4_35FKg</denchmark-link>
 ), by skipping the ""Simplest example"" and excuting directly '1. LightningModule'
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.5
 OS (e.g., Linux): ubuntu 20.04
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.8 / 3.6
 CUDA/cuDNN version: 10.2
 GPU models and configuration: GeForce GTX 1060
 Any other relevant information:
 
 	",11069c87845ea9a14e6fe807094313a67f9946dc,William Falcon,2020-07-07 12:24:56-04:00,MODIFY,1,pytorch_lightning\core\decorators.py,pytorch_lightning\core\decorators.py,1.0,16,16,1.0,pvnieo,2020-06-29T02:25:22Z,"
 		Not sure if it's related - I'm getting hparams but not metrics <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2406>#2406</denchmark-link>
 
 		",2.0,pvnieo,2020-07-01T16:52:07Z,"
 		UPDATE: I used the same code, with the same version in new folder (just copy past the code), and now I have the hparams, but no metrics!
 		",3.0,pvnieo,2020-07-07T17:55:51Z,"
 		putting a reminder here that we saw this problem popping up recently in one of the tests.
 See here, <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2512#discussion_r451011814>#2512 (comment)</denchmark-link>
 
 need to look at the test there to debug the problem.
 		",4.0,pvnieo,2020-08-03T21:50:08Z,"
 		Closing issue. <denchmark-link:https://github.com/pvnieo>@pvnieo</denchmark-link>
  please let us know if you have any issues!
 		",MODIFY,1.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,1.0,"110,111",,experiment,"self,exp",110,111,MODIFY,2.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,343,343,MODIFY,5.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"463,465,466,467,468,469,470,471,472,473,474,475,556,557,558,559,560,561,562,563,564,565,566",538,ddp_train,"self,process_idx,q,model,is_master,proc_offset",463,566,MODIFY,4.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"235,236,270,273","236,237,267,268",dp_train,"self,model",229,273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,reset_val_dataloader,"self,LightningModule",336,343,data_loader,fn,10,20,1.0,"186,187",185,single_gpu_train,"self,model",167,187,1.0,226,,tpu_train,"self,tpu_core_idx,model",189,227,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"394,432",,run_evaluation,"self,bool",352,432,,,,,,,,MODIFY,4.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"458,459,461,462,463,465,466,467,468,469,470,471,472,473,474,475","451,453,454,455,456,457,538",ddp_train,"self,process_idx,model,is_master,proc_offset",451,538,1.0,"1356,1357",,barrier,"self,name",1354,1361,1.0,"1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051","1035,1040",__run_ddp_spawn,"self,model,nprocs",1034,1051,MODIFY,3.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"510,511",510,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,224,224,reset_train_dataloader,"self,LightningModule",199,250,,,,,,,,1.0,"387,391,392,393,394,395,396,397,398,399","387,388,389,390",set_random_port,"self,force",387,401,1.0,"458,459,461","449,451,453,454,455,456,457",spawn_ddp_children,"self,model",403,461,1.0,"387,391,392","380,384,385,386,387,388,389,390",set_random_port,self,380,392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,check_checkpoint_callback,"self,should_check_val",507,513,1.0,"892,893,894,895,896",,run_training_teardown,self,863,896,MODIFY,4.0,tests\base\deterministic_model.py,tests\base\deterministic_model.py,1.0,"18,24,25","22,25",__init__,"self,weights",11,25,1.0,28,,forward,"self,x",27,28,1.0,154,,__getitem__,"self,idx",153,154,1.0,"34,35,36","31,32",step,"self,batch,batch_idx",30,40,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tests\base\develop_pipelines.py,tests\base\develop_pipelines.py,1.0,"40,49,58","49,50,51,53,61",run_model_test,"trainer_options,model,bool,version,bool",39,76,1.0,22,"22,23",run_model_test_without_loggers,"trainer_options,model,float",9,36,,,,,,,,,,,,,,,MODIFY,7.0,tests\base\develop_utils.py,tests\base\develop_utils.py,1.0,64,"63,64,65,66",MODIFY,0.0,tests\callbacks\test_early_stopping.py,tests\callbacks\test_early_stopping.py,0.0,,133,,,,,,,,,,,,MODIFY,0.0,tests\loggers\test_all.py,tests\loggers\test_all.py,0.0,159,159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\loggers\test_base.py,tests\loggers\test_base.py,0.0,,61,MODIFY,1.0,tests\loggers\test_tensorboard.py,tests\loggers\test_tensorboard.py,1.0,"35,36,37,38,39,40,41,42,43,44,45","35,36,37,38,39,40,41,42,43,44,45",test_tensorboard_hparams_reload,tmpdir,17,45,,,,,,,,,,,,MODIFY,1.0,tests\models\test_amp.py,tests\models\test_amp.py,MODIFY,0.0,tests\models\test_cpu.py,tests\models\test_cpu.py,MODIFY,11.0,tests\models\test_gpu.py,tests\models\test_gpu.py,1.0,"36,37,38,39,40,41,42,43,44,45,46,47","36,38,39,45,46,48",,,,,,,,,,,,,,,,,,,,,,1.0,"332,336",328,horovod_train,"self,model",275,336,,,,,,,,1.0,"1126,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143","1111,1118,1119",run_pretrain_routine,"self,LightningModule",1071,1180,1.0,"1037,1038,1039,1040,1041,1042",1040,can_prepare_data,self,1037,1042,1.0,,745,call_optimizer_step,"self,optimizer,opt_idx,batch_idx,split_batch",715,760,,,,,,,,,,,,,,,load_model_from_checkpoint,"logger,root_weights_dir,module_class,path_expt",63,66,,,,,,,,,,,,,,,,,,,,,,1.0,106,106,test_amp_gpu_ddp_slurm_managed,tmpdir,87,121,,,,,,,,0.0,,110,,,,,,,,,,,,,,,,,,,test_multi_gpu_early_stop_ddp_spawn,tmpdir,36,51,1.0,"116,127,131","116,117,118,119,120,121,122,123,124,125,126,127",test_multi_gpu_early_stop_dp,tmpdir,116,131,1.0,"64,65,69,70,77,78,79,80,81,82","64,75,79,80,81,82",test_multi_gpu_early_stop,"tmpdir,backend",64,82,1.0,"100,101,102,103,104,105,106,107,108,109,110,111,112","110,111,112",test_single_gpu_model,"tmpdir,gpus",100,112,MODIFY,2.0,tests\models\test_horovod.py,tests\models\test_horovod.py,1.0,132,131,test_horovod_transfer_batch_to_gpu,tmpdir,116,144,1.0,105,104,test_horovod_multi_gpu,tmpdir,96,109,,,,,,,,,,,,,,,MODIFY,1.0,tests\models\test_restore.py,tests\models\test_restore.py,ADD,0.0,None,tests\models\test_test_loop.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114","93,94,95,96,97",pl_multi_process_test,func,91,114,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"99,100,101,102,103,104,105,106",,pl_multi_process_test.pl_multi_process_test.wrapper.inner_f,"queue,kwargs",99,106,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112","94,95,96,97",pl_multi_process_test.wrapper,"args,kwargs",94,112,1.0,"93,94,95,96,97,98","93,94,95,96,97",init_checkpoint_callback,"logger,path_dir",93,98,1.0,"40,43,45,46,48,56","39,43,59,60",get_data_path,"expt_logger,path_dir",38,60,1.0,"86,87",,init_checkpoint_callback,logger,86,88,1.0,"77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95","79,80,81,82,85,95",test_multi_gpu_model_ddp_spawn,tmpdir,77,95,1.0,"55,61,62,64,65,69,70","55,61,62,64",test_multi_gpu_model_dp,tmpdir,55,73,1.0,"143,144,146",,test_ddp_all_dataloaders_passed_to_fit,tmpdir,135,155,1.0,"38,39,40,41,42,43,44,45,46,47,55","38,39,45,46,48,52,53,54,55",test_multi_gpu_model,"tmpdir,backend",38,58,1.0,297,,test_parse_gpu_returns_none_when_no_devices_are_available,"mocked_device_count_0,gpus",297,299,1.0,"18,19,20,23,26,27,28,29,30,31,32","18,19,20,21,26,27,28",test_multi_gpu_none_backend,tmpdir,18,32,1.0,,267,test_parse_gpu_returns_None_when_no_devices_are_available,"mocked_device_count_0,gpus",267,269,1.0,95,"95,96,97",test_running_test_pretrained_model_cpu,tmpdir,70,101,MODIFY,3.0,tests\models\test_tpu.py,tests\models\test_tpu.py,1.0,"25,26,27,28,29,30,31,32,33,34,35,36,37",,test_base_tpu_model,"tmpdir,tpu_cores",25,37,1.0,"42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58",,test_base_tpu_16bit_model,"tmpdir,tpu_cores",42,58,1.0,,"103,104,105,106,107,108,109,110,111,112,113,114,115",test_mixed_precision_with_tpu,"tmpdir,tpu_cores",103,115,MODIFY,4.0,tests\test_deprecated.py,tests\test_deprecated.py,1.0,"20,23,27,34,37,41,44,48,50","20,23,27,34,37,41,44,48,50",test_tbd_remove_in_v0_10_0_trainer,,18,51,1.0,"71,76,79,82,85,88","71,76,79,82,85,88",test_tbd_remove_in_v0_9_0_module_imports,,69,89,1.0,"139,147,152","139,147,152",test_tbd_remove_in_v1_0_0_model_hooks,,130,156,1.0,"56,60,64","56,60,64",test_tbd_remove_in_v0_9_0_trainer,,54,66,MODIFY,4.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"542,552,561,562","537,547,556",test_dataloader_reinit_for_subclass,,536,579,1.0,,537,test_dataloader_reinit_for_subclass.__init__,"self,dataset,batch_size,shuffle,sampler,batch_sampler,num_workers,collate_fn,pin_memory,drop_last,timeout,worker_init_fn,dummy_kwarg",534,537,1.0,"329,330,331,332,333,334",329,test_dataloaders_with_limit_num_batches,"tmpdir,limit_train_batches,limit_val_batches,limit_test_batches",307,334,1.0,542,,test_dataloader_reinit_for_subclass.__init__,"self,dataset,batch_size,shuffle,sampler,batch_sampler,num_workers,collate_fn,pin_memory,drop_last,timeout,worker_init_fn,dummy_kwarg,kwargs",539,542,MODIFY,1.0,tests\trainer\test_trainer_steps.py,tests\trainer\test_trainer_steps.py,1.0,,"20,21",test_training_step_dict,tmpdir,9,50,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35",,test_num_training_batches,tmpdir,11,35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2372,xiadingZ,2020-06-26T13:35:38Z,2020-06-30T14:03:50Z,training_epoch_end's outputs doesn't have 'loss' key,"
 pytorch-lightning: build from master
 <denchmark-code>Traceback (most recent call last):
   File ""main.py"", line 140, in <module>
     main(hparams)
   File ""main.py"", line 72, in main
     trainer.fit(model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 881, in fit
     self.ddp_train(task, model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 539, in ddp_train
     self.run_pretrain_routine(model)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1091, in run_pretrain_routine
     self.train()
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 376, in train
     self.run_training_epoch()
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 510, in run_training_epoch
     self.run_training_epoch_end(epoch_output)
   File ""/mnt/lustre/maxiao1/anaconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 535, in run_training_epoch_end
     epoch_output = model.training_epoch_end(epoch_output)
   File ""/mnt/lustre/maxiao1/PVM/models/baseline.py"", line 335, in training_epoch_end
     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
   File ""/mnt/lustre/maxiao1/PVM/models/baseline.py"", line 335, in <listcomp>
     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
 KeyError: 'loss'
 </denchmark-code>
 
 This is my code:
 <denchmark-code>    def training_step(self, batch, batch_idx):
         ...
         return {'loss': loss, ""train_acc"": acc}
 
     def training_epoch_end(self, outputs):
         avg_loss = torch.stack([x['loss'] for x in outputs]).mean()
         avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()
         logs = {'loss': avg_loss, 'train_acc': avg_acc}
         progress_bar = {'train_loss': avg_loss, 'train_acc': avg_acc}
         results = {
             'log': logs,
             'progress_bar': progress_bar
         }
         return results
 </denchmark-code>
 
 	",a42a0e16ddd75dd7199ecefe4d10c2941c17ba76,William Falcon,2020-06-30 10:03:49-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"779,792",791,1.0,xiadingZ,2020-06-26T21:54:01Z,"
 		Try: avg_loss = torch.stack([x['batch_loss'] for x in outputs]).mean()
 		",2.0,xiadingZ,2020-06-27T01:35:18Z,"
 		Thanks， it works
 but 'train_acc' key doesn't exist, neither do batch_train_acc. How to access other keys returned in training_step?
 		",3.0,xiadingZ,2020-06-27T11:35:13Z,"
 		As of now in lightning you can access them using x['callback_metrics']['loss'] and x['callback_metrics']['train_acc'], but I think it should be handled in a similar way we do this with validation_epoch_end and test_epoch_end.
 		",4.0,xiadingZ,2020-06-29T16:47:24Z,"
 		Hi! One hint: for me it works with ""loss"" under windows but not under ubuntu.
 		",MODIFY,3.0,tests\base\deterministic_model.py,tests\base\deterministic_model.py,1.0,"92,93,96","92,93,96",training_step_end_dict,"self,output",76,96,MODIFY,4.0,tests\trainer\test_trainer_steps.py,tests\trainer\test_trainer_steps.py,1.0,"34,35,36,37,38,39",34,,,,,,,,,,,,,,,,,,,,,,,5.0,xiadingZ,2020-06-29T17:20:30Z,"
 		Weird!! Why is this think platform dependent?? 🤔
 		",6.0,xiadingZ,2020-06-30T07:45:37Z,"
 		<denchmark-link:https://github.com/Pet222>@Pet222</denchmark-link>
  , are u sure that versions on ubuntu and windows are same?
 		",7.0,xiadingZ,2020-06-30T10:29:19Z,"
 		Hey <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  is this intended behaviour? I was surprised to see this breaking change being introduced with no warning.
 If it is intended, why not have consistent behaviour over  and .
 If it is not intended, as it seems due to the ""bug fix"" tag, are you working on it or should I make a PR for this?
 		",8.0,xiadingZ,2020-06-30T11:43:59Z,"
 		what is the behavior? that the ""loss"" key is not in training_epoch_end? If so, that's a bug because it should be there
 		",9.0,xiadingZ,2020-06-30T11:49:52Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  , on the latest version, the  key was changed to the . I think it was changed <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/0f073819d3e0df8db7602eab489b1bad0fc0949c#diff-c45bd21c331565cbe62aaa12fa43aa0aR717>here</denchmark-link>
 
 		",10.0,xiadingZ,2020-06-30T11:50:10Z,"
 		Yes, the fact that you need to access it through 'callback metrics'.
 Got it!
 On Tue, 30 Jun 2020 at 12:44, William Falcon ***@***.***> wrote:
  what is the behavior? that the ""loss"" key is not in training_epoch_end? If
  so, that's a bug because it should be there
 
  —
  You are receiving this because you commented.
  Reply to this email directly, view it on GitHub
  <<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2372#issuecomment-651740702>#2372 (comment)</denchmark-link>
 >,
  or unsubscribe
  <<denchmark-link:https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ>https://github.com/notifications/unsubscribe-auth/ABKWP6XTUJDTEDJ2NZQ3RKTRZHFY5ANCNFSM4OJKX4KQ</denchmark-link>
 >
  .
 
 -- 
 Best Regards,
 Miguel Vera
 
 +351 915 198 452
 miguel.coimbra.vera@protonmail.com
 Github/Captainvera <<denchmark-link:http://www.github.com/captainvera>http://www.github.com/captainvera</denchmark-link>
 >
 		",11.0,xiadingZ,2020-06-30T12:19:01Z,"
 		<denchmark-link:https://github.com/captainvera>@captainvera</denchmark-link>
  would love a PR :)
 		",12.0,xiadingZ,2020-06-30T13:24:38Z,"
 		<denchmark-link:https://github.com/captainvera>@captainvera</denchmark-link>
  <denchmark-link:https://github.com/xiadingZ>@xiadingZ</denchmark-link>
  sorry about that! it was a bad bug.
 Made a PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2428>#2428</denchmark-link>
   and added tests to make sure this doesn't happen again!
 try master now!
 we’ll push a new minor again since this is a key bug (and we have a few other key bugs)
 		",13.0,xiadingZ,2020-06-30T14:11:11Z,"
 		Well, that was fast, thanks!
 		",,,,,,,,,test_training_step_dict,tmpdir,5,41,optimizer_closure,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",759,828,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"107,108,109,110,114","107,108,112",training_epoch_end_dict,"self,outputs",98,118,1.0,61,61,training_step_dict_return,"self,batch,batch_idx",54,61,,,,,,,,,,,,,,,,,,,,,,1.0,"149,150",141,test_train_step_epoch_end,tmpdir,118,152,1.0,"67,68,70,71,72,73,74","62,63,65,66,67",training_step_with_step_end,tmpdir,44,74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"109,110,112,113,114,115","102,103,105,106,107",test_full_training_loop_dict,tmpdir,77,115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
241,samhumeau,2019-09-21T16:05:45Z,2019-10-02T15:10:41Z,"In Multi GPU DDP, pytorch-lightning creates several tfevents files","
 Describe the bug
 Right now pytorch-lightning seems to create several tfevent files in the multi-gpu ddp way:
 e.g. for 2 GPUs:
 <denchmark-code>-rw-rw-r--. 1 sam sam   40 Sep 19 08:11 events.out.tfevents.1568880714.google2-compute82.3156.0
 -rw-rw-r--. 1 sam sam 165K Sep 19 08:22 events.out.tfevents.1568880716.google2-compute82.3186.0
 -rw-rw-r--. 1 sam sam   40 Sep 19 08:11 events.out.tfevents.1568880718.google2-compute82.3199.0
 </denchmark-code>
 
 I suppose the first one is created by the main process and the next 2 are created by the 2 DDP processes (one per GPU). Unfortunately, the actual events are not logged in the last created one, and that confuses tensorboard, cf <denchmark-link:https://github.com/tensorflow/tensorboard/issues/1011>tensorflow/tensorboard#1011</denchmark-link>
 
 I have to restart tensorboard if I want to see the new data.
 A clear and concise description of what the bug is.
 To Reproduce
 Launch any training on multi GPU DDP.
 Expected behavior
 Only one tfevent file is created, from the master GPU.
 	",614cb3c03bd0894238b3197f3b7f904656f284f4,Nic Eggert,2019-10-02 11:10:40-04:00,MODIFY,0,docs\Trainer\Logging.md,docs\Trainer\Logging.md,0.0,"122,125,126,127,128,129,130,131,132,133,134,135,136","122,124,125,127",1.0,samhumeau,2019-09-21T16:12:20Z,"
 		thanks for bringing this up. this has been reported a few times already. the problem is what you described.
 the solution is to init the logger from proc zero only. want to take a look at how we can approach this? <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  is working on an abstraction that will need this fix
 		",2.0,samhumeau,2019-09-23T16:24:54Z,"
 		Are you thinking we should make sure that the test tube experiment doesn't even get initialized unless we're on process 0? Right now I have it initialize, but never log, but that's easy enough to change.
 		",3.0,samhumeau,2019-09-23T16:28:12Z,"
 		yeah, i think the best thing is to make sure it’s only initialized once. This will save a ton of space in the experiment file as well
 		",4.0,samhumeau,2019-09-27T19:19:09Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Starting to take a look at this. This turns out to affect the MLFlow logger as well when doing multi-node DDP. I think what I'd like to do is make constructing the experiment / MLFlow run inside the logger lazy, so that it doesn't get created until a method that needs it is called.
 Once consequence of this is that users shouldn't call log_hyperparams themselves, since that will happen on multiple nodes in multi-node DDP. To make up for this, we should call it for them when they do training. I'm thinking we check to see if they've defined model.hparams, and if so, we can log for them. In code, it looks like adding this to __run_pretrain_routine:
 <denchmark-code>if hasattr(ref_model, ""hparams""):
     self.logger.log_hyperparams(ref_model.hparams)
 </denchmark-code>
 
 Thoughts? I guess we should document somewhere that we're expecting a hparams attribute, although I think most of the examples follow that convention already.
 (Side note: <denchmark-link:https://williamfalcon.github.io/pytorch-lightning/Trainer/Logging/#save-a-snapshot-of-all-hyperparameters>the docs claim this is already done automatically</denchmark-link>
 , but I don't see it in the code anywhere.)
 		",MODIFY,2.0,pytorch_lightning\logging\mlflow_logger.py,pytorch_lightning\logging\mlflow_logger.py,1.0,"19,20,21,23,26,28,29,32,33","20,22,23,26",run_id,self,19,33,MODIFY,6.0,pytorch_lightning\logging\test_tube_logger.py,pytorch_lightning\logging\test_tube_logger.py,1.0,"71,72,73,74",,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"893,894,895","893,894",__run_pretrain_routine,"self,model",855,926,MODIFY,1.0,tests\test_logging.py,tests\test_logging.py,1.0,,"22,23",test_testtube_logger,,13,35,5.0,samhumeau,2019-09-27T19:59:04Z,"
 		Makes sense, i think the hparams makes the most sense. I wonder if there's a way to automatically do it even if users don't define hparams. Maybe argparse has some sort of global state we can inspect? or look at vars in the current frame? I'd love to remove the need for users to remember to have to use hparams.
 I'm converned about people who don't use argparse and/or init their models  using actual args.
 Case 1:
 MyObj(lr=0.1, ..., arg_2=0.3)
 Case 2:
 MyObj(hparams)
 Case 3:
 MyObj(hparams, lr=0.1, ...)
 		",6.0,samhumeau,2019-09-28T00:11:19Z,"
 		Yeah, I'd definitely welcome other ideas that would cover those cases. Maybe ask users to define hparams as a property if they're not doing case 2, but they still want lightning to log their parameters for them?
 		",7.0,samhumeau,2019-10-21T16:08:44Z,"
 		in 0.5.1.3 multiple tfevents files are still being created with ddp (I'm not logging any hparams if that matters), did that PR fix it? or is there more work to be done?
 		",8.0,samhumeau,2019-10-21T16:11:46Z,"
 		Are you interacting with the logger manually at all before training starts? Are you doing single-node or multi-node DDP?
 		",9.0,samhumeau,2019-10-22T02:56:08Z,"
 		single node, and the only time I manually call logger is in optimizer step (self.logger.log_metrics) otherwise I only return log entries in training step and validation end
 		",10.0,samhumeau,2019-11-05T08:48:06Z,"
 		I removed that call and I'm still getting multiple tfevents, no other calls to logging besides metrics returned by train and val steps. Currently using the experimental --reload_multifile=true in tensorboard to get around the issue.
 		",11.0,samhumeau,2020-05-11T08:40:12Z,"
 		is this problem solved?
 		",12.0,samhumeau,2020-07-02T07:43:50Z,"
 		I see same problem, why was this closed?
 		",13.0,samhumeau,2020-07-02T08:19:33Z,"
 		multiple tfevents files are still created but tensorboard updates made this a non-issue for me with everything displaying and updating correctly
 		",14.0,samhumeau,2020-07-02T08:37:53Z,"
 		<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
 
 Thank you for your answer.
 What version of tensorboard are you using?
 I'm using tensorboard-2.2.1, but when I set logdir to a folder that contains multiple tfevents, I get the following error:
 
 		",15.0,samhumeau,2020-07-02T08:44:30Z,"
 		I'm also on 2.2.1 but I'm using  within the <denchmark-link:https://ngc.nvidia.com/catalog/containers/nvidia:pytorch>ngc pytorch container</denchmark-link>
  so I don't manually setup logdir
 		",version,self,70,74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"15,16",12,__init__,"self,experiment_name,tracking_uri",12,16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"22,23,24,25,26,27,28,29,30,31,33",,experiment,self,22,33,1.0,"57,58,59,60",60,rank,self,56,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"64,65,66,67","64,65",rank,"self,value",63,67,1.0,82,,__getstate__,self,80,83,1.0,"86,87",,__setstate__,"self,state",85,88,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,samhumeau,2020-07-02T08:51:05Z,"
 		<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
 
 Oh, It (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/241#issuecomment-549722983>#241 (comment)</denchmark-link>
 ) meant adding --reload_multifile= true to the tensorboard line!
 I solved my problem. Thank you very much.
 I would like to ask you another question.
 Is there no error if you don't specify a Trainer logger using pl_loggers.TensorBoardLogger()?
 Unless I specify the logger of Trainer() separately, the error that tensorboard path already exists will occur.
 		",17.0,samhumeau,2020-07-02T08:58:30Z,"
 		I don't remember what I did back then to get logging working... but currently I use TestTubeLogger and call it in trainer as logger=TestTubeLogger(""."", ""lightning_logs"")
 this logs losses/metrics and hparams in self.hparams correctly (this method logs hparams under TEXT in tensorboard)
 		",18.0,samhumeau,2020-07-02T10:05:57Z,"
 		are you guys on 0.8.4?
 <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 		",19.0,samhumeau,2020-07-04T23:49:10Z,"
 		multiple tfevent files does not mean they come from differnt gpus, it's just a tensorboard thing.
 0.8.4 logs only on rank 0.
 previously we had the problem that other ranks would log as well, this would lead to multiple directories (version0, version1, ...) but this is fixed now.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,samhumeau,2020-07-04T23:50:49Z,"
 		if you manually log things, then do this:
 if self.trainer.is_global_zero:
     # your custom non-Lightning logging
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2411,s-rog,2020-06-29T08:16:57Z,2020-06-30T18:51:40Z,0.8.2 calls backward on '_GeneratorContextManager',"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 0.8.2 calls backward on '_GeneratorContextManager' and crashes training.
 0.8.1 works correctly. my training_step returns {'loss':loss, 'log':{'learn_rate':self.lr}}
 <denchmark-code>Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap
     fn(i, *args)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 538, in ddp_train
     self.run_pretrain_routine(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1100, in run_pretrain_routine
     self.train()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 370, in train
     self.run_training_epoch()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 452, in run_training_epoch
     batch_output = self.run_training_batch(batch, batch_idx)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 630, in run_training_batch
     self.hiddens
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 804, in optimizer_closure
     model_ref.backward(self, closure_loss, optimizer, opt_idx)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/hooks.py"", line 189, in backward
     loss.backward()
 AttributeError: '_GeneratorContextManager' object has no attribute 'backward'
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 backward is called on the loss and training runs correctly
 	",e8bb4165b76496089d24c74891f2167350e594be,William Falcon,2020-06-30 14:51:39-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"805,806,807,808,809,813,814,815,816,817,818,819,820",,1.0,s-rog,2020-06-29T11:19:12Z,"
 		did you override optimizer step?
 could you try master? we just pushed a fix to a typo we had
 		",2.0,s-rog,2020-06-29T12:47:22Z,"
 		Can confirm this happens on 0.8.3
 		",3.0,s-rog,2020-06-29T12:48:36Z,"
 		ok. Can you post a colab example that replicates this?
 		",4.0,s-rog,2020-06-30T00:40:54Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
 
 colab please
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,s-rog,2020-06-30T02:41:15Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  my optimizer step was untouched, I can't run more testing atm but I'll get to it as soon as I can
 		",6.0,s-rog,2020-06-30T03:35:30Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Hi I also encountered this, with normal Adam optimizer. I don't have a colab to replicate this atm but from what I saw earlier, this can be replicated with any setting as long as the Trainer is set to precision=16 when using Apex. Under this condition, the following lines from training_loop.py and hooks.py will run:
 if self.precision == 16 and not self.on_tpu closure_loss = model_ref.amp_scale_loss(closure_loss, optimizer, opt_idx) 
 scaled_loss = amp.scale_loss(unscaled_loss, optimizer)
 will cause the closure_loss be a _GeneratorContextManager object. Which then cannot have a backward() method.
 It seems under the current design, pytorch lighting's scale_loss function can only be used as a context?
 		",7.0,s-rog,2020-06-30T07:03:11Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Here's a colab example (my first time using colab so let me know if you have issues seeing it) <denchmark-link:https://colab.research.google.com/drive/1G08jVDpx-T-5HE2c89RLJdq4u67mM2-o?usp=sharing>https://colab.research.google.com/drive/1G08jVDpx-T-5HE2c89RLJdq4u67mM2-o?usp=sharing</denchmark-link>
 
 I suspect the issue lies with Apex AMP as suggested above by <denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>
 
 		",8.0,s-rog,2020-06-30T13:58:53Z,"
 		ummm. I think this is an apex issue. I can't replicate it with 16-bit native.
 <denchmark-link:https://user-images.githubusercontent.com/3640001/86135032-4c97ff80-bab8-11ea-942e-ffaae17aff07.png></denchmark-link>
 
 		",9.0,s-rog,2020-06-30T14:03:54Z,"
 		<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>
  min share a minimal example to reproduce?
 		",10.0,s-rog,2020-06-30T16:21:22Z,"
 		hi sorry for the delay: <denchmark-link:https://colab.research.google.com/drive/1rjaRRwgBTm4CKPfe9po_WSxnKqY4jDRv?usp=sharing>https://colab.research.google.com/drive/1rjaRRwgBTm4CKPfe9po_WSxnKqY4jDRv?usp=sharing</denchmark-link>
 
 I agree this is an apex issue, i.e. only occur when NATIVE_AMP_AVALAIBLE is false in the hooks.py
 		",11.0,s-rog,2020-06-30T18:52:12Z,"
 		<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>
  , <denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  ,<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  this is fixed on master. Give it a try?
 		",12.0,s-rog,2020-06-30T20:25:13Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
   yes, the master version works for me now. Thanks!
 		",13.0,s-rog,2020-07-01T00:35:18Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  can confirm as well! and sorry couldn't be more helpful earlier
 		",14.0,s-rog,2020-07-01T03:47:06Z,"
 		Hi <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  thanks for the quick fix. I just upgraded but am now seeing a different error:
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0,1]
 Using APEX 16bit precision.
 initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
 Loaded pretrained weights for efficientnet-b0
 /home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py:140: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.
   train_single_fold(args)
 Using APEX 16bit precision.
 initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
 ----------------------------------------------------------------------------------------------------
 distributed_backend=ddp
 All DDP processes registered. Starting ddp with 2 processes
 ----------------------------------------------------------------------------------------------------
 Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.
 
 Defaults for this optimization level are:
 enabled                : True
 opt_level              : O1
 cast_model_type        : None
 patch_torch_functions  : True
 keep_batchnorm_fp32    : None
 master_weights         : None
 loss_scale             : dynamic
 Processing user overrides (additional kwargs that are not None)...
 After processing overrides, optimization options are:
 enabled                : True
 opt_level              : O1
 cast_model_type        : None
 patch_torch_functions  : True
 keep_batchnorm_fp32    : None
 master_weights         : None
 loss_scale             : dynamic
 
   | Name      | Type             | Params
 -----------------------------------------------
 0 | critereon | CrossEntropyLoss | 0     
 1 | net       | EfficientNet     | 4 M   
 Validation sanity check:  50%|███████████████████████▌                       | 1/2 [00:00<00:00,  1.01it/s]Traceback (most recent call last):
   File ""/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py"", line 140, in <module>
     train_single_fold(args)
   File ""/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/train.py"", line 64, in train_single_fold
     trainer.fit(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 957, in fit
     self.ddp_train(task, model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 538, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1141, in run_pretrain_routine
     eval_results = self._evaluate(model,
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 346, in _evaluate
     self.reduce_eval_ddp(eval_results)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 363, in reduce_eval_ddp
     self.reduce_eval_ddp(v)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 365, in reduce_eval_ddp
     dist.all_reduce(v, op=dist.reduce_op.SUM)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 898, in all_reduce
     work = _default_pg.allreduce([tensor], opts)
 RuntimeError: Tensors must be CUDA and dense
 Traceback (most recent call last):
   File ""train.py"", line 140, in <module>
     train_single_fold(args)
   File ""train.py"", line 64, in train_single_fold
     trainer.fit(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 973, in fit
     self.spawn_ddp_children(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 449, in spawn_ddp_children
     self.ddp_train(local_rank, model, is_master=True)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/distrib_data_parallel.py"", line 538, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1141, in run_pretrain_routine
     eval_results = self._evaluate(model,
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 346, in _evaluate
     self.reduce_eval_ddp(eval_results)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 363, in reduce_eval_ddp
     self.reduce_eval_ddp(v)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 365, in reduce_eval_ddp
     dist.all_reduce(v, op=dist.reduce_op.SUM)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 898, in all_reduce
     work = _default_pg.allreduce([tensor], opts)
 RuntimeError: Tensors must be CUDA and dense
 </denchmark-code>
 
 I'm not manually assigning tensors to a device (i.e. PL should be assigning all tensors as CUDA tensors) and I am not using sparse tensors (at least not that I am aware of).
 EDIT: I found the issue. I guess metrics need to be CUDA tensors now. Thanks again :)
 		",15.0,s-rog,2020-07-01T06:15:30Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  mind send a new issue?
 		",,,,,optimizer_closure,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",759,841,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2429,Uroc327,2020-06-30T13:58:35Z,2020-07-01T11:53:20Z,Batched iterative dataloading disables validation,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Setting the batch_size parameter for torch.utils.data.DataLoader to a number greater than 1, prevents validation_step and validation_epoch_end from being called.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run python main.py with bs = 1
 Observe exception raised in validation_step
 Run python main.py after changing to bs = 2
 Observe the model train successfully
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import pytorch_lightning as pl
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.utils.data import DataLoader, IterableDataset
 
 class Dataset(IterableDataset):
     def __init__(self):
         super().__init__()
 
     def __iter__(self):
         for _ in range(1024):
             yield torch.randn(20)
 
     def __len__(self):
         return 1024
 
 class Model(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.fst = nn.Linear(20, 1)
         self.snd = nn.Linear(1, 20)
 
     def forward(self, x):
         x = self.fst(x)
         x = F.relu(x)
         x = self.snd(x)
         return x
 
     def training_step(self, batch, batchIdx):
         x = self.forward(batch)
         return {'loss': F.mse_loss(x, batch)}
 
     def validation_step(self, batch, batchIdx):
         raise NotImplementedError()
         x = self.forward(batch)
         return {'val_loss': F.mse_loss(x, batch)}
 
     def validation_epoch_end(self, outputs):
         return {'val_loss': torch.mean(torch.stack([x['val_loss'] for x in outputs]))}
 
     def configure_optimizers(self):
         return torch.optim.AdamW(self.parameters())
 
 if __name__ == '__main__':
     trainer = pl.Trainer(num_sanity_val_steps=0)
     net = Model()
     dataset = Dataset()
 
     bs = 2
     trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=bs), val_dataloaders=DataLoader(dataset, batch_size=bs))
 <denchmark-code>> python main.py # with bs=2, should fail
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name | Type   | Params
 --------------------------------
 0 | fst  | Linear | 21    
 1 | snd  | Linear | 40    
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 Epoch 1000:  25%|████████████████████████████████████████████████████████████▌                                                                                                                                                                                     | 512/2048 [00:00<00:01, 1396.66it/s, loss=0.890, v_num=20]
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-code>> python main.py # with bs=1
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name | Type   | Params
 --------------------------------
 0 | fst  | Linear | 21    
 1 | snd  | Linear | 40    
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 /home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 Epoch 1:  50%|████████████████Traceback (most recent call last):████████████████████████████████████████████████████████████████████████                                                                                                                          | 1024/2048 [00:00<00:00, 1337.66it/s, loss=1.032, v_num=19]
   File ""main.py"", line 57, in <module>
     trainer.fit(net, train_dataloader=DataLoader(dataset, batch_size=bs), val_dataloaders=DataLoader(dataset, batch_size=bs))
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 954, in fit
     self.run_pretrain_routine(model)
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1093, in run_pretrain_routine
     self.train()
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 375, in train
     self.run_training_epoch()
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 490, in run_training_epoch
     self.run_evaluation(test_mode=self.testing)
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 379, in run_evaluation
     eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 281, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 452, in evaluation_forward
     output = model.validation_step(*args)
   File ""main.py"", line 39, in validation_step
     raise NotImplementedError()
 NotImplementedError
 Exception ignored in: <object repr() failed>
 Traceback (most recent call last):
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py"", line 1086, in __del__
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py"", line 1293, in close
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py"", line 1471, in display
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py"", line 1089, in __repr__
   File ""/home/constantin/.virtualenvs/tensor/lib64/python3.6/site-packages/tqdm/std.py"", line 1433, in format_dict
 TypeError: 'NoneType' object is not iterable
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>Collecting environment information...
 PyTorch version: 1.5.1+cu101
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Gentoo Base System release 2.7
 GCC version: (Gentoo 9.3.0 p1) 9.3.0
 CMake version: version 3.17.3
 
 Python version: 3.6
 Is CUDA available: Yes
 CUDA runtime version: 10.1.243
 GPU models and configuration: GPU 0: GeForce GT 730
 Nvidia driver version: 440.82
 cuDNN version: /opt/cuda/targets/x86_64-linux/lib/libcudnn.so.7.6.5
 
 Versions of relevant libraries:
 [pip3] numpy==1.19.0
 [pip3] pytorch-lightning==0.8.1
 [pip3] torch==1.5.1+cu101
 [pip3] torchvision==0.6.1+cu101
 [conda] Could not collect
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Basically a reopen of <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2351>#2351</denchmark-link>
 , as this issue is not fixed by changing batch size and dataset size.
 	",927f305f7e556828b5cdd45e3977c67f3c54b8fc,Adrian Wälchli,2020-07-01 07:53:19-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"13,14",,1.0,Uroc327,2020-06-30T14:06:39Z,"
 		Same behavior with pytorch-lightning==0.8.3.
 		",2.0,Uroc327,2020-06-30T22:18:57Z,"
 		I can reproduce. It is caused by pl.Trainer(num_sanity_val_steps=0)
 For num_sanity_val_steps>0 it works fine
 		",3.0,Uroc327,2020-06-30T22:49:11Z,"
 		trying to fix his right now. another observation: only happens with iterable dataset.
 EDIT: iterable dataset that has also length defined (see comment below)
 		",4.0,Uroc327,2020-07-01T00:42:43Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
   thanks for looking into this! If I remember correctly, then for  it only raised the exception during sanity checks on my machine. When I try to raise the exception only in actual training (for example by raising the exception on the fourth time  is called), then this completes sucessfully (i.e. does not call validation) as well.
 		",MODIFY,3.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"148,149",,auto_add_sampler,"self,DataLoader,bool",144,167,MODIFY,0.0,requirements\base.txt,requirements\base.txt,0.0,9,9,MODIFY,3.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"506,507",,test_warning_with_iterable_dataset_and_len.__len__,self,506,507,,,,,,,,,,,,5.0,Uroc327,2020-07-01T01:09:28Z,"
 		I think the issue is that your dataset is of type Iterable but has  defined. PL interprets that wrongly. In fact, there is a weird issue with dataloaders from iterable datasets that have len defined, check out this colab that I made:
 <denchmark-link:https://colab.research.google.com/drive/1RQyNLGORe4vOL_RS6khcFNObcxyFxtEm?usp=sharing>https://colab.research.google.com/drive/1RQyNLGORe4vOL_RS6khcFNObcxyFxtEm?usp=sharing</denchmark-link>
 
 isn't it strange?
 if I remove the len from the dataset definition, your code sample works.
 		",6.0,Uroc327,2020-07-01T13:00:04Z,"
 		<denchmark-link:https://github.com/Uroc327>@Uroc327</denchmark-link>
  fyi, it turned out there is no bug, but rather a technical thing with iterable datasets. We deciced to add a warning message when IterableDataset defines also length.
 In your case you have the following options:
 
 remove __len__ from your IterableDataset (this is the preferred option you want 99.9%)
 convert it to a regular torch.utils.data.Dataset
 keep the batch size 1 (not great)
 write a custom BatchSampler (I guess, I have not tried it)
 
 		",7.0,Uroc327,2020-07-01T20:50:21Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Ok, thanks! I'll remove the  implementation.
 Btw, the pytorch docs for <denchmark-link:https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader>DataLoader</denchmark-link>
  explicitly allow an  with a .
 		",8.0,Uroc327,2020-07-01T20:59:26Z,"
 		Yep! However as the docs say in the note at the bottom of your link the user has to to their own batching to avoid duplicate data. That's why I added the warning to PL.
 Note that for example in your case, leaving it as is len(dataloader) would always return 1024 as the length, regardless of the batch size. That would be incorrect for batch size > 1.
 		",9.0,Uroc327,2020-07-01T21:02:16Z,"
 		Makes sense, thank you 😄
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"503,504",,test_warning_with_iterable_dataset_and_len.__iter__,self,503,504,,,,,,,,,,,,,,,,,,,,,,1.0,"46,47,48",46,_has_iterable_dataset,DataLoader,46,48,1.0,"53,54,60,62,64,65,66,67,68,69,70,71,72","52,54,56",_has_len,DataLoader,51,72,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519",,test_warning_with_iterable_dataset_and_len,tmpdir,496,519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2436,awaelchli,2020-06-30T21:54:06Z,2020-07-07T18:54:08Z,Fix horovod tests that try to access filepath on global rank &gt; 0,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 We had to skip two tests in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2425>#2425</denchmark-link>
 , namely
 
 test_horovod_cpu
 test_horovod_cpu_implicit
 
 Problem is that since they run in ddp and the test tries to access the trainer internal variable for the checkpoint path, it gets a NoneType error when trying to os.join() None paths.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run these two tests:
 
 test_horovod_cpu
 test_horovod_cpu_implicit
 
 	",78db847e42457ce3dcd89a2a5eccc8e79f60e731,Adrian Wälchli,2020-07-07 14:54:07-04:00,MODIFY,3,tests\base\develop_pipelines.py,tests\base\develop_pipelines.py,1.0,,19,1.0,awaelchli,2020-06-30T21:55:38Z,"
 		<denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  might be able to help us, but I will look into it too.
 		",,,,,,,,,,,,,MODIFY,1.0,tests\models\data\horovod\train_default_model.py,tests\models\data\horovod\train_default_model.py,1.0,"49,51,55,56,57,58,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84","50,54",run_test_from_config,trainer_options,46,88,MODIFY,4.0,tests\models\test_horovod.py,tests\models\test_horovod.py,1.0,81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_horovod_cpu_implicit,tmpdir,77,89,run_model_test_without_loggers,"trainer_options,model,float",9,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"63,64",65,run_model_test,"trainer_options,model,bool,version,bool",37,75,1.0,80,"81,82,83",run_prediction,"dataloader,trained_model,dp,min_acc",78,98,,,,,,,,1.0,64,,test_horovod_cpu,tmpdir,60,73,1.0,"43,44,45,49","46,56",_run_horovod,"trainer_options,on_gpu",41,56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,99,,test_horovod_multi_gpu,tmpdir,95,109,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2438,Llannelongue,2020-07-01T00:46:26Z,2020-07-01T11:38:01Z,`validation_epoch_end` and `test_epoch_end` can't return nothing,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 If validation_epoch_end or test_epoch_end returns nothing (as presented as an option in the documentation), an error occurs.
 (Happy to work on a PR to fix this)
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Overwrite test_epoch_end and remove return (same for validation_epoch_end
 <denchmark-code>File ""/.conda/envs/PPI-env/lib/python3.7/site-packages/pytorch_lightning/trainer/logging.py"", line 106, in process_output
     for k, v in output.items():
 AttributeError: 'NoneType' object has no attribute 'items'
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 from torchvision import transforms
 from pytorch_lightning.core.lightning import LightningModule
 from pytorch_lightning import Trainer, seed_everything
 
 class LitModel(LightningModule):
 
     def __init__(self):
         super().__init__()
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.001)
 
     def train_dataloader(self):
         dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())
         loader = DataLoader(dataset, batch_size=32, num_workers=4, shuffle=True)
         return loader
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         # return {'val_loss': avg_loss, 'log': tensorboard_logs}
 
     def val_dataloader(self):
         # TODO: do a real train/val split
         dataset = MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())
         loader = DataLoader(dataset, batch_size=32, num_workers=4)
         return loader
 
 
 def main():
     seed_everything(42)
 
     model = LitModel()
     # most basic trainer, uses good defaults
     trainer = Trainer(fast_dev_run=True)
     trainer.fit(model)
 
 if __name__ == '__main__':
     main()
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 It should check if there is nothing returned and carry on.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.17.4
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.8.3
 tensorboard:       2.2.2
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Darwin
 architecture:
 
 64bit
 
 
 
 processor:         i386
 python:            3.7.6
 version:           Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",325852c6df93f749bb843bff1a3cdba41698722c,William Falcon,2020-07-01 07:38:00-04:00,ADD,0,None,docs\source\bolts.rst,,,,1.0,Llannelongue,2020-07-01T03:05:55Z,"
 		yes, please submit a PR? you should be allowed to return nothing
 		",2.0,Llannelongue,2020-07-01T07:15:11Z,"
 		Yes, working on it 👍
 		",,,,,,,,,MODIFY,0.0,docs\source\callbacks.rst,docs\source\callbacks.rst,0.0,"52,53,54,55,56,57,58,59","52,53,54,55,56,57,58,59",,,,,MODIFY,0.0,docs\source\conf.py,docs\source\conf.py,0.0,142,,MODIFY,0.0,docs\source\hooks.rst,docs\source\hooks.rst,0.0,"23,24,35,36",,,,,,MODIFY,0.0,docs\source\index.rst,docs\source\index.rst,0.0,"30,31,32,33,34,35,36","38,103",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1394,1395,1396,1397,1398,1399,1400,1401,1402,1407",,test_dataloader,self,1387,1439,1.0,"1340,1341,1342,1343,1344,1345,1346,1347,1352",1340,train_dataloader,self,1330,1375,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,,,,,,,,1.0,"411,412,413,415,416,418,419,420,421,422,423,425,426,427,428,429","409,411,412,414,415,416,417,418,419,421,422,424,425",run_evaluation,"self,bool",370,447,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"1140,1141,1142,1143,1144","1145,1146",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_pretrain_routine,"self,LightningModule",1056,1156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2442,Anjum48,2020-07-01T06:56:52Z,2020-07-21T19:18:58Z,validation_epoch_end needs to return CUDA tensors,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I'm not sure if this is expected behaviour or not, but upgrading to the latest version (from 0.8.1) caused my validation_epoch_end to break. It appears that a CUDA tensor is expected for the metric where before the tensor was device agnostic.
 This was using sklearn's roc_auc_score. I haven't yet got around to testing PL's new metrics.
 Feel free to close if this is expected behaviour.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 This is my validation_epoch_end. Uncommenting .to(avg_loss.device) allows this to run with the dev version of PL.
 This was run with ddp, precision=16 using Apex AMP.
 <denchmark-code>def validation_epoch_end(self, outputs):
         avg_loss = torch.stack([x[""val_loss""] for x in outputs]).mean()
         y_pred = torch.cat([x[""y_pred""].squeeze() for x in outputs]).cpu().numpy()
         y_true = torch.cat([x[""y_true""].squeeze() for x in outputs]).cpu().numpy()
 
         metric = torch.tensor(roc_auc_score(y_true, y_pred))  # .to(avg_loss.device)
 
         tensorboard_logs = {
             ""loss/validation"": avg_loss,
             ""auc"": metric,
         }
         return {""val_loss"": avg_loss, ""log"": tensorboard_logs, ""auc"": metric}
 </denchmark-code>
 
 The error message can be seen here: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2411#issuecomment-652171874>#2411 (comment)</denchmark-link>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 
 PyTorch Version (e.g., 1.0): 1.5
 OS (e.g., Linux): Ubuntu 20.04
 How you installed PyTorch (conda, pip, source):
 Build command you used (if compiling from source):
 Python version: 3.8.2
 CUDA/cuDNN version: 10.2
 GPU models and configuration: Dual GPU (ddp)
 Any other relevant information: Apex AMP
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",a5538af3558cf544dffd92b1b8bab3a5793f0ba0,Adrian Wälchli,2020-07-21 15:18:57-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,24,24,1.0,Anjum48,2020-07-01T07:11:15Z,"
 		I also have a sklearn generated metric that is only logged, with no business being on gpu. Hopefully not intended behavior?
 		",2.0,Anjum48,2020-07-01T09:57:44Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  mind have look? 
 		",3.0,Anjum48,2020-07-02T06:41:19Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  this is probably caused by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2434>#2434</denchmark-link>
  in combination with your dip backend (from here: <denchmark-link:https://pytorch.org/docs/stable/distributed.html#backends>https://pytorch.org/docs/stable/distributed.html#backends</denchmark-link>
 ) which does not support CPU tensors (are you using NCCL backend?)
 <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  can you elaborate on your problem? Maybe in a separate issue since I'm not sure, if this is the same problem here.
 cc <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  ^^
 		",4.0,Anjum48,2020-07-02T07:52:20Z,"
 		Hi <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
 , I'm using the default PyTorch install from conda, so I believe NCCL is being used (both GPUs are in the same machine). The weird thing is that this only started happening when I upgraded from 0.8.1 to 0.8.4 so I guess something has changed in ddp?
 		",MODIFY,1.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,,"71,72,73,74,75,76",__init__,"self,args,kwargs",37,78,MODIFY,1.0,pytorch_lightning\metrics\metric.py,pytorch_lightning\metrics\metric.py,1.0,,"30,31",MODIFY,10.0,pytorch_lightning\utilities\device_dtype_mixin.py,pytorch_lightning\utilities\device_dtype_mixin.py,1.0,137,138,float,self,131,138,ADD,0.0,None,tests\utilities\test_dtype_device_mixin.py,,,,,,,,5.0,Anjum48,2020-07-02T07:56:58Z,"
 		This is not on ddp itself, but we added a often requested feature and synced the outputs across ddp nodes. And since you probably used nccl backend (which only supports gpu), this probably causes the drawback. Can you try to switch the backend of DDP? Maybe to Gloo or MPI?
 		",6.0,Anjum48,2020-07-02T08:22:09Z,"
 		I added torch.distributed.Backend('gloo') to the top of my training script and am seeing the same error
 		",7.0,Anjum48,2020-07-02T08:24:14Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  I have the same issue as OP where my  returns a cpu tensor like:
 
 Edit:
 does this mean that the previous implementation was only returning/logging the rank 0 output?
 		",8.0,Anjum48,2020-07-02T08:28:45Z,"
 		
 Hi @justusschock, I'm using the default PyTorch install from conda, so I believe NCCL is being used (both GPUs are in the same machine). The weird thing is that this only started happening when I upgraded from 0.8.1 to 0.8.4 so I guess something has changed in ddp?
 
 FYI, We are testing also against Conda PyTorch distributions
 		",9.0,Anjum48,2020-07-02T08:29:20Z,"
 		
 does this mean that the previous implementation was only returning/logging the rank 0 output?
 
 I was also wondering this. I noticed my validation loss double after upgrading to 0.8.4
 		",10.0,Anjum48,2020-07-02T10:20:56Z,"
 		It is synced/reduced back to process 0 and only logged there :) DO you think it would be better to log from each process with the respective rank as a prefix?
 		",11.0,Anjum48,2020-07-02T10:23:15Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  regarding your persisting error:
 adding it at the top of your script doesn't help, since lightning trainer overwrites this. You have to set it in the trainer: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L166>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L166</denchmark-link>
 
 		",12.0,Anjum48,2020-07-02T11:11:06Z,"
 		Ok I tried it in trainer, and got this error (after disabling AMP):
 <denchmark-code>Loaded pretrained weights for efficientnet-b0
 train.py:141: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.
   train_single_fold(args)
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 
   | Name      | Type             | Params
 -----------------------------------------------
 0 | critereon | CrossEntropyLoss | 0     
 1 | metric    | AUROC            | 0     
 2 | net       | EfficientNet     | 4 M   
 Traceback (most recent call last):
   File ""train.py"", line 141, in <module>
     train_single_fold(args)
   File ""train.py"", line 65, in train_single_fold
     trainer.fit(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1020, in fit
     self.run_pretrain_routine(model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1128, in run_pretrain_routine
     self.reset_val_dataloader(ref_model)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py"", line 342, in reset_val_dataloader
     self._reset_eval_dataloader(model, 'val')
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py"", line 268, in _reset_eval_dataloader
     dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader'))
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py"", line 363, in request_dataloader
     dataloader = dataloader_fx()
   File ""/home/anjum/PycharmProjects/kaggle/siim_isic_melanoma_classification/engine.py"", line 194, in val_dataloader
     sampler = DistributedSampler(dataset)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/utils/data/distributed.py"", line 43, in __init__
     num_replicas = dist.get_world_size()
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 582, in get_world_size
     return _get_group_size(group)
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 196, in _get_group_size
     _check_default_pg()
   File ""/home/anjum/anaconda3/envs/kaggle/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py"", line 186, in _check_default_pg
     assert _default_pg is not None, \
 AssertionError: Default process group is not initialized
 </denchmark-code>
 
 I'm not familiar with gloo so I don't know what I'm looking at here. Isn't gloo for CPU only training?
 		",13.0,Anjum48,2020-07-02T11:19:45Z,"
 		fixed on master for now.
 Yes, currently only rank 0 is used for val calculation. The problem is that reducing blows up memory and causes these kinds of issues.
 however, this will be enabled again in an upcoming PR with an object that can do this kind of syncing automatically. however we’ll need to solve this problem again, so we still need to identify what the issue is.
 		",14.0,Anjum48,2020-07-03T00:24:45Z,"
 		
 It is synced/reduced back to process 0 and only logged there :)
 
 
 Yes, currently only rank 0 is used for val calculation.
 
 <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  ming clarifying? these two statements seem to be contradicting, unless I'm misunderstanding something...
 		",15.0,Anjum48,2020-07-03T05:08:49Z,"
 		Originally we didn't reduce on val - it was on our todo. Then we added it, but it had weird issues such as this one. So we removed it and will test it more before adding back in.
 		",__init__,"self,str",22,31,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"158,159,160,161,162,163,164,165,166,167,168",,__update_properties,"self,None,None",158,168,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,108,"108,109",cuda,"self,None",95,109,1.0,"160,161,162,163,164,165,166",,__update_properties.apply_fn,module,160,166,1.0,128,129,type,"self,str",119,129,1.0,"85,86,87,88,92","85,86,87,88,89,90,91,92",to,"self,args,kwargs",32,93,1.0,146,147,double,self,140,147,1.0,116,117,cpu,self,111,117,1.0,"9,10,11,12",9,__init__,self,9,12,1.0,155,156,half,self,149,156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,Anjum48,2020-07-07T08:21:47Z,"
 		I suppose the metric package fixes/bypasses this issue?
 I just converted my vanilla sk metrics to the pl sk interface, will test soon.
 Edit:
 <denchmark-code>  File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/metric.py"", line 145, in __call__
     return apply_to_collection(self._orig_call(*args, **kwargs), torch.Tensor,
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/converters.py"", line 67, in new_func
     return func_to_apply(result, *dec_args, **dec_kwargs)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/utilities/apply_func.py"", line 35, in apply_to_collection
     return function(data, *args, **kwargs)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/metrics/converters.py"", line 252, in _sync_ddp_if_available
     async_op=False)
   File ""/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py"", line 898, in all_reduce
     work = _default_pg.allreduce([tensor], opts)
 RuntimeError: Tensors must be CUDA and dense
 </denchmark-code>
 
 same issue using the built in metric interface it seems... I'm guessing the metrics need to include cuda conversion (for now) to be able to be reduced?
 		",17.0,Anjum48,2020-07-07T09:58:56Z,"
 		are you on master?
 We won't include a cuda conversion here (we already have a push to self.device), since this also depends on your distributed backend (and you don't have access tp/want the computation to run on a gpu all the times.
 		",18.0,Anjum48,2020-07-08T00:23:48Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  I just updated and tried again with the same error
 		",19.0,Anjum48,2020-07-08T09:05:06Z,"
 		do you have any CPU tensors there?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,Anjum48,2020-07-09T00:26:35Z,"
 		nope both inputs to the lightning sklearn metric are cuda tensors
 		",21.0,Anjum48,2020-07-09T07:55:11Z,"
 		I think I know the issue. Currently there is a problem with device propagation in utilities.device_dtype_mixin which causes the device property to be set only for the most outer instance. E.g. when you assign the metric to your LightningModule, the device of your metric isn't updated. So far I don't know why though.
 Any Idea <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 ?
 		",22.0,Anjum48,2020-07-15T12:34:55Z,"
 		not sure at this moment, will check it...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,23.0,Anjum48,2020-07-16T02:21:34Z,"
 		Should this be a separate issue? as it's an issue with metrics and not validation_epoch_end specifically
 		",24.0,Anjum48,2020-07-21T13:39:16Z,"
 		Just some findings tho
 Currently, classification metrics return cpu tensor while regression metrics return cuda tensor, haven't tested with sk-metrics.
 <denchmark-link:https://colab.research.google.com/drive/102qrhFRbH3Jkh28-icSkuVkgelKmxlAt?usp=sharing>COLAB EXAMPLE</denchmark-link>
 
 		",25.0,Anjum48,2020-07-21T13:39:29Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  The problem is simply that when you call .to on the nn.Module, it is not calling .to on the submodules. What the pytorch nn module does instead is only move the tensors (buffers, parameters) of these submodules, since pytorch has no such thing as a device property, the device is simply defined by the device the tensors are on.
 We can come up with a creative solution that involves going over all submodules and checking if they are instance of dtypedevicemixin and calling .to on these
 But this is not a real solution because if you nest multiple levels of dtype-mixin-subclass and nn.Module in a particular way, it can still lead to the same bug
 EDIT: I think I have a fix that involves using the apply function on the module, and can submit a PR.
 		",26.0,Anjum48,2020-07-21T19:41:23Z,"
 		<denchmark-link:https://github.com/Anjum48>@Anjum48</denchmark-link>
  the bug should be fixed now, although I was not able to check your use case with precision=16. Let me know if it works or not.
 		",27.0,Anjum48,2020-07-22T03:28:36Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  pulled from master and still experiencing the same issue with FP16
 		",28.0,Anjum48,2020-07-22T07:54:00Z,"
 		Yes, but this is probably something different. If you're using AMP, they don't convert inputs and outputs once, but they monkey patch every function to convert it's inputs and convert it's outputs back...
 Are you on slack? Maybe it's best to write me a message there, so that we can easily discuss your issue without spamming all the others :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2444,NumesSanguis,2020-07-01T08:42:24Z,2020-08-28T07:07:44Z,self.hparam silently removes params that are not serializable,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Following the approach found under <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/hyperparameters.html#lightningmodule-hyperparameters>hyperparameters in the docs</denchmark-link>
 , step 3, I passed a dict with parameters to my .
 In the  printing  shows all contents I passed.
 However, in the function , some hparams are gone.
 This might be related to that not all param values are YAML serializable, and therefore automatically removed? Because the 2 removed params are ""criterion"": torch.nn.BCELoss() and ""optimizer"": partial(optim.Adam, lr=0.001).
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run the following script:
 
 from functools import partial
 import torch
 import torch.optim as optim
 from torch.utils.data import Dataset
 import pytorch_lightning as pl
 from pytorch_lightning import Trainer
 
 # partial to give all params, except the data
 hparams = {
     ""criterion"": torch.nn.BCELoss(),  # F.cross_entropy(),  # loss function
     ""optimizer"": partial(optim.Adam, lr=0.001),  # (lr=0.001),
     # ""learning_rate"": 0.001,
     ""filters"": 64,
     ""layers"": 2
 }
 
 class EmptyDataset(Dataset):
     def __init__(self, transform=None):
         pass
     
     def __len__(self):
         return 32
     
     def __getitem__(self, idx):
         return {""input"": np.array([1]), ""output"": ""nothing""}
 
 class LitLake(pl.LightningModule):
     def __init__(self, hparams: dict, transforms: dict = None):
         super().__init__()
         
         self.hparams = hparams
         print(""self.hparams\n"", self.hparams)
         
     def forward(self, x):
         pass
     
     def training_step(self, batch, batch_idx):
         """"""
         Lightning calls this inside the training loop with the data from the training dataloader
         passed in as `batch`.
         """"""
         # forward pass
         x, y = batch
         y_hat = self(x)
         loss = self.hparams[""criterion""](y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
         
     def configure_optimizers(self):
         print(""self.hparams\n"", self.hparams)
         optimizer = self.hparams[""optimizer""](self.parameters())
         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
         return [optimizer], [scheduler]
     
     def train_dataloader(self):
          return DataLoader(EmptyDataset(), batch_size=4, num_workers=1)
 
 model = LitLake(hparams=hparams)
 # most basic trainer, uses good defaults
 trainer = Trainer()  # gpus=1, num_nodes=1
 trainer.fit(model)  # KeyError: 'optimizer'
 
 See error
 
 Script output (CLICK ME)
 
 self.hparams
  ""criterion"": BCELoss()
 ""filters"":   64
 ""layers"":    2
 ""optimizer"": functools.partial(<class 'torch.optim.adam.Adam'>, lr=0.001)
 GPU available: True, used: False
 TPU available: False, using: 0 TPU cores
 self.hparams
  ""filters"": 64
 ""layers"":  2
 Traceback (most recent call last):
   File ""lightning_hparams_bug.py"", line 61, in <module>
     trainer.fit(model)  # KeyError: 'optimizer'
   File ""/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 965, in fit
     self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)
   File ""/home/*user*/anaconda3/envs/onseilake/lib/python3.7/site-packages/pytorch_lightning/trainer/optimizers.py"", line 18, in init_optimizers
     optim_conf = model.configure_optimizers()
   File ""lightning_hparams_bug.py"", line 51, in configure_optimizers
     optimizer = self.hparams[""optimizer""](self.parameters())
 KeyError: 'optimizer'
 
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Either:
 
 self.hparams keeping the non-serializable parameters (will give problems with loading?)
 Throw an error explaining why those param values are not acceptable and how to approach it, instead of silently removing them.
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.5.0
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): conda
 Build command you used (if compiling from source): -
 Python version: 3.7.6
 CUDA/cuDNN version: 10.2
 GPU models and configuration: 1x GeForce GTX 1080 Ti
 Any other relevant information: -
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Bug reproduced by <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 	",d5254ff9dfb67fba388de224a320f3a562561a80,monney,2020-08-28 09:07:43+02:00,MODIFY,0,pytorch_lightning\utilities\__init__.py,pytorch_lightning\utilities\__init__.py,0.0,9,9,1.0,NumesSanguis,2020-07-01T19:13:22Z,"
 		Happens to me too. I'm also waiting for this fix, since i really want to use the new feature to put anything in hparams.
 		",2.0,NumesSanguis,2020-07-02T16:33:19Z,"
 		<denchmark-link:https://github.com/dscarmo>@dscarmo</denchmark-link>
  you can take it over and send a PR :] or I ll check it tomorrow...
 		",3.0,NumesSanguis,2020-08-04T19:29:05Z,"
 		Need to add warning about this.
 		",4.0,NumesSanguis,2020-08-18T23:14:11Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  this should be fixed with PR, right?
 		",MODIFY,2.0,pytorch_lightning\utilities\parsing.py,pytorch_lightning\utilities\parsing.py,1.0,"56,58,60,61,62,63,64,65,66","55,56,57,58,59,60,61",clean_namespace,hparams,55,66,MODIFY,3.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,"425,426,427,428,429,430",,,,,,,,,,,,,,,,,,,,,,,,5.0,NumesSanguis,2020-08-18T23:19:45Z,"
 		<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  yes the linked PR solves this.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_hparams_pickle_warning,tmpdir,425,430,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"45,46,47,48,49,50,51,52","46,47,48,49,50,51,52",is_picklable,object,45,52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,285,285,test_collect_init_arguments,"tmpdir,cls",251,293,1.0,"419,420,421,422",,__init__,"self,foo,pickle_me",419,422,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2456,griff4692,2020-07-01T21:45:01Z,2020-07-02T11:04:19Z,multi-gpu training triggers CUDA out of memory error,"
 Hi -
 I am running into issues when going from single to multi-gpu training.  Specifically, if I switch the line
 pl.Trainer(gpus=1, precision=16, distributed_backend='ddp')
 to
 pl.Trainer(gpus=4, precision=16, distributed_backend='ddp')
 I get the dreaded CUDA out of memory error.  Is there any reason why the parallelism causes the GPU to receive more data?
 	",afdfba1dc6061c5e1ee6eaf215500d6a56e95482,William Falcon,2020-07-02 07:04:18-04:00,MODIFY,1,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,,"357,358,359,360,361,362,363,364,365,366,367,368",1.0,griff4692,2020-07-01T21:46:18Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,griff4692,2020-07-02T06:34:56Z,"
 		Hi, what are your outputs of the validation_step? If there are any large tensors, it's likely they get synced back to root GPU by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2434>#2434</denchmark-link>
  . We're working on that.
 cc <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
   ^^
 		",3.0,griff4692,2020-07-02T12:27:05Z,"
 		Hi - I actually haven't implemented the validation step yet.  this just occurs on the training side
 		",4.0,griff4692,2020-07-02T13:04:39Z,"
 		what is your gpu consumption on a single gpu (used/available)?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,griff4692,2020-07-02T13:34:42Z,"
 		On single gpu, I am using 5/11 GB.  The problem seems to be that when I switch over to multiple GPUs, there is an explosion of processes created on the first GPU.  Any ideas what could be causing this?
 <denchmark-link:https://user-images.githubusercontent.com/12277915/86365204-32405c00-bc47-11ea-9211-0e6ccfb57f8d.png></denchmark-link>
 
 		",6.0,griff4692,2020-07-02T13:39:04Z,"
 		Fixed it!.  I was calling .to('cuda') on my input tensors in my Dataset __get__item function which caused all the data to be uploaded to the first GPU.  Removed that and solved the problem.
 		",7.0,griff4692,2020-07-02T13:45:20Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  does that mean we should add back the all reduce for val?
 		",8.0,griff4692,2020-07-02T15:17:56Z,"
 		No, there were other issues with that as well :D Let's just keep it out for now.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,reduce_eval_ddp,"self,eval_results",357,368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2458,anthonytec2,2020-07-02T01:43:12Z,2020-07-08T05:45:26Z,Wandb Flatten Dict,"
 Wandb logger should flatten the dictionary of parameters before logging. Every other logger has the bellow pattern of code:
 <denchmark-code> params = self._convert_params(params)
  params = self._flatten_dict(params)
 </denchmark-code>
 
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Wandb logger does not flatten parameters resulting in dictionaries being logged to Wandb, which are not searchable causing for some loss of features in wandb.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run the cpu_template with wandb logger, and log a nested dictionary.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Solution, just call   params = self._flatten_dict(params) this in the wandb logger.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.0
 pytorch-lightning: 0.8.4
 tensorboard:       2.2.2
 tqdm:              4.46.1
 
 
 System:
 
 OS:                Darwin
 architecture:
 
 64bit
 
 
 
 processor:         i386
 python:            3.7.7
 version:           Darwin Kernel Version 19.4.0: Wed Mar  4 22:28:40 PST 2020; root:xnu-6153.101.6~15/RELEASE_X86_64
 
 
 
 	",899cd74044505eb308e624f15a4cb65c57973bbb,Anthony Bisulco,2020-07-08 07:45:25+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"24,25",,1.0,anthonytec2,2020-07-06T08:36:16Z,"
 		<denchmark-link:https://user-images.githubusercontent.com/5495193/86573252-659b2780-bf74-11ea-90d5-30041764941c.png></denchmark-link>
 
 Is this what you mean?
 I agree, it would be nice if we could flatten that out.
 		",2.0,anthonytec2,2020-07-06T11:09:13Z,"
 		Exactly, this commit will make hparams look like:
 <denchmark-link:https://user-images.githubusercontent.com/11021542/86587431-93be3e80-bf57-11ea-85da-c420b613e26f.png></denchmark-link>
 
 		",3.0,anthonytec2,2020-07-06T11:12:30Z,"
 		perfect!
 		",,,,,MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,128,,log_hyperparams,"self,str",126,129,MODIFY,1.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,"22,23,24,25,26,27","22,23,24",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_wandb_logger,wandb,10,32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2472,kl0211,2020-07-02T16:27:39Z,2020-07-03T17:23:31Z,DDP breaks when `python` does not refer to the correct interpreter,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 If using the DDP distributed_backend, the program breaks if python refers to python2 or does not exist.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Make sure the python command does not link to python3, such as on Ubuntu 18.04.
 Run Trainer().fit with distributed_backed='ddp'
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 The problem lies at 
 
 
 pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py
 
 
          Line 422
       in
       0697dd3
 
 
 
 
 
 
  command = ['python'] + command 
 
 
 
 
  The python command is hardcoded here. On many systems, python is a symlink to python2, or does not exist.
 	",fc61c200c085f78fa2af4850aa8dc8e832fb80d0,Jirka Borovec,2020-07-03 13:23:30-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"24,25",,1.0,kl0211,2020-07-02T16:28:37Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,kl0211,2020-07-03T03:33:13Z,"
 		Same question!!!!!
 		",3.0,kl0211,2020-07-03T08:08:57Z,"
 		it is based on your default python... <denchmark-link:https://github.com/kl0211>@kl0211</denchmark-link>
  <denchmark-link:https://github.com/RitchieAlpha>@RitchieAlpha</denchmark-link>
  mind send a Pr which updating the actual python interpreter?
 cc: <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",4.0,kl0211,2020-07-03T08:28:26Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  I just saw <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2482>#2482</denchmark-link>
 . Using  is exactly what I was going to suggest. According to <denchmark-link:https://www.python.org/dev/peps/pep-0394/#for-python-script-publishers>https://www.python.org/dev/peps/pep-0394/#for-python-script-publishers</denchmark-link>
 , this should ensure the same interpreter is used to spawn the child processes.
 Thanks for the quick change!
 		",MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"422,423",422,spawn_ddp_children,"self,model",394,450,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2476,aeryen,2020-07-02T23:51:29Z,2020-09-23T04:19:47Z,Model and Input not on same GPU when training with native AMP and DP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Training with distributed_backend='dp' and precision=16 result in the error that some input/output of the model is not on the same GPU.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-code>from pytorch_lightning import Trainer, seed_everything
 from pl_examples.models.lightning_template import LightningTemplateModel
 seed_everything(234)
 
 def main():
     # model = LightningTemplateModel(**vars(args))
     model = LightningTemplateModel()
 
     trainer = Trainer(
         gpus=2,
         num_nodes=1,
         distributed_backend='dp',
         precision=16
         )
 
     trainer.fit(model)
 
 if __name__ == '__main__':
     main()
 </denchmark-code>
 
 Run the above example code will result in following error:
 <denchmark-code>Traceback (most recent call last):
   File ""./_debug_pytorch_lightning_16gpu.py"", line 28, in <module>
     main()
   File ""./_debug_pytorch_lightning_16gpu.py"", line 24, in main
     trainer.fit(model)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 973, in fit
     self.dp_train(model)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 267, in dp_train
     self.run_pretrain_routine(model)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1139, in run_pretrain_routine
     False)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 291, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 441, in evaluation_forward
     output = model(*args)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
     result = self.forward(*input, **kwargs)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py"", line 65, in forward
     outputs = self.parallel_apply(replicas, inputs, kwargs)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py"", line 69, in parallel_apply
     return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py"", line 209, in parallel_apply
     raise output
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pytorch_lightning/overrides/data_parallel.py"", line 172, in _worker
     output = module.validation_step(*input, **kwargs)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/pl_examples/models/lightning_template.py"", line 102, in validation_step
     val_loss = F.cross_entropy(y_hat, y)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py"", line 2422, in cross_entropy
     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
   File ""/home/ubuntu/anaconda3/envs/trfm/lib/python3.7/site-packages/torch/nn/functional.py"", line 2218, in nll_loss
     ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
 RuntimeError: Assertion `THCTensor_(checkGPU)(state, 4, input, target, output, total_weight)' failed. Some of weight/gradient/input tensors are located on different GPUs. Please move them to a single one. at /opt/conda/conda-bld/pytorch_1593673617738/work/aten/src/THCUNN/generic/ClassNLLCriterion.cu:31
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The model should train successfully on 2 GPUs with native PyTorch AMP.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - Tesla V100-SXM2-16GB
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.18.5
 - pyTorch_debug:     False
 - pyTorch_version:   1.7.0.dev20200702
 - pytorch-lightning: 0.8.5-dev
 - tensorboard:       2.2.2
 - tqdm:              4.47.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.7
 - version:           #30~18.04.1-Ubuntu SMP Mon Jun 22 15:48:21 UTC 2020
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 This only seems to be an issue with native AMP, not Apex AMP.
 	",031274c25dedc92e383d2715e283a55a2b102d29,William Falcon,2020-09-23 00:19:46-04:00,MODIFY,0,pl_examples\README.md,pl_examples\README.md,0.0,"2,3,7,8,10,11,12,14,16,17,18,19","2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,35,36,37,38,39,40,41,42,44,46,47,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,66,67",1.0,aeryen,2020-07-02T23:52:26Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,aeryen,2020-07-03T00:22:17Z,"
 		I just realized that without precision=16 the script would still fail with a different error
 TypeError: zip argument #1 must support iteration
 Am I doing something wrong? This seems like a separate bug.
 		",3.0,aeryen,2020-07-03T00:28:53Z,"
 		I'm not using native amp yet but apex only works on ddp, and I suspect the native implementation might be as well.
 		",4.0,aeryen,2020-07-03T00:35:58Z,"
 		I see, so it's not supported yet. Thanks for the information. Unfortunately, I have to stick with DP, because PyTorch does not allow gradient checkpointing with DDP. Let me see if I can get this to work.
 		",MODIFY,0.0,pl_examples\__init__.py,pl_examples\__init__.py,0.0,,"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147",,,,,MODIFY,0.0,pl_examples\basic_examples\README.md,pl_examples\basic_examples\README.md,0.0,"4,5,7,8,10,11,13,14,15,17,18,19,21,22,24,25,27,28,31,32,33,35,36,38,39,41,42,43,44,57,64","4,6,7,9,10,11,12,13,15,16,17,20,21,23,24,26,27,28,31,32,33,34,35,36,38,39,54,61",ADD,0.0,None,pl_examples\basic_examples\autoencoder.py,,,,,,,,DELETE,0.0,pl_examples\basic_examples\cpu_template.py,None,,,,,,,,5.0,aeryen,2020-07-03T00:41:51Z,"
 		<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  regarding the issue that DP also doesn't work even without precision=16, can you kindly help me confirm that? should I open a separate issue?
 		",6.0,aeryen,2020-07-03T00:50:28Z,"
 		I've never really used dp so I really can't tell you
 		",7.0,aeryen,2020-07-05T02:22:31Z,"
 		yes that's correct... for some reason, dp and apex do not mix well.
 Maybe <denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>
  might know why when forward is called with dp the casting isn't automated?
 		",8.0,aeryen,2020-07-05T03:28:35Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Hi, sorry, the issue is actually with native amp, not apex. Also, honestly, currently the DP is completely broken, not just when using precision=16. It does feel like people have mostly abandoned DP and moved to DDP in general?
 		",9.0,aeryen,2020-07-05T03:40:49Z,"
 		yeah, dp should not really be used but we can’t remove since some people’s research depends on it.
 		",10.0,aeryen,2020-07-05T03:56:14Z,"
 		So here is my question (apologies if this is not a good place to discuss this): Isn't the PyTorch gradient checkpointing currently can only work with DP? My current conundrum, I'm trying to work with HuggingFace longformer, and without gradient checkpointing, I can only fit batch size 1 in a single GPU with 16g vram. So the best combo for me right now is Native AMP + DP + gradient checkpointing.
 I guess maybe the long term direction is either just move to TPU and not bother or wait to see if PyTorch can manage to get DDP to work with gradient checkpointing.
 edit: Sorry I'm slow so just realized there was an effort trying to make DDP work better with grad checkpoint, <denchmark-link:https://github.com/pytorch/pytorch/pull/24800>pytorch/pytorch#24800</denchmark-link>
 . Guess I'll stop bothering you guys and wait for that to work...
 		",11.0,aeryen,2020-07-05T04:55:04Z,"
 		torch.cuda.amp should support DP, and torch.cuda.amp + DP is tested in pytorch CI, so the error is unexpected.  Do you have a minimal repro that uses raw Pytorch (not lighting)?  This would help us narrow down if the problem lies with DP, torch.cuda.amp, or how lightning combines them.  Also, can you post the complete backtrace?
 		",12.0,aeryen,2020-07-05T05:05:33Z,"
 		<denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>
  My feeling is the issue is within lightning, I've done a rough test with raw Pytorch and haven't been able to reproduce the bug (but could be because I'm bad at this). I've added the trace to the top.
 		",13.0,aeryen,2020-08-03T19:48:23Z,"
 		<denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>
  can you take a look?
 		",14.0,aeryen,2020-08-03T20:38:42Z,"
 		I can look into this but probably not in the next few days, reping if I don't respond by then.  Did you ever get a repro with raw pytorch (not lightning)?
 		",15.0,aeryen,2020-08-03T21:53:28Z,"
 		<denchmark-link:https://github.com/aeryen>@aeryen</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,DELETE,0.0,pl_examples\basic_examples\gpu_template.py,None,,,,,,,,,,,,,,,ADD,0.0,None,pl_examples\basic_examples\image_classifier.py,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,pl_examples\basic_examples\mnist.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DELETE,0.0,pl_examples\basic_examples\multi_node_ddp2_demo.py,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DELETE,0.0,pl_examples\basic_examples\multi_node_ddp_demo.py,None,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pl_examples\basic_examples\submit_ddp2_job.sh,pl_examples\basic_examples\submit_ddp2_job.sh,0.0,27,27,MODIFY,0.0,pl_examples\basic_examples\submit_ddp_job.sh,pl_examples\basic_examples\submit_ddp_job.sh,0.0,27,27,,,,,,,,,,,,MODIFY,0.0,pl_examples\domain_templates\semantic_segmentation.py,pl_examples\domain_templates\semantic_segmentation.py,0.0,13,13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,pl_examples\models\unet.py,pl_examples\domain_templates\unet.py,,,,DELETE,0.0,pl_examples\models\lightning_template.py,None,,,,,,,,,,,,,,,,,,,DELETE,0.0,pl_examples\test_examples.py,None,MODIFY,3.0,pytorch_lightning\accelerators\dp_backend.py,pytorch_lightning\accelerators\dp_backend.py,MODIFY,5.0,pytorch_lightning\overrides\data_parallel.py,pytorch_lightning\overrides\data_parallel.py,1.0,"262,277,280,283,285,286,287,288","246,249,253",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"124,125",,training_step_end,"self,output",121,126,1.0,"131,132",,validation_step_end,"self,output",128,133,1.0,"138,139",,test_step_end,"self,output",135,140,parallel_apply,"modules,inputs,kwargs_tup,devices",231,325,1.0,"215,216,217,218,219,220,221,222,223,224,225,226,227,228",,warn_missing_output,fx_called,215,228,1.0,"334,335,336,337",,auto_squeeze_dim_zeros,output,328,344,1.0,"262,277,280,283,285,286,287,288",,parallel_apply._worker,"i,module,input,kwargs,device",260,296,ADD,0.0,None,pytorch_lightning\utilities\warning_utils.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\base\develop_utils.py,tests\base\develop_utils.py,RENAME,0.0,pl_examples\models\__init__.py,tests\examples\__init__.py,,,,,,,,16.0,aeryen,2020-08-03T21:59:35Z,"
 		No I still don't have repro for raw pytorch. So I still think it's within lighting.
 I tried to debug it, I even tried manually move the model to each GPU according to the location of the data within my outer forward(). The issue still occur. Looks to me the model's device jump to something else as soon as my sub modules' forward() are being called. I had the impression that threads are not using the model that belongs to them. I'm probably not explaining this as clear as I should...
 		",17.0,aeryen,2020-08-23T16:30:34Z,"
 		I had (maybe) the same issue with native AMP and DP backend after upgrading from 0.7.6 to 0.9.0.
 It turned out in my case the issue was in DataParallelBackend.setup. Native AMP wrapped the original model's forward function instead of DataParallel.forward.
 When code runs into <denchmark-link:https://github.com/pytorch/pytorch/blob/a97ca93c0e698b81599f7a0ca5cdbda947799431/torch/cuda/amp/autocast_mode.py#L135>this line</denchmark-link>
 , it can be seen that  is always the original model. Then the error occurs when the model and data mismatch.
 I changed the order of wrapping in DataParallelBackend.setup and the problem was solved for me.
 @@ -48,13 +48,13 @@ class DataParallelBackend(object):
          # hack forward to do autocast for the user
          self.model_autocast_original_forward = model.forward
  
 +        # init torch data parallel
 +        model = self.__init_torch_data_parallel(model)
 +
          # init half precision
          if self.trainer.amp_backend:
              model = self.__init_half_precision(model)
  
 -        # init torch data parallel
 -        model = self.__init_torch_data_parallel(model)
 -
          self.trainer.model = model
  
      def __init_torch_data_parallel(self, model):
 		",18.0,aeryen,2020-08-23T16:40:27Z,"
 		want to submit a PR?
 <denchmark-link:https://github.com/mcarilli>@mcarilli</denchmark-link>
 , does this make sense?
 		",19.0,aeryen,2020-08-23T16:46:46Z,"
 		Sorry I’m a bit hectic and not up for making a PR.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"164,165,177,180,183,207,208,209,210,211",,forward,"self,inputs,kwargs",162,212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,6,,,,,ADD,0.0,None,tests\examples\test_examples.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2479,jgbos,2020-07-03T00:34:47Z,2020-10-05T01:27:52Z,init_slurm_connection causing hostname errors,"
 <denchmark-h:h1>Problem</denchmark-h>
 
 Can you update this function to support checking if MASTER_ADDR and MASTER_PORT are already in os.environ?  Running into some weird errors where this code adds host to MASTER_ADDR and crashes the code.
 
 
 
 pytorch-lightning/pytorch_lightning/core/lightning.py
 
 
          Line 903
       in
       0697dd3
 
 
 
 
 
 
  def _init_slurm_connection(self) -> None: 
 
 
 
 
 
 <denchmark-h:h1>Solution</denchmark-h>
 
 Do you prefer a pull request?  Here's the check I put it
 def _init_slurm_connection(self) -> None:
         """"""
         Sets up environment variables necessary for pytorch distributed communications
         based on slurm environment.
         """"""
 
         if 'MASTER_PORT' not in os.environ:
             # use slurm job id for the port number
             # guarantees unique ports across jobs from same grid search
             try:
                 # use the last 4 numbers in the job id as the id
                 default_port = os.environ['SLURM_JOB_ID']
                 default_port = default_port[-4:]
 
                 # all ports should be in the 10k+ range
                 default_port = int(default_port) + 15000
 
             except Exception:
                 default_port = 12910
 
             # if user gave a port number, use that one instead
             try:
                 default_port = os.environ['MASTER_PORT']
             except Exception:
                 os.environ['MASTER_PORT'] = str(default_port)
 
         # figure out the root node addr
         if 'MASTER_ADDR' not in os.environ:
             try:
                 root_node = os.environ['SLURM_NODELIST'].split(' ')[0]
             except Exception:
                 root_node = '127.0.0.1'
 
             root_node = self.trainer.resolve_root_node_address(root_node)
             os.environ['MASTER_ADDR'] = root_node
 	",c6df63a58817b6414f8a3ae28edd9e6552be3914,William Falcon,2020-10-04 21:30:33-04:00,MODIFY,0,pytorch_lightning\trainer\__init__.py,pytorch_lightning\trainer\__init__.py,0.0,"361,369,382,383",368,1.0,jgbos,2020-07-13T15:05:21Z,"
 		Ok, back on this and tracked down the bug.  The problem is in this line:
   root_node = os.environ['SLURM_NODELIST'].split(' ')[0]
 On my system, SLURM_NODELIST is a comma delineated list, so split(',') instead of split(' ').
 		",2.0,jgbos,2020-07-13T15:10:02Z,"
 		Additionally, our system will use this for two systems:
  SLURM_NODE_LIST=d-10-11-[1-2]
 		",3.0,jgbos,2020-08-04T20:52:07Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  is this fixed?
 		",4.0,jgbos,2020-08-26T14:02:47Z,"
 		fyi, this is not fixed in general.  I think the problem is fixed for the example above
 SLURM_NODE_LIST=d-10-11-[1-2]
 but not the case for
 SLURM_NODE_LIST=d-10-11-1,d-11-12-1
 My current hack for PL to run on slurm is to set MASTER_ADDR in the sbatch script and then do the following:
     # fix slurm node list
     if 'MASTER_ADDR' in os.environ and 'SLURM_NODELIST' in os.environ:
         slurm_node_list = os.environ['SLURM_NODELIST'].split(',')
         master_addr = os.environ['MASTER_ADDR']
         os.environ['SLURM_NODELIST'] = ' '.join([master_addr] + slurm_node_list)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,jgbos,2020-09-22T21:27:25Z,"
 		<denchmark-link:https://github.com/jgbos>@jgbos</denchmark-link>
  mind sending a PR for this fix?
 		",6.0,jgbos,2020-09-23T03:28:37Z,"
 		<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  I currently do this before I run lightning so I  feel my solution is just a  and that there's probably a better solution.  It currently requires the user to set  first.  If I find some time I'll take a look at the code, maybe come upon a real solution once I see how lightning works here.
 		",7.0,jgbos,2020-10-05T01:27:52Z,"
 		this is fixed now! no longer needed to hack anything. You can pass your own ClusterEnvironment and do whatever logic you need.
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment>https://pytorch-lightning.readthedocs.io/en/latest/trainer.html#cluster-environment</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2480,HHousen,2020-07-03T04:08:46Z,2020-07-09T11:11:08Z,For versions &gt;0.8.2 learning rate is zero for last epoch (potentially a logging bug),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Version 0.8.2 and above changed the behavior of either my learning rate scheduler or the WandbLogger logger. I am using a linear warmup and decay scheduler. However, the learning rate graph produced by the LearningRateLogger is as shown below ever since version 0.8.2:
 <denchmark-link:https://user-images.githubusercontent.com/11785397/86428929-fc898a80-bcbb-11ea-85b1-2a42f4c54cfd.png></denchmark-link>
 
 The period where the learning rate is zero corresponds to the last epoch of training as you can see below:
 <denchmark-link:https://user-images.githubusercontent.com/11785397/86428981-22af2a80-bcbc-11ea-90ff-bf4418dfe090.png></denchmark-link>
 
 This graph raises another issue. The first epoch appears to take twice as many steps as the second and third epoch. I specified max_epochs=3. During training, each epoch takes the same amount of time, so this seems like a logging issue.
 Note that the above graphs are for a model that had its training stopped early. So the last epoch is slightly shorter than the second to last. This is not the issue.
 Both of these issues (the 0 learning rate and the twice-as-long epoch) do not exist in version 0.8.1, and both graphs look as they should.
 These issues could be caused by the logger or they might actually occur and be logged correctly. I have looked through the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/releases>changelog</denchmark-link>
  and I am guessing that these bugs are caused by ""Changed epoch indexing from 0 instead of 1"" (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2289>#2289</denchmark-link>
 ). I also may be relying on the fact that epoch indexing started at 1 somewhere in my code, but I do not believe this to be the case.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Reproducing this problem may be difficult since I can't provide the script and data I used. I used the WandbLogger logger and LearningRateLogger callback. I trained with 1400 warmup steps and accumulate_grad_batches set to 2.
 I can provide additional code samples or information that you may need.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 def lr_lambda_func(current_step, num_warmup_steps, num_training_steps):
     if current_step < num_warmup_steps:
         return float(current_step) / float(max(1, num_warmup_steps))
     return max(
         0.0,
         float(num_training_steps - current_step)
         / float(max(1, num_training_steps - num_warmup_steps)),
     )
 
 t_total = int(len(self.train_dataloader_object) * self.hparams.max_epochs // self.hparams.accumulate_grad_batches)
 
 lr_lambda = partial(
     lr_lambda_func,
     num_warmup_steps=self.hparams.warmup_steps
     * self.hparams.accumulate_grad_batches,
     num_training_steps=t_total,
 )
 
 scheduler = LambdaLR(optimizer, lr_lambda, -1)
 scheduler_dict = {""scheduler"": scheduler, ""interval"": ""step""}
 return ([optimizer], [scheduler_dict])
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The learning rate should warmup and decay in versions greater than 0.8.2 the same way it does in versions less than 0.8.2. Each epoch should be the same number of steps.
 The below graphs highlight the expected behavior. They are from a different model so they are not directly comparable, but their shape is as expected since they were captured from a model trained with pytorch_lightning version 0.8.1.
 <denchmark-link:https://user-images.githubusercontent.com/11785397/86430600-b5ea5f00-bcc0-11ea-8c07-b78670f1ef7d.png></denchmark-link>
 
 <denchmark-link:https://user-images.githubusercontent.com/11785397/86430629-c8649880-bcc0-11ea-8aca-85cdde47a0d2.png></denchmark-link>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 Tesla P100-PCIE-16GB
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.1+cu101
 pytorch-lightning: 0.8.4
 tensorboard:       2.2.2
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           #1 SMP Wed Feb 19 05:26:34 PST 2020
 
 
 
 	",992a7e2a414d052754f3579e173620baf740308a,Hayden Housen,2020-07-09 07:11:07-04:00,MODIFY,1,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,1.0,102,102,1.0,HHousen,2020-07-03T04:09:41Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,HHousen,2020-07-03T05:36:59Z,"
 		it would be good to know whether this can be observed with the other loggers as well. Could you run your example also with TensorboardLogger?
 		",3.0,HHousen,2020-07-03T15:06:33Z,"
 		Hey! I believe problem lies in configure_accumulated_gradients() when accumulate_grad_batches is integer, scheduler is set to use it from current_epoch=1, but Trainer starts from current_epoch=0, so trainer.accumulate_grad_batches = self.scheduling.get(self.epochs[i]) sets accumulate_grad_batches to default value (1) for this epoch.
 		",4.0,HHousen,2020-07-03T15:10:10Z,"
 		Update: When accumulate_grad_batches is an integer, Scheduler gets {1: accumulate_grad_batches} as input, and then scheduling.update({0: 1}) inserts ""default"" 1 for first epoch.
 		",MODIFY,1.0,tests\trainer\test_lr_finder.py,tests\trainer\test_lr_finder.py,1.0,157,157,test_accumulation_and_early_stopping,tmpdir,136,158,MODIFY,4.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,"143,144,145","144,145",,,,,,,,,,,,,,,,,,,,,,,5.0,HHousen,2020-07-03T15:18:41Z,"
 		<denchmark-link:https://github.com/HHousen>@HHousen</denchmark-link>
  You could do workaround and set  in pl.Trainer (I did so), but you might have problems with restoring from checkpoint, as
 <denchmark-code>n_accum = 1 if self.accumulate_grad_batches is None else self.accumulate_grad_batches
 expected_steps = self.num_training_batches / n_accum
 </denchmark-code>
 
 in restore_training_state will try to use dict in division.
 		",6.0,HHousen,2020-07-03T19:56:20Z,"
 		<denchmark-link:https://github.com/szymonzareba>@szymonzareba</denchmark-link>
  Yep, setting  to  fixed this problem (I create my  like so: ). Both the learning rate and epoch graphs are now correct. It seems like your reasoning is correct.
 		",7.0,HHousen,2020-07-03T20:20:50Z,"
 		<denchmark-link:https://github.com/HHousen>@HHousen</denchmark-link>
  mind send a PR?
 		",8.0,HHousen,2020-07-03T20:51:23Z,"
 		Sure
 		",9.0,HHousen,2020-07-03T22:45:38Z,"
 		<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2490>#2490</denchmark-link>
  fixes this. There was a problem with the  test. See <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2490>#2490</denchmark-link>
  for more information.
 		",,,,,,,,,,,,,,,,,,,,,,,,,test_gradient_accumulation_scheduling._optimizer_step,"epoch,batch_idx,optimizer,optimizer_idx,second_order_closure,on_tpu,using_native_amp,using_lbfgs",143,145,configure_accumulated_gradients,"self,accumulate_grad_batches",98,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"106,107,108,109,110,111,112,113,114,134,135,136,137,138,139,140,141,143,144,145,150,151,154,156,157,161,162,165,167,168,171,172,175,177,178","106,127,128,133,134,137,139,140,144,145,148,150,151,154,155,158,160,161,168,169,170,171,172,173,174,175,176,178",test_gradient_accumulation_scheduling,tmpdir,106,181,1.0,,"127,128",test_gradient_accumulation_scheduling._optimizer_step,"self,epoch,batch_idx,optimizer,optimizer_idx,second_order_closure",127,128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"114,134,135,136,137,138,139,140,141,143,144,145,150,151,154,156,157,161,162,165,167,168,171,172,175,177,178,186","127,128,133,134,137,139,140,144,145,148,150,151,154,155,158,160,161,168,169,170,171,172,173,174,175,176,178",test_gradient_accumulation_scheduling,"tmpdir,schedule,expected",114,189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2484,wietsedv,2020-07-03T09:48:35Z,2020-08-19T23:01:56Z,Trainer.scale_batch_size requires model.batch_size instead of model.hparams.batch_size,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Trainer.scale_batch_size only works if a model has the batch_size property and does not work with model.hparams.batch_size even though all documentation points to the reverse.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 All of my hyperparameters are available as model.hparams like suggested in the documentation: (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/hyperparameters.html#lightningmodule-hyperparameters>hyperparameters, option 3</denchmark-link>
 .
 This means that my  is available as .
 This should be fully compatible with the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/0.8.4/training_tricks.html#auto-scaling-of-batch-size>documented example code</denchmark-link>
  of  since that code also uses  instead of .
 However, when I put my model in Trainer.scale_batch_size, I get the following error:
 <denchmark-code>pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in `model.hparams`
 </denchmark-code>
 
 <denchmark-h:h3>Example code</denchmark-h>
 
 <denchmark-code>class LitModel(pl.LightningModule):
     def __init__(self, hparams):
         super().__init__()
         self.hparams = args
 
 model = LitModel(args)
 trainer = Trainer()
 trainer.scale_batch_size(model)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Either  should work with  or the error message, linked documentation examples and docstrings should all change (i.e. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L139>here</denchmark-link>
 , <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L228>here</denchmark-link>
  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_tricks.py#L233>here</denchmark-link>
 ).
 (I would prefer the second option. I think that it should work with both model.batch_size and model.hparams.batch_size.)
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 pytorch-lightning         0.8.4
 
 	",7b917de94642f63eedaffde79fb973705d2288dd,Adrian Wälchli,2020-08-19 19:01:55-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"151,152,155",,1.0,wietsedv,2020-07-03T09:49:38Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,wietsedv,2020-07-03T10:53:40Z,"
 		it seems like a nice first issue, <denchmark-link:https://github.com/wietsedv>@wietsedv</denchmark-link>
  mind send a PR? 
 		",3.0,wietsedv,2020-07-04T05:10:40Z,"
 		Appears to be the same with the learning rate parameter.
 		",4.0,wietsedv,2020-07-20T19:05:23Z,"
 		A clean fix on the user side while waiting for the PR is to actually use self.hparams.batch_size and define self.batch_size as a property of your module:
 @property
 def batch_size(self):
     return self.hparams.batch_size
 
 @batch_size.setter
 def batch_size(self, batch_size):
     self.hparams.batch_size = batch_size
 That way you keep your hyper parameters together in case you want to dump them somewhere without having to add specific code.
 		",MODIFY,0.0,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,0.0,"27,30,162,163,164,165,166,167,168,169,170,276,278,286","27,161,162,163,164,165,271,272,273,274,276,277,278,279,287",,,,,MODIFY,5.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"240,241,242,243,244,245,246,247,248,249",,,,,,,,,,,,,,,,,,,,,,,,5.0,wietsedv,2020-07-24T15:29:35Z,"
 		From <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1896>#1896</denchmark-link>
  it seems that the problem is rather on the docs side than 's implementation.
  is the correct location to look for the parameter, not .
 My above fix is thus also obsolete.
 		",6.0,wietsedv,2020-07-24T15:30:51Z,"
 		
 From #1896 it seems that the problem is rather on the docs side than scale_batch_size()'s implementation.
 My above fix is thus also obsolete.
 
 I just tried your fix and it seemed to work :)
 		",7.0,wietsedv,2020-07-24T15:37:53Z,"
 		Yes it does work, but from what they said in the PR I linked, hparams was just there as a temporary solution, and all hyper parameters are intended to be set as direct instance attributes in __init__.
 My fix is obsolete regarding the intended usage of PL.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_auto_scale_batch_size_duplicate_attribute_warning,tmpdir,240,249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"200,201,206,213,214,215,216,217,218","200,201,203,206,208,209,210,211,212,213,214",test_trainer_arg,"tmpdir,scale_arg",200,218,1.0,"200,201,206","200,201,203,206,208,209,210",test_auto_scale_batch_size_trainer_arg,"tmpdir,scale_arg",200,210,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237",214,test_auto_scale_batch_size_set_model_attribute,"tmpdir,use_hparams",214,237,1.0,"223,224,225,226,227,228,229",,test_auto_scale_batch_size_set_model_attribute.dataloader,"self,args,kwargs",223,229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
249,awaelchli,2019-09-25T13:11:17Z,2019-09-26T17:20:56Z,"UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars","
 Describe the bug
 Not sure if this is a bug. It shows me this warning at the beginning of training:
 /home/adrian/research/envs/research/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
 To Reproduce
 The minimal MNIST example from the docs has this problem when trained on multiple GPUs. Attached the Python script:
 <denchmark-code>import os
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 import torchvision.transforms as transforms
 
 import pytorch_lightning as pl
 
 
 class CoolModel(pl.LightningModule):
 
     def __init__(self):
         super(CoolModel, self).__init__()
         # not the best model...
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_nb):
         # REQUIRED
         x, y = batch
         y_hat = self.forward(x)
         return {'loss': F.cross_entropy(y_hat, y)}
 
     def validation_step(self, batch, batch_nb):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         return {'avg_val_loss': avg_loss}
 
     def test_step(self, batch, batch_nb):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'test_loss': F.cross_entropy(y_hat, y)}
 
     def test_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
         return {'avg_test_loss': avg_loss}
 
     def configure_optimizers(self):
         # REQUIRED
         return [torch.optim.Adam(self.parameters(), lr=0.02)]
 
     @pl.data_loader
     def tng_dataloader(self):
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def val_dataloader(self):
         # OPTIONAL
         # can also return a list of val dataloaders
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def test_dataloader(self):
         # OPTIONAL
         # can also return a list of test dataloaders
         return DataLoader(MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor()), batch_size=32)
 
 
 if __name__ == '__main__':
     from pytorch_lightning import Trainer
 
     trainer = Trainer(
         gpus=[2, 3],
         distributed_backend='dp',
     )
 
     model = CoolModel()
     trainer.fit(model)
 
 
 </denchmark-code>
 
 Expected behavior
 There are no scalars involved in the forward pass, so the warning does not make sense and should not be shown.
 Desktop (please complete the following information):
 
 OS: Ubuntu 18.04
 Version: the latest pip install
 
 	",8b2a2aeda3066fe30cc496a58368a523ef90ad9b,William Falcon,2019-09-26 13:20:54-04:00,ADD,0,None,pytorch_lightning\trainer\ignored_warnings.py,,,,1.0,awaelchli,2019-10-14T10:30:30Z,"
 		I'm using the newest pip version 0.5.2.1 and I'm still getting this warning (0.5.1.3 also), do I have to toggle it somehow?
 		",2.0,awaelchli,2020-05-02T13:16:12Z,"
 		It happens also to me.
 		",3.0,awaelchli,2020-05-02T13:17:26Z,"
 		update  to 0.7.5
 		",4.0,awaelchli,2020-05-02T13:22:38Z,"
 		I'm currently on cutting edge.
 When manually adding. Unsqueeze to the loss it vanishes of course
 		",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,awaelchli,2020-05-02T13:29:05Z,"
 		weird, can you post a colab with a minimal example? we added auto squeezing in dp for this. (i assume this is dp).
 but on a much greater note, use DDP instead of DP as DP use is discouraged in general (by pytorch and us)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2495,tadejsv,2020-07-04T12:30:29Z,2020-07-05T02:52:50Z,`precision=16` displaying wrong loss in progress bar,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training on a GPU (using native AMP) and setting precision=16, the loss displayed by the progress bar is some crazy large number.
 Stopping the example bellow in the middle of Epoch 1 gives a loss of ~15 000. If I train with precision=32, this loss is the true value of ~0.23.
 The loss tensor is OK, if I add a print statement in the training loop it displays normal values.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import torch
 from torch.nn import functional as F
 from torch import nn
 
 from pytorch_lightning.core.lightning import LightningModule
 from pytorch_lightning import Trainer
 
 from torch.utils.data import DataLoader, random_split
 from torchvision.datasets import MNIST
 
 import os
 from torchvision import datasets, transforms
 
 class LitMNIST(LightningModule):
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 256)
         self.layer_3 = torch.nn.Linear(256, 10)
 
     def forward(self, x):
         batch_size, channels, width, height = x.size()
         x = x.view(batch_size, -1)
         x = self.layer_1(x)
         x = torch.relu(x)
         x = self.layer_2(x)
         x = torch.relu(x)
         x = self.layer_3(x)
         x = torch.log_softmax(x, dim=1)
         return x
 
     def train_dataloader(self):
         transform=transforms.Compose([transforms.ToTensor(),
                                       transforms.Normalize((0.1307,), (0.3081,))])
         mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)
         return DataLoader(mnist_train, batch_size=64)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=1e-3)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
 
         # add logging
         logs = {'loss': loss}
         return {'loss': loss, 'log': logs}
     
 
 model = LitMNIST()
 trainer = Trainer(gpus=1, precision=16)
 trainer.fit(model)
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version:  v1.7.0.dev20200704 (nightly)
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): conda
 Python version: 3.8
 CUDA/cuDNN version: 10.2 (installed with conda from pytorch chanel)
 GPU models and configuration: RTX 2070 SUPER
 
 	",9924c76faa7789294811a27c392ba6b33e07f3f1,William Falcon,2020-07-04 22:52:49-04:00,MODIFY,5,pl_examples\models\lightning_template.py,pl_examples\models\lightning_template.py,1.0,163,,1.0,tadejsv,2020-07-04T12:31:18Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,tadejsv,2020-10-30T10:18:01Z,"
 		I have the same issue, but not on the sample code provided. The only difference in my sample code is that using a custom dataset and L1 loss the numbers are (initially) over 200.0 (as opposed to MNIST where it starts very low).
 All is fine until after ~10 epochs where the loss in the pbar becomes 'inf'; whereas the logs shows the correct value being around 20.
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,,194,amp_scale_loss,"self,unscaled_loss,optimizer,optimizer_idx",191,198,MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,,439,MODIFY,3.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,639,,run_training_batch,"self,batch,batch_idx",580,687,MODIFY,1.0,tests\base\deterministic_model.py,tests\base\deterministic_model.py,1.0,"137,138,139,140",137,backward,"self,trainer,loss,optimizer,optimizer_idx",136,141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,spawn_ddp_children,"self,model",394,450,test_dataloader,self,162,163,,,,,,,,,,,,,,,MODIFY,1.0,tests\trainer\test_trainer_steps.py,tests\trainer\test_trainer_steps.py,1.0,"20,21,48,49,50",36,test_training_step_dict,tmpdir,9,50,,,,,,,,,,,,1.0,"801,802,803,842",837,optimizer_closure,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",762,847,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,160,159,val_dataloader,self,159,160,1.0,182,,add_model_specific_args,"parent_parser,root_dir",166,191,1.0,48,,__init__,"self,float,int,int,float,str,str,int,int,int,kwargs",40,51,,,,,,,,,,,,,,,1.0,745,,call_optimizer_step,"self,optimizer,opt_idx,batch_idx,split_batch",715,760,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,157,156,train_dataloader,self,156,157,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2498,williamFalcon,2020-07-04T14:25:36Z,2020-08-03T22:00:37Z,TPU hangs when using only a train loop (ie: no val loop),"
 I think it's somehow related to checkpointing.
 Easiest way to debug is to get on colab.
 	",0fe933e23d026fce6fd065f87e66c2637693e963,Jirka Borovec,2020-07-27 19:07:09-04:00,MODIFY,0,.circleci\config.yml,.circleci\config.yml,0.0,"65,66,69,105,106","65,68,104",1.0,williamFalcon,2020-07-04T14:26:26Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,williamFalcon,2020-07-05T07:36:53Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I struggle to reproduce this. PL does not even recognize the TPUs in the runtime, XLA_AVAILABLE gets set to false (mnist tpu colab). Tried different pytorch and xla versions, all the same.
 Could you share the colab in which you observed the hangs?
 		",3.0,williamFalcon,2020-07-12T23:43:07Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  checked again and now I am able to run the mnist colab without validation and it does not hang anymore (latest master). Not sure what fixed it.
 		",,,,,MODIFY,0.0,.github\workflows\ci-testing.yml,.github\workflows\ci-testing.yml,0.0,"33,34,35,36",,,,,,MODIFY,0.0,.github\workflows\tpu-testing.yml,.github\workflows\tpu-testing.yml,0.0,"17,18,87,90,93,94,97","85,86,89,92,95",MODIFY,0.0,CHANGELOG.md,CHANGELOG.md,0.0,"41,42",,,,,,MODIFY,0.0,docs\source\new-project.rst,docs\source\new-project.rst,0.0,,"9,10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\__init__.py,pytorch_lightning\__init__.py,0.0,57,61,,,,,,,,,,,,MODIFY,6.0,pytorch_lightning\accelerator_backends\ddp_spawn_backend.py,pytorch_lightning\accelerator_backends\ddp_spawn_backend.py,,,,,,,,1.0,"63,69,170","62,68,169",ddp_train,"self,process_idx,q,model,is_master,proc_offset",62,172,1.0,43,43,train,"self,model,nprocs",42,43,MODIFY,3.0,pytorch_lightning\accelerator_backends\gpu_backend.py,pytorch_lightning\accelerator_backends\gpu_backend.py,1.0,57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_setup_nvidia_apex,"self,LightningModule",57,61,1.0,49,48,setup,"self,model",31,51,MODIFY,12.0,pytorch_lightning\accelerator_backends\tpu_backend.py,pytorch_lightning\accelerator_backends\tpu_backend.py,1.0,"121,123,125","123,124,125",__save_end_of_training_weights,"self,LightningModule,trainer",121,125,1.0,"76,81,85","78,79,83,86",train,"self,LightningModule",76,88,1.0,"99,103,104,105,106,110,113,116,118,119,121,123,125,127,129,132,133","98,100,103,104,105,107,108,111,112,115,116,118,119,123,124,125,126,129,132,133",__setup_tpu_training,"self,model",98,133,1.0,"55,56,57,58,59,61,62,63,65,66,67,68,69,70,74","56,60,74",teardown,"self,model",55,74,1.0,"99,103,104,105,106,110,113,116,118,119","100,103,104,105,107,108,111,112,115,116,118,119",tpu_train_in_process,"self,int,LightningModule,trainer,mp_queue",99,119,1.0,40,,__init__,"self,trainer",37,40,1.0,"127,129,132,133,134,135,137,138,141,142,145,146,148,149,153,154,155,156,159,162,163,164","129,132,133",__setup_tpu_training,"self,LightningModule,trainer",127,164,1.0,"74,76,81,85","74,78,79,83,86,89",tpu_train_in_process,"self,tpu_core_idx,model",74,89,MODIFY,0.0,pytorch_lightning\core\__init__.py,pytorch_lightning\core\__init__.py,0.0,"308,309,310,311,312",308,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\core\decorators.py,pytorch_lightning\core\decorators.py,0.0,,"4,5",MODIFY,10.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"264,273,284,294","266,275,286,296",set_distributed_mode,"self,distributed_backend",259,326,1.0,617,"610,619",save_spawn_weights,"self,model",610,619,MODIFY,1.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,131,132,copy_trainer_model_properties,"self,model",115,134,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,619,619,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"54,55,1065,1074,1079,1082,1088,1098,1106,1107,1112,1115,1118","54,1064,1073,1078,1081,1087,1097,1105,1106,1111,1114,1117",,,,,,,,,,,,evaluation_forward,"self,model,batch,batch_idx,dataloader_idx,bool",600,638,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,MODIFY,2.0,tests\base\datasets.py,tests\base\datasets.py,MODIFY,5.0,tests\base\develop_utils.py,tests\base\develop_utils.py,1.0,63,63,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"63,69,170","68,169",ddp_train,"self,process_idx,mp_queue,model,is_master,proc_offset",63,173,1.0,40,40,setup,self,35,40,1.0,57,56,_setup_nvidia_apex,"self,model",56,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1071,1071,training_forward,"self,batch,batch_idx,opt_idx,hiddens",1033,1111,,,,,,,,1.0,212,,prepare_data,"self,bool",203,214,1.0,"108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125",,_try_load,"path_data,int,float",108,125,,,,,,,,load_model_from_checkpoint,"logger,root_weights_dir,module_class,path_expt",63,66,1.0,"92,104,109,110,111,112,114","103,108,109,110,112",pl_multi_process_test,func,91,116,1.0,104,103,pl_multi_process_test.pl_multi_process_test.wrapper.inner_f,"queue,kwargs",100,107,1.0,"104,109,110,111,112,114","103,108,109,110,112",pl_multi_process_test.wrapper,"args,kwargs",95,114,ADD,0.0,None,tests\base\test_datasets.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tests\models\test_grad_norm.py,tests\models\test_grad_norm.py,MODIFY,23.0,tests\models\test_tpu.py,tests\models\test_tpu.py,1.0,"142,149,152,157","144,145,148,150,151,152,153,154,157",test_model_16bit_tpu_index_1,tmpdir,142,158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"47,48,49,53","47,48,49",teardown,"self,model",45,61,1.0,33,33,__init__,"self,trainer",31,33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,254,256,init_tpu,self,252,257,1.0,476,479,spawn_ddp_children,"self,model",421,479,1.0,"597,599,600,601,608","591,592,600,602,603,610",transfer_ddp_spawn_state_on_fit_end,"self,model,q,results",591,610,1.0,633,635,load_spawn_weights,"self,original_model",621,642,1.0,"588,589,597,599,600,601,608","591,592,600,602,603",transfer_distrib_spawn_state_on_fit_end,"self,model,mp_queue,results",588,608,1.0,,367,determine_local_rank,self,364,369,1.0,"487,580,588,589","484,490,583",ddp_train,"self,process_idx,q,model,is_master,proc_offset",484,589,1.0,"481,487,580","484,490,583",ddp_train,"self,process_idx,mp_queue,model,is_master,proc_offset",481,586,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,63,63,load_model_from_checkpoint,"logger,root_weights_dir,module_class",63,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,46,46,test_grad_tracking,"tmpdir,norm_type,rtol",46,74,MODIFY,1.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,900,900,test_trainer_config,"trainer_kwargs,expected",893,901,,,,,,,,,,,,,,,RENAME,0.0,tests\models\test_test_loop.py,tests\trainer\test_trainer_test_loop.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"46,47,48,49",46,teardown,self,46,49,1.0,,"91,94,96",__save_end_of_training_weights,"self,model",91,96,1.0,"51,52,53,54,55,56,57,58,59,61,62,63","51,56,60",train,"self,model",51,63,1.0,"46,47,48,49,50,51,52,53","43,44,46,51",setup,self,42,53,1.0,46,46,test_grad_tracking,"tmpdir,norm_type,rtol",46,74,1.0,"61,62,68,71,76","58,64,66,72,73,74,75,76",test_base_tpu_model_8,tmpdir,58,78,1.0,"100,101,102,103,104,105,106,107,108,109,110,111,112,114","100,109",test_base_tpu_16bit_model_idx_core,tmpdir,100,114,1.0,"162,163,170,173","161,167,168,169,170,173",test_single_tpu_core_model,tmpdir,161,173,1.0,,"72,73,74",test_base_tpu_model_8.long_train_loader,,72,74,1.0,"163,170,173,178,179,181","167,168,169,170,173,177,178",test_model_16bit_tpu_cores_8,tmpdir,163,181,1.0,"178,179,181,185,186,187","177,178,182,183,184,185,188",test_multi_core_tpu_model,tmpdir,177,188,1.0,"81,87,88,90,94,95","82,91",test_model_tpu_index_5,tmpdir,81,95,1.0,,"133,134,135",test_base_tpu_16bit_model_8_cores.long_train_loader,,133,135,1.0,"122,129,132","127,131,133,134,135,136,137",test_model_16bit_tpu_cores_1,tmpdir,122,137,1.0,"44,50,53",50,test_model_tpu_cores_1,tmpdir,44,57,1.0,"186,187,191,192,193,194,195,196,199,200",188,test_model_16bit_tpu_index_5,tmpdir,186,200,1.0,"205,209,212,213,214,215,216,219",,test_early_stop_checkpoints_on_tpu,tmpdir,205,219,1.0,251,,test_dataloaders_passed_to_fit,tmpdir,243,259,1.0,"121,122,129,132","118,127,131,133,134,135,136,137,139,140",test_base_tpu_16bit_model_8_cores,tmpdir,118,140,1.0,"100,101,102,103,104,105,106,107,108,109,110,111,112,114,115","100,109",test_model_tpu_cores_8,tmpdir,100,117,1.0,"273,274,275,276,277,278,279",,test_tpu_misconfiguration,,273,279,1.0,"27,28,29,30,31,32,33,34,35,36,37,38","26,34",test_base_tpu_model_1,tmpdir,26,38,1.0,"87,88,90,94,95,96","82,91",test_base_tpu_16bit_model_core_1,tmpdir,82,96,1.0,"62,68,71,76","64,66,72,73,74,75,76",test_model_tpu_index_1,tmpdir,62,76,1.0,"35,36,37,38,39",,_serial_train_loader,,35,39,1.0,"292,296",,test_exception_when_no_tpu_found,tmpdir,284,297,1.0,"43,44,50,53","42,50",test_base_tpu_model_idx_1,tmpdir,42,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2531,lucmos,2020-07-06T17:15:51Z,2020-07-27T21:56:56Z,IndexError with multiple validation loaders and fast_dev_run,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 An IndexError when using multiple validation datasets and fast_dev_run=True
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Use multiple val_dataloaders
 Use fast_dev_run=True
 
 <denchmark-h:h3>Code sample</denchmark-h>
 
 <denchmark-link:https://colab.research.google.com/drive/107nKJxF4ttWPtQbo8-Wb0RG3Sa_fxjQP?usp=sharing>https://colab.research.google.com/drive/107nKJxF4ttWPtQbo8-Wb0RG3Sa_fxjQP?usp=sharing</denchmark-link>
 
 <denchmark-h:h3>Traceback</denchmark-h>
 
 <denchmark-code>Traceback (most recent call last):
   File ""/home/luca/Repositories/set-operations/src/run_experiment.py"", line 73, in <module>
     trainer.fit(model,)
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 979, in fit
     self.single_gpu_train(model)
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 185, in single_gpu_train
     self.run_pretrain_routine(model)
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1156, in run_pretrain_routine
     self.train()
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 370, in train
     self.run_training_epoch()
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 470, in run_training_epoch
     self.run_evaluation(test_mode=False)
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 409, in run_evaluation
     eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 270, in _evaluate
     dl_max_batches = max_batches[dataloader_idx]
 IndexError: list index out of range
 
                               Exception ignored in: <function tqdm.__del__ at 0x7fe5848ba710>
 Traceback (most recent call last):
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/tqdm/std.py"", line 1086, in __del__
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/tqdm/std.py"", line 1293, in close
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/tqdm/std.py"", line 1471, in display
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/tqdm/std.py"", line 1089, in __repr__
   File ""/home/luca/.cache/pypoetry/virtualenvs/set-operations-GbjOlTQ2-py3.7/lib/python3.7/site-packages/tqdm/std.py"", line 1433, in format_dict
 TypeError: cannot unpack non-iterable NoneType object
 
 Process finished with exit code 1
 </denchmark-code>
 
 <denchmark-h:h3>Reason</denchmark-h>
 
 If fast_dev_run=True here max_batches is set to [1]
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
         Lines 376 to 377
       in
       afdfba1
 
 
 
 
 
 
  if self.fast_dev_run: 
 
 
 
  max_batches = [1] 
 
 
 
 
 
 Thus, later on, it does not pass this test and it remains stuck to [1]:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
         Lines 256 to 257
       in
       afdfba1
 
 
 
 
 
 
  if isinstance(max_batches, int): 
 
 
 
  max_batches = [max_batches] * len(dataloaders) 
 
 
 
 
 
 Then, the loop iterates over all the dataloaders, causing a IndexError at line 270 at the second iteration:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
         Lines 260 to 270
       in
       afdfba1
 
 
 
 
 
 
  for dataloader_idx, dataloader in enumerate(dataloaders): 
 
 
 
  dl_outputs = [] 
 
 
 
  
 
 
 
  # on TPU we have to wrap it under the ParallelLoader 
 
 
 
  if self.use_tpu: 
 
 
 
  device = xm.xla_device(self.tpu_id) 
 
 
 
  dataloader = xla_pl.ParallelLoader(dataloader, [device]) 
 
 
 
  dataloader = dataloader.per_device_loader(device) 
 
 
 
  
 
 
 
  # each dataloader has a max num batches 
 
 
 
  dl_max_batches = max_batches[dataloader_idx] 
 
 
 
 
 
 <denchmark-h:h3>Possible solution</denchmark-h>
 
 
 Let fast_dev_run=True use all validation loaders
 Modify the evaluation for loop to use only the first val loader
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.1+cu101
 pytorch-lightning: 0.8.4
 tensorboard:       2.2.2
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           1 SMP Wed Feb 19 05:26:34 PST 2020
 
 
 
 	",84c507c4df5f5c336deb19ce7f70fa02329f39f6,Rohit Gupta,2020-07-27 17:56:55-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"35,36",,1.0,lucmos,2020-07-06T18:23:05Z,"
 		
 Let fast_dev_run=True use all validation loaders
 
 This is a better choice since Dataset of different dataloaders can be different and we need to check all of them using fast_dev_run.
 		",2.0,lucmos,2020-07-07T08:06:17Z,"
 		<denchmark-link:https://github.com/lucmos>@lucmos</denchmark-link>
  seems you digged in... mind send a PR?
 		",,,,,,,,,MODIFY,3.0,pytorch_lightning\callbacks\progress.py,pytorch_lightning\callbacks\progress.py,1.0,"101,102,103","101,103,104",total_val_batches,self,94,104,MODIFY,0.0,pytorch_lightning\trainer\__init__.py,pytorch_lightning\trainer\__init__.py,0.0,396,396,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,,"516,520,521,522,523",run_evaluation,"self,bool",497,559,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"281,508,509,510,511,512,535,536,537","281,508",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,551,551,check_checkpoint_callback,"self,should_check_val",548,554,1.0,404,404,train,self,329,422,MODIFY,5.0,pytorch_lightning\utilities\debugging.py,pytorch_lightning\utilities\debugging.py,,,,,,,,1.0,"14,15",,__init__,"self,trainer",7,17,1.0,"88,89,90,91,92,93",,num_seen_test_check_batches,self,88,93,MODIFY,1.0,tests\callbacks\test_progress_bar.py,tests\callbacks\test_progress_bar.py,1.0,"119,120",,1.0,113,107,total_test_batches,self,107,113,1.0,91,91,total_train_batches,self,85,91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"510,533","510,533",run_training_epoch,self,452,546,test_progress_bar_fast_dev_run,tmpdir,111,141,,,,,,,,MODIFY,2.0,tests\models\test_grad_norm.py,tests\models\test_grad_norm.py,1.0,46,46,test_grad_tracking,"tmpdir,norm_type,rtol",46,74,1.0,46,46,test_grad_tracking,"tmpdir,norm_type,rtol",46,74,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,2.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"310,311,329,330,331,332,338,340,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362",332,test_dataloaders_with_limit_num_batches,"tmpdir,limit_train_batches,limit_val_batches,limit_test_batches",308,362,1.0,"365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396",,test_dataloaders_with_fast_dev_run,tmpdir,365,396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"29,30,31,32,33,34,35,36,37,38,39,40,41,42",,track_eval_loss_history,"self,test_mode,batch_idx,dataloader_idx,output",29,42,1.0,"75,76,77",,num_seen_sanity_check_batches,self,75,77,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"80,81,82,83,84,85",,num_seen_val_check_batches,self,80,85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2532,lucmos,2020-07-06T18:01:39Z,2020-10-05T03:25:03Z,TypeError with multiple validation loaders and overfit_batches,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 A TypeError when using multiple validation datasets and  overfit_batches != 0
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Use multiple val_dataloaders
 Use overfit_batches != 0, e.g. overfit_batches=0.5
 
 <denchmark-h:h3>Code sample</denchmark-h>
 
 <denchmark-link:https://colab.research.google.com/drive/1BtQBCoP5fK-aZm_2uLMOUbf2c9cu-yFb?usp=sharing>https://colab.research.google.com/drive/1BtQBCoP5fK-aZm_2uLMOUbf2c9cu-yFb?usp=sharing</denchmark-link>
 
 <denchmark-h:h3>Traceback</denchmark-h>
 
 <denchmark-code>TypeError                                 Traceback (most recent call last)
 
 <ipython-input-5-c33b987ae54f> in <module>()
       1 trainer = pl.Trainer(overfit_batches=0.5)
 ----> 2 trainer.fit(model)
 
 3 frames
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders)
    1018             self.optimizers, self.lr_schedulers, self.optimizer_frequencies = self.init_optimizers(model)
    1019 
 -> 1020             self.run_pretrain_routine(model)
    1021 
    1022         # callbacks
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    1137                                           self.val_dataloaders,
    1138                                           max_batches,
 -> 1139                                           False)
    1140 
    1141             # allow no returns from eval
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)
     291                         output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
     292                 else:
 --> 293                     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
     294 
     295                 # on dp / ddp2 might still want to do something with the batch parts
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in evaluation_forward(self, model, batch, batch_idx, dataloader_idx, test_mode)
     483             output = model.test_step(*args)
     484         else:
 --> 485             output = model.validation_step(*args)
     486 
     487         return output
 
 TypeError: validation_step() missing 1 required positional argument: 'dataloader_idx'
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 If the codebase is working with multiple validation loaders, it should continue to work even when using overfit_batches != 0
 <denchmark-h:h3>Possible solution</denchmark-h>
 
 
 Check if there were multiple val dataloaders, in case call validation_step with dataloader_idx=0
 Repeat the train loader to match the number of val dataloaders
 Add the possibility to overfit on train but validate and test normally. It is already possible with limit_train_batches, so it would be only a doc change ""If there are multiple val_dataloaders, use limit_train_batches instead of overfit_batches""
 
 <denchmark-h:h3>Reason</denchmark-h>
 
 When using multiple validation loaders, validation_step takes a dataloader_idx.
 However if later on we set the overfit_batches to something that is not 0, line 268 is executed to use the train loader instead than the validation loaders:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/data_loading.py
 
 
         Lines 266 to 270
       in
       a91b06e
 
 
 
 
 
 
  # use the training loader as val and test when overfitting 
 
 
 
  if self.overfit_batches > 0: 
 
 
 
  dataloaders = self.request_dataloader(getattr(model, 'train_dataloader')) 
 
 
 
  else: 
 
 
 
  dataloaders = self.request_dataloader(getattr(model, f'{mode}_dataloader')) 
 
 
 
 
 
 Now there is only one validation loader, thus the validation_step function that had a dataloader_idx parameter breaks.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.1+cu101
 pytorch-lightning: 0.8.4
 tensorboard:       2.2.2
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           1 SMP Wed Feb 19 05:26:34 PST 2020
 
 
 
 	",d787208e768085b608198f3e0313e2be28d4cbfe,William Falcon,2020-10-04 23:25:02-04:00,MODIFY,0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,0.0,"30,237,241,242,243,244,245,246,247","236,238,239,240,241",1.0,lucmos,2020-08-03T16:35:56Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  is this issue fixed on master?
 		",,,,,,,,,,,,,ADD,0.0,None,tests\trainer\flags\test_overfit_batches.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2539,YuxianMeng,2020-07-07T11:30:39Z,2020-07-10T01:28:12Z,TPU fp16 requires apex installed,"
 When I tried to use precision=16 on TPU, pytorch-lightning is trying to find amp, which is unnecessary.
 The backtrace is
 <denchmark-code>GPU available: False, used: False
 TPU available: True, using: 8 TPU cores
 Traceback (most recent call last):
   File ""bert_ner/light/fp16_debug.py"", line 16, in <module>
     trainer = pl.Trainer(tpu_cores=8, precision=16)
   File ""/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 607, in __init__
     self.init_amp()
   File ""/anaconda3/envs/torch-xla-1.5/lib/python3.6/site-packages/pytorch_lightning/trainer/auto_mix_precision.py"", line 27, in init_amp
     ""You set `use_amp=True` but do not have apex installed.""
 ModuleNotFoundError: You set `use_amp=True` but do not have apex installed.Install apex first using this guide and rerun with use_amp=True:https://github.com/NVIDIA/apex#linux his run will NOT use 16 bit precision
 </denchmark-code>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 build a whatever Trainer in TPU and use fp16
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import pytorch_lightning as pl
 
 trainer = pl.Trainer(tpu_cores=8, precision=16)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Should have nothing error.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.5.0):
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): conda
 Build command you used (if compiling from source):
 Python version:
 CUDA/cuDNN version:
 GPU models and configuration:
 Any other relevant information: actually I directly use pytorch-xla-1.5 docker on Google Cloud
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",e068af9ea8c86df8ed5eb20e57a36fbb38c70462,William Falcon,2020-07-09 21:28:11-04:00,MODIFY,1,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,212,212,1.0,YuxianMeng,2020-07-07T11:31:35Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,YuxianMeng,2020-07-07T12:26:33Z,"
 		If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
 I could get this working using nightly version of pytorch:
 <denchmark-code>pl.Trainer(precision=16, tpu_cores=8)
 >>>GPU available: False, used: False
 >>>TPU available: True, using: 8 TPU cores
 >>>Using native 16bit precision.
 </denchmark-code>
 
 		",3.0,YuxianMeng,2020-07-08T03:23:51Z,"
 		
 If you want to do 16 bit precision training, you either need to have the nightly version of pytorch install or have apex installed. Based on the traceback I guess that you do not have any of them.
 I could get this working using nightly version of pytorch:
 pl.Trainer(precision=16, tpu_cores=8)
 >>>GPU available: False, used: False
 >>>TPU available: True, using: 8 TPU cores
 >>>Using native 16bit precision.
 
 
 Thanks for the quick reply. But <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/apex.html>the document</denchmark-link>
  does not point out that I must have nightly version of pytorch installed or have apex installed when training on TPU with fp16. Maybe it's better to revise that part of document?
 		",4.0,YuxianMeng,2020-07-08T09:56:58Z,"
 		Yes, I agree that from the documentation it would look like it is only a requirement for gpu training. I guess that the specific requirement for TPU is to have pytorch version 1.6 or higher.
 		",MODIFY,1.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"243,250","243,250",dp_train,"self,model",229,273,MODIFY,0.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,0.0,289,289,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"1303,1304,1305,1306,1307",,__test_using_best_weights,"self,ckpt_path,test_dataloaders",1287,1330,MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,361,361,dump_checkpoint,"self,bool",316,381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_forward_example_input,self,204,226,,,,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,705,705,run_batch_backward_pass,"self,split_batch,batch_idx,opt_idx,optimizer",690,714,1.0,"770,820","770,820",optimizer_closure,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",762,847,,,,,1.0,1121,1121,run_pretrain_routine,"self,LightningModule",1104,1213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,753,753,call_optimizer_step,"self,optimizer,opt_idx,batch_idx,split_batch",716,760,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2551,moi90,2020-07-08T10:57:09Z,2020-10-05T03:02:36Z,TrainerEvaluationLoopMixin activates model.train() at the end,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 According to the <denchmark-link:https://github.com/jbschiratti/pytorch-lightning/blob/master/pl_examples/domain_templates/computer_vision_fine_tuning.py>example on fine-tuning</denchmark-link>
 , it is important to set the frozen sub-modules to eval mode. This is sensitive because when in training mode, BatchNorm and Dropout change state.
 However, at the end of TrainerEvaluationLoopMixin._evaluate there is following code:
 # enable train mode again
 model.train()
 So after the first validation run, the model is again completely in training mode and the freezing is partially undone (for layers like BatchNorm and Dropout).
 	",f58c7604093fc37c765ac88e46aaf52b403332fe,William Falcon,2020-10-04 23:02:35-04:00,MODIFY,4,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,"211,212",,1.0,moi90,2020-07-09T07:05:14Z,"
 		I agree, this should not happen in the validation loop. Only the training loop should switch the model to training mode.
 		",2.0,moi90,2020-07-09T10:57:04Z,"
 		so, we have to track the state of all the frozen modules before?
 		",3.0,moi90,2020-07-09T15:33:30Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  When accepting that the training mode could have been customized, not even the training loop should change it carelessly.
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  That would be one solution.
 Another would be to just advice people to set the mode in on_epoch_start. (Would this be late enough so it is not reset by the Trainer?)
 		",4.0,moi90,2020-09-07T16:13:44Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",MODIFY,2.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"91,92,93,94,95,96",,on_evaluation_model_eval,"self,args,kwargs",91,96,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"546,547,619","547,618",ADD,0.0,tests\trainer\model_hooks\__init__.py,tests\trainer\model_hooks\__init__.py,,,,,,,,ADD,0.0,None,tests\trainer\model_hooks\test_model_hooks.py,,,,,,,,5.0,moi90,2020-09-18T00:35:29Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/moi90>@moi90</denchmark-link>
  what if we just made this a hook? The default hook will implement what we have today, set the model to train or eval mode. But if user wants the fine tuning use case, they can override the hook and just set their layers to train/eval mode manually.
 
 old behaviour is preserved
 no complicated tracking needed
 in the finetuning case, it will be fully transparent to the reader of the code which layers are eval and which are training
 easy to implement
 
 		",6.0,moi90,2020-09-19T07:29:58Z,"
 		Where and when would this hook be called? How would this be different from using on_epoch_start?
 		",7.0,moi90,2020-09-28T04:30:46Z,"
 		
 Where and when would this hook be called?
 
 Wherever we call model.eval / model.train today in the training loop, we would call the hook instead, which by default also just does that same thing as before, unless user overrides it.
 
 How would this be different from using on_epoch_start?
 
 I think this would be different from an on epoch start because it would allow you to prevent exactly what happens as described in the title: activating model.train for all layers at the end of an epoch.
 		",8.0,moi90,2020-10-02T20:49:22Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  can this be added post v1?
 		",9.0,moi90,2020-10-02T21:30:08Z,"
 		Yes
 		",10.0,moi90,2020-10-05T08:51:34Z,"
 		Thank you, this is much appreciated!
 		",11.0,moi90,2020-12-10T15:41:42Z,"
 		Shouldn't the hooks be defined like:
 <denchmark-code>def on_validation_model_eval_end(self) -> None:
   """"""
   Called in the validation loop after the model is set to eval
   """"""
   # do something when the model is set to eval
 </denchmark-code>
 
 How do we use these hooks in the current version? If I don't want to switch to eval mode ever (a pretty common case with GANs), do I just override
 <denchmark-code>class DummyCallback(pl.Callback):
   def on_validation_model_eval():
     pass
 </denchmark-code>
 
 or do I call on_validation_model_train() and then the eval hook won't be called?
 		",,,,,,,,,,,,,,,,,run_evaluation,"self,bool,max_batches",537,625,on_test_model_eval,self,211,212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"98,99,100,101,102,103",,on_evaluation_model_train,"self,args,kwargs",98,103,,,,,,,,1.0,"151,152",,on_validation_model_train,self,151,152,1.0,"143,144",,on_validation_model_eval,self,143,144,1.0,"219,220",,on_test_model_train,self,219,220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2555,ruotianluo,2020-07-08T21:13:02Z,2020-08-05T17:43:51Z,apex amp state dict,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_io.py
 
 
          Line 310
       in
       25ee51b
 
 
 
 
 
 
  if self.use_amp and NATIVE_AMP_AVALAIBLE and 'native_amp_scaling_state' in checkpoint: 
 
 
 
 
 
 It seems for native amp support, the scalar state dict is saved. But for non native amp, the amp state dict is not saved?
 	",bef27c58eda4c4425c8aa750d38e16522bfcbe39,Ruotian(RT) Luo,2020-08-05 13:43:50-04:00,MODIFY,3,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"537,538",,1.0,ruotianluo,2020-07-10T01:14:24Z,"
 		anyone think it should be fixed?
 		",2.0,ruotianluo,2020-08-04T21:02:44Z,"
 		<denchmark-link:https://github.com/ruotianluo>@ruotianluo</denchmark-link>
  mind send a PR? 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,hpc_load,"self,folderpath,on_gpu",522,549,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"327,328",,restore,"self,str,bool",296,331,1.0,"380,381",,dump_checkpoint,"self,bool",333,400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2574,ruotianluo,2020-07-10T04:36:46Z,2020-08-04T21:00:56Z,horovod mode increase lr,"
 <denchmark-h:h2>Not really a 🐛 Bug</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/distrib_parts.py#L299>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/distrib_parts.py#L299</denchmark-link>
 
 Under horovod mode, the learning rate will automatically be increased by hvd.size().
 This behavior is different from ddp, so it may confuse the users.
 	",1369012bc71f257dcf7423ec65146d055ddc1cc7,Travis Addair,2020-07-23 12:14:57-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"28,29",,1.0,ruotianluo,2020-07-10T09:52:58Z,"
 		mind check <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
   ^^
 		",2.0,ruotianluo,2020-07-10T10:27:11Z,"
 		I was not aware of this. It's seems that it is a good practice, that really can help boost training speed (see this paper <denchmark-link:https://arxiv.org/abs/1706.02677>https://arxiv.org/abs/1706.02677</denchmark-link>
  and this issue <denchmark-link:https://github.com/horovod/horovod/issues/384>horovod/horovod#384</denchmark-link>
 ). So maybe we should implement something similar for ddp backend.
 		",3.0,ruotianluo,2020-07-10T12:47:01Z,"
 		Yes, in Horovod (and I believe DDP), increasing number of workers is analogous to increasing the total batch size during training.  As such, scaling the learning rate proportionately is considered a best practice.  It's good to handle it internally to the distributed_backend, because some backends may behave differently.  For example, in Horovod, enabling the Adasum optimizer only requires scaling by the number of GPUs per host.
 		",4.0,ruotianluo,2020-07-10T14:17:53Z,"
 		I agree it is a good practice. However if it's not the only way, I don't think it should be the default, Especially without notifying the users.
 It could instead be an argument of the trainer.
 		",MODIFY,1.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,"302,303,304,305,306,307",302,horovod_train,"self,model",276,342,MODIFY,2.0,tests\models\test_horovod.py,tests\models\test_horovod.py,1.0,"183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212",,,,,,,,,,,,,,,,,,,,,,,,5.0,ruotianluo,2020-07-10T16:24:18Z,"
 		I think making it configurable is reasonable.  However, I do think it should be enabled by default.  Part of the goal of the Trainer abstraction is to make distributed training accessible to people who are not familiar with distributed training concepts / best practices.
 For most users, unless they are using custom learning rate schedules or unusual optimizers, they will want to scale the learning rate.  At the same time, most users would not know to do this themselves, so I fear without enabling it by default, they would not do so, and their models would converge worse as a result.
 		",6.0,ruotianluo,2020-07-11T16:33:49Z,"
 		BTW, there may be a problem when lr_scheduler is LambdaLR. It seems LambdaLR will collect the lrs in optimizer and save as base_lrs. The lambda function will take place on the base_lrs. Even you change the lr later, the lr scheduler would ignore it.
 <denchmark-link:https://github.com/pytorch/pytorch/blob/879cf0b15a54c7848ae710e3d0ec62c4a9d7d3dd/torch/optim/lr_scheduler.py#L43>https://github.com/pytorch/pytorch/blob/879cf0b15a54c7848ae710e3d0ec62c4a9d7d3dd/torch/optim/lr_scheduler.py#L43</denchmark-link>
 
 LambdaLR scheduler I believe is a commonly used scheduler. For now I think it's safer to delete that line for now, and then think of what is the best way to implement it.
 Of course, correct me if I am wrong.
 		",7.0,ruotianluo,2020-07-16T22:11:00Z,"
 		Hey <denchmark-link:https://github.com/ruotianluo>@ruotianluo</denchmark-link>
 , that's a good point regarding interaction with LambdaLR and other LR schedulers.  Can you take a look at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2626>#2626</denchmark-link>
  and see if it addresses your concern?
 		",8.0,ruotianluo,2020-07-23T00:27:00Z,"
 		<denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  I still want to defend against scaling learning rate by default. By primitive search, it doesn't seem to me that in nlp, people do the same learning rate scaling. Bert uses batch size 256 and learning rate 1e-4; Roberta uses batch size 8k and max learning rate is 4e-4/6e-4(depending on the model size). I think it may be related to optimizer(in nlp it's usually adam). I don't know if this fact can convince you.
 		",9.0,ruotianluo,2020-07-23T00:35:49Z,"
 		Hey <denchmark-link:https://github.com/ruotianluo>@ruotianluo</denchmark-link>
 , even when training BERT with Horovod, it's common practice to scale the learning rate.  See:
 
 https://github.com/LeoWood/bert-horovod/blob/master/optimization_hvd.py#L61
 https://github.com/google-research/bert/pull/568/files#diff-717a6b63e0d2e51c2ff68a440534783eR61
 
 Fundamentally, when you add more workers, you are increasing the batch size.  That holds true whether it is a vision task, NLP, or other scenarios.  So you need to account for that somehow (most commonly through LR scaling, though I imagine other means are possible as well).
 I do agree we should make this configurable, though.  I'm interested in putting together a separate PR for this, but it should include changes to DDP as well.
 		",10.0,ruotianluo,2020-07-23T01:17:51Z,"
 		<denchmark-link:https://arxiv.org/pdf/1904.00962.pdf>https://arxiv.org/pdf/1904.00962.pdf</denchmark-link>
 . This paper uses square root scaling for bert(and also imagenet classification too). Aand albert(from google) uses this approach (<denchmark-link:https://arxiv.org/pdf/1909.11942.pdf>https://arxiv.org/pdf/1909.11942.pdf</denchmark-link>
 ).
 The first link you provide doesn't have any results. For the second, I didn't see any quantitative results either how that would affects.(and it is not merged yet.)
 Do other frameworks do learning rate scaling by default too(Keras, fastai?)? If it's common across other libraries, I think it's fine too.
 		",11.0,ruotianluo,2020-07-23T17:31:01Z,"
 		Hey <denchmark-link:https://github.com/ruotianluo>@ruotianluo</denchmark-link>
 , in my experience, frameworks that expose distributed training to users as an API (like ) will mention in their docs that it's good practice to scale the LR, but will leave it to the user to do so (this is what we do with Horovod as well).
 However, frameworks that attempt to completely abstract away distributed training (like PyTorch Lightning is seeking to do) should provide a good reasonable default.
 I agree with you that in practice, it may be that linearly scaling the learning rate does not provide the best model performance, in which cases the researchers will often hand-tune the combination of learning rate and total batch size (i.e., number of workers) to obtain the best performance.  To support that, there is definitely a need to make learning rate adjustment configurable.
 At the same time, whatever solution we come up with needs to be backend-agnostic.  One of the selling points of PL is the ability to swap out different distributed backends.  If we couple the LR scaling to the backend (e.g., require the user to put lr * hvd.size() in the LightningModule), we lose a lot of the benefit.
 With that in mind, here's what I'm currently thinking could be a good solution:
 
 Provide a good reasonable default for users who are not experts in distributed training (linear learning rate scaling) for Horovod and DDP.
 Provide an optional method in the LightningModule that allows the user to adjust the learning rate as a function of the number of workers, independent of the specific backend being used, which will override the default in (1):
 
 <denchmark-code>class MyModule(LightningModule):
 
     ...
 
     def adjust_learning_rate(self, base_lr, world_size):
         return base_lr * sqrt(world_size)
 </denchmark-code>
 
 <denchmark-link:https://github.com/ruotianluo>@ruotianluo</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  what do you think?
 		",12.0,ruotianluo,2020-07-23T17:57:40Z,"
 		cc: <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
 
 		",13.0,ruotianluo,2020-08-04T21:00:56Z,"
 		shall be resolved in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2626>#2626</denchmark-link>
 
 		",,,,,,,,,test_horovod_multi_optimizer_with_scheduling_stepping,tmpdir,183,212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,116,test_horovod_transfer_batch_to_gpu,tmpdir,115,143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2600,p-wein,2020-07-13T13:26:48Z,2020-09-15T09:07:28Z,Trainer flag overfit_batches does not overwrite train dataloaders shuffle flag,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Setting the trainer flag overfit_batches (e.g. =10) does not overwrite the shuffle flag set in the training dataloader, even though the warning reads:
 UserWarning: You requested to overfit but enabled training dataloader shuffling. We are turning it off for you.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Create lightning module with method train_dataloader with flag shuffle=True:
 
 <denchmark-code>   def train_dataloader(self) -> loading.DataLoader:
         dataset = ProstateX(train=True)
         batch_transforms, gpu_transforms, sample_transforms = self.get_transformations()
         dataloader = loading.DataLoader(dataset,
                                         batch_size=self.hparams.tr_batch_size,
                                         batch_transforms=batch_transforms,
                                         shuffle=True,
                                         sample_transforms= sample_transforms,
                                         gpu_transforms=gpu_transforms,
                                         pseudo_batch_dim=True,
                                         num_workers=self.hparams.num_workers)
         return dataloader 
 </denchmark-code>
 
 ( I use a rising dataloader, bug should also occur with pytorch dataloaders though)
 
 Create main.py with:
 
 <denchmark-code>mymodel = model.Model3D(cfg)
 trainer = pl.Trainer(gpus=1, precision=16, overfit_batches=10)
 trainer.fit(mymodel)`
 </denchmark-code>
 
 
 Run main.py
 Find out that your model does not converge.
 set shuffle=False when creating Dataloader in train_dataloader
 See that your model converges after some epochs.
 
 (Or log the samples loaded by the dataloader and check if they are the same each epoch.)
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Either model also converges with shuffle=True, since warning says that it got overwritten (assuming model converges with shuffle=False) or at least warning should read that user has to change shuffle to False.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - GeForce GTX 1080 Ti
 - available:         True
 - version:           10.1
 Packages:
 - numpy:             1.19.0
 - pyTorch_debug:     False
 - pyTorch_version:   1.7.0.dev20200705+cu101
 - pytorch-lightning: 0.8.5
 - tensorboard:       2.2.2
 - tqdm:              4.47.0
 System:
 - OS:                Linux
 - architecture:
 - 64bit
 -
 - processor:         x86_64
 - python:            3.7.7
 - version:           #109-Ubuntu SMP Fri Jun 19 11:33:10 UTC 2020
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",b5dc6998ae80b026bb6adc4040980a153390307a,Phil,2020-09-15 05:07:27-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"54,55",,1.0,p-wein,2020-07-13T13:27:53Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,p-wein,2020-09-12T11:39:18Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",3.0,p-wein,2020-09-28T13:45:33Z,"
 		I am seeing the same issue when using . <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/859ec92da51ff56b49e3e1f1beefa75767edc175/pytorch_lightning/trainer/trainer.py#L132> From a comment in the code, </denchmark-link>
 I believe that option is to be removed in 1.0.0, but is it worth it to fix it anyways? The same code will do fix the issue just checking  instead.
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"170,171,172,173,174,175",,reset_train_dataloader,"self,LightningModule",162,219,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"59,61,62,92,93","63,90",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_overfit_batch_limits,tmpdir,41,142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2622,matt-peters,2020-07-16T18:35:39Z,2020-08-07T13:31:32Z,set_epoch isn't called for TPU training,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 This line <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L350>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_loop.py#L350</denchmark-link>
  doesn't call  when training on TPUs unless using  as  is False.
 	",2cc60c625ed6593aea01d237fa047ad1863dc79c,Iz Beltagy,2020-08-07 09:31:30-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,372,372,1.0,matt-peters,2020-07-16T18:36:36Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,train,self,331,424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2635,ibeltagy,2020-07-17T19:38:29Z,2020-07-28T20:29:47Z,Loss value in the progress bar is wrong when `accumulate_grad_batches &gt; 1`,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The loss value reported in the progress bar is the_correct_loss_value / accumulate_grad_batches, so this value is wrong when accumulate_grad_batches > 1.
 This is happening because <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.5/pytorch_lightning/trainer/training_loop.py#L799>here</denchmark-link>
  the loss is divided by , then <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/0.8.5/pytorch_lightning/trainer/training_loop.py#L663>here</denchmark-link>
  the running loss is the  of these losses.
 To fix this, either remove the first line (no division by accumulate_grad_batches) or replace mean with sum in the second line.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 
 Train any model with accumulate_grad_batches=1 and note the loss value reported in the progress bar
 Train the same model with accumulate_grad_batches=2 and half the batch size,  now the loss value in the progress bar will be half the value from step 1.
 
 <denchmark-h:h3>Expected Behaviour</denchmark-h>
 
 The loss in steps (1) and (2) should be the same
 <denchmark-h:h3>Environment</denchmark-h>
 
 pytorch-lightning==v0.8.5
 	",c047676fae8cdbfe77189c218cfde73d863acc91,Iz Beltagy,2020-07-28 16:29:46-04:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,775,775,1.0,ibeltagy,2020-07-19T20:02:40Z,"
 		duplicate of <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2569>#2569</denchmark-link>
 ?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_training_batch,"self,batch,batch_idx",677,801,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2636,ehsanmok,2020-07-17T20:49:57Z,2020-10-05T11:36:13Z,nan metric breaking ModelCheckpoint,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Comparing any numbers to float('nan') is False in Python so as a result if a non-loss metric score is nan initially in training, then callback cannot checkpoint any scores after.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Ignore a nan metric score. This is orthogonal to when grad or weights become nan.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.5.0
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.6
 CUDA/cuDNN version: 10.0
 GPU models and configuration: Tesla V100
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Previous issue wasn't addressed completely <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/1008>#1008</denchmark-link>
 
 	",6ac0958166c66ed599c96737b587232b7a33d89e,Jirka Borovec,2020-10-05 07:36:12-04:00,MODIFY,0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,0.0,"520,521,522,523",,1.0,ehsanmok,2020-07-17T20:50:49Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,ehsanmok,2020-09-15T17:43:27Z,"
 		<denchmark-link:https://github.com/ehsanmok>@ehsanmok</denchmark-link>
  mind send a PR? 
 		",3.0,ehsanmok,2020-09-26T10:02:44Z,"
 		Assign this to me please.
 		",4.0,ehsanmok,2020-09-27T07:44:38Z,"
 		So I can reproduce this issue <denchmark-link:https://colab.research.google.com/drive/1dbvG_Gth8KvBsNHr5hQ4npIjmf_GAbNS?usp=sharing#scrollTo=ssQ_w97DvdzV>like this</denchmark-link>
 .
 I am not exactly clear on what the expected behavior is though. In <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  's PR for <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1097/files>nan detection and intervention</denchmark-link>
 , training is stopped when loss or weights contain  or infinite values.
 What do we want to do:
 
 
 if a metric that was passed as monitor param to ModelCheckpointgoes nan/inf:
 
 raise an error (like when loss/weights do the same)
 raise a warning but continue training and do not save any more checkpoints
 raise a warning but continue training and do not save checkpoints when monitor is nan/inf. What if it returns to non nan/inf values (as mentioned in #1008)?
 
 
 
 separate from this issue perhaps if any metric goes nan/inf (regardless of whether ModelCheckpoint) is used:
 
 raise a warning / error ? (if the former than this should be addressed in a different issue perhaps)
 
 
 
 when do we want to detect nan/infmetrics (whether monitor or any)?
 
 ASAP which would be in the first validation step when it happens (perhaps even in Trainer.run_sanity_check)
 on_validation_end (after all batches are processed) and model checkpoint is being saved
 
 
 
 		",MODIFY,2.0,tests\checkpointing\test_model_checkpoint.py,tests\checkpointing\test_model_checkpoint.py,1.0,"463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484",,test_checkpointing_with_nan_as_first,"tmpdir,mode",463,484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"469,470,471",,test_checkpointing_with_nan_as_first.validation_epoch_end,"self,outputs",469,471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2637,JiangYize,2020-07-18T09:57:26Z,2020-08-11T23:28:39Z,to() got an unexpected keyword argument 'non_blocking' for DGLGraph,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 I use dgl library to make a gnn and batch the DGLGraph.
 No problem during training, but in test, I got a TypeError: to() got an unexpected keyword argument 'non_blocking'
 <class 'dgl.graph.DGLGraph'> .to() function has no keyword argument 'non_blocking'
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 OS: Linux
 CUDA: 10.1
 Python Version: 3.7
 PyTorch Version: 1.5.1
 DGL Version: 0.4.3post2
 PyTorch-Lightning Version: 0.8.5
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 <denchmark-code>   File ""../src/main.py"", line 131, in <module>
     run(params)
   File ""../src/main.py"", line 92, in run
     trainer.test(model)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1279, in test
     results = self.__test_given_model(model, test_dataloaders)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1346, in __test_given_model
     results = self.fit(model)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1003, in fit
     results = self.single_gpu_train(model)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 186, in single_gpu_train
     results = self.run_pretrain_routine(model)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1166, in run_pretrain_routine
     results = self.run_evaluation(test_mode=True)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 391, in run_evaluation
     eval_results = self._evaluate(self.model, dataloaders, max_batches, test_mode)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 293, in _evaluate
     output = self.evaluation_forward(model, batch, batch_idx, dataloader_idx, test_mode)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 458, in evaluation_forward
     batch = self.transfer_batch_to_gpu(batch, root_gpu)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 159, in transfer_batch_to_gpu
     return self.__transfer_batch_to_device(batch, device)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 164, in __transfer_batch_to_device
     return model.transfer_batch_to_device(batch, device)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py"", line 242, in transfer_batch_to_device
     return move_data_to_device(batch, device)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 109, in move_data_to_device
     return apply_to_collection(batch, dtype=(TransferableDataType, Batch), function=batch_to)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 40, in apply_to_collection
     for k, v in data.items()})
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 40, in <dictcomp>
     for k, v in data.items()})
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 35, in apply_to_collection
     return function(data, *args, **kwargs)
   File ""/home/jiangyize/miniconda3/envs/galixir/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 107, in batch_to
     return data.to(device, non_blocking=True)
 TypeError: to() got an unexpected keyword argument 'non_blocking'
 </denchmark-code>
 
 	",69d241c82e10cf40e5787fb39bb808687d693b57,Adrian Wälchli,2020-08-11 19:28:37-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"126,127",,1.0,JiangYize,2020-07-18T09:58:17Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,JiangYize,2020-07-22T14:16:42Z,"
 		Having the same problem; it's because  (<denchmark-link:https://docs.dgl.ai/en/0.4.x/generated/dgl.DGLGraph.to.html#dgl.DGLGraph.to>docs</denchmark-link>
 , <denchmark-link:https://docs.dgl.ai/en/0.4.x/_modules/dgl/graph.html#DGLGraph.to>source</denchmark-link>
 ) doesn't take the non_blocking argument. Example:
 dgl.DGLGraph().to('cuda', non_blocking=True)
 Here's my temporary solution:
 class LightningDGLGraph(DGLGraph):
     def to(self, ctx, *args, **kwargs):
         return super().to(torch.device(ctx))
 
 g = LightningDGLGraph()
 g.to('cuda', non_blocking=True)
 Works, but probably not ideal.
 		",3.0,JiangYize,2020-07-22T20:23:12Z,"
 		
 Having the same problem; it's because DGLGraph.to (docs, source) doesn't take the non_blocking argument. Example:
 dgl.DGLGraph().to('cuda', non_blocking=True)
 Here's my temporary solution:
 class LightningDGLGraph(DGLGraph):
     def to(self, ctx, *args, **kwargs):
         return super().to(torch.device(ctx))
 
 g = LightningDGLGraph()
 g.to('cuda', non_blocking=True)
 Works, but probably not ideal.
 
 Hi, I wonder how this will work if using dgl.batch? the class type they return to you is a DGLGraph.
 Ok, they also have this quick fix here: <denchmark-link:https://github.com/dmlc/dgl/pull/1600>dmlc/dgl#1600</denchmark-link>
 .
 so uninstall the stable version and install the latest version from main solves my problem:
 <denchmark-code>pip install --pre dgl           # For CPU Build
 pip install --pre dgl-cu90      # For CUDA 9.0 Build
 pip install --pre dgl-cu92      # For CUDA 9.2 Build
 pip install --pre dgl-cu100     # For CUDA 10.0 Build
 pip install --pre dgl-cu101     # For CUDA 10.1 Build
 </denchmark-code>
 
 		",4.0,JiangYize,2020-07-22T20:54:33Z,"
 		It seems to work for me for now.
 		",MODIFY,1.0,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,"312,313,314,315,316,317,318","312,313",transfer_batch_to_device,"self,Any,device",276,324,MODIFY,2.0,pytorch_lightning\utilities\apply_func.py,pytorch_lightning\utilities\apply_func.py,1.0,"107,108",107,MODIFY,2.0,tests\models\test_gpu.py,tests\models\test_gpu.py,1.0,"402,403",,test_non_blocking.to,"self,args,kwargs",402,403,,,,,,,,,,,,5.0,JiangYize,2020-07-26T16:16:30Z,"
 		
 Ok, they also have this quick fix here: dmlc/dgl#1600.
 so uninstall the stable version and install the latest version from main solves my problem:
 
 Just saw your edit. This seems to work if I don't specify the number of gpus; when I do, same error. E: It's the distributed backend; it never calls graph.to. You can throw a 0/0 in there and it'll never break with distributed_backend ddp.
 		",6.0,JiangYize,2020-08-01T17:12:01Z,"
 		For a clean solution in Lightning, override <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html?highlight=transfer#pytorch_lightning.core.hooks.ModelHooks.transfer_batch_to_device>this</denchmark-link>
  model hook and call .to() yourself on the graph object.
 		",7.0,JiangYize,2020-08-01T17:14:14Z,"
 		Regarding ddp, is DGLGraph supposed to work with that (I mean in plain pytorch)? I don't think it can work with scatter and gather.
 		",8.0,JiangYize,2020-08-11T00:05:50Z,"
 		
 For a clean solution in Lightning, override this model hook and call .to() yourself on the graph object.
 
 <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Is this supposed to be overridden in the model? It doesn't seem to get called for me in a distributed setting.
 		",9.0,JiangYize,2020-08-11T02:57:55Z,"
 		<denchmark-link:https://github.com/jacobdanovitch>@jacobdanovitch</denchmark-link>
  Yes, this hook only works for single gpu, because in distributed we need to scatter and gather a batch, and if it is a custom object we don't know how to do that. For this, you would have to define your own DistributedDataParallel module and configure it in the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.configure_ddp>configure_ddp </denchmark-link>
 model hook. We should probably update the docs regarding that.
 		",,,,,,,,,,,,,,,,,,,,,,,,,move_data_to_device.batch_to,data,96,108,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408",,test_non_blocking,,391,408,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"107,108",107,move_data_to_device,"Any,device",78,110,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2653,s-rog,2020-07-21T01:30:04Z,2020-09-02T13:36:45Z,Checkpoints cannot be loaded in non-pl env,"
 ## 🚀 Feature
 Add an option to save only state_dict for ModelCheckpoint callbacks
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 PL checkpoints cannot be loaded in non-pl envs
 <denchmark-h:h3>Motivation</denchmark-h>
 
 To be able to move trained models and weights into pytorch only environments
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Currently  when you do torch.load() on a pl generated checkpoint in an environment without pl, there is a pickling error. For my current use case I have to load the checkpoints in my training environment and save them again with only state_dict for the weights.
 See <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2653#issuecomment-681303007>reply below</denchmark-link>
  for more info
 	",65e6687c54937db0f9bdbdb089ac9d288457d6f8,s-rog,2020-09-02 15:36:42+02:00,MODIFY,1,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"374,377,378",373,1.0,s-rog,2020-07-21T18:04:55Z,"
 		You can use save_weights_only parameter in ModelCheckpoint to save weights only. Although it will save epoch, global_step and pl_version but that won't be a problem there, I guess. Also can you show the pickling error you are getting?
 		",2.0,s-rog,2020-07-22T00:23:59Z,"
 		I am using save_weights_only  and that causes a pickling error with module lightning not found (don't have the extact error atm)
 		",3.0,s-rog,2020-07-22T04:46:34Z,"
 		Can you check when you load that checkpoint manually in pl env, what keys does that file have?
 		",4.0,s-rog,2020-08-27T02:20:20Z,"
 		Error in non-pl env
 <denchmark-code>ModuleNotFoundError                       Traceback (most recent call last)
 <ipython-input-10-dbc5018f5317> in <module>
 ----> 1 pretrained_dict = torch.load('../input/weights/test.ckpt', map_location=torch.device('cpu'))
 
 /opt/conda/lib/python3.7/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
     591                     return torch.jit.load(f)
     592                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
 --> 593         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
     594 
     595 
 
 /opt/conda/lib/python3.7/site-packages/torch/serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
     771     unpickler = pickle_module.Unpickler(f, **pickle_load_args)
     772     unpickler.persistent_load = persistent_load
 --> 773     result = unpickler.load()
     774 
     775     deserialized_storage_keys = pickle_module.load(f, **pickle_load_args)
 
 ModuleNotFoundError: No module named 'pytorch_lightning'
 </denchmark-code>
 
 keys in pl env
 dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'hparams_name', 'hyper_parameters'])
 <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  sorry about the late reply, completely forgot about this issue
 Edit:
 found the issue, I'll look into a fix
 <denchmark-code>for k, v in pretrained_dict.items():
     print(type(k), type(v))
 
 <class 'str'> <class 'int'>
 <class 'str'> <class 'int'>
 <class 'str'> <class 'str'>
 <class 'str'> <class 'collections.OrderedDict'>
 <class 'str'> <class 'str'>
 <class 'str'> pytorch_lightning.utilities.parsing.AttributeDict
 </denchmark-code>
 
 Edit 2:
 I'll submit a PR after refactor week!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,s-rog,2020-08-31T03:06:08Z,"
 		I got around to testing and can load checkpoints now in non-pl envs. The only change needed was to cast hyper_parameters to dict in dump_checkpoint of  pytorch_lightning/trainer/training_io.py
 - checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = model.hparams
 + checkpoint[LightningModule.CHECKPOINT_HYPER_PARAMS_KEY] = dict(model.hparams)
 Thoughts?
 		",6.0,s-rog,2020-08-31T10:38:08Z,"
 		Yeah this looks good to avoid such error since AttributeDict is a PL thing.
 		",7.0,s-rog,2020-08-31T18:04:46Z,"
 		<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  , I tried on master with  and these are the dict keys I got. No hyperparams.
 dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict'])
 		",8.0,s-rog,2020-09-01T00:37:25Z,"
 		<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  Did the model have ?
 If you look at dump_checkpoint() the weights_only arg only controls:
 callbacks, optimizer_states, lr_schedulers, native_amp_scaling_state and amp_scaling_state
 hparams loggging is only controlled by if model.hparams:
 		",9.0,s-rog,2020-09-01T16:56:16Z,"
 		ok, yeah my bad :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,dump_checkpoint,"self,bool",325,383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2669,ananyahjha93,2020-07-22T09:29:05Z,2020-07-24T08:26:06Z,--gpus flag with add_argparse_args bug,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using
 <denchmark-code>parser = Trainer.add_argparse_args(parser)
 args = parser.parse_args()
 
 trainer = Trainer.from_argparse_args(args)
 </denchmark-code>
 
 if the user does not provide the --gpus flag, the code allocates some memory on the GPU and reports
 GPU available: True, used: True
 but does not use this GPU. This issue has been discovered in bolts:
 
 PyTorchLightning/pytorch-lightning-bolts#35
 PyTorchLightning/pytorch-lightning-bolts#124
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Use the following method to create the trainer object
 <denchmark-code>parser = Trainer.add_argparse_args(parser)
 args = parser.parse_args()
 
 trainer = Trainer.from_argparse_args(args)
 </denchmark-code>
 
 and don't pass the --gpus flag.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 If --gpus flag is not provided in the script call, then Lightning should report
 GPU available: True, used: False
 with a warning, and not allocate any memory on the GPU. This way, the user can set the --gpus flag if they have missed out on it.
 	",6780214b27e6ebace9cf38b6f5701224204e28ad,Ananya Harsh Jha,2020-07-24 08:26:05+00:00,MODIFY,1,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"313,314",,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"533,534","466,467",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,set_distributed_mode,"self,distributed_backend",247,314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2678,andrewredd,2020-07-23T12:44:45Z,2020-10-05T11:33:47Z,training_epoch_end seems to fail when returning nothing,"
 I'm trying to log weight histograms to tensorboard at the end of each training epoch. I have the following code:
 <denchmark-code>    def training_epoch_end(self, outputs):
         self.log_hists()
 </denchmark-code>
 
 This is in line with the documentation. ""If you don't need to display anything, Don't return anything""
 However when this function runs I get the following error:
 <denchmark-code>...
 File ""/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 499, in run_training_epoch
 │    self.run_training_epoch_end(epoch_output)
 │  File ""/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 533, in run_training_epoch_end
 │    _processed_outputs = self.process_output(epoch_output)
 │  File ""/venv/lib/python3.6/site-packages/pytorch_lightning/trainer/logging.py"", line 106, in process_output
 │    for k, v in output.items():
 │AttributeError: 'NoneType' object has no attribute 'items'
 │Exception ignored in: <object repr() failed>
 │Traceback (most recent call last):
 │  File ""/venv/lib/python3.6/site-packages/tqdm/std.py"", line 1086, in __del__
 │  File ""/venv/lib/python3.6/site-packages/tqdm/std.py"", line 1293, in close
 │  File ""/venv/lib/python3.6/site-packages/tqdm/std.py"", line 1471, in display
 │  File ""/venv/lib/python3.6/site-packages/tqdm/std.py"", line 1089, in __repr__
 │  File ""/venv/lib/python3.6/site-packages/tqdm/std.py"", line 1433, in format_dict
 │TypeError: 'NoneType' object is not iterable```
 
 The fix that I've found is to return a dictionary according to the expected typed return, as follows:
 
 </denchmark-code>
 
 <denchmark-code>def training_epoch_end(self, outputs):
     self.log_hists()
     return {'dummy': torch.Tensor()}
 </denchmark-code>
 
 <denchmark-code>
 This seems like a bandaid rather than a true fix since I don't need to return anything.
 
 1) Is there a better way to log the histograms to tensorboard?
 2) Should I be doing something different in this function?
 </denchmark-code>
 
 	",b014223f72ee457285fa3eb336d1d4039cedb651,William Falcon,2020-10-05 07:33:46-04:00,MODIFY,6,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"311,312,313",,1.0,andrewredd,2020-07-23T12:45:34Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,andrewredd,2020-07-24T16:59:03Z,"
 		training_epoch_end by default doesn't log histogram.
 If you want to plot histogram on tensorboard, try self.logger.experiment.add_histogram().
 It's <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard>the docs</denchmark-link>
 .
 PL supports every method of  class with .
 		",3.0,andrewredd,2020-07-24T18:09:50Z,"
 		Thanks Jeff!
 
 The log_hists function that I have in the example is logging the entire the
 model weights, the biases and the gradients and it does use the referenced
 method.  Is the fact that if I don't return something from the function a
 bug or user error?
 <denchmark-link:#>…</denchmark-link>
 
 
 On Fri, Jul 24, 2020 at 12:59 PM Jeff Yang ***@***.***> wrote:
  training_epoch_end by default doesn't log histogram.
  If you want to plot histogram on tensorboard, try
  self.logger.experiment.add_histogram().
 
  It's the docs
  <https://pytorch-lightning.readthedocs.io/en/latest/experiment_logging.html#tensorboard>
  .
  PL supports every method of SummaryWriter class with
  self.logger.experiment.summary_writer_methods.
 
  —
  You are receiving this because you authored the thread.
  Reply to this email directly, view it on GitHub
  <#2678 (comment)>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/ALQO4BRY7WOVCZWYLZDUSPLR5G4WPANCNFSM4PFV6LKA>
  .
 
 
 
 		",4.0,andrewredd,2020-07-27T09:23:04Z,"
 		<denchmark-link:https://github.com/andrewredd>@andrewredd</denchmark-link>
  sorry for the delayed reply, currently we need to return empty dict in  to make it work. But, Lightning will be supporting  in the future.
 		",MODIFY,3.0,tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py,tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py,1.0,"196,197,198,199",,test_train_step_no_return.training_step,"self,batch,batch_idx",196,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,andrewredd,2020-08-26T19:10:32Z,"
 		Running into the same issue
 		",6.0,andrewredd,2020-09-22T16:11:59Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  are we going to support return None?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,training_step,"self,split_batch,batch_idx,opt_idx,hiddens",294,334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217",,test_train_step_no_return,tmpdir,191,217,1.0,"201,202",,test_train_step_no_return.training_epoch_end,"self,outputs",201,202,1.0,"827,828,829",,process_train_step_outputs,"self,all_train_step_outputs,early_stopping_accumulator,checkpoint_accumulator",816,846,1.0,"666,667,668",,run_training_batch,"self,batch,batch_idx,dataloader_idx",608,735,1.0,"744,745,746,747",,training_step_and_backward,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",737,754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"339,340,341,342",,_process_training_step_output,"self,training_step_output,split_batch",336,378,1.0,44,,__init__,"self,trainer",39,46,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2680,shtoshni92,2020-07-23T19:39:33Z,2020-08-08T10:02:44Z,Checkpoint saving order,"
 The last model save action, <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/1369012bc71f257dcf7423ec65146d055ddc1cc7/pytorch_lightning/callbacks/model_checkpoint.py#L295>here</denchmark-link>
 , should be after saving the top k model because the best model and best score could have changed. Swapping the order allows resuming the training from the last checkpoint, with the last checkpoint having the latest information about the best model path/score.
 	",f798cffd02a0b6cbdc3033c981501c1a0c4677bd,Adrian Wälchli,2020-08-08 06:02:43-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"109,110",,1.0,shtoshni92,2020-07-23T19:40:28Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,shtoshni92,2020-08-08T05:11:41Z,"
 		<denchmark-link:https://github.com/shtoshni92>@shtoshni92</denchmark-link>
  Thanks for the bug report and sorry for the long wait, it was actually an easy fix (see linked PR)
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"343,344,345","305,306,307,308",on_validation_end,"self,trainer,pl_module",284,345,MODIFY,2.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"439,440,448","439,440,448",MODIFY,1.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,"99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130",,test_model_checkpoint_save_last_checkpoint_contents,tmpdir,99,130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,restore_training_state,"self,checkpoint",421,484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"357,358","357,358",dump_checkpoint,"self,bool",333,400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2688,thschaaf,2020-07-24T19:28:26Z,2020-07-31T11:53:09Z,Training on GPU failed with Torchtext when using include_lengths=True in torchtext.data.Field,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The issues raises in pytorch_lightning/utilities/apply_func.py which assumes that the attributes of a Batch from trochtext are Tensors, however if torchtext.data.Field is configured to include a length Tensor (include_lengths=True) the field is a tuple.
 A bugfix is prepared and a PR can be submitted soon.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Use Torchtext Field with include_lengths=True on a GPU machine and fit model.
 Training works on CPU but fails on GPU with: TypeError: cannot unpack non-iterable NoneType object
 
 <denchmark-h:h3>Full Error Message</denchmark-h>
 
 <denchmark-code>Traceback (most recent call last):
  File ""debug_torchtext.py"", line 105, in <module>
   trainer.fit(model)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1003, in fit
   results = self.single_gpu_train(model)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 186, in single_gpu_train
   results = self.run_pretrain_routine(model)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1213, in run_pretrain_routine
   self.train()
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 370, in train
   self.run_training_epoch()
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 452, in run_training_epoch
   batch_output = self.run_training_batch(batch, batch_idx)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 632, in run_training_batch
   self.hiddens
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 776, in optimizer_closure
   hiddens)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 944, in training_forward
   batch = self.transfer_batch_to_gpu(batch, gpu_id)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 159, in transfer_batch_to_gpu
   return self.__transfer_batch_to_device(batch, device)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/trainer/distrib_parts.py"", line 164, in __transfer_batch_to_device
   return model.transfer_batch_to_device(batch, device)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/core/hooks.py"", line 242, in transfer_batch_to_device
   return move_data_to_device(batch, device)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 128, in move_data_to_device
   return apply_to_collection(batch, dtype=(TransferableDataType, Batch), function=batch_to)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 35, in apply_to_collection
   return function(data, *args, **kwargs)
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/pytorch_lightning/utilities/apply_func.py"", line 103, in batch_to
   device_field = getattr(data, field).to(device, non_blocking=True)
 AttributeError: 'tuple' object has no attribute 'to'
 Exception ignored in: <function tqdm.__del__ at 0x7fcb5e0b2680>
 Traceback (most recent call last):
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py"", line 1086, in __del__
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py"", line 1293, in close
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py"", line 1471, in display
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py"", line 1089, in __repr__
  File ""/home1/thschaaf/miniconda3/envs/p37/lib/python3.7/site-packages/tqdm/std.py"", line 1433, in format_dict
 TypeError: cannot unpack non-iterable NoneType object
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import torch
 from torch import nn, Tensor
 import pytorch_lightning as pl
 from pytorch_lightning import Trainer, seed_everything
 from torchtext import data
 seed_everything(1234)
 def get_debug_data_loader():
     text_field = data.Field(sequential=True, pad_first=False,
                             init_token=""<s>"", eos_token=""</s>"", include_lengths=True)
     example1 = data.example.Example.fromdict({""text"": ""a b c a c""}, {""text"": (""text"", text_field)})
     example2 = data.example.Example.fromdict({""text"": ""b c a a""}, {""text"": (""text"", text_field)})
     example3 = data.example.Example.fromdict({""text"": ""c b a""}, {""text"": (""text"", text_field)})
     dataset = data.Dataset([example1, example2, example3], {""text"": text_field})
     text_field.build_vocab(dataset)
     iterator = data.Iterator(dataset, batch_size=3,
                              sort_key=None, device=None, batch_size_fn=None,
                              train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None)
     return iterator, text_field
 class DebugModel(pl.LightningModule):
     def __init__(self):
         super(DebugModel, self).__init__()
         # setup data loader
         self.debug_data_loader, self.text_field = get_debug_data_loader()
         self.learning_rate = 0.001
         self.hid_dim = 4
         pad_idx = self.text_field.vocab.stoi['<pad>']
         self.criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)
         self.INPUT_DIM = len(self.text_field.vocab)
         self.ENC_EMB_DIM = 4  # keep it small for debugging
         self.embedding = nn.Embedding(self.INPUT_DIM, self.ENC_EMB_DIM)
         self.rnn = nn.GRU(self.ENC_EMB_DIM, self.hid_dim, 1, bidirectional=False)
         self.out = nn.Linear(self.hid_dim, self.embedding.num_embeddings)
         self.OUTPUT_DIM = len(self.text_field.vocab)
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=self.learning_rate)
     def forward(self, input_seq, length):
         embedded: Tensor = self.embedding(input_seq)
         packed_embedded: Tensor = torch.nn.utils.rnn.pack_padded_sequence(embedded, length, batch_first=False,
                                                                           enforce_sorted=False)
         packed_outputs, hidden = self.rnn(packed_embedded)  # [sent len, batch size, emb dim]
         outputs, length = torch.nn.utils.rnn.pad_packed_sequence(packed_outputs)
         # outputs -> [sent len, batch size, hid dim * n directions]
         # hidden -> [n layers * n directions, batch size, hid dim]
         output = outputs.squeeze(0)
         prediction = self.out(output)
         return prediction
     @staticmethod
     def _parse_batch(batch):
         source = batch.text[0]
         source_length = batch.text[1]
         return source, source_length
     def training_step(self, batch, batch_nb):
         x = self._parse_batch(batch)
         target, target_length = x
         output = self.forward(target, target_length)
         loss = self.criterion(output[:-1].view(-1, output.shape[2]), target[1:].view(-1))
         prefix = 'train'
         tensorboard_logs = {f'{prefix}_loss': loss.item()}
         result = {'loss': loss, 'log': tensorboard_logs}
         return result
     def train_dataloader(self):
         return self.debug_data_loader
 model = DebugModel()
 cuda_device_cnt = torch.cuda.device_count()
 if cuda_device_cnt > 0:
     use_num_cuda_devices = 1
 else:
     use_num_cuda_devices = None
 trainer = Trainer(fast_dev_run=False, max_steps=None,
                   gradient_clip_val=10,
                   weights_summary='full', gpus=use_num_cuda_devices,
                   show_progress_bar=True)
 trainer.fit(model)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Should not raise an error :-)
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code> CUDA:
     - GPU:
         - TITAN X (Pascal)
     - available:     True
     - version:      10.2
 * Packages:
     - numpy:       1.17.3
     - pyTorch_debug:   False
     - pyTorch_version:  1.5.1
     - pytorch-lightning: 0.8.5
     - tensorboard:    2.2.2
     - tqdm:       4.47.0
 * System:
     - OS:        Linux
     - architecture:
         - 64bit
         - 
     - processor:     x86_64
     - python:      3.7.4
     - version:      #1 SMP Tue Mar 17 23:49:17 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",a6719f09f0a383034f4285d65cba880208a03ae4,Thomas Schaaf,2020-07-31 07:53:08-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"50,51",,,,,,,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\utilities\apply_func.py,pytorch_lightning\utilities\apply_func.py,1.0,"104,107,108","102,103,106,107",move_data_to_device.batch_to,data,97,108,ADD,0.0,None,tests\utilities\test_apply_func_torchtext.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"96,104,107,108","102,103,106,107",move_data_to_device,"Any,device",79,110,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2691,yukw777,2020-07-24T20:00:08Z,2020-07-28T20:33:29Z,Subprocess launched in ddp have the wrong cwd when using hydra.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Details: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2639#issuecomment-663601262>#2639 (comment)</denchmark-link>
 . I've talked to <denchmark-link:https://github.com/omry>@omry</denchmark-link>
  about the issue and I will send out a fix soon.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Please see the comment I posted above.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The CWD for subprocesses should be the same as that of the parent, and relative paths should work.
 	",b7f613ba6da32941bc86e9188629b572b10db4ad,Peter Yu,2020-07-28 16:33:28-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"468,469,470,471,472,473",467,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,spawn_ddp_children,"self,model",422,485,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
272,kvhooreb,2019-10-01T08:19:47Z,2019-10-02T15:11:09Z,Trainer track_grad_norm always results in 0,"
 Describe the bug
 The Trainer has a flag track_grad_norm which allows us to log the gradient norms to Tensorboard. This flag is checked in the run_tng_epoch function after the training step and validation step. However, the training step (__run_tng_batch) calls model.optimizer_step(), which, in the default implementation, calls optimizer.zero_grad(). This results in the tracked gradient norms to be always zero.
 Moreover, there is an optional __run_evaluation call in the validatin_step. This results in a call to model.zero_grad(), which I assume will also result in zero gradient norms.
 To Reproduce
 Steps to reproduce the behavior:
 
 Train a model with the track_grad_norm flag set to True and Tensorboard logging enabled
 Go to Tensorboard
 Check the gradient_norms
 
 Expected behavior
 The gradient norms should not always be zero.
 Desktop (please complete the following information):
 
 OS: Windows
 Version 0.4.9
 
 	",41236c7bbbe2a22714c19b625bdf557854d747e8,kvhooreb,2019-10-02 11:11:08-04:00,MODIFY,2,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"995,1026","995,1026,1027,1028,1029",1.0,kvhooreb,2019-10-01T08:29:55Z,"
 		I temporarily worked around this by overwriting the  function of my LightningModule to not set the gradients to zero, and moved that to the  hook. However, it's clear that the validation runs also cause the gradients to be zero. See the image attached, where there is one validation batch every 5 training batches. The gradient clearly periodically drops to zero.
 <denchmark-link:https://user-images.githubusercontent.com/7951058/65946306-29132c00-e436-11e9-8c91-2522c4df7212.png></denchmark-link>
 
 		",2.0,kvhooreb,2019-10-01T10:14:16Z,"
 		good catch!
 
 evaluation does not calculate gradients, so the norms should be zero. the real solution is not to track grad norm during evaluation.
  these lines, should be moved right before optimizer_step (this line).
 
 Would love a PR to fix if you have some time!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_training_epoch,self,971,1045,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1178,1179,1180,1182,1190,1229,1230,1231,1232,1233,1234,1235,1260","1182,1190,1253",__run_training_batch,"self,batch,batch_nb",1177,1260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2724,jpblackburn,2020-07-27T14:32:27Z,2020-09-14T08:05:52Z,Issues with Confusion Matrix normalization and DDP computation,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I started using the ConfusionMatrix metric to compute normalized confusion matrices within a mult-GPU DDP environment.  However, I found the following issues:
 
 The normalization divisor is computed correctly in a row-wise manner; however, the division is applied column-wise.
 The normalization does not protect against divide by zero if there is no data in a particular row.  While this is not a usual case for a well-designed validation set, it is possible when you have a large number of unbalanced classes and limit_val_batches is small (such as when debugging).
 There is no way to specify the number of classes for the confusion matrix.  This is critical when performing DDP reduction as it is possible that the automatic computation of the number of classes could produce different answers for each process.  I encountered this possibility when using a large number of unbalanced classes such that one of the DDP processes did not see any true data or declarations of the last class, causing its number of classes to be one less than for the other processes.  This bug sometimes causes the entire training process to hang such that I needed to manually kill each DDP process.
 When computing a normalized confusion matrix with DDP reduction, the sum reduction needs to happen prior to the normalization.
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 As a means to reproduce these issues, I have attached two python scripts.  I had to change the extension to .txt such that they would upload the Github.  The script confusion_matrix.py exposes the first two issues with normalization within a single process.  The script confusion_matrix_ddp.py exposes the final two issues by computing the metric within a model trained using DDP over two GPUs.  For both scripts, they create a 4 class problem with 20 samples unevenly divided among the classes.  The true confusion matrix is computed within the script and printed to standard out, along with the confusion matrix computed by Pytorch Lightning.
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4982230/confusion_matrix.py.txt>confusion_matrix.py.txt</denchmark-link>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/files/4982231/confusion_matrix_ddp.py.txt>confusion_matrix_ddp.py.txt</denchmark-link>
 
 Steps to reproduce the behavior:
 
 Download both scripts
 Rename them to remove the .txt extension
 ./confusion_matrix.py on a machine with at least 1 GPU.  Compare the true and test confusion matrices.
 ./confusion_matrix_ddp.py on a machine with at least 2 GPUs.  This computes the unnormalized confusion matrix.  Compare the true and test confusion matrices.
 
 WARNING: This may hang the process and require you to manually kill each process.
 
 
 ./confusion_matrix_ddp.py --normalize on a machine with at least 2 GPUs.   This computes the normalized confusion matrix.  Compare the true and test confusion matrices.
 
 WARNING: This may hang the process and require you to manually kill each process.
 
 
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The computed confusion matrix will be identical to the true confusion matrix printed within the scripts provided above.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0):  1.5.1=py3.8_cuda10.2.89_cudnn7.6.5_0
 OS (e.g., Linux):  CentOS
 How you installed PyTorch (conda, pip, source):  conda
 Build command you used (if compiling from source):  NA
 Python version:  3.8.3
 CUDA/cuDNN version:  Conda cudatoolkit=10.2.89=hfd86e86_1
 GPU models and configuration:  GeForce GTX 1070 and GeForce GTX 970
 
 <denchmark-h:h3>Solution</denchmark-h>
 
 I have forked Pytorch Lightning and created fixes for all of these issues:  <denchmark-link:https://github.com/jpblackburn/pytorch-lightning/tree/bugfix/confusion_matrix>https://github.com/jpblackburn/pytorch-lightning/tree/bugfix/confusion_matrix</denchmark-link>
 .  I am willing to turn this into a pull request.
 The solution to the first two issues did not require any major changes.  The third issue required the addition of a new argument to ConfusionMatrix for the number of classes.  It is optional and the final argument, so that the API is backwards compatible.  The fourth issue was most involved as it required modifying ConfusionMatrix to derive from Metric rather than TensorMetric.  It then uses an internal class that drives from TensorMetric.  In this manner, forward delegates the computation of the unnormalized confusion matrix and the DDP reduction to the internal class, performing the normalization itself after the DDP reduction is complete.
 I incrementally fixed the issues for easier understanding:
 
 Issues 1 and 2: 8b8b635
 Issue 3: 83927a9 and 913c1e3
 Issue 4: e3e0743
 
 <denchmark-h:h4>To validate the solution:</denchmark-h>
 
 
 ./confusion_matrix.py on a machine with at least 1 GPU.  Compare the true and test confusion matrices.
 ./confusion_matrix_ddp.py --set-num-classes on a machine with at least 2 GPUs.  Compare the true and test confusion matrices.
 ./confusion_matrix_ddp.py --set-num-classes --normalize on a machine with at least 2 GPUs.   Compare the true and test confusion matrices.
 
 Note the addition of --set-num-classes to the DDP script.
 	",a552d4a2d5056705c68f2eed570a83ee3160b3bc,Cookie_thief,2020-09-14 10:05:51+02:00,MODIFY,0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,0.0,"315,316,317,318,319",315,1.0,jpblackburn,2020-07-27T14:33:21Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,jpblackburn,2020-09-12T09:26:27Z,"
 		Hi!
 Issue 3 already fixed at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3450>#3450</denchmark-link>
  and Issue 2 under process at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3465>#3465</denchmark-link>
 . I'm interested in ConfusionMatrix fix for DDP mode too, so maybe we can collaborate to apply your changes for modern version of source code?
 UPD: As I understand,  for DDP already fixed at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528>#2528</denchmark-link>
 , so there is no need to change it for 
 		",3.0,jpblackburn,2020-09-12T10:46:12Z,"
 		<denchmark-link:https://github.com/c00k1ez>@c00k1ez</denchmark-link>
  could you also take care of issue 1 in your <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3465>#3465</denchmark-link>
  PR?
 Should be a simple change from  to  in the normalization.
 		",4.0,jpblackburn,2020-09-12T10:56:14Z,"
 		Yeah, just a moment 👍
 		",MODIFY,1.0,tests\metrics\functional\test_classification.py,tests\metrics\functional\test_classification.py,1.0,"190,191,192,193,194,195,196,197,198",,test_confusion_matrix,,174,198,MODIFY,1.0,tests\metrics\test_classification.py,tests\metrics\test_classification.py,1.0,"59,60,61,62,63,64,65,66,67,68",,,,,,,,,,,,,,,,,,,,,,,,5.0,jpblackburn,2020-09-12T11:26:04Z,"
 		Fix Issue 1  at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3465>#3465</denchmark-link>
  (commit <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/fd19c78879e17198282cdc29317405dde5fdf96a>fd19c78</denchmark-link>
 ).
 		",6.0,jpblackburn,2020-09-14T09:31:50Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  As I remember, <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3465>#3465</denchmark-link>
  do not solve issue 4
 		",7.0,jpblackburn,2020-09-14T10:45:40Z,"
 		<denchmark-link:https://github.com/c00k1ez>@c00k1ez</denchmark-link>
  you are correct, it was auto closed when <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3465>#3465</denchmark-link>
  was merged. However, I do have a fix for this, hope to do it soon (write to me on slack if you want to take over)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_confusion_matrix_norm,"normalize,num_classes",59,68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2742,remisphere,2020-07-28T15:54:40Z,2020-08-02T00:17:58Z,[DataModule] `prepare_data()` and `setup()` not called,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 It seems that when using DataModule to separate training logic and data loading,
 of the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html#methods>five methods</denchmark-link>
  that should be called that are
 , , ,  and ,
 only the last three are actually used, witch is problematic since the datasets used by the data-loaders should be assigned in the .
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run this:
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import torch
 from pytorch_lightning import LightningDataModule
 from pytorch_lightning.core.lightning import LightningModule
 from pytorch_lightning.trainer import Trainer
 from torch.nn import L1Loss, Linear
 from torch.optim import SGD
 from torch.utils.data import DataLoader
 
 
 class MyDataModule(LightningDataModule):
 
     def __init__(self):
         super().__init__()
 
     def prepare_data(self):
         print('in prepare_data, '
               'this should be called before train_dataloader() but is not.')
 
     def setup(self, stage):
         print('in setup, '
               'this should be called before train_dataloader() but is not.')
         self.train_dataset = 'whatever'
 
     def train_dataloader(self):
         print('in train_dataloader')
         return DataLoader(self.train_dataset)
 
 
 class MyLightningModule(LightningModule):
 
     def __init__(self):
         super().__init__()
         self.layer = Linear(1, 1)
         self.loss_function = L1Loss()
 
     def forward(self, x):
         return self.layer(x)
 
     def configure_optimizers(self):
         return SGD(self.parameters(), lr=0.01)
 
     def training_step(self, batch, batch_idx):
         print(""you won't even get here"")
         raise NotImplementedError
 
 
 data_module = MyDataModule()
 model = MyLightningModule()
 trainer = Trainer(gpus=1)
 trainer.fit(model, data_module)
 this gives AttributeError: 'MyDataModule' object has no attribute 'train_dataset'.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 When entering train_dataloader(), prepare_data() and setup() should already have been executed, and thus the train_dataset attribute should exist.
 <denchmark-h:h3>Additional context</denchmark-h>
 
 IMHO, it comes from <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/590e7fb1fd1729b732128b3b96c919ebdf524077/pytorch_lightning/trainer/trainer.py#L1161-L1167>here</denchmark-link>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 GeForce RTX 2080 Ti
 GeForce RTX 2080 Ti
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.19.1
 pyTorch_debug:     False
 pyTorch_version:   1.5.1+cu101
 pytorch-lightning: 0.9.0rc2
 tensorboard:       2.3.0
 tqdm:              4.48.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 processor:         x86_64
 python:            3.7.6
 version:           #41-Ubuntu SMP Tue Dec 3 00:27:35 UTC 2019
 
 
 
 	",036bcea4992865e8a82a5939f0d374530e17b778,Nathan Raw,2020-08-01 20:17:57-04:00,MODIFY,0,docs\source\datamodules.rst,docs\source\datamodules.rst,0.0,"14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,93,94,95,96,97,98,99,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,135,140,141,142,143,144,145,146,147,148,149,150,151,167,168,169,170,171,172,173,174,175,176,177,178,184,185,186,187,188,189,190,191,192,193,194,195,201,219,224,225,243,248,249","15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,63,64,65,66,67,80,81,82,83,84,85,86,87,88,89,90,91,99,100,101,102,103,104,105,106,122,123,124,125,126,127,128,129,135,136,137,138,139,140,141,142,165,170,187",1.0,remisphere,2020-07-28T16:18:17Z,"
 		
 
 you're not specifying the datamodule kwarg in trainer.fit() - your last line should look like this: trainer.fit(model, datamodule=data_module)
 
 
 In this first iteration of LightningDataModule, you have to call setup and prepare_data manually for the datamodule instance. We have it set up this way so if you don't want to use Lightning, you can use your datamodule's loaders with pure Pytorch. I thought of having them called implicitly in the PR, but ended up landing on this for now. I'm not sure if users would always want these to run implicitly.
 
 
 TL;DR: you can update your code to look like this:
 # Init a datamodule
 dm = MyDataModule()
 
 # Manually call prepare_data and setup. You could put this at end of __init__ if you want
 dm.prepare_data()
 dm.setup()
 
 model = MyLightningModule()
 trainer = Trainer(gpus=1)
 trainer.fit(model, datamodule=dm)
 <denchmark-h:hr></denchmark-h>
 
 That being said, we're open to any ideas on making this more intuitive, so feel free to throw out some alternatives. 😄
 		",2.0,remisphere,2020-07-28T16:39:39Z,"
 		
 
 is not true in 0.9.0rc2: a data module as second positional argument is taken care of here.
 
 
 I don't have a global enough view to know what other users might want, so if it is a feature i'm fine with it.
 I just saw that the manual call was in the docs, my bad for not looking far enough.
 
 
 Anyway thank you for the clear answer ^^
 		",3.0,remisphere,2020-07-29T19:54:45Z,"
 		<denchmark-link:https://github.com/remisphere>@remisphere</denchmark-link>
  I totally didn't notice! You were completely right on the dm arg. things move fast haha.
 Reopening actually, as I think your intended use is more user friendly.
 		",,,,,MODIFY,1.0,pytorch_lightning\accelerator_backends\cpu_backend.py,pytorch_lightning\accelerator_backends\cpu_backend.py,1.0,29,"29,30,31",setup,"self,model",23,36,MODIFY,1.0,pytorch_lightning\accelerator_backends\ddp_spawn_backend.py,pytorch_lightning\accelerator_backends\ddp_spawn_backend.py,1.0,109,"109,110,111",MODIFY,1.0,pytorch_lightning\accelerator_backends\dp_backend.py,pytorch_lightning\accelerator_backends\dp_backend.py,1.0,36,"36,37,38",setup,"self,model",34,58,MODIFY,1.0,pytorch_lightning\accelerator_backends\gpu_backend.py,pytorch_lightning\accelerator_backends\gpu_backend.py,1.0,34,"34,35,36",setup,"self,model",31,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ddp_train,"self,process_idx,mp_queue,model,is_master,proc_offset",63,171,,,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\accelerator_backends\tpu_backend.py,pytorch_lightning\accelerator_backends\tpu_backend.py,1.0,"105,106","105,106,107",tpu_train_in_process,"self,int,LightningModule,trainer,mp_queue",99,118,,,,,,,,MODIFY,8.0,pytorch_lightning\core\datamodule.py,pytorch_lightning\core\datamodule.py,,,,,,,,1.0,"237,244",,setup,"self,None",237,247,1.0,"189,190,191,192,193,194,195",,has_prepared_data,self,189,195,MODIFY,2.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,539,"533,534,535",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ddp_train,"self,process_idx,mp_queue,model,is_master,proc_offset",497,600,1.0,"216,217",,call_setup_hook,"self,args",216,217,MODIFY,2.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,187,185,horovod_train,"self,model",185,249,1.0,"92,93",,call_setup_hook,"self,args",92,93,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,6.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,1079,1072,__attach_datamodule,"self,model,datamodule",1072,1084,1.0,,"1343,1344",__test_given_model,"self,model,test_dataloaders",1342,1362,1.0,"1079,1086,1087,1088,1089,1090,1091,1092,1100",,__attach_datamodule,"self,model,datamodule,stage",1079,1100,1.0,1298,1297,__test_using_best_weights,"self,ckpt_path,test_dataloaders",1295,1340,MODIFY,3.0,tests\base\datamodules.py,tests\base\datamodules.py,1.0,12,,MODIFY,13.0,tests\core\test_datamodules.py,tests\core\test_datamodules.py,1.0,"216,217,229",,test_full_loop,tmpdir,215,235,1.0,"37,38,39,40,41","37,38,39,40,41",test_dm_pickle_after_setup,tmpdir,37,41,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"158,165",setup,"self,args,kwargs",158,168,1.0,"198,199,200,201,202,203,204",,has_setup_fit,self,198,204,,,,,,,,,,,,,,,,,,,,,,__init__,"self,str",9,12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84",,track_data_hook_calls.wrapped_fn,"args,kwargs",62,84,1.0,"207,208,209,210,211,212,213",,has_setup_test,self,207,213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"18,19,20,21,22,23,24,25,26,27,28,29","18,19,20",setup,"self,str",18,29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"294,295,309",,test_full_loop_ddp_spawn,tmpdir,290,315,1.0,"120,121,122,123,124,125,126,127,128,129,130,131","120,121,131",test_data_hooks_called_with_stage_kwarg,tmpdir,120,131,1.0,"98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117","105,108",test_data_hooks_called_verbose,tmpdir,98,117,1.0,"265,266,280",,test_full_loop_dp,tmpdir,264,286,1.0,"13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65","37,38,39,40,41,42,43,46,47,62,65",test_can_prepare_data,tmpdir,13,65,1.0,"81,82,83,84,85,86,87,88,89,90,91,92,93,94,95","85,88,90,95",test_data_hooks_called,tmpdir,81,95,1.0,"173,175","158,161",test_train_loop_only,tmpdir,155,175,1.0,"240,241,254",,test_full_loop_single_gpu,tmpdir,239,260,1.0,"179,180,195,197","188,191",test_train_val_loop_only,tmpdir,178,197,1.0,"74,75,76,77,78",,test_base_datamodule_with_verbose_setup,tmpdir,74,78,1.0,"200,201,202,203,204,205,206,207,208,209,210,211,212",,test_test_loop_only,tmpdir,200,212,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"18,19,20","16,17,18,19,20",setup,self,16,20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"32,35,36,37,38","33,34",__call__,"cls,args,kwargs",27,43,1.0,"46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86",,track_data_hook_calls,fn,46,86,1.0,"1058,1059,1060,1061,1063,1065",1058,can_prepare_data,self,1057,1065,1.0,"1384,1385,1386,1387,1388,1389,1390,1391,1392",,call_setup_hook,"self,model",1384,1392,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2769,acturner,2020-07-30T22:03:12Z,2020-10-05T16:44:24Z,Bug in `LightningModule.load_from_checkpoint`,"
 Suppose you have a class Model(LightningModule) (a LightningModule subclass) and as parameters it takes
 params: argparse.Namespace (without a default value), along with other keyword arguments which might take default arguments. Loading from a checkpoint
 model = Model(params=params, **kwargs)
 ...  # saves a checkpoint
 model = Model.load_from_checkpoint(params=params, **kwargs)  
 # same behavior arises if params is passed as positional argument
 will throw the error TypeError: __init__() got multiple values for argument 'params'.
 This comes from the fact that in line 205 in pytorch_lightning/core/saving.py
 model = cls(*cls_args, **cls_kwargs)
 when inspected, cls_args is ({'params': params},) and cls_kwargs is, of course, {'params': params}.
 This stems from the fact in lines 189-196 (in saving.py):
             if args_name == 'kwargs':
                 # in case the class cannot take any extra argument filter only the possible
                 cls_kwargs.update(**model_args)
             elif args_name:
                 if args_name in cls_init_args_name:
                     cls_kwargs.update({args_name: model_args})
             else:
                 cls_args = (model_args,) + cls_args
 the else clause is getting called and model_args contains a copy of kwargs!
 To be honest, it's very unclear to me exactly what model_args is supposed to be here, but what is clear is that
 this block needs more checking so that we don't just stick copies of items in cls_kwargs into cls_args. Just doing
 something silly like replacing the else clause with
 from typing import Sequence
 ...
              elif isinstance(model_args, Sequence):
                 cls_args = tuple(model_args) + cls_args
 fixes the problem for me, but as I said, I'm unclear on exactly how model_args is being used, so I'm sure there is a better
 solution.
 	",cea5f1f53876399dfaa0d37accdc527af7ca39af,Jean-Baptiste SCHIRATTI,2020-10-05 12:44:23-04:00,MODIFY,4,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,,55,1.0,acturner,2020-07-30T22:04:14Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,acturner,2020-08-24T14:34:24Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  any updates on this?
 		",3.0,acturner,2020-10-02T14:16:41Z,"
 		<denchmark-link:https://github.com/acturner>@acturner</denchmark-link>
  it is fixed by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2776>#2776</denchmark-link>
  mind try...
 		",,,,,MODIFY,1.0,tests\base\model_template.py,tests\base\model_template.py,1.0,"42,43,44,45,46,47,48,49,50,51,52","42,43,44,45,46,47,48,49,50,51,52",__init__,"self,float,int,int,float,str,str,int,int,float,float",41,52,MODIFY,1.0,tests\models\test_hparams.py,tests\models\test_hparams.py,1.0,"541,542","541,542",MODIFY,1.0,tests\models\test_restore.py,tests\models\test_restore.py,1.0,"221,222,223,224,225,226,237",,test_load_model_from_checkpoint,"tmpdir,model_template",197,242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_args,tmpdir,533,542,load_from_checkpoint,"cls,str,args,str,str,device,int,None,None,bool,kwargs",52,59,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"157,160,161,162,163,164,165,169,170,171,173,174,175,177,178,179,180,182,183,185,186,187,191,193","159,164,166,167,168,169,171,175,176,177,178,179,180,181,182,186,188,189,190,192",_load_model_state,"cls,str,bool,cls_kwargs_new",157,201,1.0,244,,_convert_loaded_hparams,"dict,Callable,None",243,252,1.0,"160,161,162,163,164,165,169,170,171,173,174,175,177,178,179,180,182,183,185,186,187,191,193","159,164,166,167,168,169,171,175,176,177,178,179,180,181,182,186,188,189,190,192",_load_model_state,"cls,str,cls_args,bool,cls_kwargs",159,199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2782,bkhakshoor,2020-07-31T17:56:06Z,2020-08-02T03:25:58Z,Use of shell=True could lead to shell injection,"
 File: pytorch_lightning/trainer/training_io.py
 Line Number: 227-233
 Relevant Code:
 `            # find job id
 job_id = os.environ['SLURM_JOB_ID']
 cmd = 'scontrol requeue {}'.format(job_id)
 <denchmark-code>        # requeue job
         log.info(f'requeing job {job_id}...')
         result = call(cmd, shell=True)`
 </denchmark-code>
 
 From <denchmark-link:https://docs.python.org/2/library/subprocess.html>here</denchmark-link>
 , ""Executing shell commands that incorporate unsanitized input from an untrusted source makes a program vulnerable to shell injection, a serious security flaw which can result in arbitrary command execution. For this reason, the use of shell=True is strongly discouraged in cases where the command string is constructed from external input...shell=False disables all shell based features, but does not suffer from this vulnerability""
 Meaning anything that can set the SLURM_JOB_ID environment variable can perform code execution.
 The documentation also describes why you might need/want shell=True, ""This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user’s home directory.""
 Looking at the code above, it doesn't look like we need any of these features and we can switch to shell=False with no change in functionality while gaining the security benefits of shell=False.
 	",96eb6ebacd5b8bba2dea4741355f576e8f1c6a16,bkhakshoor,2020-08-01 23:25:57-04:00,MODIFY,1,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"229,233","229,233",1.0,bkhakshoor,2020-07-31T17:57:03Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,bkhakshoor,2020-07-31T18:10:25Z,"
 		<denchmark-link:https://github.com/bkhakshoor>@bkhakshoor</denchmark-link>
  would you mind submitting a PR for it with the fix?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,sig_handler,"self,signum,frame",221,242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2844,ananthsub,2020-08-06T01:26:00Z,2020-08-07T13:13:22Z,Tensorboard logger fails to save model OmegaConf hparams,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The Tensorboard logger fails to log module hyperparameters configured with OmegaConf. This happens when updating the logger hparams here:
 
 The trainer calls the logger's log_hyperparams here:
 Inside log_hyperparams the logger's hparams are updated here. This causes the hparams type to now be dict instead of DictConfig
 As a result, this branch in [save_hparams_to_yaml](https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/core/saving.py#L330-L333) is never triggered
 
 This is the stacktrace when logging hyperparams: <denchmark-link:https://gist.github.com/ananthsub/7acfdb0e0f551ed030f05f7674c37b46>https://gist.github.com/ananthsub/7acfdb0e0f551ed030f05f7674c37b46</denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 A hacky fix would be something like changing the hparams update to use this inside the tensorboard logger:
 <denchmark-code>if isinstance(params, Container):
    self.hparams = OmegaConf.merge(self.hparams, params)
 else:
     self.hparams.update(params)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 
 PyTorch Version (e.g., 1.0):
 OS (e.g., Linux):
 How you installed PyTorch (conda, pip, source):
 Build command you used (if compiling from source):
 Python version:
 CUDA/cuDNN version:
 GPU models and configuration:
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",b39f4798a6859d2237b48b29b39a2390164612c1,ananthsub,2020-08-07 09:13:21-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"42,43",,,,,,,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,332,330,save_hparams_to_yaml,"config_yaml,dict",323,345,MODIFY,0.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,0.0,"20,21,22,23,24,25,26,122,123,124,125",115,MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,395,393,dump_checkpoint,"self,bool",335,402,MODIFY,6.0,tests\loggers\test_tensorboard.py,tests\loggers\test_tensorboard.py,1.0,97,97,test_tensorboard_no_name,"tmpdir,name",92,97,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"134,136,137,139,140","134,136,137,139",test_tensorboard_log_hparams_and_metrics,tmpdir,127,140,1.0,"143,144,145,146,147,148,149,150,151,152,153,154,155,156,157",,test_tensorboard_log_omegaconf_hparams_and_metrics,tmpdir,143,158,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,107,107,test_tensorboard_log_metrics,"tmpdir,step_idx",101,109,1.0,"119,121,122","119,121,122",test_tensorboard_log_hyperparams,tmpdir,112,124,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"22,28,32,36","19,20,21,22,28,32,36",test_tensorboard_hparams_reload,tmpdir,19,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2859,Borda,2020-08-07T10:17:24Z,2020-08-07T13:29:10Z,Failing docker-Conda build,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 there seems to be some connection issue while creating Conda env
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/runs/957741187>https://github.com/PyTorchLightning/pytorch-lightning/runs/957741187</denchmark-link>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",ad956b5ed9add7e601dfbe57e96ea586305127d0,Jirka Borovec,2020-08-07 14:14:22+02:00,MODIFY,0,.github\workflows\docker-builds.yml,.github\workflows\docker-builds.yml,0.0,"14,63,89",,1.0,Borda,2020-08-07T13:29:10Z,"
 		<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2861>#2861</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
286,williamFalcon,2019-10-02T16:53:38Z,2019-10-09T14:23:09Z,Double check that fast_dev_run works correctly,"
 It should run a single validation and training batch and cover the full loop.
 Suggested by <denchmark-link:https://github.com/adefazio>@adefazio</denchmark-link>
 
 	",608a90a490798a743410768a832309fe40b6ab7b,William Falcon,2019-10-09 10:23:08-04:00,MODIFY,3,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"1275,1276,1277,1284,1285,1286,1287,1288,1289,1290,1291,1304,1305,1306",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__process_output,"self,output,train",1268,1341,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1123,1124,1125,1126,1127,1129,1130,1131,1136,1137,1143","1111,1112,1113,1115,1116,1121,1127",run_training_epoch,self,1090,1149,1.0,"1057,1058,1059,1079",1067,__train,self,1041,1088,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2862,sykrn,2020-08-07T12:37:41Z,2020-08-08T10:01:39Z,"Metrics error due to inplace operation, ""computation has been modified by an inplace operation"".","
 Hey, <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 , I got a new error since I upgraded the library today.
 I used the accuracy metric, but got an error.
 <denchmark-h:h3>Code sample:</denchmark-h>
 
 <denchmark-code># in lightning module
 def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         acc = accuracy(y_hat, y)      # from the functional metric classification
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 </denchmark-code>
 
 <denchmark-h:h3>Error msg:</denchmark-h>
 
 <denchmark-code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.LongTensor [32]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
 </denchmark-code>
 
 <denchmark-h:h3>Can be solved using .clone() method.</denchmark-h>
 
 However, when I clone the y before feeding to the accuracy function, no error was shown.
 <denchmark-code>acc = accuracy(y_hat, y.clone())
 </denchmark-code>
 
 But, it's inconvenience if user has to do it manually, isn't it?
 Actually, I can use the code above without clone before I upgrade to the latest. So, it might due to the latest update/rebase causing this error.
 
 The same error shown for f1_score metric.
 
 	",d9d7e91a3b68fb7bbb966c73745a932ea95a2e6b,Younghun Roh,2020-08-08 06:01:38-04:00,MODIFY,0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,0.0,"184,186","184,186",1.0,sykrn,2020-08-07T12:38:21Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,sykrn,2020-08-07T13:36:30Z,"
 		cc <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
 
 		",3.0,sykrn,2020-08-07T14:39:27Z,"
 		cc <denchmark-link:https://github.com/Diuven>@Diuven</denchmark-link>
  , I think you introduced in-place ops for speed-up, right?
 Does the speedup only come from inplace methods or can we simply replace them by out-of-place methods ?
 		",4.0,sykrn,2020-08-08T03:23:02Z,"
 		
 cc @Diuven , I think you introduced in-place ops for speed-up, right?
 Does the speedup only come from inplace methods or can we simply replace them by out-of-place methods ?
 
 Yeah, you're right. I think this is because of the  I used in . <denchmark-link:https://github.com/Diuven/pytorch-lightning/blob/7195b1dff618eb23d8341eb83fe84fb9167c93b9/pytorch_lightning/metrics/functional/classification.py#L184>This</denchmark-link>
  is the part.
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2878>This</denchmark-link>
  is a quick PR fixing this issue. If you may, please check the code still gives the issue.
 Sorry for the inconvenience!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,sykrn,2020-08-08T10:42:52Z,"
 		Great, It runs without any error, now. Thanks for the hotfix. 👍
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2868,edenlightning,2020-08-07T17:53:24Z,2020-08-17T14:29:29Z,Throw warning for changing val_loss,"
 Add warning to user that when changing val_loss to another keyword it will break checkpointing, early stopping, and other features relying on it.
 	",51de6802edd6c050ba3f2803724298eb059dc5ad,William Falcon,2020-08-17 10:29:28-04:00,MODIFY,2,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,"159,160,161,162,163,164,165,166,167,168,169",,1.0,edenlightning,2020-08-11T21:31:23Z,"
 		Hi <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 , newcomer here. Can I work on this?
 		",2.0,edenlightning,2020-08-11T21:48:13Z,"
 		<denchmark-link:https://github.com/shivin7>@shivin7</denchmark-link>
  perfect, go ahead! 
 		",,,,,,,,,MODIFY,2.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"319,320,321",,on_validation_end,"self,trainer,pl_module",314,378,MODIFY,0.0,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,0.0,"7,24,25,26",,MODIFY,1.0,tests\trainer\test_trainer_steps_result_return.py,tests\trainer\test_trainer_steps_result_return.py,1.0,"549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586",,test_result_monitor_warnings,tmpdir,549,586,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__warn_deprecated_monitor_key,self,159,169,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"301,302,303,304,305,306,307,308,309,310,311",,__warn_deprecated_monitor_key,self,301,311,,,,,,,,1.0,"177,178",,_run_early_stopping_check,"self,trainer,pl_module",171,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2891,manipopopo,2020-08-09T04:52:31Z,2020-10-04T12:32:19Z,The total number of batches shows by the progress bar of the sanity check is wrong,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The total of the sanity check progress bar is set by
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/progress.py
 
 
          Line 296
       in
       4d0406e
 
 
 
 
 
 
  self.val_progress_bar.total = convert_inf(trainer.num_sanity_val_steps * len(trainer.val_dataloaders)) 
 
 
 
 
 
 The progress bar will always show trainer.num_sanity_val_steps even if  the length of the validation DataLoader is less than trainer.num_sanity_val_steps.
 Maybe the total could be computed by
 from pytorch_lightning.trainer import data_loading
 
 num_full_val_dataloader_batches = [
     len(dataloader) if data_loading._has_len(dataloader) else float('inf')
     for dataloader in trainer.val_dataloaders
 ]
 self.val_progress_bar.total = convert_inf(
     sum(min(num_batches, trainer.num_sanity_val_steps)
             for num_batches in num_full_val_dataloader_batches))
 We use the private function data_loading._has_len to check if dataloader has __len__, maybe we could make data_loading._has_len public.
 Or we could make num_full_val_dataloader_batches (and num_full_train_dataloader_batches) a member variable of Trainer and update the value in pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 The progress bar of the sanity check in the following code (num_sanity_val_steps == 999 and len(val_data_loader) == 10) shows
 <denchmark-code>Validation sanity check:   1%|          | 9/999 [00:09<16:31,  1.00s/it]`
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import time
 
 import pytorch_lightning as pl
 from torch.utils import data
 
 
 class Dataset(data.Dataset):
 
   def __init__(self, length):
     self._elements = list(range(length))
 
   def __getitem__(self, item):
     return self._elements[item]
 
   def __len__(self):
     return len(self._elements)
 
 
 class Model(pl.LightningModule):
 
   def forward(self, *args, **kwargs):
     pass
 
   def training_step(self, *args, **kwargs):
     pass
 
   def train_dataloader(self):
     pass
 
   def configure_optimizers(self):
     pass
 
   def validation_step(self, *args, **kwargs):
     time.sleep(1)
     return pl.EvalResult()
 
 
 if __name__ == '__main__':
   model = Model()
 
   val_dataset_length = 10
   val_dataset = Dataset(val_dataset_length)
   val_data_loader = data.DataLoader(val_dataset)
 
   trainer = pl.Trainer(num_sanity_val_steps=999, limit_val_batches=999,
                        max_epochs=0)
   trainer.fit(model, val_dataloaders=val_data_loader)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The program above should be
 <denchmark-code>Validation sanity check: 100%|██████████| 10/10 [00:10<00:00,  1.00s/it]
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:
 version:
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0+cpu
 pytorch-lightning: 0.9.0rc11
 tensorboard:       1.15.0
 tqdm:              4.48.2
 
 
 System:
 
 OS:                Windows
 architecture:
 
 64bit
 WindowsPE
 
 
 processor:
 python:            3.7.3
 version:           10.0.18362
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",a628d181ee662a77b708a12c51477f912ce02f63,Rohit Gupta,2020-10-04 08:32:18-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"96,98,99,100","96,98",1.0,manipopopo,2020-08-09T07:33:03Z,"
 		<denchmark-link:https://github.com/manipopopo>@manipopopo</denchmark-link>
  I guess the first solution gets things done without changing anything else. Mind submitting a PR?
 		",2.0,manipopopo,2020-08-09T09:30:11Z,"
 		I think once <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882>#2882</denchmark-link>
  gets resolved this issue will be resolved automatically.
 		",3.0,manipopopo,2020-08-09T10:58:00Z,"
 		It seems that resolve <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882>#2882</denchmark-link>
  will make the sanity check of  run exactly  steps (or  ) if the validation  has at least 5 batches.
 Resolve this issue will make the total of the progress bar for the sanity check be the size of the validation DataLoader if it is less than num_sanity_val_steps (except num_sanity_val_steps == -1).
 		",4.0,manipopopo,2020-08-09T12:02:11Z,"
 		then I would suggest fixing <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2882>#2882</denchmark-link>
  first in which we can make self.num_sanity_val_steps to be a list of correct num_steps for each val_dataloader and then we can just do  here to assign it to progress_bar.
 		",MODIFY,1.0,pytorch_lightning\callbacks\progress.py,pytorch_lightning\callbacks\progress.py,1.0,"343,344,345","343,344",on_validation_start,"self,trainer,pl_module",341,345,MODIFY,3.0,tests\callbacks\test_progress_bar.py,tests\callbacks\test_progress_bar.py,1.0,"207,208,209",,MODIFY,2.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,,960,test_num_sanity_val_steps,"tmpdir,limit_val_batches",946,960,,,,,,,,,,,,5.0,manipopopo,2020-08-09T13:53:40Z,"
 		It seems that we have several ways to tackle the problem.
 
 
 Make self.num_sanity_val_steps a list of correct numbers of steps as @rohitgr7 suggests. ProgressBar can update self.val_progress_bar.total using self.num_sanity_val_steps. Accessing the member num_sanity_val_steps may get values different from the one passed into Trainer.__init__.
 
 
 Add a new member to save a list of correct numbers of steps or save a list of the sizes of validation DataLoaders. ProgressBar can update self.val_progress_bar.total using the new member.  Users of Trainer can still get num_sanity_val_steps passed into Trainer.__init__ by accessing the member num_sanity_val_steps. (Supposing #2882 is resolved and the member num_sanity_val_steps is independent of limit_val_batches )
 
 
 Compute the correct number of steps in the ProgressBar with the help of pytorch_lightning.trainer.data_loading._has_len
 
 
 It seems that the values of the public member Trainer.num_sanity_val_steps in stable 0.8.5 and master are different.
 
 
 In 0.8.5, Trainer.num_sanity_val_steps is num_sanity_val_steps passed in __init__.
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 491
       in
       1e68968
 
 
 
 
 
 
  self.num_sanity_val_steps = float(""inf"") if num_sanity_val_steps == -1 else num_sanity_val_steps 
 
 
 
 
 
 sets Trainer.num_sanity_val_steps to float('inf') when the one passed in __init__ is -1.
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 461
       in
       a59e140
 
 
 
 
 
 
  self.num_sanity_val_steps = min(num_sanity_val_steps, limit_val_batches) 
 
 
 
 
 
 decides Trainer.num_sanity_val_steps according to limit_val_batches  and num_sanity_val_steps passed in __init__.
 
 
 		",6.0,manipopopo,2020-08-22T04:07:27Z,"
 		Fixed by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2917>#2917</denchmark-link>
 .
 		",7.0,manipopopo,2020-08-27T21:53:34Z,"
 		It's broken again due to refactors, opening this so that we don't forget to fix this. If required, let's fix this after a week once refactors are done.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/trainer.py
 
 
          Line 1256
       in
       85cd558
 
 
 
 
 
 
  _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
          Line 246
       in
       85cd558
 
 
 
 
 
 
  self.evaluation_loop.on_evaluation_start() 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluate_loop.py
 
 
         Lines 53 to 57
       in
       85cd558
 
 
 
 
 
 
  def on_evaluation_start(self, *args, **kwargs): 
 
 
 
  if self.testing: 
 
 
 
  self.trainer.call_hook('on_test_start', *args, **kwargs) 
 
 
 
  else: 
 
 
 
  self.trainer.call_hook('on_validation_start', *args, **kwargs) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/progress.py
 
 
         Lines 341 to 344
       in
       85cd558
 
 
 
 
 
 
  def on_validation_start(self, trainer, pl_module): 
 
 
 
  super().on_validation_start(trainer, pl_module) 
 
 
 
  self.val_progress_bar = self.init_validation_tqdm() 
 
 
 
  self.val_progress_bar.total = convert_inf(self.total_val_batches) 
 
 
 
 
 
 		",8.0,manipopopo,2020-09-30T18:26:14Z,"
 		this issue is annoying. will fix this.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_num_sanity_val_steps_progress_bar.__init__,self,207,209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,984,test_num_sanity_val_steps_neg_one,"tmpdir,limit_val_batches",969,984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"211,212",,test_num_sanity_val_steps_progress_bar.on_validation_epoch_end,"self,trainer,pl_module",211,212,1.0,"202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229",,test_num_sanity_val_steps_progress_bar,"tmpdir,limit_val_batches,expected",202,229,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2916,angshine,2020-08-11T15:41:05Z,2020-08-12T10:31:18Z,ModelCheckpoint with custom filepath don't support training on multiple nodes,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training on multiple nodes using  with custom , it will raise  caused by the following line of code: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/97e6f35b34437c89d422bd440dca4a8d2c4d5a9f/pytorch_lightning/callbacks/model_checkpoint.py#L127>model_checkpoint.py#L127</denchmark-link>
 .
 Maybe a try-except block is needed?
 	",56396abe9839fa075bcc087c32f098145b0bdc9f,Brendan Fahy,2020-08-12 06:31:17-04:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"177,178,179,180,181,182",177,1.0,angshine,2020-08-11T16:50:09Z,"
 		No I think we just need to pass in exist_ok=True into makedirs :)
 		",2.0,angshine,2020-08-11T16:52:41Z,"
 		<denchmark-link:https://github.com/f4hy>@f4hy</denchmark-link>
  your PR added this line. Do you know a good way to fix it?
 		",3.0,angshine,2020-08-11T18:02:58Z,"
 		Ah sorry. I think I know what's up. I'll get a patch out this evening. Sorry!
 		",4.0,angshine,2020-08-12T05:07:03Z,"
 		<denchmark-link:https://github.com/angshine>@angshine</denchmark-link>
  I found a few issues with the model checkpoint path stuff. Not 100% sure I found the particular bug you were seeing but I think this should fix it. Can you give my branch in the above PR a test? Sorry to have introduced this bug for you.
 		",MODIFY,1.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,"440,442,444,445,446","439,441",transfer_distrib_spawn_state_on_fit_end,"self,model,mp_queue,results",418,447,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,884,883,MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"273,278,280,281,282","272,277,279,280",_atomic_save,"self,checkpoint,str",260,282,MODIFY,2.0,pytorch_lightning\utilities\cloud_io.py,pytorch_lightning\utilities\cloud_io.py,1.0,"72,73",,makedirs,pathlike,70,75,5.0,angshine,2020-09-03T06:19:39Z,"
 		Sorry for the late reply, but it seems that this bug has not been fully fixed. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/ee72271d205beb2c012e34425cb16e189cc56c7d/pytorch_lightning/utilities/cloud_io.py#L88>This line</denchmark-link>
  still raises an exception:   when training with DDP. I still need to manually add a  to ignore the exception.
 		",6.0,angshine,2020-09-04T04:17:42Z,"
 		<denchmark-link:https://github.com/angshine>@angshine</denchmark-link>
  That line has been completely replaced now on master. Can you give it another try. I hope <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3320>#3320</denchmark-link>
   has finally resolved this.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,default_root_dir,self,879,887,_del_model,"self,filepath",169,182,1.0,"31,32,33,34,35,36",,is_remote_path,pathlike,31,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,895,894,weights_save_path,self,890,898,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2936,import-antigravity,2020-08-12T18:38:40Z,2020-10-06T03:15:52Z,"Trainer ""optimizers"" attribute is None when saving checkpoint and callbacks list is not empty","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I'm training a GAN and I'm running a few custom callbacks as well. When the model attempts to save at the end of the first epoch, it crashes. Here's the very strange thing: I have the exact same code in a Jupyter notebook and the error doesn't occur.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 The bug does not occur when the callbacks list passed into the trainer is empty. None of the callbacks I'm using have anything to do with saving checkpoints, they're all for logging certain things about the model. Enabling any one of them causes the error. Running the exact same code in Jupyter results in no crashes.
 Stack trace:
 <denchmark-code>Traceback (most recent call last):███████████████████████████████████-| 98.33% [590/600 00:05<00:00 loss: -0.558, v_num: 1, d_loss: -1.120, g_loss: -0.016]
   File ""mnist-dense-gan-convergence.py"", line 55, in <module>
     main(args)
   File ""mnist-dense-gan-convergence.py"", line 45, in main
     trainer.fit(gan)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1044, in fit
     results = self.run_pretrain_routine(model)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 1213, in run_pretrain_routine
     self.train()
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 370, in train
     self.run_training_epoch()
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 502, in run_training_epoch
     self.check_checkpoint_callback(should_check_val)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 513, in check_checkpoint_callback
     [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_loop.py"", line 513, in <listcomp>
     [c.on_validation_end(self, self.get_model()) for c in checkpoint_callbacks]
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py"", line 12, in wrapped_fn
     return fn(*args, **kwargs)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 309, in on_validation_end
     self._do_check_save(filepath, current, epoch)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 346, in _do_check_save
     self._save_model(filepath)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 168, in _save_model
     self.save_function(filepath, self.save_weights_only)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py"", line 268, in save_checkpoint
     checkpoint = self.dump_checkpoint(weights_only)
   File ""/Users/robbie/.conda/envs/ganresearch/lib/python3.7/site-packages/pytorch_lightning/trainer/training_io.py"", line 350, in dump_checkpoint
     for i, optimizer in enumerate(self.optimizers):
 TypeError: 'NoneType' object is not iterable
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Here is the relevant part of my setup code:
 inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))
 
 log_dir = os.path.abspath('../logs/mnist-dense-gan-convergence')
 
 params = ParameterMatrixCallback()
 
 callbacks = [
     GANProgressBar(),
     GANTensorboardImageView(),
     params,
     inception_callback
 ]
 
 trainer_args = {
         'max_epochs': 100,
         'default_root_dir': log_dir,
         'callbacks': callbacks,
         'progress_bar_refresh_rate': 0
     }
 
     print(log_dir)
     try:
         trainer = Trainer(gpus=1, **trainer_args)
     except MisconfigurationException:
         trainer = Trainer(**trainer_args)
 
     trainer.fit(gan)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 and the same code in Jupyter:
 inception_callback = GANInceptionScorer(classifier, logits=True, sample_size=1000, input_shape=(-1, 1, 28, 28))
 
 log_dir = os.path.abspath('../logs/mnist-gan-dense')
 
 params = ParameterMatrixCallback()
 
 trainer_args = {
     'max_epochs': 200, 
     'callbacks': [GANProgressBar(), GANTensorboardImageView(n=4), params, inception_callback],
     'progress_bar_refresh_rate': 0, 
     'default_root_dir': log_dir
 }
 
 t = Trainer(**trainer_args)
 
 PyTorch Version (e.g., 1.0): 1.3.1
 OS (e.g., Linux): macOS
 How you installed PyTorch (conda, pip, source): conda
 Python version: 3.7
 Any other relevant information: pytorch-lightning 0.8.5
 
 	",cb2a3265e5eb329a48fb44df6ab8fd74df62b85a,William Falcon,2020-10-05 23:15:52-04:00,MODIFY,6,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,"313,314",,1.0,import-antigravity,2020-08-12T18:41:55Z,"
 		Additional info, here are the relevant methods in my GAN class:
 class GAN(LightningModule, ABC):
    ...
 
     @abstractmethod
     def g_optimizer(self) -> Optimizer:
         pass
 
     @abstractmethod
     def d_optimizer(self) -> Optimizer:
         pass
 
     def configure_optimizers(self):
         return self.g_optimizer(), self.d_optimizer()
 
 class MnistGanDense(GAN):
     ...
 
     def g_optimizer(self) -> Optimizer:
         return optim.RMSprop(self.G.parameters(), self.hparams['learning_rate'])
 
     def d_optimizer(self) -> Optimizer:
         return optim.RMSprop(self.D.parameters(), self.hparams['learning_rate'])
 		",2.0,import-antigravity,2020-08-14T01:51:49Z,"
 		could you try 0.9.0rc12?
 		",3.0,import-antigravity,2020-08-14T02:34:34Z,"
 		Is there a way to do that with conda?
 		",4.0,import-antigravity,2020-08-14T06:44:05Z,"
 		inside your Conda environment you could also install it with pip
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,import-antigravity,2020-08-15T11:47:42Z,"
 		Inside conda you can always install with pip:
 <denchmark-code>pip install pytorch-lightning==0.9.0rc13
 </denchmark-code>
 
 If this is still an issue, happy to reopen
 		",6.0,import-antigravity,2020-09-01T21:41:55Z,"
 		This is still a problem for me. I updated to 0.9.1rc1 and still get this error. Here is my trace.
 <denchmark-code>Traceback (most recent call last):
   File ""train_unet.py"", line 270, in <module>
     trainer.save_checkpoint(args.save_checkpoint_path)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py"", line 275, in
  save_checkpointe
     checkpoint = self.dump_checkpoint(weights_only)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_io.py"", line 360, in
  dump_checkpoint
     for i, optimizer in enumerate(self.optimizers):
 TypeError: 'NoneType' object is not iterable
 </denchmark-code>
 
 		",7.0,import-antigravity,2020-09-06T20:04:11Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  could you open this again? I'm still getting the error as well
 		",8.0,import-antigravity,2020-09-30T06:35:44Z,"
 		<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  didn't we recently make optimizers init to an empty list instead of None? I think this should solve the problem. Could you check?
 		",9.0,import-antigravity,2020-09-30T06:44:51Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  yes its an empty list now. But the code for lightning model defined above has optimizers defined, so am not sure yet what's the issue there.
 <denchmark-link:https://github.com/import-antigravity>@import-antigravity</denchmark-link>
  mind check this on master?
 		",10.0,import-antigravity,2020-10-02T19:11:37Z,"
 		<denchmark-link:https://github.com/deekshadangwal>@deekshadangwal</denchmark-link>
  mind share full sample code so we can reproduce your issue?
 		",,,,,,,,,,,,,,,,,,,,,,,,,test_multiple_optimizers_callbacks.on_train_epoch_start,"self,trainer,pl_module",313,314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"317,318,319,320",,test_multiple_optimizers_callbacks.__init__,self,317,320,1.0,"322,323,324,325,326,327,328,329,330,331",,test_multiple_optimizers_callbacks.training_step,"self,batch,batch_idx,optimizer_idx",322,331,1.0,"310,311",,test_multiple_optimizers_callbacks.on_train_batch_end,"self,trainer,pl_module,batch,batch_idx,dataloader_idx",310,311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"333,334,335,336",,test_multiple_optimizers_callbacks.configure_optimizers,self,333,336,1.0,"304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348",,test_multiple_optimizers_callbacks,tmpdir,304,348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2943,amitness,2020-08-13T08:02:34Z,2020-08-14T01:44:57Z,Issue with pl.Trainer.from_argparse_args(...),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Use parser = pl.Trainer.add_argparse_args(parser)
 Run python main.py --overfit_batches 1
 The training runs over the whole dataset instead of running on a single batch
 
 <denchmark-link:https://user-images.githubusercontent.com/8587189/90109358-6ab97680-dd6b-11ea-92d9-f1d47aea8435.png></denchmark-link>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Only one batch should have run.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 Tesla P100-PCIE-16GB
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0+cu101
 pytorch-lightning: 0.8.5
 tensorboard:       2.3.0
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",48f658fbb551e5f65a32938224dd782dd4605820,William Falcon,2020-08-13 21:44:55-04:00,MODIFY,1,pytorch_lightning\core\datamodule.py,pytorch_lightning\core\datamodule.py,1.0,320,320,1.0,amitness,2020-08-14T01:11:07Z,"
 		thanks. looking at it
 		",,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,739,739,add_argparse_args,"cls,ArgumentParser",697,777,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,add_argparse_args,"cls,ArgumentParser",310,357,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2955,SiddhantRanade,2020-08-13T17:26:43Z,2020-08-13T21:06:18Z,Using IterableDatasets without __len__ for Training,"
 Calling fit(model, trainloader, evalloader) internally calls enforce_datamodule_dataloader_override. This function
 has the if statement if (train_dataloader or val_dataloaders) and datamodule:. 
 
 
 pytorch-lightning/pytorch_lightning/trainer/configuration_validator.py
 
 
          Line 13
       in
       2c935d0
 
 
 
 
 
 
  if (train_dataloader or val_dataloaders) and datamodule: 
 
 
 
 
 
 This is similar to the PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/1560>#1560</denchmark-link>
 , the problem is that the  translates to , but there's no dataloader. so bool() uses dataloader. > 0. But... dataloader. uses IterableDataset. for IterableDatasets for which  is undefined.
 The fix is also the same, the if dl should be replaced by if dl is not None.
 I will open a PR fixing this.
 	",88bfed371e9597e813384b3d951b0e5280be71bd,SiddhantRanade,2020-08-13 23:06:17+02:00,MODIFY,1,pytorch_lightning\trainer\configuration_validator.py,pytorch_lightning\trainer\configuration_validator.py,1.0,13,13,1.0,SiddhantRanade,2020-08-13T17:27:24Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,enforce_datamodule_dataloader_override,"self,train_dataloader,val_dataloaders,datamodule",11,16,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2956,lezwon,2020-08-13T17:37:50Z,2020-08-13T22:57:24Z,'NoneType' object has no attribute 'lower'  while training on TPU,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 distributed_backend is not set to 'tpu' which breaks it here: line 
 
 
 pytorch-lightning/pytorch_lightning/trainer/distrib_data_parallel.py
 
 
          Line 412
       in
       2c935d0
 
 
 
 
 
 
  if self.distributed_backend.lower() not in ['ddp_spawn', 'ddp_cpu', 'tpu']: 
 
 
 
 
 
 distributed_backend has to be explicitly specified in Trainer params to have it working which is misleading from the docs: <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/tpu.html#distributed-backend-with-tpu>https://pytorch-lightning.readthedocs.io/en/latest/tpu.html#distributed-backend-with-tpu</denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 <denchmark-link:https://www.kaggle.com/lezwon/lightning-mnist-tpu>https://www.kaggle.com/lezwon/lightning-mnist-tpu</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Should automatically set distributed_backend and train successfully.
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Related to :
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2698#issuecomment-671665202>#2698 (comment)</denchmark-link>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2812#issuecomment-668659590>#2812 (comment)</denchmark-link>
 
 	",cfd06a083b47b6c5c619f5362441197cb4e93e9e,Lezwon Castelino,2020-08-13 18:57:23-04:00,MODIFY,1,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,1.0,,"241,242,243",,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,524,,,,,,MODIFY,12.0,tests\models\test_tpu.py,tests\models\test_tpu.py,1.0,"241,242,243",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_distributed_backend_set_when_using_tpu,"tmpdir,tpu_cores",241,243,init_tpu,self,240,245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"238,239,240,241,242,243",239,test_exception_when_no_tpu_found,tmpdir,231,244,1.0,,105,test_model_16bit_tpu_cores_1,tmpdir,98,113,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,220,,test_tpu_misconfiguration,,217,220,1.0,,149,test_model_16bit_tpu_cores_8,tmpdir,142,160,1.0,,189,test_tpu_grad_norm,tmpdir,183,197,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,82,test_model_tpu_cores_8,tmpdir,76,93,1.0,,175,test_model_tpu_early_stop,tmpdir,165,178,1.0,,63,test_model_tpu_index,"tmpdir,tpu_core",57,71,1.0,,44,test_model_tpu_cores_1,tmpdir,38,51,1.0,"199,200,201,202,203",,test_dataloaders_passed_to_fit,tmpdir,194,205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,128,test_model_16bit_tpu_index,"tmpdir,tpu_core",119,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2961,lezwon,2020-08-13T19:24:40Z,2020-10-01T10:57:27Z,AttributeError: 'NoneType' object has no attribute 'best_model_path' when `checkpoint_callback` = False,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Model does not complete training on TPU and error's out with error AttributeError: 'NoneType' object has no attribute 'best_model_path' when checkpoint_callback=False.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 trainer=Trainer(tpu_cores=8, max_epochs=5, checkpoint_callback=False, distributed_backend='tpu')
 trainer.fit(model)  
 <denchmark-code>---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-6-6de8b6915beb> in <module>
       1 trainer = Trainer(tpu_cores=8, precision=16, max_epochs=5, checkpoint_callback=False, distributed_backend='tpu')
 ----> 2 trainer.fit(model)
 
 /opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
      32             if entering is not None:
      33                 self.state = entering
 ---> 34             result = fn(self, *args, **kwargs)
      35 
      36             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted
 
 /opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    1058             self.accelerator_backend.setup()
    1059             self.accelerator_backend.train(model)
 -> 1060             self.accelerator_backend.teardown(model)
    1061 
    1062         else:
 
 /opt/conda/lib/python3.7/site-packages/pytorch_lightning/accelerators/tpu_backend.py in teardown(self, model)
      60 
      61         # transfer back the best path to the trainer
 ---> 62         self.trainer.checkpoint_callback.best_model_path = best_path
      63         # todo, pass also bets score
      64 
 
 AttributeError: 'NoneType' object has no attribute 'best_model_path'
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Should complete training.
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Seems similar to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2547>#2547</denchmark-link>
 
 	",8be002ccc7c2e8371ab426ea07c953f72747269e,Lezwon Castelino,2020-10-01 06:57:26-04:00,MODIFY,1,pytorch_lightning\accelerators\tpu_backend.py,pytorch_lightning\accelerators\tpu_backend.py,1.0,"70,71",70,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,teardown,self,61,83,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3000,rohitgr7,2020-08-16T11:50:39Z,2020-10-04T16:56:35Z,valdation_epoch_end won't log if no logging is done in validation_step,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  looks like setting both logger=False and prog_bar=False won't do anything. If this is intended, maybe we should add a warning or something.
 Also saw another issue, if I don't log anything in validation_step then logged values in validation_epoch_end won't be logged too even if we set logger=True. Updated the notebook attatched above for the same to verify.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-link:https://colab.research.google.com/drive/1Os9oSPK_rwwutcFdZTDsBaMc2IrqkSrP#scrollTo=3BJmPqHwX6WA>https://colab.research.google.com/drive/1Os9oSPK_rwwutcFdZTDsBaMc2IrqkSrP#scrollTo=3BJmPqHwX6WA</denchmark-link>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 pl: master
 env: colab
 	",3453bba898a8cab7e0a6fd73e988291740f295d0,William Falcon,2020-08-19 20:34:09-04:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"342,343,346","342,345",1.0,rohitgr7,2020-10-04T16:26:17Z,"
 		Edited the original bug with the current issue.
 		",2.0,rohitgr7,2020-10-04T16:56:15Z,"
 		<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  it's resolved in a recent PR. Forgot to close it. Will close now.
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,"79,80",,log_metrics,"self,metrics,grad_norm_dic,step",45,81,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,377,,MODIFY,2.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,"158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186",,test_ckpt_metric_names_results,tmpdir,158,186,MODIFY,4.0,tests\trainer\test_eval_loop_dict_return.py,tests\trainer\test_eval_loop_dict_return.py,1.0,257,257,test_no_val_step_end,tmpdir,222,262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",314,379,1.0,300,300,test_full_val_loop,tmpdir,265,305,1.0,139,139,test_validation_step_dict_return,tmpdir,105,144,MODIFY,2.0,tests\trainer\test_trainer_steps_scalar_return.py,tests\trainer\test_trainer_steps_scalar_return.py,1.0,154,154,test_train_step_epoch_end_scalar,tmpdir,134,174,1.0,111,111,test_full_training_loop_scalar,tmpdir,87,131,,,,,1.0,"134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155",,test_ckpt_metric_names,tmpdir,134,155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,214,214,test_val_step_step_end,tmpdir,178,219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3001,ydcjeff,2020-08-16T12:10:00Z,2020-09-27T15:09:57Z,ModelCheckpoint does not create full path,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run checkpoint_callback = ModelCheckpoint('my/path/')
 Only my folder is created.
 I think <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/callbacks/model_checkpoint.py#L126>this line</denchmark-link>
  discard the last trailing slash. So the directories are not created as intended when the paths are getting split.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Path should be fully created.
 	",580b04b490d4d6819133a5604ea0ef82e2a21727,Carlos Mocholí,2020-09-18 23:09:11+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"67,68",,1.0,ydcjeff,2020-08-16T12:46:04Z,"
 		I do not think that the trailing slash is the root problem. We also want that ""my/path"" should be interpreted as a path to a folder.
 I think a reasonable fix is to force the user to provide the extension when they want a file. This way, we can easily differentiate between folders and files.
 		",2.0,ydcjeff,2020-08-16T13:00:45Z,"
 		how about having two parameters, dirpath and filename? dirpath should be required (or maybe default to '.') and filename can be optional?
 		",3.0,ydcjeff,2020-08-16T13:09:43Z,"
 		I don't think we can make it required, because we have default_root_dir in Trainer, which should work as expected.
 		",4.0,ydcjeff,2020-08-16T13:11:35Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  I mean trailing slash isn't the problem, discarding the last trailing slash is the problem.
 		",MODIFY,2.0,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,"191,192,193,194,195","191,192,193,194,195",__warn_deprecated_monitor_key,self,186,195,MODIFY,6.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"376,380,386,387,388,389,391,392","374,378,384",MODIFY,3.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,"154,158,159,167,168,169,190",,test_model_checkpoint_save_last_checkpoint_contents,tmpdir,153,190,MODIFY,1.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,"416,417,418","416,417",test_model_checkpoint_options,"tmpdir,save_top_k,save_last,file_prefix,expected_files",394,422,5.0,ydcjeff,2020-08-16T13:11:38Z,"
 		yeah it can be optional, no problem
 		",6.0,ydcjeff,2020-08-16T13:12:57Z,"
 		but <denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>
  it should work with both 'my/path/' and 'my/path'.
 		",7.0,ydcjeff,2020-08-16T13:14:22Z,"
 		I think what jeff wants to say is that
 path, filename = os.path.split(""a/b/c"") 
 vs.
 path, filename = os.path.split(""a/b/c/"")
 in the former, filename = ""c"", and in the latter filename = """"
 		",8.0,ydcjeff,2020-08-16T13:15:17Z,"
 		yes, it's that.
 When we give filepath=mnist/ckpt/ or filepath=mnist/ckpt, os.path.realpath() outputs mnist/ckpt
 and when we do splitting, it becomes like path=mnist and filename=ckpt
 		",9.0,ydcjeff,2020-08-16T13:18:23Z,"
 		I think separate parameters dirpath and filename would be more flexible for the user. Not sure if it will break any backward compatibility with other parameters from Trainer or logger.
 		",10.0,ydcjeff,2020-09-17T20:01:00Z,"
 		Anyone can send a PR?
 		",11.0,ydcjeff,2020-09-27T15:09:57Z,"
 		let's discuss it on <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3583>#3583</denchmark-link>
 . The approach suggested there will solve this issue.
 		",,,,,,,,,,,,,,,,,on_validation_end,"self,trainer,pl_module",319,392,,,,,,,,,,,,,,,,,,,MODIFY,1.0,tests\trainer\test_trainer_steps_result_return.py,tests\trainer\test_trainer_steps_result_return.py,1.0,"612,624","612,624",test_result_monitor_warnings,tmpdir,588,625,,,,,,,,,,,,1.0,19,19,test_model_checkpoint_with_non_string_input,"tmpdir,save_top_k",18,34,,,,,,,,,,,,,,,,,,,,,,1.0,"111,112,113","111,112,113",_validate_condition_metric,"self,logs",108,123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"417,418,419","410,411,412",_do_check_save,"self,filepath,current,epoch,trainer,pl_module",394,424,1.0,"223,224,225,226,227,228,229,230,231,232,233,234,235,236,237",,_format_checkpoint_name,"cls,filename,epoch,metrics,prefix",223,237,1.0,"114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150","115,119,120,121,129,130,131,132",test_model_checkpoint_format_checkpoint_name,tmpdir,114,150,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"312,313,314,315,316","309,310,311,312,313,314",__warn_deprecated_monitor_key,self,307,316,1.0,"258,259,260,261,262","242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259",format_checkpoint_name,"self,epoch,metrics,ver",239,262,1.0,282,279,on_pretrain_routine_start,"self,trainer,pl_module",265,305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3005,szywise,2020-08-16T14:27:57Z,2020-08-18T21:51:40Z,`type_as` bug in the doc of LightningModule,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When I run <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blame/master/docs/source/lightning-module.rst#L54>this line of code</denchmark-link>
  in the doc, it complains that  shouldn't be given a .
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 The last line of the following code
 x = torch.zeros(2, device='cpu')
 new_x = torch.zeros(3, device='cuda:0')
 new_x = new_x.type_as(x.type())
 gives this error:
 <denchmark-code>TypeError: type_as(): argument 'other' (position 1) must be Tensor, not str
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Cast new_x to the same type as x.
 A potential fix:
 new_x = new_x.type_as(x)
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
                 - GeForce RTX 2080
                 - GeForce RTX 2080
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.18.5
         - pyTorch_debug:     False
         - pyTorch_version:   1.5.1
         - pytorch-lightning: 0.9.0rc5
         - tensorboard:       2.2.2
         - tqdm:              4.47.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - ELF
         - processor:         x86_64
         - python:            3.8.3
         - version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020
 </denchmark-code>
 
 	",9f6be96f845cabb114ef0df7a04498af6d5d8874,Davian Yang,2020-08-18 21:51:38+00:00,MODIFY,0,docs\source\introduction_guide.rst,docs\source\introduction_guide.rst,0.0,"4,263,265","262,264",1.0,szywise,2020-08-16T14:28:39Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,szywise,2020-08-16T15:35:02Z,"
 		In the example you linked, it creates a new tensor.
 This way
 new_x = x.new_empty(2, 3)
 the tensor will have the same device and dtype and does not need to be copied to the new device.
 We should prevent this from getting into the docs by replacing the code block by
 .. testcode:: 
 which will run the code and reveal such errors immediately.
 		",,,,,,,,,MODIFY,0.0,docs\source\lightning-module.rst,docs\source\lightning-module.rst,0.0,54,54,,,,,MODIFY,0.0,docs\source\new-project.rst,docs\source\new-project.rst,0.0,"4,361,363,411,419,424,436,448","360,362,410,418,423,435,447",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3019,awaelchli,2020-08-17T16:53:33Z,2020-08-19T00:27:49Z,Results gathering with varying tensor shapes (e.g. last batch),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Results object reduction when batch sizes are different won't work because torch.stack get's different input shapes. This can happen if your dataloader returns a smaller batch for the last iteration, for example.
 def recursive_stack(result: MutableMapping):
     for k, v in result.items():
         if isinstance(v, dict):
             recursive_stack(v)
         if isinstance(v, list) and len(v) > 0 and isinstance(v[0], torch.Tensor):
             v = torch.stack(v)
             result[k] = v
 Context
 From slack discussion by <denchmark-link:https://github.com/artgor>@artgor</denchmark-link>
 
 <denchmark-link:https://pytorch-lightning.slack.com/archives/CRBLFHY79/p1597604494424600>https://pytorch-lightning.slack.com/archives/CRBLFHY79/p1597604494424600</denchmark-link>
 
 	",9031dc3b817d46dc9b36007cce1360cfcf99939f,Adrian Wälchli,2020-08-18 20:27:48-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"149,150",,1.0,awaelchli,2020-08-17T16:53:45Z,"
 		will submit a PR within the next hours
 		",,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"425,426,427,428,429,430,431,432","425,426,427,428,430,431,432",recursive_padded_stack,MutableMapping,425,432,MODIFY,5.0,tests\core\test_results.py,tests\core\test_results.py,1.0,"179,180,181,182,183,184,185,186,187,188",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_result_gather_stack,,179,188,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"423,424,425,426,427,428,429,430,431,432,433,434,436","425,426,427,428,430,431,432",collate_tensors,List,423,436,1.0,420,420,recursive_stack,MutableMapping,415,420,,,,,,,,,,,,,,,,,,,,,,1.0,"191,192,193,194,195,196,197,198,199,200",,test_result_gather_concatenate,,191,200,1.0,"228,229,230,231,232,233,234,235,236,237,238",,test_result_gather_mixed_types,,228,238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"203,204,205,206,207,208,209,210,211,212",,test_result_gather_scalar,,203,212,1.0,"215,216,217,218,219,220,221,222,223,224,225",,test_result_gather_different_shapes,,215,225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3032,AAnoosheh,2020-08-18T07:32:29Z,2020-08-20T10:27:49Z,Epoch counting is one-off in multiple instances,"
 🐛 Bug
 Two issues occur:
 
 The final epoch does not save a checkpoint during training.
 Resuming from a checkpoint N will start the epochs at N+2.
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 
 Final checkpoint should save a .ckpt file, as usual.
 Should resume from epoch N+1.
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 		- Tesla V100-DGXS-16GB
 		- Tesla V100-DGXS-16GB
 		- Tesla V100-DGXS-16GB
 		- Tesla V100-DGXS-16GB
 	- available:         True
 	- version:           10.1
 * Packages:
 	- numpy:             1.18.1
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0
 	- pytorch-lightning: 0.9.0rc12
 	- tensorboard:       2.2.1
 	- tqdm:              4.46.1
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		-
 	- processor:         x86_64
 	- python:            3.7.7
 	- version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020
 </denchmark-code>
 
 	",10150fccb001867472e3cbde298591999e321278,Ananya Harsh Jha,2020-08-20 06:27:48-04:00,MODIFY,2,pytorch_lightning\callbacks\progress.py,pytorch_lightning\callbacks\progress.py,1.0,333,333,1.0,AAnoosheh,2020-08-18T18:04:58Z,"
 		<denchmark-link:https://github.com/AAnoosheh>@AAnoosheh</denchmark-link>
  Honestly I do not understand how the PR you linked relates to the bug your report. Did you mean to link another issue?
 
 The final epoch does not save a checkpoint during training.
 
 I don't experience this. The epoch number is 0-indexed, and by default it only saves best checkpoints. Could one of these reasons be why you may think this is a bug?
 How can I reproduce the second issue?
 		",2.0,AAnoosheh,2020-08-18T18:22:15Z,"
 		Sorry I should have clarified I use the following to save every epoch:
 pl.callbacks.ModelCheckpoint(save_top_k=-1, verbose=True)
 The second is done via Trainer(resume_from_checkpoint=some_ckpt_file)
 I assume some change was made to move epochs to 0-index, when previously they were 1-indexed, and there's a mismatch now.
 EDIT:
 I also have no idea how a PR was linked in my comment. Those numbers came out of nowhere from the auto-generated issue template.
 		",3.0,AAnoosheh,2020-08-19T14:35:57Z,"
 		<denchmark-link:https://github.com/AAnoosheh>@AAnoosheh</denchmark-link>
  so when you run  all the checkpoints are saved, however we do not save the last one as 'last.ckpt'. Also, the checkpoints are numbered from 0, so if you run for 4 epochs, the last checkpoint saved will be 'epoch=3.ckpt' and when you resume, it resumes from the expected 5th epoch.
 Updating tests and code for this
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_epoch_start,"self,trainer,pl_module",322,333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,116,116,total_val_batches,self,108,118,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3035,junwen-austin,2020-08-18T15:31:56Z,2020-09-15T12:36:15Z,Incorrect Precision/Recall/F1 score compared to sklearn,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Copy the code
 Run the code from top to bottom
 Compare print results
 See Difference between sklearn and Lightning
 
 <denchmark-h:h4>Code</denchmark-h>
 
 <denchmark-code>import torch
 import numpy as np
 import pytorch_lightning as pl
 from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
 
 print(pl.__version__)
 
 
 #### Generate binary data
 pl.seed_everything(2020)
 n = 10000  # number of samples
 y = np.random.choice([0, 1], n)
 y_pred = np.random.choice([0, 1], n, p=[0.1, 0.9])
 y_tensor = torch.tensor(y)
 y_pred_tensor = torch.tensor(y_pred)
 
 
 # Accuracy appears alright
 print('accuracy from sklearn', accuracy_score(y, y_pred))
 print('accuracy from lightning functional', pl.metrics.functional.accuracy(y_pred_tensor, y_tensor, num_classes=2))
 print('accuracy from lightning tensor', pl.metrics.Accuracy(num_classes=2)(y_pred_tensor, y_tensor))
 
 ## results
 ## accuracy from sklearn 0.4986
 ## accuracy from lightning functional tensor(0.4986)
 ## accuracy from lightning tensor tensor(0.4986)
 
 # Precision appears to be off, compared to sklearn
 print('precision from sklearn', precision_score(y, y_pred))
 print('precision from lightning functional', pl.metrics.functional.precision(y_pred_tensor, y_tensor, num_classes=2))
 print('precision from lightning tensor', pl.metrics.Precision(num_classes=2)(y_pred_tensor, y_tensor))
 
 ## precision from sklearn 0.5005544466622311
 ## precision from lightning functional tensor(0.4906)
 ## precision from lightning tensor tensor(0.4906)
 
 #Recall appears to be off, compared to sklearn
 print('recall from sklearn', recall_score(y, y_pred))
 print('recall from lightning functional', pl.metrics.functional.recall(y_pred_tensor, y_tensor, num_classes=2))
 print('recall from lightning tensor', pl.metrics.Recall(num_classes=2)(y_pred_tensor, y_tensor))
 
 ## recall from sklearn 0.8984872611464968
 ## recall from lightning functional tensor(0.4967)
 ## recall from lightning tensor tensor(0.4967)
 
 #F1 appears to be off, compared to sklearn
 print('F1 from sklearn', f1_score(y, y_pred))
 print('F1 from lightning functional', pl.metrics.functional.f1_score(y_pred_tensor, y_tensor, num_classes=2))
 print('F1 from lightning tensor', pl.metrics.F1(num_classes=2)(y_pred_tensor, y_tensor))
 
 ## F1 from sklearn 0.6429283577837915
 ## F1 from lightning functional tensor(0.4007)
 ## F1 from lightning tensor tensor(0.4007)
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Precision/Recall/F1 results are expected to be consistent with those from sklearn.
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 
 PyTorch Version : 1.5.1
 OS (e.g., Linux):  MacOS
 How you installed PyTorch (conda, pip, source):  Pip
 Build command you used (if compiling from source):
 Python version:  3.7
 CUDA/cuDNN version:   None
 GPU models and configuration:    @@None
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",28af34bc5134fddf544425fed9ffe04445b237e3,Nicki Skafte,2020-09-15 14:36:14+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"20,29,30,31,32",,1.0,junwen-austin,2020-08-18T15:41:46Z,"
 		By the way, Precision/Recall/F1 scores are also off in Pytorch-lightning 0.8.5
 		",2.0,junwen-austin,2020-08-18T16:21:39Z,"
 		i thought we tested against sklearn?
 		",3.0,junwen-austin,2020-08-18T16:26:33Z,"
 		we do test it here:
 
 
 
 pytorch-lightning/tests/metrics/functional/test_classification.py
 
 
         Lines 38 to 59
       in
       321fb8b
 
 
 
 
 
 
  @pytest.mark.parametrize(['sklearn_metric', 'torch_metric'], [ 
 
 
 
   pytest.param(sk_accuracy, accuracy, id='accuracy'), 
 
 
 
   pytest.param(partial(sk_precision, average='macro'), precision, id='precision'), 
 
 
 
   pytest.param(partial(sk_recall, average='macro'), recall, id='recall'), 
 
 
 
   pytest.param(partial(sk_f1_score, average='macro'), f1_score, id='f1_score'), 
 
 
 
   pytest.param(partial(sk_fbeta_score, average='macro', beta=2), partial(fbeta_score, beta=2), id='fbeta_score'), 
 
 
 
   pytest.param(sk_confusion_matrix, confusion_matrix, id='confusion_matrix') 
 
 
 
  ]) 
 
 
 
  def test_against_sklearn(sklearn_metric, torch_metric): 
 
 
 
  """"""Compare PL metrics to sklearn version."""""" 
 
 
 
  device = 'cuda' if torch.cuda.is_available() else 'cpu' 
 
 
 
  
 
 
 
  # iterate over different label counts in predictions and target 
 
 
 
  for n_cls_pred, n_cls_target in [(10, 10), (5, 10), (10, 5)]: 
 
 
 
  pred = torch.randint(n_cls_pred, (300,), device=device) 
 
 
 
  target = torch.randint(n_cls_target, (300,), device=device) 
 
 
 
  
 
 
 
  sk_score = sklearn_metric(target.cpu().detach().numpy(), 
 
 
 
  pred.cpu().detach().numpy()) 
 
 
 
  sk_score = torch.tensor(sk_score, dtype=torch.float, device=device) 
 
 
 
  pl_score = torch_metric(pred, target) 
 
 
 
  assert torch.allclose(sk_score, pl_score) 
 
 
 
 
 
 		",4.0,junwen-austin,2020-08-18T16:27:05Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  mind have look, pls 
 		",MODIFY,5.0,pytorch_lightning\metrics\classification.py,pytorch_lightning\metrics\classification.py,1.0,55,55,__init__,"self,None,str,Any",52,56,MODIFY,12.0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,1.0,"328,329",328,MODIFY,1.0,pytorch_lightning\metrics\functional\reduction.py,pytorch_lightning\metrics\functional\reduction.py,1.0,"27,28,29,30",,class_reduce,"Tensor,Tensor,Tensor,str",27,30,MODIFY,3.0,pytorch_lightning\metrics\functional\regression.py,pytorch_lightning\metrics\functional\regression.py,1.0,"194,195","190,192",_gaussian_kernel.gaussian,"kernel_size,sigma,device",190,195,5.0,junwen-austin,2020-08-18T16:31:49Z,"
 		Its because we calculate the macro average instead of the micro average which is the default in sklearn
 		",6.0,junwen-austin,2020-08-18T16:34:08Z,"
 		At some point we should probably support the different averaging methods that sklearn also have as one averaging method may be more meaningful in some cases (like very unbalanced datasets)
 		",7.0,junwen-austin,2020-08-18T16:59:05Z,"
 		I figured out the reason why this is a discrepancy:
 for binary classification, to recover sklearn, precision/recall/F1 should be done something like below:
 <denchmark-code>pl.metrics.functional.precision(y_pred_tensor, y_tensor, num_classes=2, reduction='none')[1])
 
 </denchmark-code>
 
 where reduction by default is elementwise_mean instead of none, the [1] returns the score for class 1
 We can close the issue for now, but it would be really good to update the document to reflect these subtle differences.
 For multi-classes, I assume there will be more nuances between Lightning and Sklearn, given different ways of doing average (macro,
 micro and so on
 		",8.0,junwen-austin,2020-08-19T15:23:23Z,"
 		<denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>
  mind update it docs so we avoid similar questions in future...
 		",9.0,junwen-austin,2020-08-19T18:49:00Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  Yes I plan to do more testing on metrics if you do not mind and then update the docs so that we have more examples. Does this sound good to you?
 		",10.0,junwen-austin,2020-08-21T17:07:50Z,"
 		
 @Borda Yes I plan to do more testing on metrics if you do not mind and then update the docs so that we have more examples. Does this sound good to you?
 
 that would be perfect!
 		",11.0,junwen-austin,2020-09-01T06:03:08Z,"
 		<denchmark-h:h1>🐛 Bug</denchmark-h>
 
 
 We can not produce sklearn's micro f1 with PL, right?
 
 
 For some scenario, like classifying 200 classes, with most of the predicted class index is right, micro f1 makes a lot more sense than macro f1
 Macro f1 for multi-classes problem suffers great fluctuation from batch size, as many classes neither appeared in prediction or label, as illustrated below the tiny batch f1 score.
 
 Steps to reproduce the behavior:
 
 Copy the code
 Run the code from top to bottom
 Compare print results
 See Difference between sklearn and Lightning
 
 from sklearn.metrics import f1_score as sklearn_f1
 from pytorch_lightning.metrics import F1
 import torch
 
 # create sample label
 y = torch.randint(high = 199,size = (210,))
 
 print(""dummy label/prediction"")
 print(y)
 
 sk_macro_f1 = sklearn_f1(y.numpy(),y.numpy(),labels=list(range(200)),average = 'macro')
 sk_macro_f1_tiny_batch = sklearn_f1(y[:10].numpy(),y[:10].numpy(),
                                     labels=list(range(200)),average = 'macro')
 sk_micro_f1 = sklearn_f1(y.numpy(),y.numpy(),labels=list(range(200)),average = 'micro')
 
 pl_f1 = F1(200,reduction = ""elementwise_mean"")
 pl_ele_f1 = pl_f1(y,y)
 
 print(f""""""sklearn macro f1:\t{sk_macro_f1}
 sklearn macro f1 (tiny batch):\t{sk_macro_f1_tiny_batch}
 skelarn micro f1:\t{sk_micro_f1}
 pl_elementwise f1:\t{pl_ele_f1}
 """""")
 will output the following, while PL produce the macro f1 0.625, the tiny batch macro f1 is much worse, but the model predicted perfectly
 <denchmark-code>dummy label/prediction
 tensor([  4,  61, 120,  64,  60,  18, 182, 123,  65, 149, 145,   2, 182, 154,
          46, 125,  39, 142, 144,  93, 164,  45,  70,  60, 102, 121,  39, 150,
          54, 109,  61, 120, 180,  52, 184, 189,   4,  89,  56,   5,  24, 100,
         194, 148, 152, 133,  75, 141,   6,  76,  93, 160, 173, 164,  13, 134,
         186, 176, 103,  30, 179, 172, 110, 164,  45, 157, 188, 187,  80,  54,
          77,   3,  80, 146,  42,  65,  84, 195, 132,  15,  35, 167, 110,  61,
          38, 197, 151, 102, 193,  78,  77, 169,  93, 129, 162, 168,  97, 190,
         129, 117,  38, 118, 145,  95, 173, 148,  70,  69, 147, 121, 138,  95,
          47,  41, 160, 131, 167, 116, 188, 171,  68, 196,  29,  22, 183,  29,
          90, 157, 179,  13,  26,  89, 148, 166, 193, 125, 100,  74, 130, 187,
          79, 166, 166, 131, 147, 191,  11, 147, 101, 139,  94,  20,  22, 187,
         149,  61,  55, 141, 176, 120, 152, 187, 146, 197, 192, 180, 180,  68,
           1, 115, 142,   5, 161,  77,  54, 115, 175,  39, 110,  68, 151,  98,
         102, 147,  37,  42, 154,  53, 105, 170, 114, 109,  53,  16,  62,  57,
          75,  79,  33,  42,  74,  92, 130, 151,  50, 112, 174, 113,  69,  34])
 sklearn macro f1:	0.65
 sklearn macro f1 (tiny batch):	0.05
 skelarn micro f1:	1.0
 pl_elementwise f1:	0.6499999761581421
 </denchmark-code>
 
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
 
 		",12.0,junwen-austin,2020-09-01T06:44:08Z,"
 		<denchmark-link:https://github.com/raynardj>@raynardj</denchmark-link>
  We are already tracking it in this issue and it will be part of our new aggregation system. However this may take a while to lay out.
 		",13.0,junwen-austin,2020-09-02T02:14:17Z,"
 		
 @raynardj We are already tracking it in this issue and it will be part of our new aggregation system. However this may take a while to lay out.
 
 I'm also in the slack by the same user name, anything I can contribute to the matter?
 		",14.0,junwen-austin,2020-09-02T07:40:58Z,"
 		<denchmark-link:https://github.com/raynardj>@raynardj</denchmark-link>
  if you want to help, please write to me on slack (username Nicki Skafte), as I already have some code ready that you could help finish :]
 		",,,,,precision_recall,"Tensor,Tensor,None,str,bool",324,329,,,,,1.0,"184,186,187,188,189","190,192",_gaussian_kernel._gaussian,"kernel_size,sigma,device",184,192,1.0,"184,186,187,188,189,194,195,196","190,192,197,198",_gaussian_kernel,"channel,kernel_size,sigma,device",183,198,MODIFY,0.0,pytorch_lightning\metrics\regression.py,pytorch_lightning\metrics\regression.py,0.0,"50,51,52,53,54,93,94,95,96,97,136,137,138,139,140,179,180,181,182,183,226,227,228,229,230,278,279,280,281,282","50,51,52,53,54,93,94,95,96,97,136,137,138,139,140,179,180,181,182,183,226,227,228,229,230,278,279,280,281,282",,,,,,,,,,,,MODIFY,5.0,tests\metrics\functional\test_classification.py,tests\metrics\functional\test_classification.py,,,,,,,,1.0,"171,172,173,174,175,178",,test_multilabel_accuracy,,166,178,1.0,"227,228",,test_precision_recall,"pred,target,expected_prec,expected_rec",226,231,MODIFY,1.0,tests\metrics\functional\test_reduction.py,tests\metrics\functional\test_reduction.py,1.0,"18,19,20,21,22,23,24,25,26,27,28,29,30",,1.0,,421,__init__,"self,float,None,str,Any",417,422,1.0,55,55,__init__,"self,None,str,Any",52,56,,,,,,,,,,,,,,,,,,,,,,1.0,242,242,accuracy,"Tensor,Tensor,None,str",238,242,1.0,491,"487,488,489,490",f1_score,"Tensor,Tensor,None,str",487,491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_class_reduce,,18,30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,412,recall,"Tensor,Tensor,None,str",408,412,1.0,,447,fbeta_score,"Tensor,Tensor,float,None,str",442,447,1.0,,377,precision,"Tensor,Tensor,None,str",373,377,,,,,,,,,,,,,,,1.0,"69,70,71,72,73,74,75,76,77,78,79,80",,test_different_reduction_against_sklearn,"class_reduction,sklearn_metric,torch_metric",69,80,1.0,"240,243",,test_fbeta_score,"pred,target,beta,exp_score",239,244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,436,432,__init__,"self,float,None,str,Any",432,437,1.0,"86,87",83,forward,"self,Tensor,Tensor",75,87,,,,,,,,1.0,"253,256",,test_f1_score,"pred,target,exp_score",252,257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,497,f1_score,"Tensor,Tensor,None,reduction",493,497,1.0,371,"367,368,369",precision,"Tensor,Tensor,None,str",367,371,1.0,441,"436,439",fbeta_score,"Tensor,Tensor,float,None,str",436,441,1.0,328,328,precision_recall,"Tensor,Tensor,None,str",324,328,1.0,242,242,accuracy,"Tensor,Tensor,None,reduction",238,242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,406,405,recall,"Tensor,Tensor,None,str",402,406,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3053,yukw777,2020-08-19T20:11:59Z,2020-08-20T11:19:12Z,load_from_checkpoint() doesn't work when a LightningModule inherits from typing.Generic,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When a LightningModule with saved hyperparameters inherits from , hyperparameters saved in the checkpoint file are not loaded automatically, causing an error. When  calls  to gather the list of arguments of the LightningModule that inherits from ,  returns  instead of the actual arguments, because  implements an empty  (the execution path ends up here: <denchmark-link:https://github.com/python/cpython/blob/3.8/Lib/inspect.py#L2324>https://github.com/python/cpython/blob/3.8/Lib/inspect.py#L2324</denchmark-link>
 ). As a result, PL filters out all the saved hyperparameters from the checkpoint, which results in an error when trying to instantiate the LightningModule. I'd assume this would happen when a LightningModule inherits from any class that implements  such as .
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Create a LightningModule that inherits from typing.Generic with some hyperparameters, fit it, then try to load it from a checkpoint.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import torch
 import torch.nn.functional as F
 import pytorch_lightning as pl
 
 from typing import Generic, TypeVar
 from torch.utils.data import DataLoader
 
 T = TypeVar(""T"")
 
 
 class GenericLitClassifier(Generic[T], pl.LightningModule):
     def __init__(self, dim):
         super().__init__()
         self.l1 = torch.nn.Linear(28 * 28, dim)
         self.save_hyperparameters()
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_nb):
         x, y = batch
         loss = F.cross_entropy(self(x), y)
         tensorboard_logs = {""train_loss"": loss}
         return {""loss"": loss, ""log"": tensorboard_logs}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
 
 class LitClassifier(GenericLitClassifier[str]):
     pass
 
 
 class Dataset:
     def __getitem__(self, idx):
         return torch.ones(1, 784), 1
 
     def __len__(self):
         return 5
 
 
 train_loader = DataLoader(Dataset(), batch_size=2)
 model = LitClassifier(10)
 trainer = pl.Trainer(max_epochs=5)
 trainer.fit(model, train_loader)
 
 for path, _ in trainer.checkpoint_callback.best_k_models.items():
     lm = LitClassifier.load_from_checkpoint(path)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Even when a LightningModule inherits from any class that implements __new__() (e.g. typing.Generic), its hyperparameters should be loaded automatically from a checkpoint.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
         - available:         False
         - version:           None
 * Packages:
         - numpy:             1.19.1
         - pyTorch_debug:     False
         - pyTorch_version:   1.5.1
         - pytorch-lightning: 0.8.5
         - tensorboard:       2.3.0
         - tqdm:              4.48.2
 * System:
         - OS:                Darwin
         - architecture:
                 - 64bit
                 - 
         - processor:         i386
         - python:            3.7.7
         - version:           Darwin Kernel Version 18.7.0: Mon Apr 27 20:09:39 PDT 2020; root:xnu-4903.278.35~1/RELEASE_X86_64
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",88886ace7232c8e25ece431969c5d8d101f3368d,Peter Yu,2020-08-20 07:19:11-04:00,MODIFY,3,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,159,159,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\base\__init__.py,tests\base\__init__.py,0.0,4,4,,,,,MODIFY,5.0,tests\base\model_template.py,tests\base\model_template.py,1.0,"42,43,44,45,46,47,48,49,50,51,52","41,42,43,44,45,46,47,48,49,50",MODIFY,6.0,tests\models\test_restore.py,tests\models\test_restore.py,1.0,"367,389,394","358,359,360,365,366,367,384,385,386,387",test_strict_model_load_less_params,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",355,395,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,float,int,int,float,str,str,int,int,float,float",41,52,_load_model_state,"cls,str,bool,cls_args,cls_kwargs",157,197,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"154,155,157,158,179","154,156,157,178",test_load_model_from_checkpoint,tmpdir,154,192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"51,52,53,54,55,56,57","51,52,53,54,55,56,57",load_from_checkpoint,"cls,str,args,str,str,device,int,None,None,bool,kwargs",50,57,1.0,"51,52,53,54,55,56,57","51,52,53,54,55,56,57",load_from_checkpoint,"cls,str,args,str,str,device,int,None,None,bool,kwargs",50,57,,,,,,,,1.0,"42,43,44,45,46,47,48,49,50","40,41,42,43,44,45,46,47,48,49,50",__init__,"self,float,int,int,float,str,str,int,int,float,float",39,50,1.0,"139,140,141,142,143,144,145,146,147,148,149,150",,__init__,"self,float,int,int,float,str,str,int,int,float,float",139,150,1.0,"274,304","278,279,280,281",test_model_saving_loading,tmpdir,265,310,1.0,"155,157,158,179","156,157,178",test_load_model_from_checkpoint,"tmpdir,model_template",155,193,1.0,"328,345,350","314,338,339,340,341",test_strict_model_load_more_params,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",314,351,1.0,202,"201,202,203,204,205,206",test_dp_resume,tmpdir,197,262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,129,,get_default_hparams,"bool,int",113,132,1.0,"88,92","86,87,88,89",__build_model,self,83,92,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3097,abrahambotros,2020-08-21T23:04:35Z,2020-09-17T08:37:50Z,IoU metric returns 0 score for classes not present in prediction or target,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The iou metric implementation always returns a score of 0 for a class that is not present in either the prediction or the target. This can lead to a deflated score even for perfectly-predicted examples.
 Case 1: one example of an affected case is multi-class semantic segmentation of an image that does not contain one of the classes. This can be outlined as follows:
 
 We have 3 possible classes in this dataset (0, 1, and 2, where 0 can optionally be the background class).
 Ground-truth target for an image consists only of classes 0 and 2.
 Model perfectly predicts the target.
 The IoU score should be 1.0 (perfect), but the actual score will be deflated (0.67) since there will be an unnecessary penalty for class 1.
 
 Case 2: another example that is a bit more implementation-dependent to explain:
 
 Target contains only 1's.
 Prediction perfectly assigns all 1's.
 The IoU score should be 1.0 (perfect), but the actual score will be deflated (0.5) since there will be an unnecessary penalty for class 0.
 This only applies when a higher-numbered class is present, and lower-numbered classes are not present.
 
 Case 3: All the above are also affected by any num_classes parameter passed to the functional iou implementation - if num_classes=N is given, then all classes with ids <N that did not appear in the target or prediction will always be assigned 0 IoU score. For example, if N=10, and only classes 0 and 1 are present and correct in target and prediction, then classes 2-9 will all have IoU score 0.0.
 Especially in aggregate for a dataset with substantial neutral ground-truth values (i.e., semantic segmentation dataset with lots of images where not all classes are present), this can significantly deflate the (m)IoU score(s). This can also undesirably interact with checkpointing that looks at IoU-based metrics.
 <denchmark-h:h3>To Reproduce / Code sample</denchmark-h>
 
 Case 1 above:
 import torch
 from pytorch_lightning.metrics.functional.classification import iou
 
 target = torch.tensor([0, 2])
 pred = torch.tensor([0, 2])
 
 iou(pred, target) # Returns tensor(0.6667)
 # Same computation, but with 'none' reduction to illustrate what score each class gets:
 iou(pred, target, reduction='none') # Returns tensor([1., 0., 1.])
 Case 2 above:
 target = torch.tensor([1])
 pred = torch.tensor([1])
 
 iou(pred, target) # Returns tensor(0.5)
 iou(pred, target, reduction='none') # Returns tensor([0., 1.])
 Case 3 above:
 target = torch.tensor([0, 1])
 pred = torch.tensor([0, 1])
 
 iou(pred, target, num_classes=10) # Returns tensor(0.2), or 2/10
 iou(pred, target, num_classes=10, reduction='none') # Returns tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The fallback IoU score to use for classes not in the target and correctly not in the prediction should be configurable. This should probably default to 1.0, which seems more expected behavior to me.
 Case 1:
 target = torch.tensor([0, 2])
 pred = torch.tensor([0, 2])
 iou(pred, target) # Should return tensor(1.)
 iou(pred, target, reduction='none') # Should return tensor([1., 1., 1.])
 Case 2:
 target = torch.tensor([1])
 pred = torch.tensor([1])
 iou(pred, target) # Should return tensor(1.)
 iou(pred, target, reduction='none') # Should return tensor([1., 1.])
 Case 3:
 target = torch.tensor([0, 1])
 pred = torch.tensor([0, 1])
 iou(pred, target, num_classes=10) # Should return tensor(1.)
 iou(pred, target, num_classes=10, reduction='none') # Should return tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 		- GeForce RTX 2070 with Max-Q Design
 	- available:         True
 	- version:           10.2
 * Packages:
 	- numpy:             1.19.1
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.5.1
 	- pytorch-lightning: 0.9.0rc18
 	- tensorboard:       2.2.0
 	- tqdm:              4.48.0
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.7.8
 	- version:           #38~1596560323~20.04~7719dbd-Ubuntu SMP Tue Aug 4 19:12:34 UTC 2
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 I have a  open at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3098>#3098</denchmark-link>
  that attempts to implement the expected behavior described above, and adds some tests for this. Any feedback welcome!
 Somewhat-related issues:
 
 #2736
 #2753
 
 	",76c4afb840b0ae5fcafee07d527c58e9245d099d,Abe Botros,2020-09-17 10:37:49+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"33,34,35,36",,1.0,abrahambotros,2020-08-21T23:05:17Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,MODIFY,3.0,pytorch_lightning\metrics\classification.py,pytorch_lightning\metrics\classification.py,1.0,"807,808,809,810,811,812","807,810,811,812",__init__,"self,None,float,None,str",807,812,MODIFY,2.0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,1.0,"965,966,968","967,968",MODIFY,4.0,tests\metrics\functional\test_classification.py,tests\metrics\functional\test_classification.py,1.0,"419,420,421,422,423,424,425,426,427,428",,test_iou_absent_score,"pred,target,ignore_index,absent_score,num_classes,expected",419,428,MODIFY,2.0,tests\metrics\test_classification.py,tests\metrics\test_classification.py,1.0,"230,231","230,231",test_iou,ignore_index,230,237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,iou,"Tensor,Tensor,None,float,None,str",962,968,,,,,1.0,"230,231","230,231",test_iou,remove_bg,230,237,,,,,,,,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,,66,test_overfit_batch_limits,tmpdir,41,142,,,,,,,,,,,,1.0,"444,445,446,447,448,449,450,451,452",,test_iou_ignore_index,"pred,target,ignore_index,num_classes,reduction,expected",444,452,,,,,,,,,,,,,,,,,,,,,,1.0,"807,808,809,810,811,812,813,816,817,818,819,820,821,822,823","807,810,811,812,813,814,815,816,817,818,819,822",__init__,"self,bool,str",807,823,1.0,"841,842,843,844,845,846,847,848",,forward,"self,Tensor,Tensor,None",837,848,,,,,,,,,,,,,,,,,,,,,,1.0,"965,966,968","967,968",iou,"Tensor,Tensor,None,bool,str",963,968,,,,,,,,1.0,"373,378,379,380,381,382,383",376,test_iou,"half_ones,reduction,ignore_index,expected",373,384,1.0,"371,373","371,376",test_iou,"half_ones,reduction,remove_bg,expected",371,377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3104,dalmia,2020-08-22T19:24:01Z,2020-10-06T17:54:38Z,TPU available: true when there are no TPUs,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I am using a DGX machine (and so, no TPUs), but on initiating Trainer, it logs TPU available: True. This ends up returning Missing XLA configuration when I run my script.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Simply running the following lines on my machine:
 >> trainer = pl.Trainer(gpus=[0])                                                                                                                 
 GPU available: True, used: True
 TPU available: True, using: 0 TPU cores
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 >> trainer = pl.Trainer(gpus=[0])                                                                                                                 
 GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
                 - Tesla V100-SXM2-32GB
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.18.2
         - pyTorch_debug:     False
         - pyTorch_version:   1.6.0
         - pytorch-lightning: 0.9.0
         - tensorboard:       2.2.0
         - tqdm:              4.45.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - 
         - processor:         x86_64
         - python:            3.6.9
         - version:           #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019
 </denchmark-code>
 
 	",69833dad5b2a0e7e68ed60a91a5a8c32ae22f707,Lezwon Castelino,2020-10-06 19:54:37+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"34,35",,1.0,dalmia,2020-08-24T23:10:11Z,"
 		sounds like some misconfiguration issue, are interested in sending a PR? 🐰
 		",2.0,dalmia,2020-08-28T01:39:04Z,"
 		Sure. I realized that the bug is in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/8ebf4fe1739aae04c14ddb3ad572a57775018673/pytorch_lightning/trainer/distrib_parts.py>this</denchmark-link>
  script.
 Specifically:
 <denchmark-code>try:
     import torch_xla.core.xla_model as xm
 except ImportError:
     XLA_AVAILABLE = False
 else:
     XLA_AVAILABLE = True
 </denchmark-code>
 
 So, if the environment has  installed but no TPU, then this error is thrown. If I use an environment without , it works fine. So, is this something that should be fixed in the codebase or something that the user should take care of? <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 		",3.0,dalmia,2020-08-28T07:35:19Z,"
 		yes, we had the XLA detection as a temporal solution as we did not expect someone would install XLA without having TPU...
 so, pls send a PR, I think that we have this patter in several files...
 		",4.0,dalmia,2020-10-01T18:35:37Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",MODIFY,2.0,pytorch_lightning\accelerators\tpu_backend.py,pytorch_lightning\accelerators\tpu_backend.py,1.0,"49,50",50,setup,"self,model",46,60,MODIFY,1.0,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,188,189,MODIFY,0.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,0.0,"33,34,48,49,50","46,48,49,50,51",,,,,MODIFY,0.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,0.0,"30,33,39","32,38,41,42,43,44,45",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_early_stopping_check,"self,trainer,pl_module",170,204,,,,,,,,,,,,,,,,,,,ADD,0.0,None,pytorch_lightning\utilities\xla_device_utils.py,,,,,,,,,,,,,,,MODIFY,0.0,tests\models\test_tpu.py,tests\models\test_tpu.py,,,,,,,,0.0,"2,9,13,18,19,20","15,20,21,22,23,219",,,,,,,,,,,,ADD,0.0,None,tests\utilities\test_xla_device_utils.py,,,,1.0,174,174,to_device,"self,batch",160,181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3111,mutasem-mattar,2020-08-23T10:38:33Z,2020-09-09T00:30:59Z,Horovod with native 16 precision not working,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 using precision=16 with distributed_backend=horovod
 
 <denchmark-code>Traceback (most recent call last):
   File ""/workspace/main_lightning.py"", line 500, in <module>
     main(hyperparams)
   File ""/workspace/main_lightning.py"", line 492, in main
     trainer.fit(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py"", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py"", line 1068, in fit
     results = self.horovod_train(model)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 213, in horovod_train
     model, optimizers = model.configure_apex(amp, model, self.optimizers, self.amp_level)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/core/lightning.py"", line 954, in configure_apex
     model, optimizers = amp.initialize(model, optimizers, opt_level=amp_level)
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>trainer = Trainer(
         precision=16,
         gpus=1,
         distributed_backend=""horovod"")
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version: 1.6.0+cu101
 How you installed PyTorch: pip
 
 	",091d37f968b593e7e3b212d53bec6395a8c546de,Travis Addair,2020-09-08 20:30:57-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"42,43",,1.0,mutasem-mattar,2020-08-25T08:14:43Z,"
 		mind have look <denchmark-link:https://github.com/tgaddair>@tgaddair</denchmark-link>
  
 		",2.0,mutasem-mattar,2020-08-25T13:07:31Z,"
 		Absolutely, let me take a look today and get back to you, <denchmark-link:https://github.com/mutasem-mattar>@mutasem-mattar</denchmark-link>
 .
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\accelerators\horovod_backend.py,pytorch_lightning\accelerators\horovod_backend.py,1.0,"90,91,92,93,94","75,76,77,78,79",setup,"self,model",42,100,MODIFY,2.0,tests\models\test_horovod.py,tests\models\test_horovod.py,1.0,"45,46","45,46",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_horovod,"trainer_options,on_gpu",42,57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131",,test_horovod_amp,tmpdir,116,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3143,24hours,2020-08-25T03:25:50Z,2020-09-10T21:01:21Z,Trainer crashed when optimizer frequency is defined.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run the following code:
 <denchmark-link:https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639>https://gist.github.com/24hours/ec67de5384bb05e28544d580ae424639</denchmark-link>
 
 <denchmark-code>Traceback (most recent call last):
   File ""pl_bug.py"", line 40, in <module>
     trainer.fit(mnist_model, train_loader) 
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/states.py"", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1073, in fit
     results = self.accelerator_backend.train(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/gpu_backend.py"", line 51, in train
     results = self.trainer.run_pretrain_routine(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1239, in run_pretrain_routine
     self.train()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 394, in train
     self.run_training_epoch()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 491, in run_training_epoch
     batch_output = self.run_training_batch(batch, batch_idx)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 883, in run_training_batch
     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
     def configure_optimizers(self):
         optimizer_G = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)
         optimizer_D = torch.optim.Adam(self.parameters(), lr=0.1, weight_decay=1e-5)
 
         return [
                 {'optimizer': optimizer_D, 'frequency': 5},
                 {'optimizer': optimizer_G, 'frequency': 1}
             ]
 the culprit is 'frequency' : 5, removing the line will allow trainer to run smoothly.
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers>https://pytorch-lightning.readthedocs.io/en/latest/lightning-module.html?highlight=optimizers#configure-optimizers</denchmark-link>
 
 The definition is correct according to this documentation.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Model should train without crash.
 The code work in 0.8.5 environment.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 GeForce GTX TITAN X
 
 
 available:         True
 version:           11.0
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0a0+9907a3e
 pytorch-lightning: 0.9.0
 tensorboard:       2.2.0
 tqdm:              4.48.2
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.10
 version:           #110-Ubuntu SMP Tue Jun 23 02:39:32 UTC 2020
 
 
 
 	",a1ea681c47004599ee5a47a05ddd1b4ea12e60d4,Rohit Gupta,2020-09-10 23:01:20+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"40,41",,1.0,24hours,2020-08-25T03:26:32Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,24hours,2020-08-25T07:08:55Z,"
 		this is the case with multiple optimizers, you need to spec them.. so you would prefer having default 1 if freq is not specified?
 		",3.0,24hours,2020-08-25T10:24:40Z,"
 		the exception occur because trainer_loop.py incorrect count number of optimizer if frequency is defined. Since this configuration work in version 0.8.5, this look like regression error.
 Unless of course if the configuration is unsupported in version 0.9.0
 		",4.0,24hours,2020-08-25T22:23:14Z,"
 		
 so you would prefer having default 1 if freq is not specified?
 
 That is a totally different case I think. If the frequency is not specified we run train_step for both optimizers but if it is specified to 1 for both then in such case it will run 1st batch for opt_1, 2nd for opt_2, 3rd for opt_1, 4th for opt_2...
 A simple fix here can be:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 872
       in
       a7705c8
 
 
 
 
 
 
  batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end) 
 
 
 
 
 
 if len(batch_outputs) == 1:  # when frequencies are defined
     batch_outputs[0].append(opt_closure_result.training_step_output_for_epoch_end)
 else:  # no frequencies
     batch_outputs[opt_idx].append(opt_closure_result.training_step_output_for_epoch_end)
 		",MODIFY,0.0,docs\source\converting.rst,docs\source\converting.rst,0.0,"19,21,39,41,51,53,81,83","19,21,39,41,51,53,81,83",,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"503,504",503,MODIFY,1.0,tests\base\model_optimizers.py,tests\base\model_optimizers.py,1.0,"37,38,39,40,41,42,43",,configure_optimizers__multiple_optimizers_frequency,self,37,43,MODIFY,1.0,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,"245,246,247,248,249,250,251,252,253,254,255,256,257",,test_configure_optimizers_with_frequency,tmpdir,245,257,5.0,24hours,2020-08-27T14:12:39Z,"
 		ok, can someone write a test and submit a PR for this? show the test failing on master first.
 <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
   or <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  ?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,run_training_batch,"self,batch,batch_idx,dataloader_idx",431,553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3144,lezwon,2020-08-25T03:35:04Z,2020-08-26T16:22:20Z,ONNX model does not save on GPU,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Attempting to export on ONNX after training model on GPU, throws an error is the input_sample or example_input_array is not a CUDA tensor.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Train a model on GPU
 Try to export to ONNX when  self.example_input_array = torch.zeros(1, 1, 500, 500) or input_sample = torch.zeros(1, 1, 500, 500)
 
 <denchmark-code>---------------------------------------------------------------------------
 RuntimeError                              Traceback (most recent call last)
 <ipython-input-32-cd8009a0b6a3> in <module>
       1 filepath = 'model.onnx'
 ----> 2 model.to_onnx(filepath, export_params=True)
 
 /opt/conda/lib/python3.7/site-packages/pytorch_lightning/core/lightning.py in to_onnx(self, file_path, input_sample, **kwargs)
    1721         if 'example_outputs' not in kwargs:
    1722             self.eval()
 -> 1723             kwargs['example_outputs'] = self(input_data)
    1724 
    1725         torch.onnx.export(self, input_data, file_path, **kwargs)
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     548             result = self._slow_forward(*input, **kwargs)
     549         else:
 --> 550             result = self.forward(*input, **kwargs)
     551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
 
 <ipython-input-24-51cae3b5e57f> in forward(self, inputs)
      20 
      21     def forward(self, inputs):
 ---> 22         return self.model(inputs)
      23 
      24     def training_step(self, batch, batch_idx):
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     548             result = self._slow_forward(*input, **kwargs)
     549         else:
 --> 550             result = self.forward(*input, **kwargs)
     551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py in forward(self, input)
      98     def forward(self, input):
      99         for module in self:
 --> 100             input = module(input)
     101         return input
     102 
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
     548             result = self._slow_forward(*input, **kwargs)
     549         else:
 --> 550             result = self.forward(*input, **kwargs)
     551         for hook in self._forward_hooks.values():
     552             hook_result = hook(self, input, result)
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in forward(self, input)
     351 
     352     def forward(self, input):
 --> 353         return self._conv_forward(input, self.weight)
     354 
     355 class Conv3d(_ConvNd):
 
 /opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py in _conv_forward(self, input, weight)
     348                             _pair(0), self.dilation, self.groups)
     349         return F.conv2d(input, weight, self.bias, self.stride,
 --> 350                         self.padding, self.dilation, self.groups)
     351 
     352     def forward(self, input):
 
 RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 filepath = 'model.onnx'
 model.to_onnx(filepath, export_params=True)
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Should automatically convert example_input_array or input_sample to the device type and save the model to ONNX.
 	",d9ea25590e95ca9e70401123a0f1f59de711e2ff,Lezwon Castelino,2020-08-26 16:22:19+00:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"29,30",,1.0,lezwon,2020-08-25T07:09:53Z,"
 		I would say that the problem could be the distributed way, mind check running only on a single GPU?
 		",2.0,lezwon,2020-08-25T09:31:24Z,"
 		I ran this on Kaggle notebook. When I tried to save after training, it threw the error.
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1719,1720,1721,1722,1723,1724,1727,1728","1719,1720,1723",to_onnx,"self,str,None,kwargs",1689,1730,MODIFY,9.0,tests\models\test_onnx.py,tests\models\test_onnx.py,1.0,128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_if_inference_output_is_valid,tmpdir,118,141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"101,102,103",102,test_error_if_no_input,tmpdir,97,104,1.0,"20,21,22,23,24",20,test_model_saves_with_input_sample,tmpdir,14,24,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"107,108,109,110,111,112,113,114,115",,test_error_if_input_sample_is_not_tensor,tmpdir,107,115,1.0,91,88,test_verbose_param,"tmpdir,capsys",88,94,1.0,83,"69,77",test_model_saves_on_multi_gpu,tmpdir,65,85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,47,44,test_model_saves_with_example_output,tmpdir,41,52,1.0,"28,29,30,31,32,33,34",33,test_model_saves_on_gpu,tmpdir,28,38,1.0,58,,test_model_saves_with_example_input_array,tmpdir,55,61,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3162,Sordie,2020-08-25T15:23:39Z,2020-08-26T12:02:54Z,RMSLE metric appears to be incorrect,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The usage of mse <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/22b9642117394d3c50587ae137dbf94c6dd5173c/pytorch_lightning/metrics/functional/regression.py#L138>in the rmsle function</denchmark-link>
  looks wrong to me. It looks like this function currently computes  instead of .
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I would expect that rmsle looks like this:
 rmsle = rmse(torch.log(pred + 1), torch.log(target + 1), reduction=reduction)
 	",888340d17ed91eeee1b576cda36f13f0ef3e5459,Sordie,2020-08-26 08:02:53-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"27,28",,1.0,Sordie,2020-08-25T15:24:18Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Sordie,2020-08-25T17:35:46Z,"
 		yep it's a bug. mind send a PR :)
 		",,,,,,,,,MODIFY,0.0,pytorch_lightning\metrics\functional\regression.py,pytorch_lightning\metrics\functional\regression.py,0.0,"135,138","135,138",,,,,MODIFY,0.0,pytorch_lightning\metrics\regression.py,pytorch_lightning\metrics\regression.py,0.0,169,169,MODIFY,1.0,tests\metrics\functional\test_regression.py,tests\metrics\functional\test_regression.py,1.0,"32,33,34,35,36,37,38,39,40,41,42,43,44",,test_against_sklearn,"sklearn_metric,torch_metric",32,44,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3168,carmocca,2020-08-25T17:28:04Z,2020-08-26T01:21:03Z,Max line length mismatch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/.pep8speaks.yml>.pep8speaks.yml</denchmark-link>
  has  set to 119 whereas it is 120 in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pyproject.toml>pyproject.toml</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 line-length should be consistent
 	",59fb332677c0f865de1e42f2fc80caa2460915ff,Carlos Mocholí,2020-08-25 21:21:02-04:00,MODIFY,0,.pep8speaks.yml,.pep8speaks.yml,0.0,8,8,1.0,carmocca,2020-08-25T19:58:00Z,"
 		mind send a fix PR? :]
 		",2.0,carmocca,2020-08-25T20:54:02Z,"
 		Sure!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3172,ozen,2020-08-25T18:31:49Z,2020-09-19T22:29:07Z,"""Unsupported `ReduceOp` for distributed computing"" warning when using Result without distributed","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 step_result.py imports pytorch_lightning.metrics.converters
 converters.py raises the following warning if torch.distributed.ReduceOp cannot be imported:
 
 rank_zero_warn('Unsupported ReduceOp for distributed computing.')
 
 I don't use and don't want to use distributed training, but this warning is printed to stdout non-stop at one warning per second rate.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Use Result object without a distributed package available.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 This warning should be printed once at most.
 If I don't use distributed, I don't need to see this warning at all.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - GeForce RTX 2080
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.18.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.6.0
 - pytorch-lightning: 0.9.0
 - tensorboard:       2.2.0
 - tqdm:              4.47.0
 System:
 - OS:                Windows
 - architecture:
 - 64bit
 - WindowsPE
 - processor:         Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
 - python:            3.7.1
 - version:           10.0.19041
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",99f05ed23f818d4f21c2c6925a66e75df606c859,Adrian Wälchli,2020-09-19 18:29:06-04:00,MODIFY,0,pytorch_lightning\metrics\converters.py,pytorch_lightning\metrics\converters.py,0.0,"31,33","29,32,34,35,39,40",1.0,ozen,2020-08-25T20:01:29Z,"
 		Yeah, that is PyTorch issue as most it does not support several distributed features on Windows...
 May send a PR with some friendly message or update it docs?
 		",2.0,ozen,2020-08-25T20:35:46Z,"
 		I think I understand what you are saying. But, pytorch-lightning code is the one that prints the warning message. Since it keeps printing the same warning like it is in an infinite loop, upgrading from 0.8 to 0.9 made pytorch-lightning unusable for me, and for anyone using Windows I imagine.
 If you say pytorch-lightning 0.9+ won't support Windows, that's one thing. Otherwise, this is an issue.
 		",3.0,ozen,2020-08-25T20:49:44Z,"
 		
 I think I understand what you are saying. But, pytorch-lightning code is the one that prints the warning message.
 
 yes, we warn you that you requested something which is not supported on your OS, pt does not give it to you either, the diff is the PT even does not tell you and is standard computing
 
 Since it keeps printing the same warning like it is in an infinite loop, upgrading from 0.8 to 0.9 made pytorch-lightning unusable for me, and for anyone using Windows I imagine.
 
 so you just complain that you get this message several times? <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 
 If you say pytorch-lightning 0.9+ won't support Windows, that's one thing. Otherwise, this is an issue.
 
 this is a misunderstanding, it is not PL who does not support distributed on Win but PyTorch itself, PL is just wrapper above PT... so you shall create an issue there...
 		",4.0,ozen,2020-08-25T20:56:45Z,"
 		
 this is a misunderstanding, it is not PL who does not support distributed on Win but PyTorch itself, PL is just wrapper above PT... so you shall create an issue there...
 
 I think I failed to explain the problem. I use PT without any issues, because I don't use distributed anyway. I don't use distributed when I use PL either, but PL core imports distributed anyway, and prints that warning, and prints it once per every second (I don't know why).
 Edit: and this only started to happen in 0.9 because the import line came with the Result object.
 		",MODIFY,0.0,pytorch_lightning\metrics\sklearns.py,pytorch_lightning\metrics\sklearns.py,0.0,"23,25","22,24,26,27,31,32",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,ozen,2020-08-25T21:21:10Z,"
 		ok, that is fair that we shall tell you just once or even do not import it if you do not need it...
 		",6.0,ozen,2020-08-25T21:22:24Z,"
 		<denchmark-link:https://github.com/ozen>@ozen</denchmark-link>
  mind send a PR with patching this importing?
 cc: <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  <denchmark-link:https://github.com/yukw777>@yukw777</denchmark-link>
 
 		",7.0,ozen,2020-08-25T22:03:51Z,"
 		OK, couple of findings:
 
 As I understand rank_zero_only covers distributed processes, but not DataLoader worker processes. The worker processes are started at the beginning of the training and validation runs at every epoch, resulted in a lot of warnings, especially if you use a large number of workers.
 from pytorch_lightning.metrics.converters import _sync_ddp_if_available line in core/step_result.py can be moved inside the if clause where it is used if sync_dist=True. But this alone won't solve the problem because metric converters are imported elsewhere too.
 The ReduceOp import in metrics/converters.py that causes the warning is used for two things: type hints and in the function _sync_ddp_if_available. If we moved the import inside _sync_ddp_if_available the problem would be solved, but I don't know if it's OK to remove the type hints.
 
 		",8.0,ozen,2020-09-07T12:32:49Z,"
 		To keep it simple, you can also ignore the warnings in the command-line with
 <denchmark-code> >>> python -W ""ignore::UserWarning::0"" file_to_run.py
 </denchmark-code>
 
 		",9.0,ozen,2020-09-19T01:15:02Z,"
 		I have mentioned multi-process data loading as the cause of the spam of warnings. In fact, this is a problem only in Windows when  statement is outside of  block. The reason of this is <denchmark-link:https://pytorch.org/docs/stable/data.html#platform-specific-behaviors>described here</denchmark-link>
 . Moving the import statements inside the if block solves the problem.
 We may still consider changing the code a little bit. The origin of the warning is pytorch_lightning.metrics.converters module.
 try:
     from torch.distributed import ReduceOp
 except ImportError:
     class ReduceOp:
         SUM = None
     rank_zero_warn(""Unsupported `ReduceOp` for distributed computing"")
 Note that ReduceOp.SUM is not used. ReduceOp class is imported only for the type hints (i.e. reduce_op: Optional[ReduceOp] = None). However on the same module, group parameters do not follow this practice for type hinting and use Optional[Any].
 An example:
 def tensor_metric(group: Optional[Any] = None, reduce_op: Optional[ReduceOp] = None) -> Callable:
 If we change the hints into
 def tensor_metric(group: Optional[Any] = None, reduce_op: Optional[Any] = None) -> Callable:
 we can remove the import ReduceOp statement together with the warning.
 Or maybe we want type hints for group as well:
 def tensor_metric(group: Optional[group] = None, reduce_op: Optional[ReduceOp] = None) -> Callable:
 and add a try-catch import for group too since it has the same problem with the ReduceOp class.
 I can send a PR depending on the decision.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3185,VirajBagal,2020-08-26T05:19:53Z,2020-09-01T13:17:53Z,"Value out of range (expected to be in range of [-1, 0], but got 1)","
 <denchmark-h:h2>Value out of range (expected to be in range of [-1, 0], but got 1)</denchmark-h>
 
 Exception in device=TPU:5: torch_xla/csrc/helpers.cpp:97 : Check failed: min_shape_dim <= dim && dim <= max_shape_dim
 Following is the stack trace when I am using all 8 TPU cores on Kaggle. The exact same code with
 
 sync_dist=False
 
 works completely fine on Kaggle GPU.
 
 Value out of range (expected to be in range of [-1, 0], but got 1)Exception in device=TPU:5: torch_xla/csrc/helpers.cpp:97 : Check failed: min_shape_dim <= dim && dim <= max_shape_dim
 *** Begin stack trace ***
 tensorflow::CurrentStackTrace()
 torch_xla::XlaHelpers::GetCanonicalDimensionIndex(long long, long long)
 torch_xla::XlaHelpers::MakeTransposePermutation(long long, long long, long long)
 torch_xla::XLATensor::transpose(torch_xla::XLATensor const&, long long, long long)
 torch_xla::AtenXlaType::t(at::Tensor const&)
 c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionIntoRuntimeFunctor_<at::Tensor ()(at::Tensor const&), at::Tensor, c10::guts::typelist::typelist<at::Tensor const&> >, at::Tensor (at::Tensor const&)>::call(c10::OperatorKernel, at::Tensor const&)
 at::t(at::Tensor const&)
 at::Tensor::t() const
 _PyMethodDef_RawFastCallKeywords
 _PyMethodDescr_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallDict
 _PyObject_Call_Prepend
 PyObject_Call
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallDict
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallDict
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallDict
 _PyObject_Call_Prepend
 _PyObject_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallDict
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 PyEval_EvalCodeEx
 PyEval_EvalCode
 _PyMethodDef_RawFastCallKeywords
 _PyCFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyGen_Send
 _PyEval_EvalFrameDefault
 _PyGen_Send
 _PyEval_EvalFrameDefault
 _PyGen_Send
 _PyMethodDef_RawFastCallKeywords
 _PyMethodDescr_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallDict
 _PyObject_Call_Prepend
 PyObject_Call
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyMethodDef_RawFastCallKeywords
 _PyCFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyMethodDef_RawFastCallKeywords
 _PyCFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyMethodDef_RawFastCallKeywords
 _PyCFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyEval_EvalCodeWithName
 _PyFunction_FastCallDict
 _PyObject_Call_Prepend
 PyObject_Call
 _PyEval_EvalFrameDefault
 _PyGen_Send
 _PyMethodDef_RawFastCallKeywords
 _PyMethodDescr_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallKeywords
 _PyEval_EvalFrameDefault
 _PyFunction_FastCallDict
 *** End stack trace ***
 
 Here is the code for my LightningModule
 class GraphAEStage1(pl.LightningModule):
   def __init__(self, config):
     super(GraphAEStage1, self).__init__()
     max_deg = config['poc_max_degree']
     input_dim = config['input_dim']
     hidden_feat = config['hidden_feat_dim']
     layerid = config['layerid']
     self.lr = config['lr']
   
     self.encoder = GraphEncoderStage1(max_deg, input_dim, hidden_feat[layerid])
     self.decoder = GraphDecoderStage1(max_deg, input_dim, hidden_feat[layerid])
     
     self.encoder.apply(self.init_weights)
     
     self.crit = torch.nn.L1Loss()
     self.node_mae = MAE()
     self.adj_mae = MAE()
     
     self.decoder.self_linear.weight = nn.Parameter(self.encoder.self_linear.weight.permute(1,0))
     for d in range(config['poc_max_degree']):
         self.decoder.neigh_linear[d].weight = nn.Parameter(self.encoder.neigh_linear[d].weight.permute(1,0))
 
   def training_step(self, batch, batch_idx):
     loss, tf_node, _, _, _ = self.shared_step(batch)
     result = pl.TrainResult(minimize=loss)
     result.log('train_loss', loss, prog_bar = True, logger = True, on_step = False, on_epoch = True, sync_dist = True)
     return result
 
   def validation_step(self, batch, batch_idx):
     loss, tf_node, recon_node, val_node_mae, val_adj_mae = self.shared_step(batch)
     
     logs = {'val_loss': loss, 'node_mae': val_node_mae, 'adj_mae': val_adj_mae}
     result = pl.EvalResult(checkpoint_on = loss)
     result.log_dict(logs, prog_bar = True, logger = True, on_step = False, on_epoch = True, sync_dist = True)
     
     return result               
 
   def shared_step(self, batch):
     node, adj, deg, mask = batch
     tf_node, avg_adj = self.encoder(node, adj, deg, mask)
     recon_node, recon_adj = self.decoder(tf_node, deg)
 
     
     node_loss = self.criterion(recon_node, node)
     adj_loss = self.criterion(recon_adj, avg_adj)
     loss = node_loss + adj_loss
     
     node_mae = self.node_mae(node, recon_node)
     adj_mae = self.adj_mae(recon_adj, avg_adj)
     
     return loss, tf_node, recon_node, node_mae, adj_mae
 
   def configure_optimizers(self):
     optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
     return optimizer
 
   def criterion(self, output, target):
     return self.crit(output, target)
 
   def init_weights(self, m):
     if type(m) == nn.Linear:
         torch.nn.init.xavier_uniform(m.weight)
         m.bias.data.fill_(0.00)  
 There are similar issues reported on PyTorch XLA repo, but many of them were using CrossEntropyLoss. I am not using CrossEntropyLoss here.
 	",3910ad033074367f6abfe0001562db725a75cb73,Lezwon Castelino,2020-09-01 09:17:52-04:00,MODIFY,1,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,232,232,1.0,VirajBagal,2020-08-26T15:24:23Z,"
 		<denchmark-link:https://github.com/lezwon>@lezwon</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  mind taking a look at this?
 		",2.0,VirajBagal,2020-08-26T15:41:59Z,"
 		<denchmark-link:https://github.com/VirajBagal>@VirajBagal</denchmark-link>
  could you share the notebook?
 		",3.0,VirajBagal,2020-08-26T16:32:46Z,"
 		<denchmark-link:https://github.com/lezwon>@lezwon</denchmark-link>
  Due to some privacy issues, I can't share it publicly here. I have shared it with you on Kaggle. Please check the ""Shared With You"" tab on Kaggle.
 		",4.0,VirajBagal,2020-08-26T16:46:24Z,"
 		Sure.. Will do 😊👍
 		",MODIFY,2.0,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,848,846,weighted_mean,"result,weights",846,850,MODIFY,1.0,tests\models\test_tpu.py,tests\models\test_tpu.py,1.0,"248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276",,,,,,,,,,,,,,,,,,,,,,,,5.0,VirajBagal,2020-08-28T17:42:29Z,"
 		It seems xla does not support the  operation on 1-dimensional tensors. <denchmark-link:https://github.com/zcain117>@zcain117</denchmark-link>
  could you help us out?  seems to be throwing the error mentioned when called. PyTorch usually returns 1d tensors as is.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_result_obj_on_tpu,tmpdir,248,276,_stop_distributed_training,"self,trainer,pl_module",221,234,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"302,303",,__copy__,self,299,305,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3189,dedeswim,2020-08-26T08:25:32Z,2020-10-07T21:55:25Z,Broken [Source] links in docs,"
 <denchmark-h:h2>📚 Documentation</denchmark-h>
 
 Hello, I was browsing the docs, and out of curiosity, I wanted to check the source for , and so I clicked on the [Source] link of the  method. The link, however, sends to <denchmark-link:https://github.com/PyTorchLightning/PyTorch-Lightning/blob/pytorch_lightning/core/lightning/LightningModule.py>https://github.com/PyTorchLightning/PyTorch-Lightning/blob/pytorch_lightning/core/lightning/LightningModule.py</denchmark-link>
 , which is not right, as the repo name is ""pytorch-lightning"".
 I am not familiar with Sphinx, but I checked the docs and the conf file, and I think that the reason can be found here:
 
 
 
 pytorch-lightning/docs/source/conf.py
 
 
         Lines 59 to 71
       in
       ee4eae8
 
 
 
 
 
 
  project = 'PyTorch-Lightning' 
 
 
 
  copyright = pytorch_lightning.__copyright__ 
 
 
 
  author = pytorch_lightning.__author__ 
 
 
 
  
 
 
 
  # The short X.Y version 
 
 
 
  version = pytorch_lightning.__version__ 
 
 
 
  # The full version, including alpha/beta/rc tags 
 
 
 
  release = pytorch_lightning.__version__ 
 
 
 
  
 
 
 
  # Options for the linkcode extension 
 
 
 
  # ---------------------------------- 
 
 
 
  github_user = 'PyTorchLightning' 
 
 
 
  github_repo = project 
 
 
 
 
 
 Since github-repo is set to project, which is, indeed, PyTorch-Lightning, as in the broken link.
 If someone more familiar with Sphinx can confirm this, I'd be glad to submit a PR to fix this.
 	",9be26d0c1b3aa2923475cd83098850d1e438a24e,Jeff Yang,2020-10-07 23:55:24+02:00,MODIFY,0,docs\source\_templates\theme_variables.jinja,docs\source\_templates\theme_variables.jinja,0.0,"2,3,4,5","2,3,4,5",1.0,dedeswim,2020-08-26T08:26:10Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,dedeswim,2020-08-26T16:12:02Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  mind have look? 
 		",3.0,dedeswim,2020-08-26T16:20:24Z,"
 		<denchmark-link:https://github.com/dedeswim>@dedeswim</denchmark-link>
  so the reason you pointed out is not exactly why a wrong link is being generated for LightningModule.
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/docs/source/conf.py#L347>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/docs/source/conf.py#L347</denchmark-link>
 
 The function linkcode_resolve generates the 'source' links in the doc and that is giving out the wrong link for LightningModule. If you see the link generated for say TrainResult class, it links to the master branch and the exact line number for the methods, which is not the case here.
 		",4.0,dedeswim,2020-08-26T16:22:34Z,"
 		btw, need to keep compatibility with tagged releases as well as with master...
 		",MODIFY,2.0,docs\source\conf.py,docs\source\conf.py,1.0,,"329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363",linkcode_resolve,"domain,info",329,363,MODIFY,0.0,docs\source\lightning_module.rst,docs\source\lightning_module.rst,0.0,"211,658,664,669,680,686,692,703,709,715,723,735,741,771,777,783,789,795,1031,1037,1047,1053,1067,1073,1079,1085,1097,1103,1108,1114,1120,1130,1136,1142,1148,1154,1160,1166,1172,1178,1184,1190,1196,1202,1208,1214,1220,1226,1232,1238","211,658,664,669,680,686,692,703,709,715,723,735,741,771,777,783,789,795,1031,1037,1047,1053,1067,1073,1079,1085,1097,1103,1108,1114,1120,1130,1136,1142,1148,1154,1160,1166,1172,1178,1184,1190,1196,1202,1208,1214,1220,1226,1232,1238",,,,,,,,,,,,,,,,,,,,,,,5.0,dedeswim,2020-08-26T19:55:24Z,"
 		Hi, thank you <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>
  for the explanation!
 I indeed realized that the link should go to ""pytorch_lightning/core/lightning.py"", instead of ""pytorch_lightning/core/lightning/LightningModule.py"".
 Interestingly, the 0.8.5 version of the docs redirects to the right link (the one above), whereas the ""0.9.0"", ""latest"" and ""stable"" versions (which are the same version, right?) don't work.
 		",6.0,dedeswim,2020-08-27T07:01:42Z,"
 		ok, then we may have the wrong path in docs, is it only this case or all docs have wrong links?
 		",7.0,dedeswim,2020-08-27T07:07:09Z,"
 		I tried with all the other classes under the ""Python API"" section of the bar on the right in the docs and it looks like this is the only one broken
 		",8.0,dedeswim,2020-08-27T07:23:33Z,"
 		ok, then it is nothing with linkcode_resolve but just wrong link in docs, mind send a PR fix
 
 
 
 pytorch-lightning/pytorch_lightning/core/lightning.py
 
 
          Line 136
       in
       4d98419
 
 
 
 
 
 
  def forward(self, *args, **kwargs): 
 
 
 
 
 
 		",9.0,dedeswim,2020-08-27T08:04:38Z,"
 		I'd be happy to, but I am not sure about how to fix this. Where is the path to the source defined? Isn't it generated by linkcode_resolve?
 		",10.0,dedeswim,2020-08-27T15:27:44Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  others are correct, LightningModule is the only one that seems broken.
 		",11.0,dedeswim,2020-09-01T18:50:12Z,"
 		it looks like this is because in the .rst docs we refer to the methods with autofunction instead of automethod. changing to automethod solves the link issue, but breaks formatting of docs, need to figure out how to  properly use it.
 		",12.0,dedeswim,2020-09-14T16:39:40Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  or <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  is there a specific reason using  extension?
 if not, shall we use this extension <denchmark-link:https://www.sphinx-doc.org/en/master/usage/extensions/viewcode.html>https://www.sphinx-doc.org/en/master/usage/extensions/viewcode.html</denchmark-link>
 ?
 this creates  folder for source code and link to that folder. PyTorch is using this also.
 It also solves this issue using ,  won't work just like <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  comment.
 I discovered that PyTorch docs doesn't refer to source code for , so we need to add some css to the forked theme.
 Here's the tried output:
 It doesn't show self which is pretty normal I think, just like methods of other classes.
 <denchmark-link:https://user-images.githubusercontent.com/32727188/93113155-d7af8b80-f6de-11ea-9840-7026948f0250.png></denchmark-link>
 
 		",13.0,dedeswim,2020-09-14T16:54:57Z,"
 		<denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>
  nice, could you make a draft PR so we can try it out?
 		",14.0,dedeswim,2020-09-14T16:58:04Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  with  extension?
 sure, I can
 		",15.0,dedeswim,2020-09-14T17:05:36Z,"
 		so you say it's a two step fix.
 
 replace linkcode with viewcode, which solves the problem with linked source code, but we lose methods source code
 add the custom css to the theme so we have methods properly linked
 
 if it can consistently link all modules, classes, methods and functions, I think this is what we want.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347",linkcode_resolve.find_source,,330,347,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,dedeswim,2020-09-14T17:09:43Z,"
 		Yes, pytorch sphinx theme doesn't include css for directly linking methods.
 So, we need to add it. I think viewcode works with all modules, classes, methods and functions.
 But i will try with all to make sure it works and i will draft a PR
 		",17.0,dedeswim,2020-09-15T22:38:50Z,"
 		just testing and sure if it is a better way as before it linked to GitHub source and you simply create a perm link...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3199,Lucas-Steinmann,2020-08-26T17:53:25Z,2020-09-18T21:08:06Z,Early Stopping + result dictionary + no validation not working.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The case where the user does not use validation and returns a dictionary (instead of a TrainResult) during training does not work in combination with early stopping.
 The test case which should check this is here:
 
 
 
 pytorch-lightning/tests/callbacks/test_early_stopping.py
 
 
         Lines 136 to 159
       in
       bd35c86
 
 
 
 
 
 
  def test_early_stopping_no_val_step(tmpdir): 
 
 
 
  """"""Test that early stopping callback falls back to training metrics when no validation defined."""""" 
 
 
 
  
 
 
 
  class CurrentModel(EvalModelTemplate): 
 
 
 
  def training_step(self, *args, **kwargs): 
 
 
 
  output = super().training_step(*args, **kwargs) 
 
 
 
  output.update({'my_train_metric': output['loss']})  # could be anything else 
 
 
 
  return output 
 
 
 
  
 
 
 
  model = CurrentModel() 
 
 
 
  model.validation_step = None 
 
 
 
  model.val_dataloader = None 
 
 
 
  
 
 
 
  stopping = EarlyStopping(monitor='my_train_metric', min_delta=0.1) 
 
 
 
  trainer = Trainer( 
 
 
 
  default_root_dir=tmpdir, 
 
 
 
  early_stop_callback=stopping, 
 
 
 
  overfit_batches=0.20, 
 
 
 
  max_epochs=2, 
 
 
 
      ) 
 
 
 
  result = trainer.fit(model) 
 
 
 
  
 
 
 
  assert result == 1, 'training failed to complete' 
 
 
 
  assert trainer.current_epoch < trainer.max_epochs 
 
 
 
 
 
 The check in the last line is wrong. It should actually compare:
     assert trainer.current_epoch < trainer.max_epochs - 1
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Fix the test case
 Run tests.
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 I guess using the test case is simpler and easier.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 That is an interesting question indeed. Possibilities are:
 
 Test case should pass with correct comparison
 The docs and  @williamFalcon in #3193 (comment) suggest that only 'loss' should work.
 
 So before fixing this issue, it should be settled what the expected behavior is.
 If you tell me, I'm happy to help.
 I could also include it in the pull request where I already tried to bring the docs in line with the test cases.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 GeForce GTX 1080 Ti
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.19.1
 pyTorch_debug:     False
 pyTorch_version:   1.6.0
 pytorch-lightning: 0.9.1dev
 tensorboard:       2.2.0
 tqdm:              4.48.2
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 ELF
 
 
 processor:         x86_64
 python:            3.8.3
 version:           #113~16.04.1-Ubuntu SMP Fri Jul 10 04:37:08 UTC 2020
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",197acd535fee5e79dafeeff14cc742095c77bd70,Lucas Steinmann,2020-09-18 23:08:04+02:00,MODIFY,0,docs\source\results.rst,docs\source\results.rst,0.0,,"104,105,106,107,108,109,110,111,112,113",1.0,Lucas-Steinmann,2020-08-26T19:00:04Z,"
 		<denchmark-link:https://github.com/Lucas-Steinmann>@Lucas-Steinmann</denchmark-link>
  mind sending a PR for this?
 		",2.0,Lucas-Steinmann,2020-08-26T19:41:11Z,"
 		I'd love to, but what is the intended behavior?
 Possibility 1, 2, or doesn't it matter?
 Maybe we should wait on <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  answer, since he already expressed his opinion.
 		",3.0,Lucas-Steinmann,2020-08-27T10:31:29Z,"
 		There is a third possibility, which is suggested by the code which would have to be changed:
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/early_stopping.py
 
 
          Line 169
       in
       4d98419
 
 
 
 
 
 
  # early stopping can also work in the train loop when there is no val loop and when using structured results 
 
 
 
 
 
 This comment says, that the current behavior is correct. (assuming ""structured result"" means Result object)
 So either code, docs or test is correct and the other two have to be adjusted. :)
 		",4.0,Lucas-Steinmann,2020-08-27T14:10:02Z,"
 		if you're not using the trainresult then the key 'loss' needs to be present
 All of these are equivalent
 return loss
 
 return {'loss': loss}
 
 return TrainResult(minimize=loss)
 
 return TrainResult(loss)
 But the recommended way is:
 <denchmark-code>return TrainResult(loss)
 </denchmark-code>
 
 We're moving away from plain dictionaries (TrainResult is just a dict also, but adds type checking)
 		",MODIFY,3.0,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,"163,164,165","160,164",on_validation_epoch_end,"self,trainer,pl_module",149,165,MODIFY,3.0,tests\callbacks\test_early_stopping.py,tests\callbacks\test_early_stopping.py,1.0,"173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191",,,,,,,,,,,,,,,,,,,,,,,,5.0,Lucas-Steinmann,2020-08-27T14:18:33Z,"
 		But return {'loss': loss} doesn't work with early stopping.
 Also the test, which suggests that any key should work still contradicts you.
 Furthermore the test is still wrong because the last line does not actually check whether early stopping was triggered (off-by-one error)
 		",6.0,Lucas-Steinmann,2020-08-28T16:11:06Z,"
 		<denchmark-link:https://github.com/Lucas-Steinmann>@Lucas-Steinmann</denchmark-link>
  reopening this then!
 		",7.0,Lucas-Steinmann,2020-09-01T13:24:41Z,"
 		Just to be clear. The next step is for you (the project owners) to decide what the correct behaviour is:
 
 The docs (only dicts with 'loss' key should work with early stopping): Then we have to fix the code and the tests.
 The code (early stopping should only work with TrainResult): Then we have to remove the test and fix the docs
 The test (early stopping should work with dicts and any key): Then we have to fix the code and the docs.
 
 I would love to help but I think it is not for me to decide what is correct.
 If you pick one, I'll implement it.
 		",8.0,Lucas-Steinmann,2020-09-01T13:33:04Z,"
 		The correct behavior is neither of the above...
 
 if using dicts, it should work with whatever you put in the monitor argument of the EarlyStopping or ModelCheckpoint.
 
 EarlyStopping(monitor='jiraffe')
 
 # must have this key
 return {'jiraffe': x}
 
 If using results it should work with whatever is in the early_stop_on argument:
 
 acc = ...
 loss = ...
 f1 = ...
 
 # pick whatever you want to early stop on
 key_i_care_about = f1 or loss or acc
 result = pl.TrainResult(early_stop_on=key_i_care_about)
 If you ALSO pass the key in the EvalResult then the EvalResult takes precedence...
 def training_step(...):
    # early stop on has NO effect because you ALSO have one in validation_step
     result = TrainResult(early_stop_on=acc)
 
 def validation_step(...)
     my_val_loss = MSE(y, y_hat)
     result = EvalResult(early_stop_on=my_val_loss)
 Finally, I'm pretty sure this is how it works today given that I personally implemented these and wrote a ton of tests...
 So, my question is:
 
 does it not work like I described? then we need to fix the bugs
 if it does work, then we need to clean up the docs to explain this better.
 
 		",9.0,Lucas-Steinmann,2020-09-01T13:44:16Z,"
 		Ok. 1. currently does not work (atleast a few days ago, when I wrote the issue, see my minimal example here: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3193#discussion_r478517184>#3193 (comment)</denchmark-link>
 )
 There is a test, which exactly checks 1.  but has an off-by-one error and would actually fail if the error is fixed (see my first comment on this issue).
 If I understand the docs correctly, they currently say it should only work with 'loss' key. Maybe it was meant to say the default behaviour is that it only works with 'loss' key: 
 
 
 pytorch-lightning/docs/source/results.rst
 
 
          Line 109
       in
       3910ad0
 
 
 
 
 
 
   # early stop + checkpoint can only use the `loss` when done manually via dictionaries 
 
 
 
 
 
 I think the bug in the code is located here, where it only checks for the early_stop_on key:
 
 
 
 pytorch-lightning/pytorch_lightning/callbacks/early_stopping.py
 
 
         Lines 170 to 173
       in
       3910ad0
 
 
 
 
 
 
  train_es_key = 'early_stop_on' 
 
 
 
  if trainer.callback_metrics.get(train_es_key, None) is not None: 
 
 
 
  self.monitor = train_es_key 
 
 
 
  should_check_early_stop = True 
 
 
 
 
 
 		",10.0,Lucas-Steinmann,2020-09-04T15:27:20Z,"
 		To prevent misunderstanding:
 <denchmark-code>EarlyStopping(monitor='jiraffe')
 
 # must have this key
 return {'jiraffe': x}
 </denchmark-code>
 
 the return {'jiraffe': x} is in training_step()?
 I've implemented a pull request. <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3347>#3347</denchmark-link>
 .
 It first corrects the test, which then fails. Then I fixed the issue.
 Afterwards another bug was unveiled, which I fixed in the subsequent commits.
 Then I also updated the docs.
 		",,,,,,,,,,,,,,,,,,,,,test_early_stopping_functionality_arbitrary_key,tmpdir,173,191,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"169,172,174,179,180,181",167,on_train_epoch_end,"self,trainer,pl_module",167,184,1.0,"199,200,201,202",,_run_early_stopping_check,"self,trainer,pl_module",198,232,,,,,,,,,,,,,,,,,,,,,,1.0,"140,145,150","140,145,150",test_early_stopping_no_val_step,tmpdir,127,150,1.0,"177,178,179,180",,test_early_stopping_functionality_arbitrary_key.validation_epoch_end,"self,outputs",177,180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3233,carmocca,2020-08-27T21:56:50Z,2020-09-03T20:07:50Z,auto_scale_batch_size not working with datamodule,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The Trainer expects the LightningModule to have self.batch_size (see scale_batch_size() in training_tricks.py). However, if one is using the new LightningDataModule, that should be the class with self.batch_size defined.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 assert hasattr(lightning_data_module, ""batch_size"")
 trainer = Trainer(auto_scale_batch_size=True)
 trainer.fit(lightning_module, datamodule=lightning_data_module)
 pytorch_lightning.utilities.exceptions.MisconfigurationException: Field batch_size not found in both `model` and `model.hparams`
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 auto_scale_batch_size should work using LightningDataModule
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* Packages:
 	- numpy:             1.18.5
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0
 	- pytorch-lightning: 0.9.1rc1
 	- tensorboard:       2.2.0
 	- tqdm:              4.48.2
 </denchmark-code>
 
 	",48c22c8bad9a47141c7160d92f2edc9e2e4ad159,Adrian Wälchli,2020-09-03 22:07:49+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"38,39",,1.0,carmocca,2020-08-27T23:26:56Z,"
 		I knew this bug report will eventually come :) same will happen for auto_lr_find.
 We need to generalize our lightning_hasattr and getattr helper functions to include the datamodule. In total, we have
 <denchmark-code>model.attribute_name
 model.hparams.attribute_name
 model.dm.attribute_name
 </denchmark-code>
 
 all of these should be considered by both lr_find and auto_scale_batch_size
 		",2.0,carmocca,2020-08-28T14:56:05Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
   So currently there is no solution to this issue?
 		",3.0,carmocca,2020-08-28T15:00:09Z,"
 		no, you need to set it manually, sorry.
 But I'll try to make a PR this weekend if not today. Should be a relatively easy fix unless I missed something.
 		",4.0,carmocca,2020-08-28T15:22:29Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  I completely agree that it should be a simple fix, since all the attribute getting/setting is handled by our own  ect functions.
 		",MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"954,955,956,957,958,959,960",954,,,,,MODIFY,6.0,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,1.0,137,137,MODIFY,3.0,pytorch_lightning\utilities\parsing.py,pytorch_lightning\utilities\parsing.py,1.0,"201,202,203,204,214,215,216,217,219,220","208,209,215",lightning_getattr,"model,attribute",200,221,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"232,233,234,237,240,241,244,245,246",235,test_auto_scale_batch_size_set_model_attribute,"tmpdir,use_hparams",215,246,5.0,carmocca,2020-09-03T20:47:58Z,"
 		<denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>
  As you may already know, we fixed this. Just a note in case you didn't see it, you now need to call .tune() instead of .fit():
 trainer.tune(lightning_module, datamodule=lightning_data_module)
 This is to better distinguish the training from the tuning step. However, it may be subject to change since there are some refactors happening right now.
 		",6.0,carmocca,2020-09-04T07:38:41Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 In which version  will be officially introduced?
 		",7.0,carmocca,2020-09-04T21:01:02Z,"
 		If it stays, in v1.0
 		",8.0,carmocca,2020-09-05T02:01:15Z,"
 		I am not familiar with the tuning interface. Few questions:
 Right now, I can use auto_scale_batch_size by passing the option to the trainer and calling fit. This does the auto scale procedure and then starts training
 If I understand correctly, when tune is released the following will be the necessary:
 <denchmark-code>should_auto_scale_bs = # comes from the user
 trainer = Trainer(auto_scale_batch_size=should_auto_scale_bs)
 
 if should_auto_scale_bs:
     trainer.tune(...)
 trainer.fit(...)
 </denchmark-code>
 
 Im assuming tune doesnt run fit automatically when it is finished.
 What would happen if the code ran trainer.tune(...) outside of the if and auto_scale_batch_size was False?
 Also, shouldn't auto_scale_batch_size be a parameter of tune instead of Trainer? (Maybe it is, I just don't know where is the tune discussion).
 		",9.0,carmocca,2020-09-05T03:33:33Z,"
 		I do not know how to answer these questions, <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  added the tune method so it would be best to ask him how he sees it being used in the future.
 		",10.0,carmocca,2020-09-05T09:34:06Z,"
 		<denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>
  I think the process in the future will be:
 <denchmark-code>trainer = Trainer(auto_scale_batch_size = should_auto_scale_bs)
 trainer.tune(model) # will do nothing if should_auto_scale_bs=False
 trainer.fit(model)
 </denchmark-code>
 
 it should still be easy for the user to use these features. The moving of the tuning from fit to tune is to disentangle the hyperparameter tuning from the actual optimization of the network. This will make it easier for us in the future to implement more tuning algorithms.
 		",11.0,carmocca,2020-10-23T21:03:21Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  , I have the same issue happening with me, my code is:
 <denchmark-code> # Reading and intilaizing the Trainer
     trainer_config = config.pop('trainer_config')
     trainer = Trainer(
         **trainer_config,
         callbacks=callbacks,
         logger=loggers,
         checkpoint_callback=checkpoint_callback,
         auto_scale_batch_size=(dataset_config['batch_size'] is None)
     )
 
     # optimizing batch size if batch_size is none
     if dataset_config['batch_size'] is None:
         print(""Batch Size is None attempting to tune batch size"")
         # tuner = Tuner(trainer)
         # optimal_batch_size = tuner.scale_batch_size(
         #     model, mode='power',
         #     batch_arg_name='batch_size',
         #     datamodule=dataset)
         optimal_batch_size = trainer.tune(model, datamodule=dataset)
         print(f""Found best batch size to be: {optimal_batch_size}"")
         dataset.batch_size = optimal_batch_size
 </denchmark-code>
 
 I tracked the issue and I think there is a problem in logic from <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L186>parsing.py Line186</denchmark-link>
  to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L193>parsing.py Line193</denchmark-link>
 
 The problem is that I have a hparams attribute in my model (I don't know where that came from),  but it doesn't contain a batch size attribute, the batch_size is an attribute that is contained in the datamodule, if <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/utilities/parsing.py#L192>this if condition</denchmark-link>
  is executed then I think there would not be a problem
 		",,,,,,,,,,,,,,,,,scale_batch_size,"self,LightningModule,str,int,int,int,str",131,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"226,227,228,229,230,231,232,233,234,238,245,246,247,248","225,226,227",lightning_setattr,"model,attribute,value",224,248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"294,302",301,_run_power_scaling,"trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs",294,314,1.0,"317,328",327,_run_binsearch_scaling,"trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs",317,353,1.0,"178,179,180,181,191,192,193","178,179,196,197",lightning_hasattr,"model,attribute",177,197,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"137,138",137,scale_batch_size,"self,LightningModule,str,int,int,int,str,fit_kwargs",131,138,1.0,"294,302","293,301",_run_power_scaling,"trainer,model,new_size,batch_arg_name,max_trials",293,313,1.0,"317,328","316,327",_run_binsearch_scaling,"trainer,model,new_size,batch_arg_name,max_trials",316,352,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3253,ShomyLiu,2020-08-29T09:06:08Z,2020-09-03T10:27:33Z,**gather_all_tensors_if_available**  share the same underlying storage for all GPUs,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Hi, one of new features in   <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2528>#2528</denchmark-link>
   has a   copy bug,  and this would lead that tensors in all GPUs  are the wrongly same as one GPU,  since they share the same storage:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L304>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/metrics/converters.py#L304</denchmark-link>
 
 <denchmark-code>gathered_result = world_size * [torch.zeros_like(result)]
 </denchmark-code>
 
 change into:
 <denchmark-code>gathered_result = [torch.zeros_like(result) for _ in range(world_size)]
 </denchmark-code>
 
 	",d521c1b1787930dd4f6375a3c61a25579ca59ee5,HT Liu,2020-09-03 12:27:32+02:00,MODIFY,0,pytorch_lightning\metrics\converters.py,pytorch_lightning\metrics\converters.py,0.0,304,304,1.0,ShomyLiu,2020-09-01T14:53:57Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
 
 		",2.0,ShomyLiu,2020-09-01T15:21:15Z,"
 		<denchmark-link:https://github.com/ShomyLiu>@ShomyLiu</denchmark-link>
  good catch, would you be up for sending a PR? Please, note that the function is not used anywhere yet, but are there for future changes to the metric package.
 		",3.0,ShomyLiu,2020-09-01T15:31:27Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
   it's my pleasure for a PR. I will finish this as soon as possible.
 Yeah, it's a new function to wrap the torch.distributed.all_gather.  But I think it is a very common use case;  especially, when using DDP mode, we always need to gather all the outputs cross all the GPUs.
 		",4.0,ShomyLiu,2020-09-01T15:35:57Z,"
 		<denchmark-link:https://github.com/ShomyLiu>@ShomyLiu</denchmark-link>
  Yes, I agree that it is a common use case.
 Please ping me when PR is ready.
 		",MODIFY,2.0,tests\metrics\test_converters.py,tests\metrics\test_converters.py,1.0,"180,181,182,183,184,185,186",,test_gather_all_tensors_ddp,,180,186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,ShomyLiu,2020-09-02T04:47:06Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
   Hi, I have sent a PR jus now for your review   <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3319>#3319</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"138,139,140,141,142,143,144,145,146",,_ddp_test_gather_all_tensors,"rank,worldsize",138,146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3259,maxjeblick,2020-08-29T22:01:59Z,2020-09-09T08:51:43Z,Cap batch size by number of training samples when using auto_scale_batch_size,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The batch size finder sets an unrealistically high batch size if all samples of the training dataset fit into one batch.
 <denchmark-code>...
 Batch size 8388608 succeeded, trying batch size 16777216
 Batch size 16777216 succeeded, trying batch size 33554432
 Batch size 33554432 succeeded, trying batch size 67108864
 Finished batch size finder, will continue with full run using batch size 67108864
 </denchmark-code>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run Mnist Example with auto_scale_batch_size=True (one needs to remove hardcoded batch size and set self.batch_size).
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Batch size search space should not be larger than number of available training samples.
 	",e245065fbcc7701da528fbe2568242d50586a0a3,Adrian Wälchli,2020-09-09 10:51:43+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"44,45",,1.0,maxjeblick,2020-08-29T22:02:38Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,0.0,,"15,16,18,26,27,28,29,30",,,,,MODIFY,4.0,pytorch_lightning\tuner\batch_size_scaling.py,pytorch_lightning\tuner\batch_size_scaling.py,1.0,"176,182,186,187,188","168,174",MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"242,244",241,test_auto_scale_batch_size_set_model_attribute,"tmpdir,use_hparams",215,247,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_run_power_scaling,"trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs",166,189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"213,215,216,217,218,219,227,232","202,204,212,224,225,226,227,228,229",_run_binsearch_scaling,"trainer,model,new_size,batch_arg_name,max_trials,fit_kwargs",192,233,1.0,240,,_adjust_batch_size,"trainer,str,float,None,str",236,240,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"274,275",,_is_valid_batch_size,"current_size,dataloader",274,275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
326,kossnick,2019-10-07T21:33:42Z,2019-10-08T11:39:55Z,Broken link in Examples readme,"
 The link to the template at the top of <denchmark-link:https://williamfalcon.github.io/pytorch-lightning/examples/Examples/>https://williamfalcon.github.io/pytorch-lightning/examples/Examples/</denchmark-link>
  doesn't work. Seems like the underlying codebase has shifted and the doc wasn't yet updated.
 	",c0bd203cffad86cc55fda2d87b7f7e0d51135166,David Kossnick,2019-10-08 07:39:54-04:00,MODIFY,0,docs\examples\Examples.md,docs\examples\Examples.md,0.0,2,2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3260,maxjeblick,2020-08-29T22:09:45Z,2020-10-06T17:54:49Z,auto_scale_batch_size won't reset current_epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When auto_scale_batch_size is enabled, the model is initially trained with varying batch sizes. When training begins, trainer.current_epoch equals 1 instead of 0.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Either observe the progress bar or use a simple callback to track the epoch number, once with  auto_scale_batch_size  enabled and once with  auto_scale_batch_size disabled.
 <denchmark-code>from pytorch_lightning import Callback
 
 class PrintCallback(Callback):
     
     def __init__(self):
         self.observed_epochs = []
         
     def on_train_epoch_start(self, trainer, pl_module):
         print(f'Current Epoch: {trainer.current_epoch}')
         self.observed_epochs.append(trainer.current_epoch)
 
 </denchmark-code>
 
 	",39b3704285e40a29a5862c4d8145b68d3b35d45e,maxjeblick,2020-10-06 19:54:48+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"107,108",,1.0,maxjeblick,2020-08-30T12:50:23Z,"
 		since it calls it with various batch_sizes did you find where it sets current_epoch to 0 before checking it on next batch_size?
 		",2.0,maxjeblick,2020-08-31T11:43:44Z,"
 		The problem is during <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/training_io.py#L335>model checkpointing</denchmark-link>
 . The checkpoint sets  . That checkpoint will be loaded after having completed the batch size finder. During batch size scaling, the epoch won't be increased.
 		",3.0,maxjeblick,2020-09-02T18:48:34Z,"
 		Currently blocked until trainer.tune is added.
 		",,,,,MODIFY,3.0,pytorch_lightning\tuner\batch_size_scaling.py,pytorch_lightning\tuner\batch_size_scaling.py,1.0,156,,__scale_batch_restore_params,trainer,154,166,MODIFY,1.0,tests\trainer\test_trainer_tricks.py,tests\trainer\test_trainer_tricks.py,1.0,"185,186",185,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_trainer_reset_correctly,tmpdir,167,200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,142,,__scale_batch_reset_params,"trainer,model,steps_per_trial",139,151,1.0,126,,__scale_batch_dump_params,trainer,122,136,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3276,s-rog,2020-08-31T01:33:53Z,2020-10-13T10:42:12Z,Logging non-tensor scalar with result breaks subsequent epoch aggregation,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Logging non-tensor scalar with result breaks subsequent epoch/tbptt aggregation
 (on both 0.9 and master)
 <denchmark-code>-- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap
     fn(i, *args)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_backend.py"", line 165, in ddp_train
     results = self.trainer.run_pretrain_routine(model)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 1237, in run_pretrain_routine
     self.train()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 396, in train
     self.run_training_epoch()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 543, in run_training_epoch
     self.run_training_epoch_end(epoch_output, checkpoint_accumulator, early_stopping_accumulator, num_optimizers)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 672, in run_training_epoch_end
     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 696, in __auto_reduce_results_on_epoch_end
     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py"", line 392, in reduce_across_time
     result[k] = tbptt_reduce_fx(value)
 TypeError: mean(): argument 'input' (position 1) must be Tensor, not list
 </denchmark-code>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-code>    def training_step(self, batch, batch_idx):
         x, y = batch[0], batch[1]
         x = self.forward(x)
         loss = self.loss(x, y)
         result = pl.TrainResult(loss)
         result.log(""non tensor scalar"", 1.0)
         result.log(""loss"", loss, on_step=False, on_epoch=True)
 </denchmark-code>
 
 <denchmark-h:h3>To Fix</denchmark-h>
 
 <denchmark-code>result.log(""non tensor scalar"", torch.tensor(1.0))
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 In log() of result objects, value should accept non tensor values as value: Any and not cause issues with other metrics to be logged
 <denchmark-h:h3>Additional context</denchmark-h>
 
 log() can be changed to only accept tensors, or have a built-in conversion, will update as I investigate further
 	",2d5a7f5e7dc686cfc8172101a81505bf421468af,William Falcon,2020-10-13 06:42:11-04:00,MODIFY,1,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"479,480,481",,1.0,s-rog,2020-09-01T14:51:45Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",2.0,s-rog,2020-09-15T16:52:44Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  is it still there? <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  mind check if it is still on master? or better add test for such case...
 		",3.0,s-rog,2020-09-16T01:26:44Z,"
 		I'll take a look again when I get a chance, haven't probed much due to the refactors... Are they mostly done?
 		",4.0,s-rog,2020-09-16T02:02:51Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  the bug is still here, the following template tests for this issue as well as <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3278>#3278</denchmark-link>
 
 For this problem the easiest fix would be to force type to tensors. Though that's probably just a bandaid solution, thoughts?
 
 Test template for reference
 #!/opt/conda/bin/python
 """"""
 Runs a model on a single node across multiple gpus.
 """"""
 import os
 from argparse import ArgumentParser
 
 import torch.nn.functional as F
 
 import pytorch_lightning as pl
 from pl_examples.models.lightning_template import LightningTemplateModel
 from pytorch_lightning import Trainer, seed_everything
 
 seed_everything(234)
 
 class custom_template(LightningTemplateModel):
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
     
     def on_epoch_start(self):
         print(""on_epoch_start"")
 
     def on_fit_start(self):
         print(""on_fit_start"")
         
     def training_step(self, batch, batch_idx):
         """"""
         Lightning calls this inside the training loop with the data from the training dataloader
         passed in as `batch`.
         """"""
         # forward pass
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         result = pl.TrainResult(loss)
         result.log(""non tensor scalar"", 1.0)
         result.log(""loss"", loss, on_step=False, on_epoch=True)
         return result
         
 
 def main(args):
     """""" Main training routine specific for this project. """"""
     # ------------------------
     # 1 INIT LIGHTNING MODEL
     # ------------------------
     model = custom_template(**vars(args))
 
     # ------------------------
     # 2 INIT TRAINER
     # ------------------------
     trainer = Trainer.from_argparse_args(args)
 
     # ------------------------
     # 3 START TRAINING
     # ------------------------
     trainer.fit(model)
 
 
 def run_cli():
     # ------------------------
     # TRAINING ARGUMENTS
     # ------------------------
     # these are project-wide arguments
     root_dir = os.path.dirname(os.path.realpath(__file__))
     parent_parser = ArgumentParser(add_help=False)
 
     # each LightningModule defines arguments relevant to it
     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
     parser = Trainer.add_argparse_args(parser)
     parser.set_defaults(gpus=1, distributed_backend=None)
     args = parser.parse_args()
 
     # ---------------------
     # RUN TRAINING
     # ---------------------
     main(args)
 
 
 if __name__ == '__main__':
     run_cli()
 
 
 		",MODIFY,2.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,"24,25,55,71",68,test__validation_step__log,tmpdir,13,72,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,s-rog,2020-09-17T01:01:44Z,"
 		I looked into it a bit and reduce_across_time is getting called on all metrics if one metric in results is logged with on_epoch=True which makes on_epoch=True only compatible with tensor scalars since the default tbptt fn is torch.mean
 This is probably not intended behavior?
 		",6.0,s-rog,2020-09-17T15:24:59Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
 
 		",7.0,s-rog,2020-09-17T15:57:06Z,"
 		<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  this is not related to metrics.
 <denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  I guess a simple type check that converts scalars to scalars tensor should do the trick? If so, could you open a PR with this fix?
 		",8.0,s-rog,2020-10-05T07:45:50Z,"
 		fixed by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3855>#3855</denchmark-link>
 
 		",9.0,s-rog,2020-10-13T08:16:31Z,"
 		<denchmark-code>Traceback (most recent call last):
   File ""/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/spawn.py"", line 59, in _wrap
     fn(i, *args)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/ddp_spawn_accelerator.py"", line 152, in ddp_train
     results = self.train_or_test()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/accelerators/accelerator.py"", line 53, in train_or_test
     results = self.trainer.train()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 483, in train
     self.train_loop.run_training_epoch()
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py"", line 598, in run_training_epoch
     self.num_optimizers
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py"", line 339, in log_train_epoch_end_metrics
     epoch_log_metrics, epoch_progress_bar_metrics = self.__auto_reduce_results_on_epoch_end(epoch_output)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py"", line 449, in __auto_reduce_results_on_epoch_end
     tbptt_outs = tbptt_outs[0].__class__.reduce_across_time(tbptt_outs)
   File ""/opt/conda/lib/python3.6/site-packages/pytorch_lightning/core/step_result.py"", line 483, in reduce_across_time
     result[k] = tbptt_reduce_fx(value.float())
 AttributeError: 'list' object has no attribute 'float'
 </denchmark-code>
 
 <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  I don't think the issue was fixed completely, this is on rc5 (using self.log)
 		",10.0,s-rog,2020-10-13T09:49:37Z,"
 		<denchmark-link:https://github.com/s-rog>@s-rog</denchmark-link>
  mind add a test for this case?
 		",11.0,s-rog,2020-10-13T10:43:22Z,"
 		william beat me to it :]
 		",,,,,,,,,,,,,,,,,,,,,reduce_across_time,"cls,time_outputs",457,489,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"24,25",,test__validation_step__log.training_step,"self,batch,batch_idx",20,27,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3280,hyukyu,2020-08-31T06:32:08Z,2020-09-11T14:55:59Z,Error in transfer_batch_to_device when None type is in the batch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 There should be no torchtext pre-installed
 Run the sample code
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>from torch.utils.data import DataLoader
 import pytorch_lightning as pl
 
 
 def collate_fn(batch):
     return batch
 
 
 class MyDataModule(pl.LightningDataModule):
     def __init__(self):
         super().__init__()
 
     def prepare_data(self):
         pass
 
     def setup(self, stage):
         self.train = [{""input"": torch.randn(1,2), ""output"": None}]
 
     def train_dataloader(self):
         return DataLoader(self.train, batch_size=1, collate_fn=collate_fn)
 
 
 class MyModel(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.linear = torch.nn.Linear(2, 1)
 
     def forward(self, x):
         return self.linear(x)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters())
 
     def training_step(self, batch, batch_idx):
         x = batch[0][""input""]
         y = batch[0][""output""]
         loss = self(x)
         result = pl.TrainResult(loss)
         result.log('train_loss', loss, on_epoch=True)
         return result
 
 
 def main():
     # Dataset
     data_module = MyDataModule()
 
     # Model
     model = MyModel()
 
     # Train
     trainer = pl.Trainer(max_steps=1)
     trainer.fit(model, datamodule=data_module)
 
 
 if __name__ == ""__main__"":
     main()
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The above code runs fine if the package torchtext is installed. However, the code raises following error if torchtext is not available and I believe this inconsistency is a bug.
 <denchmark-code> File ""python3.6/site-packages/pytorch_lightning/utilities/apply_func.py"", line 122, in batch_to
     return data.to(device, **kwargs)
 AttributeError: 'NoneType' object has no attribute 'to'
 python-BaseException
 </denchmark-code>
 
 I think this line of the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/f46318ebfeb785a659c49091a6871584ccde3ee1/pytorch_lightning/utilities/apply_func.py#L24>code</denchmark-link>
  is the cause of the problem
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 TITAN Xp
 TITAN Xp
 TITAN Xp
 TITAN Xp
 TITAN Xp
 TITAN Xp
 TITAN Xp
 TITAN Xp
 
 
 available:         True
 version:           10.2
 
 
 Packages:
 
 numpy:             1.18.4
 pyTorch_debug:     False
 pyTorch_version:   1.6.0
 pytorch-lightning: 0.9.0
 tensorboard:       2.2.1
 tqdm:              4.46.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 ELF
 
 
 processor:         x86_64
 python:            3.6.8
 version:           #81~16.04.1-Ubuntu SMP Tue Nov 26 16:34:21 UTC 2019
 
 
 
 	",bd5f53c51994e14c79404c9dcededae53b21b664,Adrian Wälchli,2020-09-11 10:55:58-04:00,MODIFY,1,pytorch_lightning\utilities\apply_func.py,pytorch_lightning\utilities\apply_func.py,1.0,"124,125",124,1.0,hyukyu,2020-08-31T06:32:47Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,hyukyu,2020-09-01T13:36:08Z,"
 		Thanks! Mind sending us a PR with the fix?
 		",3.0,hyukyu,2020-09-11T00:08:43Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  mind taking a look?
 		",4.0,hyukyu,2020-09-11T07:08:23Z,"
 		I submitted a fix.
 <denchmark-link:https://github.com/hyukyu>@hyukyu</denchmark-link>
  Be aware, torchtext will remove/deprecate this Batch object soon. So in the future you should switch to their new dataloading pipeline that integrates better with PyTorch and Lightning.
 		",MODIFY,1.0,tests\models\test_gpu.py,tests\models\test_gpu.py,1.0,"344,345,346,347,348,349",,test_single_gpu_batch_parse,,340,431,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,move_data_to_device,"Any,device",92,125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3303,NumesSanguis,2020-09-01T09:37:48Z,2020-09-21T09:46:50Z,AUROC metric should throw an error when used for multi-class problems,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 AUROC accepts multi-class input without throwing an error. Instead, it gives a random value, which gives the illusion that it is working.
 Background: <denchmark-link:https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7>https://forums.pytorchlightning.ai/t/pytorch-lightning-auroc-value-for-multi-class-seems-to-be-completely-off-compared-to-sklearn-using-it-wrong/61/7</denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Manually create some multi-class arrays
 Use PyTorch Lightning's AUROC() metric
 Use sklearn's AUROC metric
 Observe values not matching
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import torch
 import sklearn.metrics
 import pytorch_lightning as pl
 from pytorch_lightning.metrics.classification import AUROC
 
 pl.seed_everything(0)
 auroc = AUROC()
 
 def test_auroc_sk_multiclass():
     for i in range(100):
         target = torch.randint(0, 3, size=(10,))  # 2 --> 3
         pred = torch.rand(10, 3).softmax(dim=1)  # torch.randint(0, 2, size=(10, ))
         score_sk = sklearn.metrics.roc_auc_score(target.numpy(), pred.numpy(), multi_class='ovo', labels=[0, 1, 2])
         score_pl = auroc(pred, target)
         print(score_sk, score_pl)
         assert torch.allclose(torch.tensor(score_pl).float(), torch.tensor(score_sk).float())
 
 test_auroc_sk_multiclass()
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 
 Throw error that multi-class AUROC has not been implemented (yet).
 Note in documentation that the AUROC metric does not support multi-class yet ()
 
 <denchmark-h:h3>Actual behavior</denchmark-h>
 
 Giving a random value, giving a false sense that it is working.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 		- GeForce GTX 1080 Ti
 	- available:         True
 	- version:           10.2
 * Packages:
 	- numpy:             1.19.1
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0
 	- pytorch-lightning: 0.9.0
 	- tensorboard:       2.2.0
 	- tqdm:              4.48.2
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.7.7
 	- version:           #113-Ubuntu SMP Thu Jul 9 23:41:39 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 The output value is nonsense in the multi-class classification, instead of an error. That's why I thought it was appropriate to file it as a bug instead of a feature request / documentation improvement. I'm aware that the AUROC implementation is not intended to be for multi-class after the discussion on the PyTorch Lightning forum.
 I used the AUROC value and noticed it was wrong after training a few models, but it will take many people off-guard in it's current form.
 Feature request for MulticlassAUROC: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3304>#3304</denchmark-link>
 
 	",b1347c956af4752560b53b891d352c48c6050305,Nicki Skafte,2020-09-21 11:46:48+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,20,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\metrics\classification.py,pytorch_lightning\metrics\classification.py,0.0,"377,380","377,380",,,,,MODIFY,0.0,pytorch_lightning\metrics\functional\classification.py,pytorch_lightning\metrics\functional\classification.py,0.0,"587,590,592,685,688,690,861,863,865,866,867,868","587,590,592,685,688,690,861,863",MODIFY,3.0,tests\metrics\functional\test_classification.py,tests\metrics\functional\test_classification.py,1.0,"48,49,50,51,52,53,55,56,59,60,61","48,49,52,53,59,61",test_against_sklearn,"sklearn_metric,torch_metric",48,61,MODIFY,1.0,tests\metrics\test_classification.py,tests\metrics\test_classification.py,1.0,121,121,test_auroc,pos_label,117,123,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"406,407,408,409,410,411",,test_error_on_multiclass_input,metric,406,411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"55,56,59,60,61,62,63,64,71,72,73,74,75,76,77,78,79","59,61",test_against_sklearn,"sklearn_metric,torch_metric,only_binary",55,79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3335,junwen-austin,2020-09-02T21:51:08Z,2020-10-05T17:52:47Z,Cannot replicate training results with seed_everything and deterministic flag = True with DDP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I noticed this when I was adding more metrics calculation to the LightningModule, for example, adding the confusion matrix at the end of validation/test epoch.  Before and after I added these functions (which do not appear to be dependent on any random seed), I noticed the training results are not the exactly the same.
 However, once I added these function and re-ran again, yes I got the same training results.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The training results should be identical even if some deterministic functions are added
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 
 PyTorch Version (e.g., 1.0):  1.4
 OS (e.g., Linux):  Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.7
 CUDA/cuDNN version: 10.1
 GPU models and configuration: 4 GPUs DDP
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",a71d62d8409f4960a4b438b8d19c924d3636c73f,Adrian Wälchli,2020-09-20 19:42:58-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,71,,1.0,junwen-austin,2020-09-04T04:12:37Z,"
 		Thanks for the report. We're also seeing something similar in our CI, unfortunately we don't have a lead yet.
 I did not exactly understand how you came to the conclusion that it is related to metrics. Did you try several runs with and without having metric? I don't think it is metrics, but rather just a ddp issue.
 		",2.0,junwen-austin,2020-09-04T16:20:52Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Thanks for your follow-up. No, I do not think this issue has anything to do with the metrics that I was adding but more likely to DDP.
 		",3.0,junwen-austin,2020-09-20T21:37:48Z,"
 		<denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>
  I may have fixed this problem with the linked PR. But to be sure it closes this issue, could you let me know the trainer flags you used? Did you use  (is also the default) together with
 ?
 		",4.0,junwen-austin,2020-09-20T21:46:17Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Thanks for letting me know the progress. I used the ddp as the backend, not the ddp_spawn. Thanks
 		",MODIFY,1.0,pytorch_lightning\accelerators\ddp_base_backend.py,pytorch_lightning\accelerators\ddp_base_backend.py,1.0,"101,102,103,104,105",,ddp_train_tmp,"self,process_idx,mp_queue,model,is_master,proc_offset",89,182,MODIFY,1.0,pytorch_lightning\utilities\seed.py,pytorch_lightning\utilities\seed.py,1.0,"28,29,30,31,32,33,34,35,36,37,44,45,57","28,29,36,37,38",,,,,,,,,,,,,,,,,,,,,,,5.0,junwen-austin,2020-09-20T21:54:56Z,"
 		hmm, that's unfortunate because my fix only applies to ddp_spawn.
 for ddp, I ran several scripts (including my research) and always got deterministic behavior, so I am not sure what what the cause is in your case :(
 		",6.0,junwen-austin,2020-09-20T22:25:38Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  it is rather a tricky one. I have two versions of the Lightning models v1 and v2. The only difference between them is that I added additional metric (confusion matrix in this case) in v2, and I noticed the training/validation/test results are slightly off, with both case having ddp as backend, same seed for seed_everything and deterministic flag = True.
 Since the additional code about the confusion matrix has nothing to do with randomization, I expect I should get the exactly the same results.
 S
 		",7.0,junwen-austin,2020-09-20T23:41:29Z,"
 		but the confusion matrix is not used for training, right? It is just there for visualization?
 So, just to clarify to be 100% sure we're talking about the same
 
 you get the same training loss if you rerun v1 multiple times
 you get the same training loss if you rerun v2 multiple times
 the training loss between v1 and v2 are however different
 
 sorry if I misunderstood
 		",8.0,junwen-austin,2020-09-21T00:56:38Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 
 
 Confusion matrix is used at the end of validation/test loop but not during the training loop
 
 
 you get the same training loss if you rerun v1 multiple times: Yes, also same validation/test loss
 
 
 you get the same training loss if you rerun v2 multiple times: Yes, also same validation/test loss
 
 
 the training loss between v1 and v2 are however different: Yes, also different validation/test loss
 
 
 Sorry about any miscommunication on my part. Thanks
 		",9.0,junwen-austin,2020-09-22T20:41:58Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  able to repro? any clue?
 		",10.0,junwen-austin,2020-09-22T21:14:16Z,"
 		Unfortunately not. I tried to reproduce by adding the confusion matrix to the validation epoch end as described, but this gives me same val and train loss as it is expected with setting the seed. <denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>
  I think I need an example code from you or I don't know where to look for the problem.
 		",11.0,junwen-austin,2020-09-22T21:38:18Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  Thanks for looking into it. Sure, the pseudo code looks like:
 In v1, there is no confusion matrix at all or any calculation related to it.
 in v2, I added the following in the __init__ function:
 self.confusion_matrix = ConfusionMatrix()
 then at the end of validation/test epoch, I calculated confusion matrix based on sync'ed y and y_pred and then log each component of confusion matrix. Note: in both v1 and v2, y and y_pred are sync'ed before calculating any metric
 <denchmark-code>           def validation_epoch_end(self, result):
                y = result['y']
                y_pred = result['y_pred']
                
                if use_ddp:
                    sync y
                    sync y_pred
             
               # calculate other metrics
                    ...
              
               # start the code only in v2:
               cm = self.confusion_matrix(y_pred, y)
               
              # log each component of cm:
              self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)
              self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)
              self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)
              self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)
              
              # end the code only in v2
 
 
 
 </denchmark-code>
 
 		",12.0,junwen-austin,2020-09-23T09:11:35Z,"
 		I had it almost like this, but without sync ddp. Can you share the whole runnable script? At this point I can only make guesses.
 		",13.0,junwen-austin,2020-09-23T14:13:21Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 Unfortunately I cannot share the entire script but I will provide more information in hoping of getting what you need.
 I have the following helper function with the Lightning Module:
 <denchmark-code>def calculate(self, batch):
     data, y = batch
     logit = self.model(data).     # y_hat is probability
     loss = torch.nn.functional.CrossEntropy(logit, y)
     y_hat = torch.nn.Softmax(dim=1)(logit)     
     _, y_pred = torch.max(y_hat, dim=1)    # y_pred is the predicted label
    
    return loss, y, y_hat, y_pred
 
 def compute_metrics(self, y, y_hat, y_pred):
       # compute metrics given y, y_hat, y_pred
       acc = self.accuracy(y_pred, y)/self.trainer.world_size
       f1 = self.f1(y_pred,y) / self.trainer.world_size
       cm = self.confusion_matrix(y_pred, y)    # added in v2
      
       return acc, f1, cm
  
 @staticmethod
 def log_metrics(result, kind, scalars=('loss','accuracy','f1'):
         for scalar in scalars:
                   result.log(scalar + '/' + kind, scalar, on_step=False, on_epoch=True, sync_dist=True, sync_dist_op='mean')
 
 
 def train_valid_test_step(self, batch, kind):
           loss, y, y_hat, y_pred = self.calculate(batch)
           if kind == 'train':
                 result = pl.TrainResult(loss)
           else:
                 result = pl.EvalResult(checkpoint_on=loss)
                 result.val_loss = loss
 
           result.y =y
           result.y_hat = y_hat
           result.y_pred = y_pred
  
 def sync_across_gpus(self, t):   # t is a tensor
        
         gather_t_tensor = [torch.ones_like(t) for _ in range(self.trainer.world_size)]
         torch.distributed.all_gather(gather_t_tensor, t)
         return torch.cat(gather_t_tensor)          
 
 def valid_test_epoch_end(self, result, kind):
     loss = result.val_loss
     y = result.y
     y_hat = result.y_hat
     y_pred = result.y_pred
 
    if self.trainer.use_ddp:
           loss = self.sync_across_gpus(loss)
           y = self.sync_across_gpus(y)
           y_pred = self.sync_across_gpus(y_pred)
           y_hat = self.sync_across_gpus(y_hat)
          
    acc, f1, cm = self.compute_metrics(y, y_hat, y_pred)
    result.loss = loss.mean()
    result.accurancy = acc
    result.f1 = f1
    result.cm = cm # added in v2
    
    log_metrics(result, kind, scalars=('loss','accuracy','f1')
 
    # added in v2
    self.logger.add_scalar('TP', cm[1,1], global_step=self.global_step)
    self.logger.add_scalar('FP', cm[0,1], global_step=self.global_step)
    self.logger.add_scalar('FN', cm[1,0], global_step=self.global_step)
    self.logger.add_scalar('TN', cm[0,0], global_step=self.global_step)
 
 
 def training_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'train')
 
 def train_step_end(self, result):
      loss = result.minimize
      result.log('loss/train', loss, on_step=True, sync_dist=True, sync_dist_op='mean')
      return result
 
 
 def validation_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'valid')
 
 def validation_epoch_end(self, result):
      
      return self.valid_test_epoch_end(result,'valid')
 
 def test_step(self, batch, batch_idx):
      return self.train_valid_test_step(batch, 'test')
 
 def test_epoch_end(self, result):
      
      return self.valid_test_epoch_end(result,'test')
     
 </denchmark-code>
 
 		",14.0,junwen-austin,2020-09-28T05:42:10Z,"
 		still no luck reproducing this. since you cannot share all the code, I had tried to combine what you show here with an existing example code, but no luck.
 maybe if you tried yourself to adapt our <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/basic_examples/mnist.py>mnist example</denchmark-link>
  with your confusion matrix use case we get a reproducible script. I tried but I failed.
 		",15.0,junwen-austin,2020-09-28T16:58:43Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 Thanks I will do that and let you know.
 		",seed_everything,None,27,62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,junwen-austin,2020-10-05T17:52:47Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  My apology I cannot replicate the issue seen from my work project. Let's close the issue for now and if I spot a similar one, I will have better documentation of it instead of trying to getting it from top of my head. Thanks.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3393,patrickorlando,2020-09-08T06:28:36Z,2020-09-09T09:38:27Z,"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using the MLFlow logger, with a remote server, logging per step introduces latency which slows the training loop.
 I have tried to configure logging of metrics only per epoch, however it seems this still results in much slower performance. I suspect the logger is still communicating with the MLFlow server on each training step.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 
 Start an MLFlow server locally
 
 <denchmark-code>mlflow ui
 </denchmark-code>
 
 
 Run the minimal code example below as is, (with MLFlow logger set to use the default file uri.)
 Uncomment out the tracking_uri to use the local MLFlow server and run the code again. You will see a 2-3 times drop in the iterations per second.
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import torch
 from torch.utils.data import TensorDataset, DataLoader
 import pytorch_lightning as pl
 
 class MyModel(pl.LightningModule):
     def __init__(self):
         super().__init__()
         self.num_examples = 5000
         self.num_valid = 1000
         self.batch_size = 64
         self.lr = 1e-3
         self.wd = 1e-2
         self.num_features = 2
         self.linear = torch.nn.Linear(self.num_features, 1)
         self.loss_func = torch.nn.MSELoss()
         self.X = torch.rand(self.num_examples, self.num_features)
         self.y = self.X.matmul(torch.rand(self.num_features, 1)) + torch.rand(self.num_examples)
         
     def forward(self, x):
         return self.linear(x)
 
     def train_dataloader(self): 
         ds = TensorDataset(self.X[:-self.num_valid], self.X[:-self.num_valid])
         dl = DataLoader(ds, batch_size=self.batch_size)
         return dl
 
     def val_dataloader(self): 
         ds = TensorDataset(self.X[-self.num_valid:], self.X[-self.num_valid:])
         dl = DataLoader(ds, batch_size=self.batch_size)
         return dl
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         yhat = self(x)
         loss = self.loss_func(yhat, y)
         result = pl.TrainResult(minimize=loss)
         result.log('train_loss', loss, on_epoch=True, on_step=False)
         return result
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         yhat = self(x)
         loss = self.loss_func(yhat, y)
         result = pl.EvalResult(early_stop_on=loss)
         result.log('val_loss', loss, on_epoch=True, on_step=False)
         return result
 
 if __name__ == '__main__':
     from pytorch_lightning.loggers import TensorBoardLogger, MLFlowLogger
     mlf_logger = MLFlowLogger(
         experiment_name=f""MyModel"",
         # tracking_uri=""http://localhost:5000""
     )
     trainer = pl.Trainer(
         min_epochs=5,
         max_epochs=50,
         early_stop_callback=True,
         logger=mlf_logger
     )
     model = MyModel()
     trainer.fit(model)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 When using the TrainResult and EvalResult, or manually handling metric logging using the training_epoch_end and validation_epoch_end callbacks. It should be possible to avoid the MLFlow logger from communicating with the server in each training loop.
 This would make it feasible to implement the MLFlow when a remote server is used for experiment tracking.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 	- available:         False
 	- version:           None
 * Packages:
 	- numpy:             1.18.2
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0+cpu
 	- pytorch-lightning: 0.9.0
 	- tensorboard:       2.2.0
 	- tqdm:              4.48.2
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		-
 	- processor:         x86_64
 	- python:            3.7.9
 	- version:           #1 SMP Tue May 26 11:42:35 UTC 2020
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 We host a MLFlow instance in AWS and would like to be able to track experiments without affecting the training speed.
 It appears that in general the MLFlow logger is much less performant than the default Tensorboard Logger, but this would not be much of a problem if we could avoid calls to the logger during the training loop.
 <denchmark-h:h3>Solution</denchmark-h>
 
 I've done a bit of debugging in the codebase and have been able to isolate the cause in two places
 
 
 
 pytorch-lightning/pytorch_lightning/loggers/mlflow.py
 
 
         Lines 125 to 129
       in
       d438ad8
 
 
 
 
 
 
  @property 
 
 
 
  def run_id(self): 
 
 
 
  # create the experiment if it does not exist to get the run id 
 
 
 
  _ = self.experiment 
 
 
 
  return self._run_id 
 
 
 
 
 
 Here self.experiment is called regardless of whether self._run_id exists. If we add an if not self._run_id here we avoid calling self._mlflow_client.get_experiment_by_name(self._experiment_name) on each step.
 However we still call it each time we log metrics to MFlow, because of the property self.experiment.
 
 
 
 pytorch-lightning/pytorch_lightning/loggers/mlflow.py
 
 
         Lines 100 to 112
       in
       d438ad8
 
 
 
 
 
 
  @property 
 
 
 
  @rank_zero_experiment 
 
 
 
  def experiment(self) -> MlflowClient: 
 
 
 
  r"""""" 
 
 
 
          Actual MLflow object. To use MLflow features in your 
 
 
 
          :class:`~pytorch_lightning.core.lightning.LightningModule` do the following. 
 
 
 
   
 
 
 
          Example:: 
 
 
 
   
 
 
 
              self.logger.experiment.some_mlflow_function() 
 
 
 
   
 
 
 
          """""" 
 
 
 
  expt = self._mlflow_client.get_experiment_by_name(self._experiment_name) 
 
 
 
 
 
 Here if we store expt within the logger and only call self._mlflow_client.get_experiment_by_name when it does not exist, we eliminate all overhead, it runs as fast as fast as the tensorboard logger and all the mlflow logging appears to be working as expected.
 I'd be happy to raise a PR for this fix.
 	",656c1af0df0cd0a8102a69c9c5045e86dc2b6b3a,Patrick Orlando,2020-09-09 11:38:26+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"46,47",,1.0,patrickorlando,2020-09-08T06:29:17Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,patrickorlando,2020-09-08T08:11:43Z,"
 		have you tried to just increase the row_log_interval, its a trainer flag that controls how often logs are sent to the logger.
 I mean, your network is a single linear layer, you probably run through epochs super fast.
 I am not yet convinced it is a bug, but I'll try your example code
 		",3.0,patrickorlando,2020-09-08T08:18:48Z,"
 		hey <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 , Thanks for replying!
 The model above is a contrived example, upon further testing I have realised that the performance difference between MFLow logger and the Tensorboard logger is not inherent to the MLFlow client.
 I've done some debugging and added a solution section to the issue. It appears to be in in the experiment property of the MLFlowLogger. Each time .experiment is accessed, self._mlflow_client.get_experiment_by_name(self._experiment_name) is called, which communicates with the MLFlow server.
 It seems we can store the response of this method thereby needing to call it only once, and this seems to resolve the dramatic difference between the Tensorboard and MLFlow Logger.
 		",4.0,patrickorlando,2020-09-08T08:21:54Z,"
 		oh ok, that makes sense. Would you like to send a PR with your suggestion and see if the tests pass? Happy to review it.
 		",MODIFY,1.0,pytorch_lightning\loggers\mlflow.py,pytorch_lightning\loggers\mlflow.py,1.0,"112,113,114,115,116,117,118,119,120","112,113,114,115,116,117,118,119,120",experiment,self,102,123,MODIFY,1.0,tests\loggers\test_mlflow.py,tests\loggers\test_mlflow.py,1.0,"47,48,49,50,51,52,53,54",,,,,,,,,,,,,,,,,,,,,,,,5.0,patrickorlando,2020-09-08T08:24:26Z,"
 		yeah sure, I'll link it here shortly.
 		",6.0,patrickorlando,2020-09-08T08:38:24Z,"
 		Did you encounter this <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3392>#3392</denchmark-link>
  problem as well?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_mlflow_experiment_id_retrieved_once,tmpdir,47,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3417,Vozf,2020-09-09T12:37:07Z,2020-09-09T13:27:36Z,CometLogger failing without save_dir,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Cometmllogger with api key and  without save dir results in error.
 This happens due to this if <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L135>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L135</denchmark-link>
 
 _save_dir is not set and later train loop tries to read it and fails.
 This can be fixed by setting _save_dir to None. I will supply PR in a moment
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 <denchmark-code>    model = LightningModel({})
     comet_logger = CometLogger(
         api_key=KEY,
         workspace=""workspace""
     )
 
     trainer = Trainer(logger=comet_logger)
     trainer.fit(model)
 </denchmark-code>
 
 Traceback (most recent call last):
 trainer.fit(model)
 File ""/python3.8/site-packages/pytorch_lightning/trainer/states.py"", line 48, in wrapped_fn
 result = fn(self, *args, **kwargs)
 File ""/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1073, in fit
 results = self.accelerator_backend.train(model)
 File ""/python3.8/site-packages/pytorch_lightning/accelerators/gpu_backend.py"", line 51, in train
 results = self.trainer.run_pretrain_routine(model)
 File ""/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 1239, in run_pretrain_routine
 self.train()
 File ""/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 363, in train
 self.on_train_start()
 File ""/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py"", line 111, in on_train_start
 callback.on_train_start(self, self.get_model())
 File ""/python3.8/site-packages/pytorch_lightning/utilities/distributed.py"", line 27, in wrapped_fn
 return fn(*args, **kwargs)
 File ""/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py"", line 296, in on_train_start
 save_dir = trainer.logger.save_dir or trainer.default_root_dir
 File ""/python3.8/site-packages/pytorch_lightning/loggers/comet.py"", line 253, in save_dir
 return self._save_dir
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",5b4db52851000d5e4eca8c680d851bcdaafc3a80,Alexander,2020-09-09 09:27:35-04:00,MODIFY,0,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,0.0,142,,1.0,Vozf,2020-09-09T12:37:49Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3424,GimmickNG,2020-09-09T16:18:23Z,2020-10-01T08:33:13Z,DataModule with lr_find not supported,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Create a Trainer (trainer), a LightningModule (model) and a DataModule (data_module)
 Call trainer.lr_find(model, datamodule=data_module)
 Error: TypeError: lr_find() got an unexpected keyword argument 'datamodule'
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import torch
 import torch.nn as nn
 import pytorch_lightning as pl
 from torch.utils.data import DataLoader
 class DataModule(pl.LightningDataModule):
     def __init__(self):
         super().__init__()
     def gen_set(self, start, end):
         vals = torch.tensor([float(i) for i in range(0, 800)])
         return torch.stack([torch.sin(vals), torch.cos(vals)], 1)
     def train_dataloader(self):
         return DataLoader(dataset=self.gen_set(0, 800))
     def val_dataloader(self):
         return DataLoader(dataset=self.gen_set(800, 900))
     def test_dataloader(self):
         return DataLoader(dataset=self.gen_set(900, 1000))
 
 class LitModel(pl.LightningModule):
     def __init__(self, in_features):
         super().__init__()
         self.in_layer = nn.Linear(in_features, in_features//2)
         self.out_layer = nn.Linear(in_features//2, in_features)
         self.criterion = nn.MSELoss()
         self.lr = 0.001
     def forward(self, inp):
         inp = torch.tanh(self.in_layer(inp))
         return self.out_layer(inp)
     def training_step(self, batch, batch_idx):
         x = batch
         y_hat = self(x)
         loss = self.criterion(y_hat, x.float())
         return pl.TrainResult(minimize=loss)
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=(self.lr or self.learning_rate))
     
 data_module = DataModule()
 trainer = pl.Trainer()
 model = LitModel(in_features=2)
 lr_finder = trainer.lr_find(model, datamodule=data_module)   #error
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The lr_find() function should work as if it were passed a train_loader and test_loader when invoking it, just like fit()
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.5.1
 pytorch-lightning: 0.9.0
 tensorboard:       2.2.0
 tqdm:              4.45.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 processor:         x86_64
 python:            3.7.6
 
 
 
 	",e4e60e9b82adc48482db4721ce3e1fdc3ab6d6fe,GimmickNG,2020-10-01 10:33:12+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"30,31",,1.0,GimmickNG,2020-09-09T16:19:18Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,GimmickNG,2020-09-10T17:13:45Z,"
 		<denchmark-link:https://github.com/nateraw>@nateraw</denchmark-link>
  take a look?
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\tuner\lr_finder.py,pytorch_lightning\tuner\lr_finder.py,1.0,76,,lr_find,"trainer,LightningModule,None,DataLoader,None,float,float,int,str,float,None",66,76,MODIFY,1.0,pytorch_lightning\tuner\tuning.py,pytorch_lightning\tuner\tuning.py,1.0,54,,MODIFY,1.0,tests\trainer\test_lr_finder.py,tests\trainer\test_lr_finder.py,1.0,"156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177",,test_datamodule_parameter,tmpdir,156,177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,lr_find,"self,LightningModule,None,DataLoader,None,float,float,int,str,float,None",44,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3487,Tim-Chard,2020-09-13T11:27:42Z,2020-09-15T16:41:28Z,Gradient norms are not logged unless row_log_interval==1,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 In version 0.9 the guards to calculate the gradient norms and then log the metrics can't be satisfied in the same batch unless the row_log_interval  is 1. In most places the guard seems to be (batch_idx + 1) % self.row_log_interval == 0 such as here:
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 749 to 757
       in
       b40de54
 
 
 
 
 
 
  def save_train_loop_metrics_to_loggers(self, batch_idx, batch_output): 
 
 
 
  # when metrics should be logged 
 
 
 
  should_log_metrics = (batch_idx + 1) % self.row_log_interval == 0 or self.should_stop 
 
 
 
  if should_log_metrics or self.fast_dev_run: 
 
 
 
  # logs user requested information to logger 
 
 
 
  metrics = batch_output.batch_log_metrics 
 
 
 
  grad_norm_dic = batch_output.grad_norm_dic 
 
 
 
  if len(metrics) > 0 or len(grad_norm_dic) > 0: 
 
 
 
  self.log_metrics(metrics, grad_norm_dic) 
 
 
 
 
 
 However in run_batch_backward_pass it is batch_idx % self.row_log_interval == 0
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 929 to 939
       in
       b40de54
 
 
 
 
 
 
  def run_batch_backward_pass(self, split_batch, batch_idx, opt_idx, optimizer): 
 
 
 
  # ------------------ 
 
 
 
  # GRAD NORMS 
 
 
 
  # ------------------ 
 
 
 
  # track gradient norms when requested 
 
 
 
  grad_norm_dic = {} 
 
 
 
  if batch_idx % self.row_log_interval == 0: 
 
 
 
  if float(self.track_grad_norm) > 0: 
 
 
 
  model = self.get_model() 
 
 
 
  grad_norm_dic = model.grad_norm( 
 
 
 
  self.track_grad_norm) 
 
 
 
 
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Run the code sample below (taken from #1527 ).
 Confirm that gradients are not being logged in tensorboard.
 Change row_log_interval to 1 and rerun the code.
 4 Confirm that gradients are now being logged.
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import pytorch_lightning as pl
 from torch.utils.data import TensorDataset, DataLoader
 from pytorch_lightning.loggers.tensorboard import TensorBoardLogger
 from torch.optim import SGD
 import torch.nn as nn
 import torch
 
 
 class MWENet(pl.LightningModule):
     def __init__(self):
         super(MWENet, self).__init__()
 
         self.first = nn.Conv2d(1, 1, 3)
         self.second = nn.Conv2d(1, 1, 3)
         self.loss = nn.L1Loss()
 
     def train_dataloader(self):
         xs, ys = torch.zeros(16, 1, 10, 10), torch.ones(16, 1, 6, 6)
         ds = TensorDataset(xs, ys)
         return DataLoader(ds)
 
     def forward(self, xs):
         out = self.first(xs)
         out = self.second(out)
         return out
 
     def configure_optimizers(self):
         first = SGD(self.first.parameters(), lr=0.01)
         second = SGD(self.second.parameters(), lr=0.01)
         return [second, first]
 
     def training_step(self, batch, batch_idx, optimizer_idx):
         xs, ys = batch
         out = self.forward(xs)
         return {'loss': self.loss(out, ys)}
 
 
 net = MWENet()
 logger = TensorBoardLogger('tb_logs', name='testing')
 trainer = pl.Trainer(
     track_grad_norm=2,
     row_log_interval=2,
     max_epochs=50,
     logger=logger)
 trainer.fit(net)
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Gradients should be logged if track_grad_norm is True
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU:
 - GeForce GTX 1080 Ti
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.19.1
 - pyTorch_debug:     False
 - pyTorch_version:   1.6.0
 - pytorch-lightning: 0.9.0
 - tensorboard:       2.2.1
 - tqdm:              4.48.2
 System:
 - OS:                Windows
 - architecture:
 - 64bit
 - WindowsPE
 - processor:         AMD64 Family 23 Model 113 Stepping 0, AuthenticAMD
 - python:            3.7.9
 - version:           10.0.19041
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",4ed96b2eb471124184144f96d259055e49ac97e7,Adrian Wälchli,2020-09-15 18:41:27+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"61,62",,1.0,Tim-Chard,2020-09-13T11:28:24Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,Tim-Chard,2020-09-13T11:44:15Z,"
 		yeah, it's a bug, mind send a PR?
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"379,380,383,384","379,380,383,384",_track_gradient_norm,"self,batch_idx",378,384,MODIFY,1.0,tests\models\test_grad_norm.py,tests\models\test_grad_norm.py,1.0,"80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_grad_tracking_interval,"tmpdir,row_log_interval",80,101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3549,pbmstrk,2020-09-18T13:00:13Z,2020-09-20T00:00:51Z,Bug in validation_epoch_end,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 In the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html?highlight=validation_epoch_end#validation-epoch-end>documentation</denchmark-link>
 ,  is described as running at the end of a validation epoch and does not need to necessarily return anything.
 When running a slightly modified version of the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/new_project.html>example</denchmark-link>
  in the docs,  seemingly runs once on a single batch of validation data and returns an error if nothing is returned.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 A MWE:
 <denchmark-code>import os
 import torch
 import torch.nn.functional as F
 from torchvision.datasets import MNIST
 from torchvision import transforms
 from torch.utils.data import DataLoader
 import pytorch_lightning as pl
 from torch.utils.data import random_split
 
 class LitModel(pl.LightningModule):
 
     def __init__(self):
         super().__init__()
         self.layer_1 = torch.nn.Linear(28 * 28, 128)
         self.layer_2 = torch.nn.Linear(128, 10)
 
     def forward(self, x):
         x = x.view(x.size(0), -1)
         x = self.layer_1(x)
         x = F.relu(x)
         x = self.layer_2(x)
         return x
 
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
         return optimizer
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
 
         return {'loss': loss}
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         
         return {""val_loss"": loss}
 
     def validation_epoch_end(self, validation_step_outputs):
         print(""Length of outputs: {}"".format(len(validation_step_outputs)))
         # a dummy return value to avoid error
         return {""val_loss"": 0}
 
 model = LitModel()
 
 dataset = MNIST(os.getcwd(), download=True, train=False, transform=transforms.ToTensor())
 train, val = random_split(dataset, [9000, 1000])
 
 train_loader = DataLoader(train)
 val_loader = DataLoader(val)
 
 # train
 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
 trainer.fit(model, train_loader, val_loader)
 </denchmark-code>
 
 <denchmark-h:h4>Output</denchmark-h>
 
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 CUDA_VISIBLE_DEVICES: [0]
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given
   warnings.warn(*args, **kwargs)
 Length of outputs: 1
 Length of outputs: 1000
 Length of outputs: 1000
 Length of outputs: 1000
 Length of outputs: 1000
 Saving latest checkpoint..
 Length of outputs: 1000
 </denchmark-code>
 
 If no return value is given in validation_epoch_end, the following error occurs
 <denchmark-code>---------------------------------------------------------------------------
 AttributeError                            Traceback (most recent call last)
 <ipython-input-183-013dcacba267> in <module>()
      10 # train
      11 trainer = pl.Trainer(progress_bar_refresh_rate=0, max_epochs=5, weights_summary=None, gpus=1)
 ---> 12 trainer.fit(model, train_loader, val_loader)
 
 7 frames
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/states.py in wrapped_fn(self, *args, **kwargs)
      46             if entering is not None:
      47                 self.state = entering
 ---> 48             result = fn(self, *args, **kwargs)
      49 
      50             # The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    1071             self.accelerator_backend = GPUBackend(self)
    1072             model = self.accelerator_backend.setup(model)
 -> 1073             results = self.accelerator_backend.train(model)
    1074 
    1075         elif self.use_tpu:
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/gpu_backend.py in train(self, model)
      49 
      50     def train(self, model):
 ---> 51         results = self.trainer.run_pretrain_routine(model)
      52         return results
      53 
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in run_pretrain_routine(self, model)
    1222 
    1223         # run a few val batches before training starts
 -> 1224         self._run_sanity_check(ref_model, model)
    1225 
    1226         # clear cache before training
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py in _run_sanity_check(self, ref_model, model)
    1255             num_loaders = len(self.val_dataloaders)
    1256             max_batches = [self.num_sanity_val_steps] * num_loaders
 -> 1257             eval_results = self._evaluate(model, self.val_dataloaders, max_batches, False)
    1258 
    1259             # allow no returns from eval
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in _evaluate(self, model, dataloaders, max_batches, test_mode)
     397 
     398         # log callback metrics
 --> 399         self.__update_callback_metrics(eval_results, using_eval_result)
     400 
     401         # Write predictions to disk if they're available.
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/evaluation_loop.py in __update_callback_metrics(self, eval_results, using_eval_result)
     429                         flat = {'val_loss': eval_result}
     430                     else:
 --> 431                         flat = flatten_dict(eval_result)
     432                     self.callback_metrics.update(flat)
     433             else:
 
 /usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/parsing.py in flatten_dict(source, result)
     113         result = {}
     114 
 --> 115     for k, v in source.items():
     116         if isinstance(v, dict):
     117             _ = flatten_dict(v, result)
 
 AttributeError: 'NoneType' object has no attribute 'items'
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The outputs in validation_epoch_end should contain results from all batches. Is a test validation batch running in the background, which could explain why the results of only one batch are included in the first loop.
 Return values in  were addressed in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2438>#2438</denchmark-link>
  (with the same error) and from what I can tell included in the 0.8.4 release - perhaps I am missing something?
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch: 1.6.0
 pytorch-lightning: 0.9.0
 
 	",9acee67c31c84dac74cc6169561a483d3b9c9f9d,William Falcon,2020-09-19 20:00:50-04:00,MODIFY,1,pytorch_lightning\trainer\connectors\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector.py,1.0,"107,108,109",,1.0,pbmstrk,2020-09-18T13:01:10Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,pbmstrk,2020-09-18T15:17:40Z,"
 		Hi there! What you are seeing in the first output is caused by our ""validation sanity check"". We run a configurable number of validation steps before training starts to ensure the user doesn't run into any issues after a potentially timely series of training steps. This can be configured (or turned off) with pl.Trainer(num_sanity_val_steps=0).
 Regarding returning None from validation_epoch_end, it appears that functionality has recently been removed, I will look into whether that was intentional or not and update the code/docs accordingly.
 		",3.0,pbmstrk,2020-09-18T19:10:08Z,"
 		<denchmark-link:https://github.com/teddykoker>@teddykoker</denchmark-link>
  That makes sense. Thanks for the pointer for configuring this!
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\logging.py,pytorch_lightning\trainer\logging.py,1.0,"76,77,78,79,140","76,77,78,139",process_output,"self,output,train",55,150,MODIFY,2.0,tests\base\model_valid_epoch_ends.py,tests\base\model_valid_epoch_ends.py,1.0,"45,46,47",,MODIFY,1.0,tests\trainer\test_trainer_steps_result_return.py,tests\trainer\test_trainer_steps_result_return.py,1.0,"628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649",,test_eval_loop_return_none,tmpdir,628,649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,validation_epoch_end_return_none._mean,"res,key",45,47,_log_on_evaluation_epoch_end_metrics,"self,eval_results,using_eval_result",106,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"36,37,38,39,40,41,42,43,44,45,46,47,48,49",,validation_epoch_end_return_none,"self,outputs",36,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3578,carmocca,2020-09-21T00:54:24Z,2020-09-25T12:18:07Z,"Incorrect ""Saving latest checkpoint"" warning","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 ""Saving latest checkpoint..."" warning appears regardless of whether a ModelCheckpoint exists or save_last is set to True
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 167 to 169
       in
       a71d62d
 
 
 
 
 
 
  # Save latest checkpoint 
 
 
 
  rank_zero_warn('Saving latest checkpoint..') 
 
 
 
  self.check_checkpoint_callback(should_check_val=False, force_save=True) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 196 to 204
       in
       a71d62d
 
 
 
 
 
 
  def check_checkpoint_callback(self, should_check_val, force_save=False): 
 
 
 
  model = self.trainer.get_model() 
 
 
 
  
 
 
 
  # when no val loop is present or fast-dev-run still need to call checkpoints 
 
 
 
  # TODO bake this logic into the checkpoint callback 
 
 
 
  should_activate = not is_overridden('validation_step', model) and not should_check_val 
 
 
 
  if should_activate or force_save: 
 
 
 
  checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)] 
 
 
 
          [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks] 
 
 
 
 
 
 This might confuse an user to think the last checkpoint got saved when it did not.
 <denchmark-h:h2>Proposed change:</denchmark-h>
 
 def check_checkpoint_callback(self, should_check_val, force_save=False):
     model = self.trainer.get_model()
 
     # when no val loop is present or fast-dev-run still need to call checkpoints
     # TODO bake this logic into the checkpoint callback
     should_activate = not is_overridden('validation_step', model) and not should_check_val
     if should_activate or force_save:
         checkpoint_callbacks = [c for c in self.trainer.callbacks if isinstance(c, ModelCheckpoint)]
         if any(c.save_last for c in checkpoint_callbacks):
             rank_zero_warn('Saving latest checkpoint..')
         [c.on_validation_end(self.trainer, model) for c in checkpoint_callbacks]
 	",ed12e422a42472af1acb88f870dba3d43710b31d,Carlos Mocholí,2020-09-25 14:18:06+02:00,MODIFY,7,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"195,197,199,200,201","196,197,198,199,201,202",1.0,carmocca,2020-09-21T01:15:09Z,"
 		why not just remove the log line from training_loop and defer logging about saving the latest checkpoint to be within the checkpoint callback? that seems simpler to me
 		",2.0,carmocca,2020-09-21T01:33:25Z,"
 		Because the logic to save last is inside of on_validation_end so it would appear after the first validation run
 		",3.0,carmocca,2020-09-21T12:25:59Z,"
 		
 save_last is set to True
 
 it was meant to save the checkpoint if someone interrupts the training.
 
 regardless of whether a ModelCheckpoint exists
 
 yea, should not log if no ModelCheckpoint is used.
 		",4.0,carmocca,2020-09-21T15:21:39Z,"
 		Thanks for the issue <denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>
  . Mind sending a PR?
 		",MODIFY,1.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,"266,267,268,269,270,271,272,273,274,275,276,277",,test_model_checkpoint_save_last_warning,"tmpdir,caplog,max_epochs,should_validate,save_last",266,277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,carmocca,2020-09-21T17:35:42Z,"
 		Done!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,check_checkpoint_callback,"self,should_save,is_last",195,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"670,671,673","674,675",should_check_val_fx,"self,batch_idx,is_last_batch",667,676,1.0,"197,199,200,201","196,197,198,199,201,202",check_checkpoint_callback,"self,should_check_val,force_save",196,204,1.0,"430,501,502",505,run_training_epoch,self,416,505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,39,__init__,"self,trainer",37,44,1.0,,"227,228,229",on_train_epoch_start,"self,epoch",206,236,1.0,"167,168","167,168,169",on_train_end,self,161,193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3597,sshleifer,2020-09-22T00:26:46Z,2020-10-07T11:43:18Z,distributed training: ModelCheckpoint is receiving bad data,"
 You can reproduce in 4 minutes on 0.9.0.
 I tried master and got an unrelated wandb error and gave up trying to reproduce there.
 you must be on a machine with multiple gpus
 git clone git@github.com:huggingface/transformers.git
 cd transformers
 pip install -e .
 pip install -e .[examples]  # installs pytorch-lightning==0.8.5
 git checkout pl-checkpoint-bug
 cd examples/seq2seq
 wget https://s3.amazonaws.com/datasets.huggingface.co/translation/wmt_en_ro.tar.gz
 tar -xzvf wmt_en_ro.tar.gz
 
 export MAX_LEN=128
 export m=sshleifer/student_marian_en_ro_6_3
 
 python finetune.py \
   --learning_rate=3e-4 \
   --do_train \
   --do_predict \
   --fp16 \
   --val_check_interval 0.25 \
   --data_dir wmt_en_ro \
   --max_source_length $MAX_LEN --max_target_length $MAX_LEN --val_max_target_length $MAX_LEN --test_max_target_length $MAX_LEN \
   --freeze_encoder --freeze_embeds \
   --train_batch_size=64 --eval_batch_size=64 \
   --tokenizer_name $m --model_name_or_path $m \
   --warmup_steps 500 --sortish_sampler --logger_name wandb \
   --fp16_opt_level=O1 --task translation --num_sanity_val_steps=0 \
   --model_name_or_path $m --gpus 8 --num_train_epochs=1 \
   --data_dir wmt_mar_pl --output_dir dmar_pl_only_v3 --save_top_k=10
 <denchmark-h:h3>Results</denchmark-h>
 
 ls dmar_pl_only_v3/*.ckpt
 <denchmark-code>-rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:58 dmar_pl_only_v3/val_avg_bleu=23.3951-step_count=5.ckpt
 -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:57 dmar_pl_only_v3/val_avg_bleu=23.2619-step_count=4.ckpt
 -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:56 dmar_pl_only_v3/val_avg_bleu=22.6724-step_count=3.ckpt
 -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:56 dmar_pl_only_v3/val_avg_bleu=22.2664-step_count=2.ckpt
 -rw-r--r-- 1 shleifer shleifer 351351790 Sep 21 23:55 dmar_pl_only_v3/val_avg_bleu=23.2263-step_count=1.ckpt
 </denchmark-code>
 
 There are 5 checkpoints which much lower scores. PL thinks the best checkpoint is from step 5, but
 <denchmark-code>cat dmar_pl_only_v3/metrics.json | grep bleu
 </denchmark-code>
 
 <denchmark-code>            ""val_avg_bleu"": 26.4513,
             ""val_avg_bleu"": 25.5289,
             ""val_avg_bleu"": 25.6942,
             ""val_avg_bleu"": 26.2227,
             ""val_avg_bleu"": 25.8546,
 </denchmark-code>
 
 (the best checkpoint is step 1)
 When I evaluate offline on the best checkpoint without truncation, I get val_bleu =  27+, which makes me nearly certain that the numbers in  (which I create and save in <denchmark-link:https://github.com/huggingface/transformers/blob/master/examples/seq2seq/finetune.py#L209>finetune.py</denchmark-link>
  are correct and the numbers in the saved paths are incorrect.)
 Is this a known issue with a workaround? How can I fix? Should be high priority because suboptimal checkpoint saving is a huge productivity drain.
 <denchmark-h:h3>Additional Notes:</denchmark-h>
 
 
 The numbers logged to wandb are also the low/wrong ones.
 on 1 or 2 GPU the numbers are identical!
 
 	",2aebf65241ab054df9256cc33d37236651691a48,Sean Naren,2020-10-07 07:43:17-04:00,MODIFY,1,tests\checkpointing\test_model_checkpoint.py,tests\checkpointing\test_model_checkpoint.py,1.0,"23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43",,1.0,sshleifer,2020-09-22T09:44:08Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  verifying this but you might be right, even I felt in one of my previous training runs that the epoch with minimum validation wasn't saved.
 		",2.0,sshleifer,2020-09-22T16:50:39Z,"
 		Where in the PL code does trainer.callback_metrics gather data from all nodes?
 		",3.0,sshleifer,2020-09-22T16:52:00Z,"
 		CC: <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
 
 		",4.0,sshleifer,2020-09-22T19:51:34Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  if you use our metrics they have built-in ddp reduction (each metric separately)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,sshleifer,2020-09-22T20:51:11Z,"
 		Do you have a working example of how that would work for ROUGE/BLEU?
 		",6.0,sshleifer,2020-09-22T20:52:35Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  looked at my logs, my confusion was because I was looking at online fine tuning loss values for pre-training, checking this for BLEU. BLEU needs to be verified for DDP sync right now, this is something we are already working on.
 		",7.0,sshleifer,2020-09-22T21:12:39Z,"
 		OK, so in the current state of the code, is trainer.callback_metrics always from rank 0, from the last rank to finish, or from the first rank to finish?
 		",8.0,sshleifer,2020-09-23T06:07:16Z,"
 		<denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>
  just wanted to clarify that this has nothing to do with the metric package, since this is calculating blue score using another package.
 		",9.0,sshleifer,2020-10-05T03:30:41Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  mind trying master? we added a lot of tests to model checkpoint and cleaned up the logging API.
 1.0 will be released in a few days with these changes.
 0.10.0rc1 is now available.
 		",10.0,sshleifer,2020-10-07T09:11:59Z,"
 		Let's see how it works with the new Metric API made by <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>
  and <denchmark-link:https://github.com/teddykoker>@teddykoker</denchmark-link>
 
 <denchmark-link:https://github.com/SeanNaren>@SeanNaren</denchmark-link>
  it may be solved, but it would be nice to add a test for this case to prevent it in future
 		",11.0,sshleifer,2020-10-07T09:43:53Z,"
 		I can confirm on pytorch-lightning <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/9c415d2c71ddc44ca3344ba1136c0dd546fc5ef6>master</denchmark-link>
  and transformers <denchmark-link:https://github.com/huggingface/transformers/commit/8fa0c956b34123d1f1406ae96d74c484976d0e3f>master</denchmark-link>
  metrics/ckpts are in sync. Modified the cmd slightly by disabling wandb + fixed data_dir path.
 <denchmark-code>python finetune.py \
   --learning_rate=3e-4 \
   --do_train \
   --fp16 \
   --val_check_interval 0.25 \
   --data_dir wmt_en_ro \
   --max_source_length $MAX_LEN --max_target_length $MAX_LEN --val_max_target_length $MAX_LEN --test_max_target_length $MAX_LEN \
   --freeze_encoder --freeze_embeds \
   --train_batch_size=64 --eval_batch_size=64 \
   --tokenizer_name $m --model_name_or_path $m \
   --warmup_steps 500 --sortish_sampler  \
   --fp16_opt_level=O1 --task translation --num_sanity_val_steps=0 \
   --model_name_or_path $m --gpus 8 --num_train_epochs=1 \
   --data_dir wmt_mar_pl --output_dir dmar_pl_only_v3 --save_top_k=10
 
 </denchmark-code>
 
 <denchmark-code>~/transformers/examples/seq2seq$ ls dmar_pl_only_v3/*.ckpt
 'dmar_pl_only_v3/val_avg_bleu=21.3473-step_count=1.ckpt'
 'dmar_pl_only_v3/val_avg_bleu=21.5114-step_count=2.ckpt'
 'dmar_pl_only_v3/val_avg_bleu=23.1029-step_count=3.ckpt'
 'dmar_pl_only_v3/val_avg_bleu=23.2499-step_count=4.ckpt'
 </denchmark-code>
 
 <denchmark-code>cat dmar_pl_only_v3/metrics.json | grep bleu
             ""val_avg_bleu"": 21.3473,
             ""val_avg_bleu"": 21.51145,
             ""val_avg_bleu"": 23.10295,
             ""val_avg_bleu"": 23.24995,
 </denchmark-code>
 
 When using --do_predict there is an unrelated issue in Transformers which needs to be fixed in finetune.py. I think main gets run again:
 <denchmark-code>Traceback (most recent call last):
   File ""/home/jovyan/transformers/examples/seq2seq/finetune.py"", line 440, in <module>
     main(args)
   File ""/home/jovyan/transformers/examples/seq2seq/finetune.py"", line 376, in main
     raise ValueError(""Output directory ({}) already exists and is not empty."".format(args.output_dir))
 ValueError: Output directory (dmar_pl_only_v3) already exists and is not empty.
 </denchmark-code>
 
 As <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  said I'll get a test in place to ensure the file path metrics are correct!
 		",12.0,sshleifer,2020-10-07T14:51:39Z,"
 		On pl=master in my multigpu unittest, getting
 trainer.test()
 
 ERR:   File ""/home/shleifer/miniconda3/envs/nb/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 773, in __test_using_best_weights
 ERR:     'ckpt_path is ""best"", but ModelCheckpoint is not configured to save the best model.'
 ERR: pytorch_lightning.utilities.exceptions.MisconfigurationException: ckpt_path is ""best"", but ModelCheckpoint is not configured to save the best model.`
 There is 1 checkpoint in the save directory at the correct time.
 Does ModelCheckpoint need to be configured differently?
 		",13.0,sshleifer,2020-10-07T14:53:51Z,"
 		have you set a monitor key?
 only with this will it be able to track best
 		",14.0,sshleifer,2020-10-07T15:16:11Z,"
 		yes, we removed the magical “val_loss” key for 1.0.
 by default without a checkpoint CB we save the last always.
 to change that you have to now pass in a callback with the monitor keyword.
 it makes it less magical now and more transparent. also opens the door to multiple checkpoint callbacks soon
 		",15.0,sshleifer,2020-10-07T15:17:59Z,"
 		Yes I have a checkpoint callback with a monitor
     checkpoint_callback = ModelCheckpoint(
         filepath=os.path.join(output_dir, exp),
         monitor=f""val_{metric}"",
         mode=""min"" if ""loss"" in metric else ""max"",
         save_top_k=save_top_k,
         period=0,  # Interval (number of val checks) between checkpoints.
 
     )
 It correctly saves checkpoints but trainer.test doesn't recognize it for some reason.
 		",,,,,test_model_checkpoint_correct_score,"tmpdir,save_top_k",23,43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,sshleifer,2020-10-07T15:53:03Z,"
 		do you run ddp? how do I reproduce it?
 		",17.0,sshleifer,2020-10-07T15:59:09Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  can you reproduce using the BoringModel?
 if you submit a failing test for this using BoringModel we can fix in a few mins
 		",18.0,sshleifer,2020-10-07T19:08:55Z,"
 		Tried and gave up after 30 mins.
 Had to guess install instructions deal (after scanning README) with failing horovod install. (I blindly ran pip install -r requirements/devel.txt) Then I tried to add to test_ddp and test_trainer but my test was failing for other reasons than the bug I was trying to isolate. (I'm not whining just telling you cause I would want you to tell me if the roles were reversed.)
 Is there a test that uses:
 DDP, BoringModel, and ModelCheckpoint and calls trainer.test at the end?
 If that exists and passes, I won't be able to replicate my issue.
 if it doesn't exist but passes: you increased the likelihood that my issue is my fault and improved your test coverage.
 		",19.0,sshleifer,2020-10-07T19:36:52Z,"
 		yup! this test
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/backends/test_ddp.py#L44-L57>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/tests/backends/test_ddp.py#L44-L57</denchmark-link>
 
 
 EvalModelTemplate which is a bit more involved than BoringModel
 DDP
 modelcheckpoint
 calls both fit and test at the end
 
 And... to be extra safe ;)  it even makes sure it gets 90+ acc
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,sshleifer,2020-10-19T13:56:13Z,"
 		<denchmark-link:https://github.com/sshleifer>@sshleifer</denchmark-link>
  I can confirm this works using the below boring model test:
 import os
 
 import torch
 from torch.utils.data import Dataset
 
 from pytorch_lightning import Trainer, LightningModule
 from pytorch_lightning.callbacks import ModelCheckpoint
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
 
     def __init__(self):
         """"""
         Testing PL Module
 
         Use as follows:
         - subclass
         - modify the behavior for what you want
 
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
 
         or:
 
         model = BaseTestModel()
         model.training_epoch_end = None
 
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
 
     def step(self, x):
         x = self.layer(x)
         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))
         return out
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('loss', loss)
         return {""loss"": loss}
 
     def training_step_end(self, training_step_outputs):
         return training_step_outputs
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('x', loss)
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('y', loss)
 
     def configure_optimizers(self):
         optimizer = torch.optim.AdamW(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
 
 
 def run_test():
     # fake data
     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64))
 
     # model
     model = BoringModel()
     trainer = Trainer(
         default_root_dir=os.getcwd(),
         limit_train_batches=4,
         limit_val_batches=2,
         max_epochs=1,
         accelerator='ddp',
         gpus=2,
         checkpoint_callback=ModelCheckpoint(
             monitor='x',
             mode='min',
             save_top_k=1
         )
     )
     trainer.fit(model, train_data, val_data)
     trainer.test()
 
 
 if __name__ == '__main__':
     run_test()
 I think the error message is a bit ambiguous, but the issue might be not calling self.log to report the metrics we'd like to save on.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3600,josh-gleason,2020-09-22T01:50:07Z,2020-09-22T14:19:30Z,Infinite hang when running `Trainer.test` after `Trainer.fit` with DDP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 If I run Trainer.test after running Trainer.fit with distributed_backend='ddp' then the system hangs.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 Run the following script
 # main.py
 import os
 from argparse import ArgumentParser
 from pl_examples.models.lightning_template import LightningTemplateModel
 from pytorch_lightning import Trainer, seed_everything
 
 seed_everything(234)
 
 
 def main(args):
     model = LightningTemplateModel(**vars(args))
     trainer = Trainer.from_argparse_args(args)
     trainer.fit(model)     # if this is commented out then test will complete, otherwise it hangs
     trainer.test(model)
 
 
 def run_cli():
     root_dir = os.path.dirname(os.path.realpath(__file__))
     parent_parser = ArgumentParser(add_help=False)
     parser = LightningTemplateModel.add_model_specific_args(parent_parser, root_dir)
     parser = Trainer.add_argparse_args(parser)
     parser.set_defaults(gpus=2)
     args = parser.parse_args()
 
     main(args)
 
 
 if __name__ == '__main__':
     run_cli()
 with command line arguments (assuming >= 2 GPUs)
 python main.py --gpus 2 --hidden_dim 500 --max_epochs 1 --distributed_backend ddp
 Running this script causes the program to hang during test phase.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 I would expect Trainer.test to complete rather than hanging.
 <denchmark-h:h3>Environment</denchmark-h>
 
 Output of collect_env_details.py:
 <denchmark-code>* CUDA:
         - GPU:
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.19.1
         - pyTorch_debug:     False
         - pyTorch_version:   1.6.0
         - pytorch-lightning: 0.9.1rc3
         - tqdm:              4.49.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - ELF
         - processor:         x86_64
         - python:            3.7.5
         - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020
 </denchmark-code>
 
 
 PyTorch Version: 1.6.0
 OS: Ubuntu 20.04
 How you installed PyTorch: pip
 Build command you used (if compiling from source):
 Python version: 3.7.5
 CUDA/cuDNN version: 7.6.5
 GPU models and configuration: GeForce RTX 2080 Ti (x2)
 
 List of all installed packages (output of pip freeze):
 <denchmark-code>absl-py==0.10.0
 cachetools==4.1.1
 certifi==2020.6.20
 chardet==3.0.4
 decorator==4.4.2
 fsspec==0.8.2
 future==0.18.2
 google-auth==1.21.2
 google-auth-oauthlib==0.4.1
 grpcio==1.32.0
 idna==2.10
 importlib-metadata==1.7.0
 Markdown==3.2.2
 networkx==2.5
 numpy==1.19.1
 oauthlib==3.1.0
 packaging==20.4
 Pillow==7.2.0
 pkg-resources==0.0.0
 protobuf==3.13.0
 pyasn1==0.4.8
 pyasn1-modules==0.2.8
 pyparsing==2.4.7
 pytorch-lightning==0.9.1rc3
 PyYAML==5.3.1
 requests==2.24.0
 requests-oauthlib==1.3.0
 rsa==4.6
 six==1.15.0
 tensorboard==2.2.0
 tensorboard-plugin-wit==1.7.0
 torch==1.6.0
 torchvision==0.7.0
 tqdm==4.49.0
 urllib3==1.25.10
 Werkzeug==1.0.1
 zipp==3.1.0
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 If I comment out trainer.fit then everything works as expected.
 I was able to pause the execution during hang while running in PyCharm. The following are the stack frames for the main thread, which is the only thread I could get to pause.
 <denchmark-code>select, selectors.py:418
 wait, connection.py:920
 _poll, connection.py:414
 poll, connection.py:257
 get, queues.py:104
 _worker_loop, worker.py:167
 run, process.py:99
 _bootstrap, process.py:297
 _launch, popen_fork.py:74
 __init__, popen_fork.py:20
 _Popen, context.py:277
 _Popen, context.py:223
 start, process.py:112
 __init__, dataloader.py:737
 __iter__, dataloader.py:291
 run_evaluation, trainer.py:437
 run_test, trainer.py:489
 train_or_test, base_backend.py:34
 ddp_train, ddp_backend.py:243
 train, ddp_backend.py:138
 fit, trainer.py:324
 wrapped_fn, states.py:48
 __test_given_model, trainer.py:627
 test, trainer.py:564
 wrapped_fn, states.py:48
 main, main.py:13
 run_cli, main.py:24
 <module>, main.py:28
 </denchmark-code>
 
 	",d2a3d6aa8e8b69e6f373243bd25165a0963d7a53,Adrian Wälchli,2020-09-22 16:57:01-04:00,MODIFY,0,docs\source\multi_gpu.rst,docs\source\multi_gpu.rst,0.0,"295,299","295,299",1.0,josh-gleason,2020-09-22T01:50:47Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,josh-gleason,2020-09-22T10:51:42Z,"
 		Hi, in this ddp mode you can call trainer.fit / test only once.
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel>https://pytorch-lightning.readthedocs.io/en/latest/multi_gpu.html#distributed-data-parallel</denchmark-link>
 
 
 There are cases in which it is not possible to use DDP. Examples are:
 
 Jupyter Notebook, Google COLAB, Kaggle, etc.
 You have a nested script without a root package
 Your script needs to invoke .fit or .test multiple times
 
 
 you need to switch to ddp_spawn or launch your .test in a separate script.
 		",3.0,josh-gleason,2020-09-22T18:42:48Z,"
 		Interesting. I read this but to me it seemed to indicate you could call both fit and test as long as neither were called multiple times. Perhaps the documentation could be updated to make this more clear.
 		",4.0,josh-gleason,2020-09-22T19:47:58Z,"
 		yeah granted, this may be a bit ambiguous.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,josh-gleason,2020-09-26T11:00:16Z,"
 		Hi
 I encountered that DDP couldn't work for me when running my python script as a module (python -m).
 Does it match the second limitation case? (""You have a nested script without a root package"").
 I couldn't really understand this one, and I am not sure why DDP cannot work with modules.
 When I am running the same script not as module, everything works fine.
 I tested it also on a really simple MNIST example.
 		",6.0,josh-gleason,2020-09-26T11:06:40Z,"
 		<denchmark-link:https://github.com/AvivWn>@AvivWn</denchmark-link>
  Because when you run the ddp script, it will call itself under the hood again n-1 times where n is the number of gpus you selected. So, example, if we did this:
 python train.py --gpus 4 --something-else --distributed_backend=ddp 
 this process will launch on gpu 0 and then call
 <denchmark-code>python train.py --gpus ""1,""  --something-else --distributed_backend=ddp
 python train.py --gpus ""2,""  --something-else --distributed_backend=ddp
 python train.py --gpus ""3,""  --something-else --distributed_backend=ddp
 </denchmark-code>
 
 (simplified example)
 so if we wanted to support the module way of launching the script, we would have to strip -m ... from the command and append it to the subprocess calls.
 		",7.0,josh-gleason,2020-09-26T11:17:12Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
   Sure
 And is it impossible to programmatically add the ""-m"" only if it specified? It should be just a single if, right?
 It is frustrating to know that a module cannot be run as DDP, after I have already built a full working module project.
 Transforming it into a script won't be easy...
 		",8.0,josh-gleason,2020-09-28T02:19:33Z,"
 		No, I would not assume it is impossible. If the -m option can be stripped from arglist, it should not be too difficult.
 Very sorry that it causes trouble for you but it looks like they just didn't think about this use case when the ddp backend was added.
 Let's open a feature request for this.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3619,carmocca,2020-09-23T02:00:38Z,2020-09-29T13:36:46Z,ModelCheckpoint period should not always save on the first epoch,"
 <denchmark-h:h2>🚀 Feature</denchmark-h>
 
 period should work so:
 <denchmark-code># ModelCheckpoint on_validation_end
 if (epoch + 1) % period:
     # do not save
     return
 </denchmark-code>
 
 e.g:
 
 period == 1: save on epoch 0, 1, 2...
 period == 2: save on epoch 1, 3, 5...
 period == 3: save on epoch 2, 5, 8...
 
 currently, it always runs on the first epoch and then runs every period epochs
 e.g:
 
 period == 1: save on epoch 0, 1, 2...
 period == 2: save on epoch 0, 2, 4...
 period == 3: save on epoch 0, 3, 6...
 
 This would also allow having period = 0 which would never save, just as save_top_k = 0
 <denchmark-h:h3>Motivation</denchmark-h>
 
 I want to save a checkpoint every period epochs but current behaviour forces to always save on the first one.
 	",3b2efe5b2afe664d88c9d5eda127774bba4cff4c,Carlos Mocholí,2020-09-29 15:36:45+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"82,83",,1.0,carmocca,2020-09-23T02:34:00Z,"
 		makes sense. submit a PR?
 		",2.0,carmocca,2020-09-23T17:05:57Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  why is epoch 0-indexed in one place and 1-indexed in other places? can it be consistent across lightning instead of these +1s? I think the same issue affects batch indices too
 		",3.0,carmocca,2020-09-23T17:43:14Z,"
 		yes, we did a refactor a while to make everything zero indexed. where did we miss spots?
 		",,,,,MODIFY,6.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,"486,487,488,489",,_save_top_k_checkpoints,"self,metrics,trainer,pl_module,epoch,filepath",476,496,MODIFY,4.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_model_checkpoint_topk_zero,tmpdir,301,320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,293,_save_model,"self,str,trainer,pl_module",292,304,1.0,313,,check_monitor_top_k,"self,current",299,313,,,,,,,,,,,,,,,,,,,,,,1.0,"184,190","189,190",test_model_checkpoint_save_last,tmpdir,173,191,1.0,"264,276,277","275,276",test_model_checkpoint_none_monitor,tmpdir,251,277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297",,test_model_checkpoint_period,"tmpdir,period",281,298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"184,186,187,188,189,190,191,192,193","184,185,186,187,188,189,190,191,192,193,194,196,197,203,204",save_checkpoint,"self,trainer,pl_module",180,218,1.0,,"446,447,448",_should_skip_epoch,"self,trainer",446,448,1.0,275,,__init_monitor_mode,"self,monitor,mode",263,280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3652,yoshum,2020-09-25T01:21:50Z,2020-09-25T11:09:51Z,Creation of many data module instances incurs RecursionError,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Thank you for a nice framework!
 When I repeated hundreds of experiments, each time with a new instance of a single LightningDataModule class, RecursionError was raised. I also found that creating data modules and calling setup() were enough to reproduce the issue.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Please look at the following code sample and error messages:
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import pytorch_lightning as pl
 
 class DummyDM(pl.LightningDataModule):
     def setup(self, stage=None):
         pass
 
 if __name__ == ""__main__"":
     MAX_ITERS = 1000
     for i in range(MAX_ITERS):
         try:
             dm = DummyDM()
             dm.setup()
         except RecursionError:
             print(f""RecursionError occured in the {i}-th iteration!"")
             raise
 </denchmark-code>
 
 <denchmark-h:h4>Error messages</denchmark-h>
 
 <denchmark-code>RecursionError occured in the 998-th iteration!
 Traceback (most recent call last):
   File ""test_dm.py"", line 18, in <module>
     dm.setup()
   File ""/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py"", line 85, in wrapped_fn
     return fn(*args, **kwargs)
   File ""/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py"", line 85, in wrapped_fn
     return fn(*args, **kwargs)
   File ""/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py"", line 85, in wrapped_fn
     return fn(*args, **kwargs)
   [Previous line repeated 995 more times]
   File ""/workspace/src/.venv/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py"", line 69, in wrapped_fn
     if fn.__name__ == 'setup':
 RecursionError: maximum recursion depth exceeded in comparison
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The above code sample is expected to exit without any outputs.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.6.0
 PytorchLightning Version: 0.9.0
 OS (e.g., Linux): Linux
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source): n/a
 Python version: 3.8.2
 CUDA/cuDNN version: 10.2
 GPU models and configuration: 1080Ti
 
 	",17c8c95fbc7b31f73671761430e86d881f6d6c6d,Antoine Broyelle,2020-09-25 07:09:50-04:00,MODIFY,2,pytorch_lightning\core\datamodule.py,pytorch_lightning\core\datamodule.py,1.0,"39,40,41,42,43,44","35,36,37,38,39",1.0,yoshum,2020-09-25T01:22:29Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,yoshum,2020-09-25T10:54:35Z,"
 		I faced the same issue. I opened a PR with a fix <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3654>#3654</denchmark-link>
 .
 		",,,,,,,,,MODIFY,3.0,tests\core\test_datamodules.py,tests\core\test_datamodules.py,1.0,"78,79",,test_hooks_no_recursion_error.prepare_data,"self,args,kwargs",78,79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__call__,"cls,args,kwargs",32,49,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"71,72,73,74,75,76,77,78,79,80,81,82,83,84",,test_hooks_no_recursion_error,tmpdir,71,84,1.0,"75,76",,test_hooks_no_recursion_error.setup,"self,args,kwargs",75,76,1.0,"28,29,30",,__init__,"self,args,kwargs",28,30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3668,gerardsn,2020-09-26T15:38:17Z,2020-10-06T01:30:42Z,incorrect batch_sizes when Dataloader returns a dict with multiple tensors.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Tracked batch sizes in result object are incorrect when a Dataloader returns a dict with multiple tensors.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Create data loader that returns a dict, e.g. batch = {'batchA': tensor_A, 'batchB': tensor_B}.
 Both entires have batch size N with N != 2.
 For this example a batch size of 2 will be logged since len(batch) == 2.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
         Lines 147 to 150
       in
       05e5f03
 
 
 
 
 
 
  # track batch size for weighted average 
 
 
 
  is_result_obj = isinstance(output, Result) 
 
 
 
  if is_result_obj: 
 
 
 
  output.track_batch_size(len(batch)) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 304 to 306
       in
       05e5f03
 
 
 
 
 
 
  # track batch size for weighted average 
 
 
 
  if is_result_obj: 
 
 
 
  training_step_output.track_batch_size(len(split_batch)) 
 
 
 
 
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Log correct batch size.
 I'm not sure what can be defined as the 'correct' batch size when there are multiple tensors, but I expect that each tensor in the dict has the same batch_size. So, maybe something like:
 if is_result_obj:
     if isinstance(batch, dict):
         batch = batch[list(batch.keys())[0]]
     result_obj.track_batch_size(len(batch))
 	",b34c7add23553f10f6f0d7caf4177c67ee213f3a,William Falcon,2020-10-05 21:30:41-04:00,MODIFY,8,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341",,1.0,gerardsn,2020-10-01T20:33:02Z,"
 		I think doing just len(batch) is still wrong since here if the batch is a tuple or some kind of custom batch datatype then len(batch) will be wrong. Considering the basic mnist example too it will give 2 only which is wrong.
 		",2.0,gerardsn,2020-10-02T03:17:19Z,"
 		This should probably catch most things. Might be a bit much though.
 It returns 1 if it fails to determine the batch size to prevent issues with weighted averaging in reduce_on_epoch_end.
 if is_result_obj:
     result_obj.track_batch_size(unpack_batchsize(batch))
 
 # maybe add as staticmethod to ResultObj?
 def unpack_batchsize(sample):
     """""" 
     Recursively unpack sample to find a torch.Tensor.
     returns len(tensor) when found, or 1 when it hits an empty or non iterable.
     """"""
     if isinstance(sample, torch.Tensor):
         sample = len(sample)
     elif isinstance(sample, dict):
         sample = next(iter(sample.values()), 1)
     elif isinstance(sample, Iterable):
         sample = next(iter(sample), 1)
     else:
         sample = 1  
 
     if isinstance(sample, int):
         return sample
     return unpack_batchsize(sample)
 		",3.0,gerardsn,2020-10-02T03:59:08Z,"
 		I suggest adding a function to the LightningModule batch_len_fx which defaults to len if it is not overriden. Anything could be a batch and lightning shouldn't have the responsability of supporting any batch type.
 		",4.0,gerardsn,2020-10-02T06:04:56Z,"
 		Exactly what I had in mind <denchmark-link:https://github.com/carmocca>@carmocca</denchmark-link>
 . Or maybe simple ask to put  in  itself if ??
 .log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)
 .log('some_metric', metric_value, on_epoch=False)
 		",MODIFY,1.0,pytorch_lightning\loggers\tensorboard.py,pytorch_lightning\loggers\tensorboard.py,1.0,"183,184,185,186,187,188,189,190,191",182,log_metrics,"self,str,None",177,191,MODIFY,2.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,323,323,MODIFY,3.0,tests\base\boring_model.py,tests\base\boring_model.py,1.0,"11,12,13,14",,__getitem__,"self,index",11,14,MODIFY,5.0,tests\trainer\logging\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,1.0,"378,379",,test_different_batch_types_for_sizing.val_dataloader,self,378,379,5.0,gerardsn,2020-10-02T06:07:11Z,"
 		Lightning currently defaults to weighted_mean for reduction on epoch end by substituting the reduction method if it is torch.mean:
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
         Lines 389 to 390
       in
       ebc1b23
 
 
 
 
 
 
  if fx == torch.mean: 
 
 
 
  reduced_val = weighted_mean(result[k], batch_sizes) 
 
 
 
 
 
 If this is the desired behaviour, I think Lightning should at least attempt getting a reasonable estimate for the batch size. In most use cases the dataloader will return multiple tensors, resulting in an incorrect batch estimate if  is the default. (e.g. any supervised method has at least (X, y) in its batch, producing  as mentioned by <denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
 )
 This could still be done using batch_len_fx though. On first call, if the method is not overriden, replace the batch_len_fx with a reasonable estimate based on the type of batch. (e.g. len of [tensor, first value in Iterable])
 		",6.0,gerardsn,2020-10-02T06:18:16Z,"
 		
 Exactly what I had in mind @carmocca. Or maybe simple ask to put batch_size in .log itself if on_epoch=True??
 .log('some_metric', metric_value, on_epoch=True, batch_size=batch_size)
 .log('some_metric', metric_value, on_epoch=False)
 
 this should work too. Probably default to 1 if not provided since len is likely to be wrong.
 		",7.0,gerardsn,2020-10-02T20:54:24Z,"
 		<denchmark-link:https://github.com/gerardsn>@gerardsn</denchmark-link>
  I have a problem exactly with this  function.
 I'm working with the latest Lightning version from master.
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
          Line 369
       in
       ebc1b23
 
 
 
 
 
 
  def reduce_on_epoch_end(cls, outputs): 
 
 
 
 
 
 It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].
 Because I have only one batch per epoch in validation.
 Lightning tries to reduce on epoch end.
 It feeds
 result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
          Line 897
       in
       ebc1b23
 
 
 
 
 
 
  weights = weights.to(result.device)[:result.size(0)] 
 
 
 
 
 
 AttributeError: 'list' object has no attribute 'device'
 I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.
 		",8.0,gerardsn,2020-10-02T22:15:58Z,"
 		<denchmark-link:https://github.com/fogside>@fogside</denchmark-link>
  in your example result is a tensor so  should not though an error.
 		",9.0,gerardsn,2020-10-02T22:17:58Z,"
 		
 @fogside in your example result is a tensor so result.device should not though an error.
 
 But it's a list with a tensor inside.
 		",10.0,gerardsn,2020-10-02T22:21:30Z,"
 		
 result = tensor([27.8364], device='cuda:0'), weights=tensor([2])
 
 you refering to this right?
 		",11.0,gerardsn,2020-10-02T22:29:07Z,"
 		
 
 result = tensor([27.8364], device='cuda:0'), weights=tensor([2])
 
 you refering to this right?
 
 Sorry, I just realized, that I was mistaken.
 Actually it calls this method twice for some reason.
 I added prints at the beginning of weighted_mean function and at the reduce_on_epoch_end (I also changed the number of batches in this example)
 <denchmark-code>Result[k] tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')
 result:  tensor([23.6331, 26.0617, 24.0941, 25.3255], device='cuda:0')
 weights:  tensor([2, 2, 2, 2])
 Result[k] [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]
 result:  [0.14285714285714285, 0.06451612903225806, 0.056179775280898875, 0.13793103448275862]
 weights:  tensor([2, 2, 2, 2])
 </denchmark-code>
 
 And on the second time it gives me the error.
 		",12.0,gerardsn,2020-10-02T22:30:45Z,"
 		are you logging non-tensor values? maybe doing .item() somewhere in the logs? if not, can you put .log statements here??
 		",13.0,gerardsn,2020-10-02T22:33:17Z,"
 		
 are you logging non-tensor values?
 
 Yes, I was calculating precision in numpy.. Isn't it possible to log non-tensor values?
 		",14.0,gerardsn,2020-10-02T22:34:43Z,"
 		no.. also to calculate precision or anyother metric you can try pl.metrics package which computes these metrics on the current device itself.
 or you can just do torch.tensor(numpy_value) in .log
 		",15.0,gerardsn,2020-10-02T22:42:38Z,"
 		
 no.. also to calculate precision or another metric you can try pl.metrics package which does all of these on the current device itself.
 
 I see. Thank you!
 Actually I was trying to work with <denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning>pytorch-metric-learning</denchmark-link>
  and used some function for topk precision estimation from there. But it looks quite tough to merge these 2 frameworks. I see now that topk precision should be calculated in pytorch. Another thing is that I need to have as big batch as possible to get a good topk estimation (it's even better to have the whole val set), that's why I found it hard to make this estimations in the . Maybe I should look into some Callbacks?
 But it's not related to this issue.
 		",log_evaluation_step_metrics,"self,batch,batch_idx",318,326,unpack_batch_size,"self,sample",326,341,1.0,"375,376",,test_different_batch_types_for_sizing.train_dataloader,self,375,376,1.0,"360,361,362,363,364,365",,test_different_batch_types_for_sizing.training_step,"self,batch,batch_idx",360,365,,,,,,,,,,,,,,,,,,,,,,,1.0,"16,17",,__len__,self,16,17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"442,443,444,445,446,447",423,reduce_across_time,"cls,time_outputs",421,450,1.0,"948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972",,_process_dataloader_aggregated_steps,"result,weights",948,972,1.0,"513,514,515,516,517,518,519,520,521,522",,_recursive_fx_apply,"dict,fx",513,522,1.0,168,168,evaluation_step,"self,test_mode,batch,batch_idx,dataloader_idx",155,175,,,,,,,,1.0,"7,8,9",,__init__,"self,size,length",7,9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398",,test_different_batch_types_for_sizing,tmpdir,357,398,1.0,"367,368,369,370,371,372,373",,test_different_batch_types_for_sizing.validation_step,"self,batch,batch_idx",367,373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,gerardsn,2020-10-02T22:51:57Z,"
 		already working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)
 		",17.0,gerardsn,2020-10-03T08:05:17Z,"
 		
 already working on topk accuracy. Maybe will add topk precision and recall in pl.metrics as well. Can you point me to the implementation of topk precision in pytorch-metric-learning package. It would be helpful. Thanks :)
 
 It's great!
 Sure. I used the class AccuracyCalculator like this
 <denchmark-code>accuracy_calculator = AccuracyCalculator(include=(""mean_average_precision_at_r""),  k=5)
 accuracies = self.accuracy_calculator.get_accuracy(embeddings,
                                                            embeddings,
                                                            labels,
                                                            labels,
                                                            True)
 
 </denchmark-code>
 
 Implementation:
 <denchmark-link:https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45>https://github.com/KevinMusgrave/pytorch-metric-learning/blob/10bed5ee8719a543827aa32ea658603c2fcb0130/src/pytorch_metric_learning/utils/accuracy_calculator.py#L45</denchmark-link>
 
 		",18.0,gerardsn,2020-10-03T14:32:57Z,"
 		So I guess 2 things should be fixed:
 
 Track correct batch_size
 Allow non-tensor numeric values in .log(...)
 
 		",19.0,gerardsn,2020-10-05T13:43:15Z,"
 		
 @gerardsn I have a problem exactly with this weighted_mean function.
 I'm working with the latest Lightning version from master.
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
          Line 369
       in
       ebc1b23
 
 
 
 
 
 
  def reduce_on_epoch_end(cls, outputs): 
 
 
 
 
 
 It gets outputs = [{'checkpoint_on': tensor(28.3303, device='cuda:0'), 'val_loss': tensor(28.3303, device='cuda:0'), 'val_precision1': 0.12652068126520682}].
 Because I have only one batch per epoch in validation.
 Lightning tries to reduce on epoch end.
 It feeds
 result = tensor([27.8364], device='cuda:0'), weights=tensor([2]), into the weighted_mean function and I get an error here:
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
          Line 897
       in
       ebc1b23
 
 
 
 
 
 
  weights = weights.to(result.device)[:result.size(0)] 
 
 
 
 
 
 AttributeError: 'list' object has no attribute 'device'
 I think it's related to this issue. It would be nice to not reduce anything if it's just one batch per epoch.
 
 this is fixed on master
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"221,222",220,track_batch_size,"self,batch_size",220,222,1.0,"935,936,937,938,939,940,941,942,943,944",,weighted_mean,"result,weights",934,945,1.0,"221,222",,track_batch_size,"self,batch",221,224,1.0,"493,494,495,496,497,498,500",,recursive_gather,None,486,502,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20.0,gerardsn,2020-10-05T13:44:31Z,"
 		ok, making changes to this today.
 What do we want as the default behavior? doesn't the custom reduce function solve the problem of custom batches etc?
 		",21.0,gerardsn,2020-10-05T14:43:52Z,"
 		The batches are not tracked correctly.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
367,VismantasD,2019-10-14T05:21:19Z,2019-10-23T10:27:16Z,setting gpus=-1 and gpus='-1' in Trainer give different behaviours,"
 I discovered this while looking through the code. Trainer constructor does not mention that
 gpus can be -1 or '-1'. However if such values are passed they are accepted and result in
 different behaviour: -1 will result in no gpus used, '-1' will use all available gpus.
 To Reproduce
 
 Run any model first setting trainer gpus parameter to -1. No gpus will be used.
 Run same model setting gpus to '-1'. All available gpus will be used.
 
 Being able to set -1 to indicate that all gpus should be used is and I believe useful behaviour.
 The issue is in function <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/19c2b8fc9e73326c3e320c29e8f932893bd49460/pytorch_lightning/trainer/trainer.py#L340>self.__parse_gpu_ids(gpus)</denchmark-link>
  where the handling of -1 when passed as int is not implemented.
 Solution would be to implement equivalent logic for -1 as for '-1'.
 Happy to submit a PR.
 	",2aba70e228b427b16e547e030ab3bbad736b5b00,Vismantas,2019-10-23 05:05:09-04:00,MODIFY,0,docs\Trainer\Distributed training.md,docs\Trainer\Distributed training.md,0.0,"61,64,65,66,72,73,74,75",61,1.0,VismantasD,2019-10-14T10:57:18Z,"
 		yeah, fair. would love a PR! (also docs update)
 		",2.0,VismantasD,2019-10-16T21:22:50Z,"
 		Question. What is the intended behaviour when setting gpus in the Trainer to an int? (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/e2cabb03ba86c4dc0271a4531bce7bf2dff53e94>e2cabb0</denchmark-link>
 ).
 Is it specifying number of gpus to use? Or is it specifying a specific gpu? Depending on the choice one of __set_root_gpu() or num_gpus() needs fixing:
 
 If it is number of gpus, then __set_root_gpu  would be returning 0 but it should be returning None in this case.
 If it is specific gpu (e.g. gpus=3 means running on 3rd gpu) then num_gpus would be returning 3 instead of 1.
 
 		",3.0,VismantasD,2019-10-16T21:32:59Z,"
 		number is for how many gpus to use. i guess the docs need to be updated to make that clear?
 		",4.0,VismantasD,2019-10-16T23:33:56Z,"
 		Thanks, Yeah, I think they could be more specific and it is hard to read it from code. I'll update the documentation in the PR.
 		",MODIFY,7.0,pytorch_lightning\trainer\dp_mixin.py,pytorch_lightning\trainer\dp_mixin.py,1.0,"141,142,143,144,145,146,147,148,149,150,151",,normalize_parse_gpu_input_to_list,gpus,141,151,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,280,"276,278,279,280,281",MODIFY,14.0,tests\test_models.py,tests\test_models.py,1.0,"1613,1614,1615",,test_parse_gpu_fail_on_non_existant_id_2,mocked_device_count,1613,1615,,,,,,,,,,,,5.0,VismantasD,2019-10-17T06:17:23Z,"
 		awesome!
 		",6.0,VismantasD,2019-10-23T10:27:16Z,"
 		On master
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,num_gpus,self,276,281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1586,1587",,test_determine_root_gpu_device,"gpus,expected_root_gpu",1586,1587,,,,,,,,,,,,,,,,,,,,,,1.0,"119,120,121,122,123",,get_all_available_gpus,,119,123,1.0,"205,206,207,208,209,210,211,212,213,214,215,216,217,218,219",,determine_root_gpu_device,gpus,205,219,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1620,1621,1622",,test_parse_gpu_returns_None_when_no_devices_are_available,"mocked_device_count_0,gpus",1620,1622,1.0,"1569,1570",,test_root_gpu_property_0_raising,"mocked_device_count_0,gpus,expected_root_gpu,distributed_backend",1569,1570,1.0,"1508,1509",,test_trainer_gpu_parse,"mocked_device_count,gpus,expected_num_gpus,distributed_backend",1508,1509,1.0,"1490,1491",,mocked_device_count_0.device_count,,1490,1491,1.0,"1481,1482,1483,1484,1485",,mocked_device_count,monkeypatch,1481,1485,1.0,"1482,1483",,mocked_device_count.device_count,,1482,1483,1.0,"1520,1521",,test_trainer_num_gpu_0,"mocked_device_count_0,gpus,expected_num_gpus,distributed_backend",1520,1521,1.0,"1549,1550",,test_root_gpu_property_0_passing,"mocked_device_count_0,gpus,expected_root_gpu,distributed_backend",1549,1550,1.0,"1489,1490,1491,1492,1493",,mocked_device_count_0,monkeypatch,1489,1493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"109,110,111,112,113,114,115,116",,normalize_parse_gpu_string_input,s,109,116,1.0,"126,127,128,129,130,131,132,133,134,135,136,137,138",,check_gpus_data_type,gpus,126,138,1.0,"154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169",,sanitize_gpu_ids,gpus,154,169,,,,,,,,,,,,,,,1.0,"172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202",,parse_gpu_ids,gpus,172,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1535,1536",,test_root_gpu_property,"mocked_device_count,gpus,expected_root_gpu,distributed_backend",1535,1536,1.0,"1607,1608,1609",,test_parse_gpu_fail_on_non_existant_id,"mocked_device_count_0,gpus",1607,1609,1.0,"1601,1602",,test_parse_gpu_ids,"mocked_device_count,gpus,expected_gpu_ids",1601,1602,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3693,chrismaliszewski,2020-09-28T04:34:38Z,2020-10-02T19:46:47Z,"Missing attribute ""training_step_output_for_epoch_end""","
 I used the documentation way of stopping the training (<denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end>https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html#enable-early-stopping-using-callbacks-on-epoch-end</denchmark-link>
 ).
 If on_bath_start method returns -1 at the very beginning of an epoch, the titled AttributeError exception.
 The problem is in training_loop.py line 496 (batch_output.training_step_output_for_epoch_end).
 <denchmark-h:h4>Code sample</denchmark-h>
 
 Use the method and run your code:
     def on_batch_start(self, batch):
         return -1
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Check batch_output value if equals -1 before running trainin_loop.py line 495.
 The early stopping method achieved the same way the documentation specifies should not throw an exception but rather simply stop the training.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.19.1
 pyTorch_debug:     False
 pyTorch_version:   1.6.0
 pytorch-lightning: 0.9.0
 tqdm:              4.49.0
 
 
 System:
 
 OS:                Windows
 architecture:
 
 64bit
 WindowsPE
 
 
 processor:         Intel64 Family 6 Model 60 Stepping 3, GenuineIntel
 python:            3.8.5
 version:           10.0.18362
 
 
 
 	",9942f3ebdf14d0139b1b156dd56662b425f3c777,Jeff Yang,2020-10-02 21:46:46+02:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"62,63",,1.0,chrismaliszewski,2020-09-28T04:35:19Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,chrismaliszewski,2020-10-03T04:24:44Z,"
 		<denchmark-link:https://github.com/chrismaliszewski>@chrismaliszewski</denchmark-link>
  can you confirm this now stops the training epoch?
 		",3.0,chrismaliszewski,2020-10-03T07:01:54Z,"
 		Should I update in conda command line, nothing has changed:
 <denchmark-code>Traceback (most recent call last):
   File ""XXX\__main__train.py"", line 54, in <module>
     trainer.fit(model)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\states.py"", line 48, in wrapped_fn
     result = fn(self, *args, **kwargs)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 1084, in fit
     results = self.accelerator_backend.train(model)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py"", line 39, in train
     results = self.trainer.run_pretrain_routine(model)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 1239, in run_pretrain_routine
     self.train()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 394, in train
     self.run_training_epoch()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 496, in run_training_epoch
     batch_output.training_step_output_for_epoch_end,
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\utilities\parsing.py"", line 144, in __getattr__
     raise AttributeError(f'Missing attribute ""{key}""')
 AttributeError: Missing attribute ""training_step_output_for_epoch_end""
 Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]
 
 Process finished with exit code 1
 </denchmark-code>
 
 Or should I update directly from GitHub, i.e. using the method provided here: <denchmark-link:https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github>https://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github</denchmark-link>
 ?
 		",4.0,chrismaliszewski,2020-10-03T07:06:01Z,"
 		Yes please, it's fixed on master, it hasn't been released yet. You can do the following in your conda or pip env
 <denchmark-code>pip install git+https://github.com/PyTorchLightning/pytorch-lightning.git@master --upgrade
 </denchmark-code>
 
 		",MODIFY,0.0,docs\source\early_stopping.rst,docs\source\early_stopping.rst,0.0,13,13,,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"518,519,520,521","530,531,532",MODIFY,2.0,tests\models\test_hooks.py,tests\models\test_hooks.py,1.0,"119,120,121",,test_on_train_batch_start_hook.on_train_batch_start,"self,batch,batch_idx,dataloader_idx",119,121,,,,,,,,,,,,5.0,chrismaliszewski,2020-10-03T07:27:42Z,"
 		After the update you suggested I have MAJOR problems, even crashing errors.
 In regards to the issue I posted, I have the following error, no matter if I return -1 or anything else:
 <denchmark-code>File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 442, in fit
     results = self.accelerator_backend.train()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py"", line 47, in train
     results = self.train_or_test()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\base_backend.py"", line 43, in train_or_test
     results = self.trainer.train()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 489, in train
     self.train_loop.run_training_epoch()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 516, in run_training_epoch
     batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 617, in run_training_batch
     response = self.trainer.call_hook('on_batch_start')
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 807, in call_hook
     output = hook_fx(*args, **kwargs)
 TypeError: on_batch_start() missing 1 required positional argument: 'batch'
 Epoch 0:   0%|          | 0/4 [00:00<?, ?it/s]
 </denchmark-code>
 
 I haven't changed anything in the definition of my function and it looks as follows:
 <denchmark-code>    def on_batch_start(self, batch):
         return -1
         if self.get_early_stop(self.hparams['early_stop_path']):
             return -1
         else:
             return batch
 </denchmark-code>
 
 where get_early_stop returns Boolean if the training should early stop at any given time.
 For the unknown reason, the args and kwargs in the line output = hook_fx(*args, **kwargs) are empty.
 If I remove method on_batch_start, the code follows further but crashes elsewhere, read the next comment.
 If you need further information, let me know and I try helping.
 		",6.0,chrismaliszewski,2020-10-03T07:39:45Z,"
 		In terms of other problems with the version.
 I have a crashing error or a warning is being displayed. Messages exclude each other in the potential resolving way.
 Error message 1.
 <denchmark-code> File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 442, in fit
     results = self.accelerator_backend.train()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\cpu_backend.py"", line 47, in train
     results = self.train_or_test()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\accelerators\base_backend.py"", line 43, in train_or_test
     results = self.trainer.train()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 489, in train
     self.train_loop.run_training_epoch()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 539, in run_training_epoch
     self.trainer.run_evaluation(test_mode=False)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 604, in run_evaluation
     self.evaluation_loop.on_evaluation_epoch_end()
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py"", line 298, in on_evaluation_epoch_end
     self.trainer.call_hook('on_validation_epoch_end', *args, **kwargs)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 800, in call_hook
     trainer_hook(*args, **kwargs)
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\trainer\callback_hook.py"", line 87, in on_validation_epoch_end
     callback.on_validation_epoch_end(self, self.get_model())
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\callbacks\early_stopping.py"", line 152, in on_validation_epoch_end
     if self._validate_condition_metric(trainer.logger_connector.callback_metrics):
   File ""YYY\anaconda3\envs\pt_cpu\lib\site-packages\pytorch_lightning\callbacks\early_stopping.py"", line 116, in _validate_condition_metric
     raise RuntimeError(error_msg)
 RuntimeError: Early stopping conditioned on metric `val_loss` which is not available. Either add `val_loss` to the return of `validation_epoch_end` or modify your `EarlyStopping` callback to use any of the following: ``
 </denchmark-code>
 
 Note the part Either add val_loss to the return of validation_epoch_end and that the error message is cut with nothing after the following:.
 Warning message 2.
 UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
 after I remove return {'val_loss': loss} leaving just self.log(""val_loss"", loss). So which one should I do? return or not return?
 		",7.0,chrismaliszewski,2020-10-03T07:50:12Z,"
 		Okay. Regarding the first issue,  is deprecated in 0.9 and will be removed in 1.0.
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start>https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_batch_start</denchmark-link>
 
 Please use .
 <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start>https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.hooks.html#pytorch_lightning.core.hooks.ModelHooks.on_train_batch_start</denchmark-link>
 
 Also you probably don't want to stop training epoch just at the start of the training, I assume.
     def on_train_batch_start(self, batch, batch_idx, dataloader_idx):
         if self.get_early_stop(self.hparams['early_stop_path']):
             return -1
         else:
             return batch
 		",8.0,chrismaliszewski,2020-10-03T07:53:21Z,"
 		The method you provided works without any errors. Thank you for the advice.
 		",9.0,chrismaliszewski,2020-10-03T07:56:57Z,"
 		Regarding the second issue, you can use  to use  in the early stop callback.
 For now, you can ignore the below warning, currently at fixing at <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3812>#3812</denchmark-link>
 
 <denchmark-code>UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule
 </denchmark-code>
 
 		",10.0,chrismaliszewski,2020-10-03T07:58:29Z,"
 		<denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>
 , I'll report you on that later. It's 9PM my time. Thank you.
 		",11.0,chrismaliszewski,2020-10-03T07:59:54Z,"
 		Okay. Feel free to create an another issue, if something doesn't work with earlystopping
 		",12.0,chrismaliszewski,2020-10-07T10:36:08Z,"
 		Forgot to report. The code you suggested works. Thank you again, <denchmark-link:https://github.com/ydcjeff>@ydcjeff</denchmark-link>
 .
 		",,,,,,,,,,,,,run_training_epoch,self,494,594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"117,118,119,120,121,122,123,124,125,126,127,128,129,130,131",,test_on_train_batch_start_hook,"max_epochs,batch_idx_",117,131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3778,carmocca,2020-10-01T22:03:07Z,2020-10-04T21:10:26Z,training_step log requires that tbptt_reduce_fx is also set,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 training_step log requires that tbptt_reduce_fx is also set.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 def training_step(self, batch, batch_idx):
     ...
     self.log(""train_loss"", loss, on_step=False, on_epoch=True, sync_dist=True)
     self.log(
         ""foo"",
         torch.tensor(self.current_epoch),
         on_step=False,
         on_epoch=True,
         reduce_fx=max,
         #tbptt_reduce_fx=max, ERROR when commented
     )
     return loss
 
 def validation_step(self, batch, batch_idx):
     # No issues here
     self.log(
         ""bar"",
         torch.tensor(self.global_step),
         on_step=False,
         on_epoch=True,
         reduce_fx=max,
     )
 Error:
 <denchmark-code>time_outputs = [{'tr_loss': tensor(6.2107), 'foo': tensor(0), 'minimize': tensor(6.2107)}]
 
     @classmethod
     def reduce_across_time(cls, time_outputs):
         # auto-reduce across time for tbptt
         meta = time_outputs[0]['meta']
     
         # in 1.0 the results have 'extra'. Once we deprecate 0.10.0 we may not need this
         if 'extra' in time_outputs[0]:
             [x.pop('extra', None) for x in time_outputs]
     
         result = cls()
         result = recursive_gather(time_outputs, result)
         recursive_stack(result)
     
         for k, value in result.items():
             if k in ['meta', 'extra']:
                 continue
     
             # pick the reduce fx
             if k in ['checkpoint_on', 'early_stop_on', 'minimize']:
                 tbptt_reduce_fx = torch.mean
             else:
                 tbptt_reduce_fx = meta[k]['tbptt_reduce_fx']
 >           result[k] = tbptt_reduce_fx(value)
 E           RuntimeError: Can only calculate the mean of floating types. Got Long instead.
 
 ../../venv/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py:423: RuntimeError
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 That tbptt_reduce_fx is not required
 <denchmark-h:h3>Environment</denchmark-h>
 
 Master @ commit <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/8be002ccc7c2e8371ab426ea07c953f72747269e>8be002c</denchmark-link>
 
 	",89cc12311f5eaa7860d66bce9bfe3d93255f35b6,Carlos Mocholí,2020-10-04 17:10:25-04:00,MODIFY,1,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,423,423,1.0,carmocca,2020-10-02T13:01:25Z,"
 		good catch. yeah, that's a bug.
 want to submit a PR?
 		",2.0,carmocca,2020-10-02T14:31:12Z,"
 		I am not familiar with the tbptt logic, so maybe its better someone else does it.
 I'll create a PR with the failing test so somebody can work on it.
 		",3.0,carmocca,2020-12-22T23:48:14Z,"
 		What the hell is this tbptt, this looks crazy specific, why is this necessary? Why not keep things simple?
 		",,,,,MODIFY,3.0,tests\trainer\logging\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,1.0,"249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277",,test__training_step__log_max_reduce_fx,"tmpdir,batches,fx,result",249,277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,reduce_across_time,"cls,time_outputs",402,426,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"259,260,261,262,263",,test__training_step__log_max_reduce_fx.validation_step,"self,batch,batch_idx",259,263,1.0,"254,255,256,257",,test__training_step__log_max_reduce_fx.training_step,"self,batch,batch_idx",254,257,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3780,edenlightning,2020-10-02T01:21:06Z,2020-10-06T13:13:30Z,auto_scale_batch_size doesnt use 'binsearch',"
 I tried to following and it's still using power:
 <denchmark-code>#####################
 # 1. Init Model
 ##################### 
 
 model = LitAutoEncoder()
 
 #####################
 # 2. Init Trainer
 ##################### 
 trainer = pl.Trainer(auto_scale_batch_size='binsearch')
 
 #####################
 # 3. Tune
 #####################
 trainer.fit(model)
 </denchmark-code>
 
 Did we remove support? or is that a bug?
 	",f745c4a773fa9742a1c3cc9d051af0acbfe411ec,Nicki Skafte,2020-10-06 09:13:29-04:00,MODIFY,0,docs\source\training_tricks.rst,docs\source\training_tricks.rst,0.0,"57,64,65,66,67,68,69,72,73,74,75,76,77,81,83,86,88,100,104,111,112,113,118,121,123","57,64,65,66,67,69,71,72,73,77,79,82,84,96,100,107,108,109,114,117,118",1.0,edenlightning,2020-10-02T12:48:14Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  any idea?
 		",2.0,edenlightning,2020-10-02T13:06:03Z,"
 		<denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  it seems that this was changed during one of the refactors.
 No matter what the  argument is set to, the tuning will not run as part of  anymore.
 Instead user should call .
 		",3.0,edenlightning,2020-10-02T13:42:17Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  mind fix it?
 <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  where is this example so we shall edit it also there and make it as a tested example...
 		",4.0,edenlightning,2020-10-02T13:47:05Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  i am not sure if there is anything to fix, as I think the intention with the refactors was that the user should call .
 cc: <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",MODIFY,0.0,pytorch_lightning\tuner\tuning.py,pytorch_lightning\tuner\tuning.py,0.0,"40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,edenlightning,2020-10-02T13:57:27Z,"
 		I see, then just update the docs... :]
 		",6.0,edenlightning,2020-10-02T14:04:17Z,"
 		It actually is described correctly in the <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/training_tricks.html#auto-scaling-of-batch-size>docs</denchmark-link>
 
 <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
  is it an old example you have the code from?
 		",7.0,edenlightning,2020-10-04T16:22:20Z,"
 		sorry, wrong screenshot.
 <denchmark-link:https://camo.githubusercontent.com/c9ba3dc746e23063cccb4916cb6b1baa152f4d8320f635f0d31cd98e582f8378/68747470733a2f2f696d616765732e7a656e68756275736572636f6e74656e742e636f6d2f3565643632303035666462393232336162396639633831382f39613165633566652d373162632d343735612d396164662d363465383739623839303863></denchmark-link>
 
 its just using power instead of binsearch
 		",8.0,edenlightning,2020-10-04T18:07:33Z,"
 		I don't see anything wrong here.
 Seems like you are running on mnist. Mnist has 60000 samples, and it seems like you can fit it all in gpu memory. The batch size finder never go higher than the len of the train dataloader.
 In this case there will be no difference between modes (power and binsearch), as the binary search will first kick in after the power scaling fails the first time.
 		",9.0,edenlightning,2020-10-05T17:38:32Z,"
 		ok so i guess this is just a matter of documentation. Can you help clarify this behaviour in the docs?
 		",10.0,edenlightning,2020-10-05T19:43:31Z,"
 		Yes, will send i PR :]
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3797,carmocca,2020-10-02T15:05:18Z,2020-10-02T17:51:03Z,Broken ddp_cpu backend,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Broken on current master.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 def test(tmpdir):
     import pytorch_lightning as pl
     trainer = pl.Trainer(
         default_root_dir=tmpdir,
         max_epochs=3,
         limit_train_batches=10,
         limit_val_batches=10,
         distributed_backend=""ddp_cpu"",
     )
     model = DummyModule() # Linear layer on MNIST
     trainer.fit(model)
 Error:
 <denchmark-code>E       -- Process 0 terminated with the following error:
 E       Traceback (most recent call last):
 E         File ""/home/carmocca/projects/PyLaia/venv/lib/python3.8/site-packages/torch/multiprocessing/spawn.py"", line 20, in _wrap
 E           fn(i, *args)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py"", line 137, in ddp_train
 E           results = self.train_or_test()
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/base_backend.py"", line 43, in train_or_test
 E           results = self.trainer.train()
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 464, in train
 E           self.run_sanity_check(self.get_model())
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 649, in run_sanity_check
 E           _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/trainer.py"", line 571, in run_evaluation
 E           output = self.evaluation_loop.evaluation_step(test_mode, batch, batch_idx, dataloader_idx)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py"", line 149, in evaluation_step
 E           output = self.trainer.accelerator_backend.validation_step(args)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py"", line 157, in validation_step
 E           output = self.training_step(args)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/accelerators/ddp_cpu_spawn_backend.py"", line 153, in training_step
 E           output = self.trainer.model(*args)
 E         File ""/home/carmocca/projects/PyLaia/venv/lib/python3.8/site-packages/torch/nn/modules/module.py"", line 722, in _call_impl
 E           result = self.forward(*input, **kwargs)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/overrides/data_parallel.py"", line 209, in forward
 E           warn_missing_output(fx_called)
 E         File ""/home/carmocca/projects/PyLaia/venv/src/pytorch-lightning/pytorch_lightning/overrides/data_parallel.py"", line 228, in warn_missing_output
 E           warning_cache.warn(m)
 E       UnboundLocalError: local variable 'm' referenced before assignment
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Does not fail
 	",22efce8f400ab452ad1369e6ec9e8e733cc9a93d,Jirka Borovec,2020-10-02 13:51:02-04:00,MODIFY,2,pytorch_lightning\overrides\data_parallel.py,pytorch_lightning\overrides\data_parallel.py,1.0,"215,216,218","217,218",1.0,carmocca,2020-10-02T17:22:57Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  could it be linked to you refactoring?
 seems to be similar to <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3798>#3798</denchmark-link>
 
 		",2.0,carmocca,2020-10-02T17:46:55Z,"
 		just a typo... easy fix
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,warn_missing_output,fx_called,213,218,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,209,"209,210",forward,"self,inputs,kwargs",162,210,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3811,ananthsub,2020-10-03T01:14:40Z,2020-10-03T16:33:30Z,ModelCheckpoint not picking up metrics logged from lightning module,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The Model Checkpoint raises a misconfiguration error because metrics logged from validation epoch end are mysteriously unavailable to the callback
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-code>from typing import Optional
 import torch
 from pytorch_lightning import Trainer, LightningModule
 from pytorch_lightning.callbacks import ModelCheckpoint
 from torch.utils.data.dataset import Dataset
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
     def __getitem__(self, index):
         return self.data[index]
     def __len__(self):
         return self.len
 class TestModule(LightningModule):
     def __init__(self, epoch_min_loss_override: Optional[int] = None):
         """"""LightningModule for testing purposes
         Args:
             epoch_min_loss_override (int, optional): Pass in an epoch that will be set to the minimum
                 validation loss for testing purposes (zero based). If None this is ignored. Defaults to None.
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
         self.epoch_min_loss_override = epoch_min_loss_override
     def forward(self, x):
         return self.layer(x)
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
     def training_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss, ""checkpoint_on"": loss}
     def validation_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss, ""checkpoint_on"": loss}
     def test_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss}
     def training_epoch_end(self, outputs) -> None:
         avg_loss = torch.stack([x[""loss""] for x in outputs]).mean()
         self.log(""avg_loss"", avg_loss)
     def validation_epoch_end(self, outputs) -> None:
         avg_val_loss = torch.stack(
             [torch.randn(1, requires_grad=True) for _ in outputs]
         ).mean()
         # For testing purposes allow a nominated epoch to have a low loss
         if self.current_epoch == self.epoch_min_loss_override:
             avg_val_loss -= 1e10
         self.log(""avg_val_loss"", avg_val_loss)
         self.log(""checkpoint_on"", avg_val_loss)
     def test_epoch_end(self, outputs) -> None:
         avg_loss = torch.stack(
             [torch.randn(1, requires_grad=True) for _ in outputs]
         ).mean()
         self.log(""val_loss"", avg_loss)
     def configure_optimizers(self):
         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
     def train_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
     def val_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
     def test_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
 def train():
     checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=""avg_val_loss"")
     trainer = Trainer(
         max_epochs=epoch_min_loss_override + 2,
         logger=False,
         checkpoint_callback=checkpoint_callback,
     )
     model = TestModule(epoch_min_loss_override=2)
     lightning_trainer.fit(model)
 </denchmark-code>
 
 this is the error I see
 <denchmark-code>raise MisconfigurationException(m)
 pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor='avg_val_loss') not found in the returned metrics: ['avg_loss']. HINT: Did you call self.log('avg_val_loss', tensor) in the LightningModule?
 </denchmark-code>
 
 Full stacktrace:
 <denchmark-code>    lightning_trainer.fit(model)                                                                                                        File ""pytorch_lightning/trainer/trainer.py"", line 442, in fit
     results = self.accelerator_backend.train()
   File ""pytorch_lightning/accelerators/cpu_backend.py"", line 47, in train
     results = self.train_or_test()
   File ""pytorch_lightning/accelerators/base_backend.py"", line 43, in train_or_test
     results = self.trainer.train()
   File ""pytorch_lightning/trainer/trainer.py"", line 489, in train
     self.train_loop.run_training_epoch()
   File ""pytorch_lightning/trainer/training_loop.py"", line 538, in run_training_epoch
     self.trainer.run_evaluation(test_mode=False)
   File ""pytorch_lightning/trainer/trainer.py"", line 611, in run_evaluation
     self.evaluation_loop.on_evaluation_end()
   File ""pytorch_lightning/trainer/evaluation_loop.py"", line 95, in on_evaluation_end
     self.trainer.call_hook('on_validation_end', *args, **kwargs)
   File ""pytorch_lightning/trainer/trainer.py"", line 800, in call_hook
     trainer_hook(*args, **kwargs)
   File ""pytorch_lightning/trainer/callback_hook.py"", line 177, in on_validation_end
     callback.on_validation_end(self, self.get_model())
   File ""pytorch_lightning/callbacks/model_checkpoint.py"", line 167, in on_validation_end
     self.save_checkpoint(trainer, pl_module)
   File ""pytorch_lightning/callbacks/model_checkpoint.py"", line 197, in save_checkpoint
     self._validate_monitor_key(trainer)
   File ""pytorch_lightning/callbacks/model_checkpoint.py"", line 440, in _validate_monitor_key
     raise MisconfigurationException(m)
 pytorch_lightning.utilities.exceptions.MisconfigurationException: ModelCheckpoint(monitor='avg_val_loss') not found in the returned me
 trics: ['avg_loss']. HINT: Did you call self.log('avg_val_loss', tensor) in the LightningModule?
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 We can save the top-1 checkpoint with the monitor based on ""avg_val_loss""
 <denchmark-h:h3>Environment</denchmark-h>
 
 This is based on Lightning git revision <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/0c12065efd3cad98857895da43e02c6850317405>0c12065</denchmark-link>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",d9bc95f83e163f1ef0e64012ad086d4448410817,William Falcon,2020-10-03 12:33:29-04:00,MODIFY,1,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,1.0,"109,111","110,111",1.0,ananthsub,2020-10-03T02:28:14Z,"
 		For me this is happening only when no logs are created in validation_step: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/3000#issuecomment-702978090>#3000 (comment)</denchmark-link>
 
 		",,,,,,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector.py,1.0,"174,175,176,177",174,_track_callback_metrics,"self,eval_results,using_eval_result",173,211,MODIFY,1.0,pytorch_lightning\trainer\connectors\optimizer_connector.py,pytorch_lightning\trainer\connectors\optimizer_connector.py,1.0,"46,47,48,49,50,51,52,63","46,57",MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"231,232,233,234",,__run_eval_epoch_end,"self,num_dataloaders,using_eval_result",192,238,MODIFY,2.0,pytorch_lightning\trainer\optimizers.py,pytorch_lightning\trainer\optimizers.py,1.0,"98,101,102,103,104,105,106,107,108,109","100,101,102,103",configure_schedulers,"self,list,str",98,129,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,update_learning_rates,"self,str,monitor_metrics",28,92,_validate_condition_metric,"self,logs",107,122,1.0,"98,101,102,103,104,105,106,107,108,109","97,100,101,102,103",configure_schedulers,"self,list",97,123,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"534,535,536,537,538","541,542,543,544,545",run_training_epoch,self,494,594,,,,,,,,MODIFY,0.0,tests\base\__init__.py,tests\base\__init__.py,,,,,,,,0.0,5,,,,,,,,,,,,,MODIFY,1.0,tests\base\model_valid_epoch_ends.py,tests\base\model_valid_epoch_ends.py,1.0,54,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,validation_epoch_end,"self,outputs",33,56,,,,,,,,ADD,0.0,None,tests\base\simple_model.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,3.0,tests\callbacks\test_early_stopping.py,tests\callbacks\test_early_stopping.py,1.0,162,162,test_early_stopping_functionality.validation_epoch_end,"self,outputs",159,162,1.0,40,40,test_resume_early_stopping_from_checkpoint,tmpdir,32,69,1.0,"162,168","162,168",test_early_stopping_functionality,tmpdir,156,173,,,,,,,,MODIFY,10.0,tests\callbacks\test_model_checkpoint.py,tests\callbacks\test_model_checkpoint.py,1.0,185,185,MODIFY,1.0,tests\core\test_datamodules.py,tests\core\test_datamodules.py,1.0,230,,test_dm_checkpoint_save,tmpdir,220,238,,,,,,,,MODIFY,1.0,tests\loggers\test_all.py,tests\loggers\test_all.py,1.0,"85,86,87,88,89,90,91,92,94,95,96,97,98,99","85,86,87,88,89,91,92,93",test_loggers_fit_test,"wandb,tmpdir,monkeypatch,logger_class",44,99,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\models\test_amp.py,tests\models\test_amp.py,0.0,"13,174",,MODIFY,1.0,tests\models\test_restore.py,tests\models\test_restore.py,1.0,165,165,test_load_model_from_checkpoint,"tmpdir,model_template",155,193,,,,,,,,,,,,ADD,0.0,tests\trainer\data_flow\__init__.py,tests\trainer\data_flow\__init__.py,RENAME,0.0,tests\trainer\test_eval_loop_flow_1_0.py,tests\trainer\data_flow\test_eval_loop_flow_1_0.py,RENAME,0.0,tests\trainer\test_train_loop_flow_dict_1_0.py,tests\trainer\data_flow\test_train_loop_flow_dict_1_0.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_model_checkpoint_save_last,tmpdir,180,198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\trainer\test_train_loop_flow_scalar_1_0.py,tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,tests\trainer\legacy_deprecate_flow_log_tests\__init__.py,tests\trainer\legacy_deprecate_flow_log_tests\__init__.py,RENAME,0.0,tests\trainer\test_eval_loop_dict_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_eval_loop_dict_return.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"159,161,164,168,172,175","159,161,164,168,172,175",test_model_checkpoint_format_checkpoint_name,tmpdir,140,177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,127,127,test_model_checkpoint_no_extraneous_invocations,tmpdir,123,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,307,307,test_model_checkpoint_topk_all,tmpdir,302,319,1.0,416,416,test_model_checkpoint_save_last_warning,"tmpdir,caplog,max_epochs,should_validate,save_last",409,420,1.0,393,393,test_ckpt_metric_names_results,tmpdir,375,403,1.0,429,429,test_model_checkpoint_save_last_checkpoint_contents,tmpdir,423,459,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\trainer\test_trainer_steps_dict_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_trainer_steps_dict_return.py,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\trainer\test_trainer_steps_result_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_trainer_steps_result_return.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\trainer\test_trainer_steps_scalar_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_trainer_steps_scalar_return.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\trainer\test_validation_steps_result_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_validation_steps_result_return.py,,,,,,,,ADD,0.0,tests\trainer\logging\__init__.py,tests\trainer\logging\__init__.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,48,48,test_model_checkpoint_to_yaml,"tmpdir,save_top_k",43,57,1.0,28,28,test_model_checkpoint_with_non_string_input,"tmpdir,save_top_k",23,39,1.0,333,333,test_ckpt_metric_names,tmpdir,322,343,RENAME,0.0,tests\trainer\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,RENAME,0.0,tests\trainer\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,MODIFY,4.0,tests\trainer\test_optimizers.py,tests\trainer\test_optimizers.py,1.0,"138,139,140,141",,test_reduce_lr_on_plateau_scheduling.configure_optimizers,self,138,141,1.0,"115,121,122,123,124,125,126,127,128,129,130,131",131,test_reduce_lr_on_plateau_scheduling_missing_monitor,tmpdir,115,131,1.0,"134,135,136,137,138,139,140,141,142,143,144,156",,test_reduce_lr_on_plateau_scheduling,tmpdir,134,158,1.0,"195,203,214","170,178,189",test_optimizer_return_options,,161,215,MODIFY,3.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,668,668,test_test_checkpoint_path,"tmpdir,ckpt_path,save_top_k",660,692,1.0,511,511,test_resume_from_checkpoint_epoch_restored,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",474,543,1.0,435,435,test_model_checkpoint_only_weights,tmpdir,426,462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3813,ananthsub,2020-10-03T05:59:48Z,2020-11-11T17:05:25Z,Calling module.log(...) within a callback fails,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Calling pl_module.log(...) within a Callback fails, even though this is recommended by the documentation here: <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#logging-from-a-callback>https://pytorch-lightning.readthedocs.io/en/latest/loggers.html#logging-from-a-callback</denchmark-link>
 
 <denchmark-h:h3>Error</denchmark-h>
 
 <denchmark-code>  File ""my_callback_file.py"", line XX, in on_validation_epoch_end
     pl_module.log_dict(my_metrics_dict)
   File ""/home/local/USHERBROOKE/pain5474/opt/miniconda3/envs/cav/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py"", line 287, in log_dict
     self.log(
   File ""/home/local/USHERBROOKE/pain5474/opt/miniconda3/envs/cav/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py"", line 233, in log
     self._results.log(
   File ""/home/local/USHERBROOKE/pain5474/opt/miniconda3/envs/cav/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py"", line 171, in log
     self.__set_meta(
   File ""/home/local/USHERBROOKE/pain5474/opt/miniconda3/envs/cav/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py"", line 217, in __set_meta
     _internal = self['meta']['_internal']
 KeyError: '_internal'
 python-BaseException
 </denchmark-code>
 
 cc <denchmark-link:https://github.com/nathanpainchaud>@nathanpainchaud</denchmark-link>
 
 This is happening on master
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 We can log from callbacks using the lightning module
 <denchmark-h:h3>Environment</denchmark-h>
 
 Happening on PyTorch Lightning master
 	",3d202f9ecc4137b08cb5b1ac15af276456fcfaaf,chaton,2020-11-11 17:05:24+00:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"36,37,38,39,40,42",36,1.0,ananthsub,2020-10-07T09:14:54Z,"
 		<denchmark-link:https://github.com/ananthsub>@ananthsub</denchmark-link>
  I just tried on master and cannot reproduce (I think it was solved yesterday, as I could reproduce 2 days ago).
 		",2.0,ananthsub,2020-10-07T16:12:50Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  The issue was created by <denchmark-link:https://github.com/ananthsub>@ananthsub</denchmark-link>
  based on a question/issue I initially raised in the slack. I'll try to see if the bug is now resolved on master for me (it was still present as of yesterday afternoon) and I'll update you here as soon as I can.
 Edit: <denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  The issue persists on my end, after having just pulled the master. I'll try to write a test today that can reliably reproduce this behavior, and I'll open a draft PR and link it here once I do.
 		",3.0,ananthsub,2020-10-14T11:48:46Z,"
 		Hey <denchmark-link:https://github.com/nathanpainchaud>@nathanpainchaud</denchmark-link>
 ,
 Did you manage to reliably reproduce this behaviour? And if yes, could share the draft PR associated with this issue ?
 I will try to try it out too.
 Best regards,
 Thomas Chaton.
 		",4.0,ananthsub,2020-10-14T13:37:16Z,"
 		Hey <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 ,
 Thanks for the follow up! I opened the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4144>draft PR</denchmark-link>
  where I added a test that reproduces the behavior I'm getting.
 If I can help in any other way to get this sorted, just let me know!
 		",MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,1.0,"423,430,448,449,450,459,460,461",428,update_logger_connector,"self,str",411,464,MODIFY,14.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,229,"229,231",MODIFY,17.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"169,172,175,178,179",,evaluation_step,"self,test_mode,batch,batch_idx,dataloader_idx",165,190,MODIFY,2.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"604,607,608,613,617,622,623,624,625,626,627,628,629,630","578,585,604,607,608,609,610,611,612,613,614,615,616,617,623,626,627,628,629,630,631,632",run_evaluation,"self,bool,max_batches",550,635,5.0,ananthsub,2020-10-30T19:47:48Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  Any updates on whether this is a feature that's planned to be supported, or on the contrary it's been abandoned? I'm only asking this question because the issue has been labelled priority for a while, but every PR referring to it have been closed without getting merged 
 		",6.0,ananthsub,2020-10-30T21:49:33Z,"
 		Yes! thats been worked on these weeks. Starting to be merged now (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4439>#4439</denchmark-link>
 )
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,track_metrics_deprecated,"self,deprecated_eval_results,using_eval_result,test_mode",229,231,,,,,1.0,"868,869,892,893","877,878,879,880",call_hook,"self,hook_name,args,kwargs",867,894,,,,,,,,MODIFY,1.0,tests\models\test_hooks.py,tests\models\test_hooks.py,1.0,337,336,test_trainer_model_hook_system,tmpdir,148,362,,,,,,,,MODIFY,3.0,tests\trainer\legacy_deprecate_flow_log_tests\test_eval_loop_dict_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_eval_loop_dict_return.py,1.0,299,"302,303,304,305,306",on_evaluation_batch_start,"self,batch,batch_idx,dataloader_idx",298,306,1.0,193,193,test_val_step_step_end_no_return,tmpdir,169,199,1.0,47,47,test_validation_step_no_return,tmpdir,23,53,MODIFY,3.0,tests\trainer\logging\test_logger_connector.py,tests\trainer\logging\test_logger_connector.py,1.0,"247,248,249",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"254,255,256,257,258","256,257,258",prepare_eval_loop_results,self,254,258,1.0,64,"62,65",on_evaluation_batch_start,"self,testing,batch,dataloader_idx,num_dataloaders",62,67,1.0,"235,243,247,248,263,264,265","220,226,227,228,239,242,248,251",__run_eval_epoch_end,"self,num_dataloaders,using_eval_result",220,266,1.0,"350,351,352,353","350,355",log_evaluation_step_metrics_legacy,"self,output,batch_idx",350,355,1.0,"316,317","316,318,320",on_evaluation_batch_end,"self,args,kwargs",316,320,1.0,"215,216,217","215,216,217,218",log_epoch_metrics_on_evaluation_end,self,215,218,1.0,,"357,358,359",__log_result_step_metrics,"self,output,batch_idx",357,373,1.0,"213,215,216,217","213,214,215,216,217,218,219,220",log_epoch_metrics,"self,deprecated_eval_results,epoch_logs,test_mode",213,221,1.0,"199,200,201,202,206","197,201,202,203,204,211",evaluation_epoch_end,"self,num_dataloaders",197,211,1.0,,322,evaluation_batch_end_cleanup,"self,output,batch_idx,dataloader_idx",322,330,1.0,,"112,113",on_evaluation_end,"self,args,kwargs",106,113,,,,,,,,test__logger_connector__epoch_result_store__test_multi_dataloaders.on_test_epoch_end,self,247,249,1.0,"243,244,245",,test__logger_connector__epoch_result_store__test_multi_dataloaders.on_test_batch_end,"self,args,kwargs",243,245,MODIFY,28.0,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,1.0,"487,488,489",,test_log_works_in_val_callback.on_validation_batch_start,"self,trainer,pl_module,batch,batch_idx,dataloader_idx",487,489,1.0,"655,656,657",,test_log_works_in_test_callback.on_test_start,"self,trainer,pl_module",655,657,1.0,"503,504,505",,test_log_works_in_val_callback.on_epoch_end,"self,trainer,pl_module",503,505,1.0,"556,557,558,559,560,561,562,563,564",,test_log_works_in_val_callback.get_expected_output,"func_attr,original_values",556,564,1.0,"659,660,661",,test_log_works_in_test_callback.on_epoch_start,"self,trainer,pl_module",659,661,1.0,"671,672,673,674,675,676,677,678",,test_log_works_in_test_callback.on_test_batch_end,"self,trainer,pl_module,outputs,batch,batch_idx,dataloader_idx",671,678,1.0,"479,480,481",,test_log_works_in_val_callback.on_validation_epoch_start,"self,trainer,pl_module",479,481,1.0,"667,668,669",,test_log_works_in_test_callback.on_test_batch_start,"self,trainer,pl_module,batch,batch_idx,dataloader_idx",667,669,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,39,,__init__,"self,trainer",36,46,1.0,"260,261,262","260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276",get_evaluate_epoch_results,"self,test_mode",260,276,1.0,"256,257,258,259,260,261,262,289,292,308,321","256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331",_log_on_evaluation_epoch_end_metrics,"self,epoch_logs",256,331,,,,,,,,,,,,,,,1.0,78,78,test_validation_step_scalar_return,tmpdir,56,85,,,,,,,,1.0,"243,244,245,246,247,248,249,250,271,280","263,272",test__logger_connector__epoch_result_store__test_multi_dataloaders,"tmpdir,num_dataloaders",218,285,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"349,350,351,352,353,354",__rename_keys_by_dataloader_idx,"self,metrics,dataloader_idx,num_loaders",349,354,1.0,"242,243,244,245,246,247,248,249,250,251,252",,add_to_eval_loop_results,"self,dl_idx,has_been_initialized",242,252,1.0,"289,292,308,321","278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321",_track_callback_metrics,"self,eval_results,using_eval_result",278,321,1.0,"335,336,338","323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341",__process_eval_epoch_end_results_and_log_legacy_update,"self,prog_bar_metrics,log_metrics,callback_metrics",323,341,1.0,"241,242,243,244,245,246,247,248,249,250,251,252,254",241,_get_evaluate_epoch_results,"self,test_mode",241,254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"229,233,234,235,236,237,238,239","229,231,232,235,237,238,239",on_evaluation_epoch_end,"self,deprecated_eval_results,epoch_logs,using_eval_result,test_mode",229,239,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"740,741,742,743,744,745,746",,test_log_works_in_test_callback.get_expected_output,"func_attr,original_values",740,746,1.0,"663,664,665",,test_log_works_in_test_callback.on_test_epoch_start,"self,trainer,pl_module",663,665,1.0,"438,439",,test_log_works_in_val_callback.make_logging,"self,pl_module,func_name,func_idx,on_steps,on_epochs,prob_bars",438,439,1.0,,389,test_multi_dataloaders_add_suffix_properly,tmpdir,363,391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"346,349,350,351,352,353","346,347,348,349,350,355,357,358,359",__log_result_step_metrics,"self,step_log_metrics,step_pbar_metrics,batch_idx",346,364,1.0,"339,341,342,344,346","339,340,341,342,343,344,345,346,347",log_evaluation_step_metrics,"self,batch,batch_idx",339,347,1.0,"199,200,201,202,206,213","201,202,203,204,211,213",evaluation_epoch_end,self,199,213,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"233,234,235,236,237,238,239,240","235,237,238,239",evaluation_epoch_end,"self,testing",233,240,1.0,"335,336,338","333,334,335,336,337,338,339,340,341,342,343,344,345,346,347",add_to_eval_loop_results,"self,dl_idx,num_loaders",333,347,1.0,"334,338,339,341,342,344","339,340,341,342,343,344",log_evaluation_step_metrics,"self,output,batch_idx",334,344,1.0,"308,310,312,314,315",,on_evaluation_batch_end,"self,output,batch,batch_idx,dataloader_idx",308,315,1.0,317,"318,320,322",store_predictions,"self,output,batch_idx,dataloader_idx",317,325,1.0,"306,307,308,311,314","302,303,304,305,307,308,309,312",test_eval_logging_auto_reduce,tmpdir,252,314,1.0,"507,508,509",,test_log_works_in_val_callback.on_validation_epoch_end,"self,trainer,pl_module",507,509,1.0,"680,681,682",,test_log_works_in_test_callback.on_epoch_end,"self,trainer,pl_module",680,682,1.0,"595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782",,test_log_works_in_test_callback,tmpdir,595,782,1.0,"684,685,686",,test_log_works_in_test_callback.on_test_epoch_end,"self,trainer,pl_module",684,686,1.0,"695,696,697,698,699",,test_log_works_in_test_callback.test_step,"self,batch,batch_idx,dataloader_idx",695,699,1.0,"701,702",,test_log_works_in_test_callback.test_dataloader,self,701,702,1.0,"483,484,485",,test_log_works_in_val_callback.on_batch_start,"self,trainer,pl_module",483,485,1.0,"471,472,473",,test_log_works_in_val_callback.on_validation_start,"self,trainer,pl_module",471,473,1.0,"475,476,477",,test_log_works_in_val_callback.on_epoch_start,"self,trainer,pl_module",475,477,1.0,"422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592",,test_log_works_in_val_callback,tmpdir,422,592,1.0,"513,514,515,516",,test_log_works_in_val_callback.validation_step,"self,batch,batch_idx",513,516,1.0,"491,492,493",,test_log_works_in_val_callback.on_batch_end,"self,trainer,pl_module",491,493,1.0,"612,613",,test_log_works_in_test_callback.make_logging,"self,pl_module,func_name,func_idx,on_steps,on_epochs,prob_bars",612,613,1.0,322,323,test_eval_epoch_only_logging,"tmpdir,batches,log_interval,max_epochs",318,347,1.0,"495,496,497,498,499,500,501",,test_log_works_in_val_callback.on_validation_batch_end,"self,trainer,pl_module,outputs,batch,batch_idx,dataloader_idx",495,501,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3898,rotabulo,2020-10-06T10:39:25Z,2020-10-06T16:31:50Z,"TypeError: expected str, bytes or os.PathLike object, not NoneType","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I am summarizing the source of the issue to speedup the fix.
 After this line of code
 
 
 
 pytorch-lightning/pytorch_lightning/accelerators/ddp_backend.py
 
 
          Line 119
       in
       90929fa
 
 
 
 
 
 
  env_copy['PL_GLOBAL_SEED'] = os.environ.get('PL_GLOBAL_SEED') 
 
 
 
 
 
 I have that env_copy['PL_GLOBAL_SEED'] is None and having an environment variable set to None breaks subprocess.Popen here
 
 
 
 pytorch-lightning/pytorch_lightning/accelerators/ddp_backend.py
 
 
          Line 127
       in
       90929fa
 
 
 
 
 
 
  proc = subprocess.Popen(command, env=env_copy, cwd=cwd) 
 
 
 
 
 
 My fix at the moment is to add
 <denchmark-code>if env_copy['PL_GLOBAL_SEED'] is None:
                 del env_copy['PL_GLOBAL_SEED']
 </denchmark-code>
 
 after
 
 
 
 pytorch-lightning/pytorch_lightning/accelerators/ddp_backend.py
 
 
          Line 119
       in
       90929fa
 
 
 
 
 
 
  env_copy['PL_GLOBAL_SEED'] = os.environ.get('PL_GLOBAL_SEED') 
 
 
 
 
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
 	- GPU:
 	- available:         False
 	- version:           10.2
 * Packages:
 	- numpy:             1.18.5
 	- pyTorch_debug:     False
 	- pyTorch_version:   1.6.0
 	- pytorch-lightning: 0.10.0rc1
 	- tqdm:              4.48.0
 * System:
 	- OS:                Linux
 	- architecture:
 		- 64bit
 		- 
 	- processor:         x86_64
 	- python:            3.7.7
 	- version:           #100-Ubuntu SMP Wed Apr 22 20:32:56 UTC 2020
 </denchmark-code>
 
 	",e4a56fa5cfb5b67147c2013ae444ad0cd9a1b63a,Sean Naren,2020-10-06 12:31:49-04:00,MODIFY,1,pytorch_lightning\accelerators\ddp_backend.py,pytorch_lightning\accelerators\ddp_backend.py,1.0,"119,120,121",119,1.0,rotabulo,2020-10-06T13:53:17Z,"
 		Hey <denchmark-link:https://github.com/rotabulo>@rotabulo</denchmark-link>
  thanks for the report, could you describe the case/give a code example where  is None at this step?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_call_children_scripts,self,64,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3906,nathanpainchaud,2020-10-06T16:35:46Z,2020-10-06T19:27:32Z,Infinite recursion when calling `self.log(...)` in validation loop with dataset that returns string in item dict,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 I'm not sure if this is a behavior that was intended to be supported in the first place, but PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3888>#3888</denchmark-link>
  introduced a regression on passing strings as part of the data in a batch. Now, if we pass a dictionary where one of the values is a string,  falls into an infinite recursion loop when trying to log anything during the validation step.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 See PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3907>#3907</denchmark-link>
  for a test that reproduces the bug on the current master. The test becomes functional when commenting out line 22 (the call to  in the validation loop).
 The recursion happens in this specific statement in unpack_batch_size():
 elif isinstance(sample, Iterable):
     sample = next(iter(sample), 1)
     size = self.unpack_batch_size(sample)
 which recurses infinitely when sample is a string.
 The full stacktrace I get when running the test is the following:
 <denchmark-code>test_logging.py:38: 
 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 ../../pytorch_lightning/trainer/trainer.py:487: in fit
     results = self.accelerator_backend.train()
 ../../pytorch_lightning/accelerators/cpu_backend.py:47: in train
     results = self.train_or_test()
 ../../pytorch_lightning/accelerators/base_backend.py:47: in train_or_test
     results = self.trainer.train()
 ../../pytorch_lightning/trainer/trainer.py:509: in train
     self.run_sanity_check(self.get_model())
 ../../pytorch_lightning/trainer/trainer.py:699: in run_sanity_check
     _, eval_results = self.run_evaluation(test_mode=False, max_batches=self.num_sanity_val_batches)
 ../../pytorch_lightning/trainer/trainer.py:632: in run_evaluation
     step_metrics = self.evaluation_loop.log_evaluation_step_metrics(batch, batch_idx)
 ../../pytorch_lightning/trainer/evaluation_loop.py:323: in log_evaluation_step_metrics
     results.track_batch_size(batch)
 ../../pytorch_lightning/core/step_result.py:222: in track_batch_size
     batch_size = self.unpack_batch_size(batch)
 ../../pytorch_lightning/core/step_result.py:335: in unpack_batch_size
     size = self.unpack_batch_size(sample)
 ../../pytorch_lightning/core/step_result.py:338: in unpack_batch_size
     size = self.unpack_batch_size(sample)
 ../../pytorch_lightning/core/step_result.py:338: in unpack_batch_size
     size = self.unpack_batch_size(sample)
 E   RecursionError: maximum recursion depth exceeded in comparison
 !!! Recursion detected (same locals & position)
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 See See PR <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3907>#3907</denchmark-link>
 .
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 We should be able to use strings in the data returned by the dataset, and still be able to call self.log(...) in the validation loop.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 - GPU: TITAN Xp
 - available:         True
 - version:           10.2
 Packages:
 - numpy:             1.19.2
 - pyTorch_debug:     False
 - pyTorch_version:   1.6.0
 - pytorch-lightning: 0.9.1rc4
 - tqdm:              4.49.0
 System:
 - OS:                Linux
 - architecture: 64bit, ELF
 - processor:         x86_64
 - python:            3.8.5
 - version:           #51~18.04.1-Ubuntu SMP Sat Sep 5 14:35:50 UTC 2020
 
 Edit: Added reference to draft PR
 	",c510a7f90077140d60c47adf8e1e73638c2d1017,Nathan Painchaud,2020-10-06 15:27:18-04:00,MODIFY,3,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"415,416,417,418",409,,,,,,,,,,,,,,,,,MODIFY,3.0,tests\base\boring_model.py,tests\base\boring_model.py,1.0,"28,29",,__len__,self,28,29,MODIFY,4.0,tests\trainer\logging\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,1.0,"394,395",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_validation_step_with_string_data_logging.on_train_epoch_start,self,394,395,reduce_on_epoch_end,"cls,outputs",394,427,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"25,26",,__getitem__,"self,index",25,26,1.0,"21,22,23",,__init__,"self,size,length",21,23,1.0,"337,338",,unpack_batch_size,"self,sample",330,347,1.0,"222,223,224,225,226",222,track_batch_size,"self,batch",221,228,,,,,,,,1.0,"397,398,399,400",,test_validation_step_with_string_data_logging.training_step,"self,batch,batch_idx",397,400,1.0,"402,403,404,405,406",,test_validation_step_with_string_data_logging.validation_step,"self,batch,batch_idx",402,406,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421",,test_validation_step_with_string_data_logging,,392,421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
394,neggert,2019-10-18T23:21:11Z,2019-11-05T15:42:01Z,ModelCheckpoint wipes out current directory,"
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I think this is what you were seeing in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/389>#389</denchmark-link>
 . If we let the  create the default  callback and don't use , the prefix ends up being set to the current directory. Then, when  tries to clean up previous checkpoints, it wipes out everything in the current directory.
 Relevant bits of code:
 
 default_save_path set to os.getcwd(): https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L151
 ModelCheckpoint falls back to default_save_path: https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer.py#L280
 ( ModelCheckpoint blows away pre-existing files in checkpoint directory: https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/callbacks/pt_callbacks.py#L216
 
 The most obvious fix is to provide a better default checkpoint prefix, but there would still be a lurking footgun for a user who sets default_save_path incorrectly. Should we maybe insist that the checkpoint directory not exist before training starts, or that it be empty?
 	",9fa28066059c3bda0b022c33921796c7425cd41e,Nic Eggert,2019-11-05 10:41:59-05:00,MODIFY,0,pytorch_lightning\callbacks\pt_callbacks.py,pytorch_lightning\callbacks\pt_callbacks.py,0.0,"182,183,184,185,186,187,188,189,190,191",,1.0,neggert,2019-10-19T05:09:20Z,"
 		most of the time it makes sense to have the default checkpoint inside the experiment file. why don't we do that with mlflowLogger? mlflow_exp/checkpoints
 		",2.0,neggert,2019-10-21T15:19:38Z,"
 		MLFlow doesn't always have an experiment file locally. You can set it up to do all logging to a centralized server. Presumably other loggers will have similar issues. I'm not sure it makes sense to tightly couple checkpoint saving to the logger as a general principle.
 How about this proposal:
 
 Provide a default value of default_save_path=""./checkpoints""
 Add a new (required) name property to LightningLoggerBase and make the existing version property required
 Have the default ModelCheckpoint save to os.path.join(default_save_path, logger.name, logger.version) if a logger is defined, otherwise default_save_path.
 Add a warning in ModelCheckpoint if filepath already exists and has files in it.
 
 I think this should prevent accidentally deleting files unless the user manually sets default_save_path=os.getcwd() and ignores the warning. I'm happy to do a PR if this sounds good.
 		",3.0,neggert,2019-10-22T17:40:58Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Thoughts on the above proposal? If this is what we want to do, you can assign to me.
 		",4.0,neggert,2019-10-22T18:12:30Z,"
 		yeah, i like that. But default logging should be os.path.join(default_save_path, logger.name, logger.version, '/checkpoints')
 Isn't default_save_path also used for the logger path? and also SLURM checkpointing, etc...?  it shouldn't go to ./checkpoints specifically
 		",MODIFY,2.0,pytorch_lightning\logging\base.py,pytorch_lightning\logging\base.py,1.0,76,,version,self,74,76,MODIFY,2.0,pytorch_lightning\logging\mlflow_logger.py,pytorch_lightning\logging\mlflow_logger.py,1.0,"69,70",,MODIFY,2.0,pytorch_lightning\logging\test_tube_logger.py,pytorch_lightning\logging\test_tube_logger.py,1.0,32,32,experiment,self,26,39,MODIFY,1.0,pytorch_lightning\trainer\callback_config_mixin.py,pytorch_lightning\trainer\callback_config_mixin.py,1.0,"17,18,20,21,22,23,25","15,16,18,19,20,22",configure_checkpoint_callback,self,8,42,5.0,neggert,2019-10-22T18:15:29Z,"
 		Yeah, I think with the other changes, leaving default_save_path=os.getcwd() is fine. I was thinking about the case where there's no logger, but the default checkpoint, but I think we can just fall back to checkpoints in that case.
 		",6.0,neggert,2019-10-22T18:18:03Z,"
 		I notice that for test tube, we're pre-pending version_ before the version. Is that needed? If so, I maybe we can do it in the loggers version property.
 		",7.0,neggert,2019-10-23T10:20:12Z,"
 		<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  i made the fix to the PR on version_. See comments there
 		",8.0,neggert,2019-11-05T16:26:18Z,"
 		Thanks for this fix <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
 . I just tested this with a remote databricks workspace, e.g.
 <denchmark-code>mlf_logger = MLFlowLogger(
             experiment_name=""/Users/xxxx/xxx"",
             tracking_uri=""databricks""
         )
 </denchmark-code>
 
 Now the experiment name is exposed as the name of the logger which in turn is used to create the checkpoint directory. Long story short it tries to create a directory  under the root partition. Now this a loud error and still better than cleaning out the current directory  but I'm still looking for a fix. Any pointers on how to address this and I can take a stab at it (also <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  ). I'll create an issue.
 		",9.0,neggert,2019-11-05T16:54:05Z,"
 		Why are you setting the experiment name to a file path? Is this something that's required with databricks?
 You might want to take a look at the default_save_path argument to Trainer. The checkpoints will be saved in the result of
                 ckpt_path = os.path.join(
                     self.default_save_path,
                     self.logger.name,
                     f'version_{self.logger.version}',
                     ""checkpoints""
                 )
 All of those parameters except the final ""checkpoints"" are adjustable, so you should be able to get the checkpoints to save wherever you want.
 		",10.0,neggert,2019-11-05T16:55:44Z,"
 		let’s resolve this today to make the release for tomorrow
 		",11.0,neggert,2019-11-05T17:03:55Z,"
 		Oh, I see. The databricks tracking URI is ""special"". <denchmark-link:https://mlflow.org/docs/latest/quickstart.html#quickstart-logging-to-remote-server>https://mlflow.org/docs/latest/quickstart.html#quickstart-logging-to-remote-server</denchmark-link>
 
 In this case, I'd suggest just creating your ModelCheckpoint callback manually rather than relying on the default.
 I'm not sure any of the core contributors use Databricks, so we'd probably need some help from a Databricks user to get this working smoothly.
 		",12.0,neggert,2019-11-05T17:30:50Z,"
 		<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  thanks for looking into this and the suggestion - fair enough, I'll take that route.
 		",,,,,,,,,,,,,version,self,69,70,,,,,,,,,,,,,,,,,,,MODIFY,9.0,tests\test_y_logging.py,tests\test_y_logging.py,1.0,"108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137","108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137",test_mlflow_pickle,,108,137,1.0,"162,163","162,163",test_custom_logger.name,self,162,163,,,,,1.0,"84,85,86,87,88",,name,self,84,88,,,,,,,,,,,,,,,,,,,,,,1.0,"69,70,71",71,name,self,69,71,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"65,66",,name,self,65,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105","74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105",test_mlflow_logger,,74,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186","140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182",test_custom_logger,tmpdir,140,186,1.0,"166,167","166,167",test_custom_logger.version,self,166,167,1.0,"154,155","154,155",test_custom_logger.log_metrics,"self,metrics,step_num",154,155,1.0,"150,151","150,151",test_custom_logger.log_hyperparams,"self,params",150,151,1.0,"143,144,145,146,147","143,144,145,146,147",test_custom_logger.__init__,self,143,147,1.0,"158,159","158,159",test_custom_logger.finalize,"self,status",158,159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3945,ananthsub,2020-10-07T16:53:58Z,2020-10-07T17:46:28Z,Unexpected signature for validation_step,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-code>TypeError: validation_step() takes 3 positional arguments but 4 were given
 </denchmark-code>
 
 Full stacktrace: <denchmark-link:https://gist.github.com/ananthsub/cbe20db7f5bf22fdcbe77c0e0f8c3e49>https://gist.github.com/ananthsub/cbe20db7f5bf22fdcbe77c0e0f8c3e49</denchmark-link>
 
 This test is passing for me on 0.9.1rc4, but not on master
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-code>from typing import Optional
 import unittest
 import torch
 from pytorch_lightning import LightningModule
 from torch.utils.data.dataset import Dataset
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
     def __getitem__(self, index):
         return self.data[index]
     def __len__(self):
         return self.len
 class TestModule(LightningModule):
     def __init__(self, epoch_min_loss_override: Optional[int] = None):
         """"""LightningModule for testing purposes
         Args:
             epoch_min_loss_override (int, optional): Pass in an epoch that will be set to the minimum
                 validation loss for testing purposes (zero based). If None this is ignored. Defaults to None.
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
         self.epoch_min_loss_override = epoch_min_loss_override
     def forward(self, x):
         return self.layer(x)
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
     def training_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss, ""checkpoint_on"": loss}
     def validation_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss, ""checkpoint_on"": loss}
     def test_step(self, batch, batch_idx):
         output = self.forward(batch)
         loss = self.loss(batch, output)
         return {""output"": output, ""loss"": loss}
     def training_epoch_end(self, outputs) -> None:
         avg_loss = torch.stack([x[""loss""] for x in outputs]).mean()
         self.log(""avg_loss"", avg_loss)
     def validation_epoch_end(self, outputs) -> None:
         avg_val_loss = torch.stack(
             [torch.randn(1, requires_grad=True) for _ in outputs]
         ).mean()
         # For testing purposes allow a nominated epoch to have a low loss
         if self.current_epoch == self.epoch_min_loss_override:
             avg_val_loss -= 1e10
         self.log(""val_loss"", avg_val_loss)
         self.log(""checkpoint_on"", avg_val_loss)
     def test_epoch_end(self, outputs) -> None:
         avg_loss = torch.stack(
             [torch.randn(1, requires_grad=True) for _ in outputs]
         ).mean()
         self.log(""val_loss"", avg_loss)
     def configure_optimizers(self):
         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
     def train_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
     def val_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
     def test_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64))
 class TestPL(unittest.TestCase):
     def test_strict_model_load(self):
         model = TestModule()
         model.layer = torch.nn.Linear(32, 4)
         checkpoint = ModelCheckpoint(
             save_top_k=1,
             monitor=""val_loss"",
         )
         trainer = Trainer(
             checkpoint_callback=checkpoint,
             logger=logger,
             overfit_batches=0.20,
             max_epochs=1,
         )
         result = trainer.fit(model)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The test doesn't crash, and trainer.fit() works
 	",6044cf900317ec9542fb1745976c9a96cc70b396,William Falcon,2020-10-07 13:46:27-04:00,MODIFY,0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,0.0,"238,239,240","245,246,247",1.0,ananthsub,2020-10-07T17:15:15Z,"
 		fixed!
 		",,,,,,,,,,,,,MODIFY,1.0,tests\trainer\flags\test_overfit_batches.py,tests\trainer\flags\test_overfit_batches.py,1.0,"40,41,42,43,44,45,46,47,48,49,50,51,52,53,54",,test_overfit_basic,"tmpdir,overfit",40,54,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3974,nrupatunga,2020-10-08T04:31:02Z,2020-10-08T14:20:56Z,[Bug]: Late update of Trainer `current_epoch` property for `LightningDataModule`,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Late update of Trainer current_epoch property for LightningDataModule object.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 The below code reproduces the issue:
 Please check for the print logs for the current_epoch number in train_dataloader.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torchvision import transforms
 from torchvision.datasets import MNIST
 from torch.utils.data import random_split, DataLoader
 
 import pytorch_lightning as pl
 
 
 class LitModel(pl.LightningModule):
 
     def __init__(self, channels, width, height, num_classes, hidden_size=64, learning_rate=2e-4):
 
         super().__init__()
 
         # We take in input dimensions as parameters and use those to dynamically build model.
         self.channels = channels
         self.width = width
         self.height = height
         self.num_classes = num_classes
         self.hidden_size = hidden_size
         self.learning_rate = learning_rate
 
         self.model = nn.Sequential(
             nn.Flatten(),
             nn.Linear(channels * width * height, hidden_size),
             nn.ReLU(),
             nn.Dropout(0.1),
             nn.Linear(hidden_size, hidden_size),
             nn.ReLU(),
             nn.Dropout(0.1),
             nn.Linear(hidden_size, num_classes))
 
     def forward(self, x):
         x = self.model(x)
         return F.log_softmax(x, dim=1)
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         logits = self(x)
         loss = F.nll_loss(logits, y)
         return loss
 
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
         return optimizer
 
 
 class MNISTDataModule(pl.LightningDataModule):
 
     def __init__(self, data_dir: str = './'):
         super().__init__()
         self.data_dir = data_dir
         self.transform = transforms.Compose([
             transforms.ToTensor(),
             transforms.Normalize((0.1307,), (0.3081,))
         ])
 
         # self.dims is returned when you call dm.size()
         # Setting default dims here because we know them.
         # Could optionally be assigned dynamically in dm.setup()
         self.dims = (1, 28, 28)
         self.num_classes = 10
 
     def prepare_data(self):
         # download
         MNIST(self.data_dir, train=True, download=True)
         MNIST(self.data_dir, train=False, download=True)
 
     def setup(self, stage=None):
 
         # Assign train/val datasets for use in dataloaders
         if stage == 'fit' or stage is None:
             mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)
             self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])
 
         # Assign test dataset for use in dataloader(s)
         if stage == 'test' or stage is None:
             self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)
 
     def train_dataloader(self):
         print('\n----------------------------')
         print(f'Current epoch: {self.trainer.current_epoch}')
         print('----------------------------')
         if self.trainer.current_epoch > 2:
             return DataLoader(self.mnist_train, batch_size=32)
         else:
             return DataLoader(self.mnist_train, batch_size=32)
 
 
 # Init DataModule
 dm = MNISTDataModule()
 # Init model from datamodule's attributes
 model = LitModel(*dm.size(), dm.num_classes)
 # Init trainer
 trainer = pl.Trainer(max_epochs=5, progress_bar_refresh_rate=20, gpus=1, reload_dataloaders_every_epoch=True)
 # Pass the datamodule as arg to trainer.fit to override model hooks :)
 trainer.fit(model, dm)
 </denchmark-code>
 
 <denchmark-h:h4>logs</denchmark-h>
 
 Note: current_epoch is 0 two time, which is indeed due to the late update of this property
 <denchmark-code>----------------------------
 Current epoch: 0
 ----------------------------
 /home/nthere/2020/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
   warnings.warn(*args, **kwargs)
 Epoch 0:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 1700/1719 [00:10<00:00, 157.72it/s, loss=0.284, v_num=16]
 ----------------------------
 Current epoch: 0
 ----------------------------
 Epoch 1:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 1700/1719 [00:10<00:00, 164.23it/s, loss=0.200, v_num=16]
 ----------------------------
 Current epoch: 1
 ----------------------------
 Epoch 2:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏ | 1700/1719 [00:10<00:00, 164.02it/s, loss=0.168, v_num=16]
 ----------------------------
 Current epoch: 2
 ----------------------------
 Epoch 3:  10%|████████████████▊                                                                                                                                                | 180/1719 [00:01<00:09, 163.17it/s, loss=0.185, v_num=16]
 ^C/home/nthere/2020/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
   warnings.warn(*args, **kwargs)
 Epoch 3:  10%|████████████████▊                                                                                                                                                | 180/1719 [00:01<00:10, 150.43it/s, loss=0.185, v_num=16]
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 current_epoch should be updated and reflect the right epoch number for the LightningDataModule
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>* CUDA:
         - GPU:
                 - GeForce GTX 960
         - available:         True
         - version:           10.1
 * Packages:
         - numpy:             1.18.5
         - pyTorch_debug:     False
         - pyTorch_version:   1.5.0+cu101
         - pytorch-lightning: 0.10.0
         - tqdm:              4.47.0
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - ELF
         - processor:         x86_64
         - python:            3.8.3
         - version:           #113~16.04.1-Ubuntu SMP Fri Jul 10 04:37:08 UTC 2020
 
 </denchmark-code>
 
 	",fcfa5874923000a4b391c88b6488e065aee4d671,Nrupatunga,2020-10-08 10:20:55-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,23,,1.0,nrupatunga,2020-10-08T04:31:48Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,nrupatunga,2020-10-08T05:08:41Z,"
 		great catch <denchmark-link:https://github.com/nrupatunga>@nrupatunga</denchmark-link>
  !
 
 this bug happens specifically when using the flag reload_dataloaders_every_epoch
 this is because reload_dataloaders_every_epoch  happens before train_epoch_start
 trainer.current_epoch is updated in train_epoch_start, so the epoch state when used inside of train_dataloader is stale
 
 We could:
 
 set trainer.current_epoch directly in the trainer at the beginning of the loop
 Move reload_dataloaders_every_epoch into train_epoch_start
 Or both?
 
 What do you think <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
  ?
 		",3.0,nrupatunga,2020-10-08T05:11:31Z,"
 		Suggested changes in this PR
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3975>#3975</denchmark-link>
 
 Please review let me know if there is a better way, Thank you
 		",,,,,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,,"456,457,458,459",train,self,438,503,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"209,210,211,212,215,216,217,218","217,218,219",MODIFY,5.0,tests\core\test_datamodules.py,tests\core\test_datamodules.py,1.0,"424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444",,test_dm_reload_dataloaders_every_epoch,tmpdir,424,444,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_train_epoch_start,"self,epoch",208,239,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"406,407",,prepare_data,self,406,407,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"409,410,411,412,413,414,415",,setup,"self,None",409,415,1.0,"401,402,403,404",,__init__,"self,str",401,404,1.0,"417,418,419,420,421",,train_dataloader,self,417,421,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3993,hbredin,2020-10-08T15:07:54Z,2020-10-09T03:02:24Z,Mismatch between docstring and code regarding when `on_load_checkpoint` hook is called,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The docstring of on_load_checkpoint hook says that it is called before trying to load_state_dict:
 
 
 
 pytorch-lightning/pytorch_lightning/core/saving.py
 
 
         Lines 203 to 206
       in
       cea5f1f
 
 
 
 
 
 
  def on_load_checkpoint(self, checkpoint: Dict[str, Any]) -> None: 
 
 
 
  """""" 
 
 
 
          Do something with the checkpoint. 
 
 
 
          Gives model a chance to load something before ``state_dict`` is restored. 
 
 
 
 
 
 However, in LightningModule.load_from_checkpoint, it is called after load_state_dict:
 
 
 
 pytorch-lightning/pytorch_lightning/core/saving.py
 
 
         Lines 195 to 199
       in
       cea5f1f
 
 
 
 
 
 
  # load the state_dict on the model automatically 
 
 
 
  model.load_state_dict(checkpoint['state_dict'], strict=strict) 
 
 
 
  
 
 
 
  # give model a chance to load something 
 
 
 
  model.on_load_checkpoint(checkpoint) 
 
 
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Related discussion on Slack: <denchmark-link:https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000>https://pytorch-lightning.slack.com/archives/CQXV8BRH9/p1602168345184000</denchmark-link>
 
 I think the docstring is correct and the call to on_load_checkpoint should be moved right before load_state_dict to give the model a chance to call setup.
 	",a8573b005224bde87eb7a81ccdf5f428620c121f,Hervé BREDIN,2020-10-08 23:02:23-04:00,MODIFY,1,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,"198,199,200","195,196,197",1.0,hbredin,2020-10-08T15:46:44Z,"
 		<denchmark-link:https://github.com/hbredin>@hbredin</denchmark-link>
  mind sending a PR to fix it... 
 		",2.0,hbredin,2020-10-08T15:47:39Z,"
 		I can do that. Should I fix the docstring or the code?
 I'd go with the code.
 		",3.0,hbredin,2020-10-09T03:03:33Z,"
 		I need this code change as well! I'm doing transfer learning and I want to support both loading the original model with the original weights, and modify it for a new task.
 on_load_checkpoint would allow me to redo the modifications I've done for transfer learning, so the state_dict of the transferred model can be properly restored.
 At present I need to add non-network code to the model to handle this logic, which is ugly and prone to bugs.
 This would allow to have the same model to redo the modifications I've made for transfer learning,
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_load_model_state,"cls,str,bool,cls_kwargs_new",157,201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4001,wyessen,2020-10-08T19:23:57Z,2020-10-20T19:07:27Z,on_train_epoch_end and on_epoch_end are out of order,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Consider the following order in which the  hooks are called from <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2816>#2816</denchmark-link>
  (I have confirmed that in PytorchLightning version 0.10 this is still an issue):
 <denchmark-code>on_epoch_start
 on_train_epoch_start
 on_validation_start
 on_validation_epoch_start
 on_validation_epoch_end
 on_validation_end
 on_epoch_end
 on_train_epoch_end
 </denchmark-code>
 
 Naturally one would expect the opening and closing scope hooks to match. However, on_train_epoch_end is called after on_epoch_end, which seems incorrect. It is natural to open the epoch scope before the train epoch scope (as is being done currently), in which case the epoch scope should be closed after closing the train epoch scope (which is not currently being done)
 
 PyTorch Version (e.g., 1.0): 1.6.0
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): pip
 Build command you used (if compiling from source):
 Python version: 3.8.5
 CUDA/cuDNN version: NA
 GPU models and configuration: NA
 Any other relevant information: NA
 
 	",3777988502d1013508455a5fd34dc7d1a7e8e035,Jirka Borovec,2020-10-20 13:33:46+01:00,MODIFY,1,pytorch_lightning\utilities\model_utils.py,pytorch_lightning\utilities\model_utils.py,1.0,26,"26,27,28",1.0,wyessen,2020-10-08T19:24:37Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,wyessen,2020-10-08T20:29:59Z,"
 		<denchmark-link:https://github.com/wyessen>@wyessen</denchmark-link>
  it seems that the flow is incorrect the  shall be before 
 cc: <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 
 		",3.0,wyessen,2020-10-08T21:38:37Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 True, but depends how we should look at it: should validation be considered part of the training epoch scope? If so, then the current flow is fine; otherwise, you're right, it's incorrect.
 So, the original bug report complains about incorrect closing of the scope given the order in which the scopes were opened. You raise a valid issue, and in the broader scheme of things the current flow should be reconsidered.
 		",4.0,wyessen,2020-10-08T21:57:15Z,"
 		yes. validation is part of the flow. as mentioned many times, big research requires checking val multiple times within an epoch
 Train: --------------------------- (1 epoch = 2 days)
 Val                --             --             --
 Then we have:
 e1 = on_epoch_start
 e2 = on_epoch_end
 t1 = on_train_epoch_start
 t2 = on_train_epoch_end
 v1 = on_val_epoch_start
 v2 = on_val_epoch_end
 Train:          e1 t1 ------------------------------------------------------------- t2 e2
 Val   :                    ________(e1 v1 --val--- v2 e2) ___ (e1 v1 --val---v2 e2)
 		",MODIFY,36.0,tests\models\test_hooks.py,tests\models\test_hooks.py,1.0,"252,253,254",,test_trainer_model_hook_system.on_test_start,self,252,254,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,wyessen,2020-10-08T22:06:56Z,"
 		here is an added test to check the actual flow <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4010>#4010</denchmark-link>
 
 I think the confusion comes from using smaller models or just one validation per epoch, then you would expect to have called the validation after training...
 		",6.0,wyessen,2020-10-08T22:55:43Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  So as to not hijack the original bug report, I want to clarify:
 The flow executed by PytorchLightning is incorrect in the sense that opening of a scope (with _start) does not match closing of the scope with (with _end). In particular on_epoch_end is called before on_train_epoch_end, which is not correct.
 		",7.0,wyessen,2020-10-20T17:53:27Z,"
 		<denchmark-link:https://github.com/SeanNaren>@SeanNaren</denchmark-link>
  Why did you close this issue? Your PR does not fix this.
 		",8.0,wyessen,2020-10-20T18:58:03Z,"
 		Apologies, I think the PR associated with this issue was incorrect!
 EDIT: after looking at the associated PR and the discussion here, I do think this PR addresses the issue of ensuring the order is correct. Was there anything in particular that wasn't addressed <denchmark-link:https://github.com/wyessen>@wyessen</denchmark-link>
 ?
 		",9.0,wyessen,2020-10-20T19:06:41Z,"
 		<denchmark-link:https://github.com/wyessen>@wyessen</denchmark-link>
  this was closed with <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  explanation that the behavior is as expected 
 		",10.0,wyessen,2020-10-20T19:07:27Z,"
 		Will close again for now...
 		",11.0,wyessen,2020-10-20T19:10:46Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  the behavior is not expected, please read my explanation (<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  was responding to <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 ’s message, which was different from my original issue).
 		",,,,,,,,,,,,,,,,,,,,,is_overridden,"str,LightningModule",21,43,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"168,169,170",,test_trainer_model_hook_system.on_epoch_end,self,168,170,1.0,"248,249,250",,test_trainer_model_hook_system.on_validation_epoch_end,self,248,250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"208,209,210",,test_trainer_model_hook_system.on_train_end,self,208,210,1.0,"256,257,258",,test_trainer_model_hook_system.on_test_end,self,256,258,1.0,"224,225,226",,test_trainer_model_hook_system.on_train_epoch_end,"self,outputs",224,226,,,,,,,,,,,,,,,1.0,"164,165,166",,test_trainer_model_hook_system.on_epoch_start,self,164,166,1.0,"172,173,174",,test_trainer_model_hook_system.on_fit_start,self,172,174,1.0,"204,205,206",,test_trainer_model_hook_system.on_train_start,self,204,206,1.0,"268,269,270",,test_trainer_model_hook_system.on_test_epoch_start,self,268,270,1.0,"280,281,282",,test_trainer_model_hook_system.on_validation_model_train,self,280,282,1.0,"160,161,162",,test_trainer_model_hook_system.on_before_zero_grad,"self,optimizer",160,162,1.0,"232,233,234",,test_trainer_model_hook_system.on_validation_end,self,232,234,1.0,"200,201,202",,test_trainer_model_hook_system.on_pretrain_routine_end,self,200,202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"260,261,262",,test_trainer_model_hook_system.on_test_batch_start,"self,batch,batch_idx,dataloader_idx",260,262,1.0,"196,197,198",,test_trainer_model_hook_system.on_pretrain_routine_start,self,196,198,1.0,"180,181,182",,test_trainer_model_hook_system.on_hpc_load,"self,checkpoint",180,182,1.0,"188,189,190",,test_trainer_model_hook_system.on_load_checkpoint,"self,checkpoint",188,190,1.0,"152,153,154",,test_trainer_model_hook_system.__init__,self,152,154,1.0,"216,217,218",,test_trainer_model_hook_system.on_train_batch_end,"self,outputs,batch,batch_idx,dataloader_idx",216,218,1.0,"264,265,266",,test_trainer_model_hook_system.on_test_batch_end,"self,outputs,batch,batch_idx,dataloader_idx",264,266,1.0,"176,177,178",,test_trainer_model_hook_system.on_fit_end,self,176,178,1.0,"272,273,274",,test_trainer_model_hook_system.on_test_epoch_end,self,272,274,1.0,"276,277,278",,test_trainer_model_hook_system.on_validation_model_eval,self,276,278,1.0,"228,229,230",,test_trainer_model_hook_system.on_validation_start,self,228,230,1.0,"148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358",,test_trainer_model_hook_system,tmpdir,148,358,1.0,"212,213,214",,test_trainer_model_hook_system.on_train_batch_start,"self,batch,batch_idx,dataloader_idx",212,214,1.0,"284,285,286",,test_trainer_model_hook_system.on_test_model_eval,self,284,286,1.0,"156,157,158",,test_trainer_model_hook_system.on_after_backward,self,156,158,1.0,"236,237,238",,test_trainer_model_hook_system.on_validation_batch_start,"self,batch,batch_idx,dataloader_idx",236,238,1.0,"192,193,194",,test_trainer_model_hook_system.on_save_checkpoint,"self,checkpoint",192,194,1.0,"288,289,290",,test_trainer_model_hook_system.on_test_model_train,self,288,290,1.0,"220,221,222",,test_trainer_model_hook_system.on_train_epoch_start,self,220,222,1.0,"240,241,242",,test_trainer_model_hook_system.on_validation_batch_end,"self,outputs,batch,batch_idx,dataloader_idx",240,242,1.0,"244,245,246",,test_trainer_model_hook_system.on_validation_epoch_start,self,244,246,1.0,"184,185,186",,test_trainer_model_hook_system.on_hpc_save,"self,checkpoint",184,186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4011,pbmstrk,2020-10-08T22:12:05Z,2020-10-09T23:10:49Z,Broken link in Documentation,"
 <denchmark-h:h2>📚 Documentation</denchmark-h>
 
 The Module Index link at the bottom of the main page of the Lightning documentation is broken. This seems to be because the make html command does not create a py-modindex.html file (not sure why).
 If the Module Index page is not required a solution is to remove * :ref: modindex from the index.rst file.
 Additionally, below the Module Index link there is a link to a search page, that is currently empty. Seeing as searching is possible in the sidebar, not sure if the page is required, so could remove * :ref: search as well.
 Not super familiar with Sphinx but think this wouldn't break anything.
 	",9e919763231d6f210f711515e496c953e6411a57,Paul Baumstark,2020-10-09 19:10:48-04:00,MODIFY,0,docs\source\index.rst,docs\source\index.rst,0.0,,133,1.0,pbmstrk,2020-10-09T12:21:35Z,"
 		<denchmark-link:https://github.com/pbmstrk>@pbmstrk</denchmark-link>
  not sure what you mean, mind share some screenshot?
 or better send a PR with fix... ? 
 		",2.0,pbmstrk,2020-10-09T12:49:17Z,"
 		It's a very minor issue, but seeing as the link is broken and on the main page thought it was worthwhile raising.
 Below is the screenshot (can be found by scrolling to the bottom of <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest/>https://pytorch-lightning.readthedocs.io/en/latest/</denchmark-link>
 ),
 <denchmark-link:https://user-images.githubusercontent.com/41694868/95583318-d8b0b000-0a33-11eb-852b-6c0e00327a69.png></denchmark-link>
 
 The Module Index link is broken and the Search Page link leads to an empty page. I believe Sphinx automatically adds links to Index, Module Index, and Search Page to index.rst when first creating the docs (see <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blame/master/docs/source/index.rst#L132-L134>here</denchmark-link>
 ). I can send a PR to remove them if you think the links are not necessary.
 		",3.0,pbmstrk,2020-10-09T13:58:43Z,"
 		ok, <denchmark-link:https://github.com/pbmstrk>@pbmstrk</denchmark-link>
  mind send a PR with remooving this term from index?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4020,colin-ai,2020-10-09T08:30:30Z,2020-10-10T16:20:12Z,"Validation loss Tensor object is print in progress bar, it is expected only value","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When I add validation loss in progress bar training, tensor object is printed whereas only loss value is expected.
 For example  :
 Epoch 1: 100%|██████████| 5/5 [00:04<00:00,  1.21it/s, loss=82.423, v_num=52, val_loss=tensor(76.4331, dtype=torch.float32)]
 Validation loss is added with the following command : self.log('val_loss', loss, prog_bar=True)
 I tried self.log('val_loss', loss.item(), prog_bar=True) with no effect.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 The bug is reproductible with the minimal code example (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py</denchmark-link>
 ). See code sample below with validation_step overridden :
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>class TestModel(BoringModel):
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log('val_loss', loss, prog_bar=True)
         return {""x"": loss}
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 It is expected to only obtain value of validation loss in progress bar and not tensor object.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 available:         False
 version:           None
 
 
 Packages:
 
 numpy:             1.19.1
 pyTorch_debug:     False
 pyTorch_version:   1.6.0
 pytorch-lightning: 0.10.0
 tqdm:              4.50.1
 
 
 System:
 
 OS:                Darwin
 architecture:
 
 64bit
 
 
 
 processor:         i386
 python:            3.8.6
 version:           Darwin Kernel Version 18.7.0: Thu Jun 18 20:50:10 PDT 2020; root:xnu-4903.278.43~1/RELEASE_X86_64
 
 
 
 	",bdbf84602973dc86a16f66d2902b22ee5a4c9f21,Rohit Gupta,2020-10-10 12:20:11-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,24,,1.0,colin-ai,2020-10-09T08:31:14Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,colin-ai,2020-10-10T06:17:36Z,"
 		Also encounter this problem
 		",,,,,,,,,MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector.py,1.0,"160,186","160,186",_log_on_evaluation_epoch_end_metrics,"self,epoch_logs",137,202,MODIFY,1.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,"67,68",66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test__validation_step__log,tmpdir,13,69,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
403,darleybarreto,2019-10-21T12:28:38Z,2019-10-24T03:27:20Z,Error when model checkpoint and no early stop,"
 
 When creating a , if we set a  and  we get an error at this <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer_io.py#L157>line</denchmark-link>
  here.
 To Reproduce
 Steps to reproduce the behavior:
 
 Create a ckpt = ModelCheckpoint(...)
 Create a Trainer
 Setting checkpoint_callback = ckpt and early_stop_callback=False
 See the error AttributeError: 'NoneType' object has no attribute 'wait'
 
 Expected behavior
 It should be possible to save the model without setting an EarlyStopping condition. Of course one could set an EarlyStopping with the max integer, but changing the condition from an or to an and solves the problem.
 Desktop
 
 OS: Ubuntu 19.04
 Browser: Firefox Quantum
 Version: 69.0.2
 
 	",e7c12d936e30aec96b1bf333ed9dc17c736dcc9e,Hata Ryosuke,2019-10-22 13:07:48-04:00,MODIFY,2,pytorch_lightning\trainer\trainer_io.py,pytorch_lightning\trainer\trainer_io.py,1.0,"154,157","154,157",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,dump_checkpoint,self,147,182,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"210,213","210,213",restore_training_state,"self,checkpoint",203,236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4073,willprice,2020-10-11T11:59:57Z,2020-12-04T18:10:08Z,Data Parallel bug (return outputs not being moved to same device),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Under backend='dp' doesn't handle reduction of the loss across multiple GPUs correctly. This is present in v0.10--v1.0.0rc4
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 import torch
 import pytorch_lightning as ptl
 from pytorch_lightning import LightningModule
 from torch.utils.data import Dataset
 
 
 class RandomDictDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         a = self.data[index]
         b = a + 2
         return {""a"": a, ""b"": b}
 
     def __len__(self):
         return self.len
 
 
 class RandomDictStringDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return {""id"": str(index), ""x"": self.data[index]}
 
     def __len__(self):
         return self.len
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
     def __init__(self):
         """"""
         Testing PL Module
         Use as follows:
         - subclass
         - modify the behavior for what you want
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
         or:
         model = BaseTestModel()
         model.training_epoch_end = None
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.cross_entropy(
             prediction,
             torch.ones(len(prediction), dtype=torch.long, device=prediction.device),
         )
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log(""loss"", loss)
         return loss
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log(""loss"", loss)
         return loss
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return loss
 
     def configure_optimizers(self):
         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def val_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def test_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
 
 def main():
     model = BoringModel()
     trainer = ptl.Trainer(
         distributed_backend=""dp"",
         gpus=4,
     )
     trainer.fit(model)
 
 
 if __name__ == ""__main__"":
     main()
 Produces the following
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 
 | Name  | Type   | Params
 ---------------------------------
 0 | layer | Linear | 66
 /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 Validation sanity check: 0it [00:00, ?it/s]/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
 warnings.warn('Was asked to gather along dimension 0, but all '
 /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 Epoch 1:  50%|████████████████Traceback (most recent call last):████████████████████████████████████                                                                                      | 4/8 [00:00<00:00, 184.41it/s, loss=0.497, v_num=53]
 File ""dp_bug.py"", line 118, in <module>
 main()
 File ""dp_bug.py"", line 114, in main
 trainer.fit(model)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 440, in fit
 results = self.accelerator_backend.train()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py"", line 97, in train
 results = self.train_or_test()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py"", line 53, in train_or_test
 results = self.trainer.train()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 483, in train
 self.train_loop.run_training_epoch()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 557, in run_training_epoch
 self.trainer.run_evaluation(test_mode=False)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 599, in run_evaluation
 eval_loop_results = self.evaluation_loop.log_epoch_metrics(deprecated_eval_results, epoch_logs, test_mode)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/evaluation_loop.py"", line 210, in log_epoch_metrics
 eval_loop_results = self.trainer.logger_connector.on_evaluation_epoch_end(
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py"", line 113, in on_evaluation_epoch_end
 self._log_on_evaluation_epoch_end_metrics(epoch_logs)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector.py"", line 178, in _log_on_evaluation_epoch_end_metrics
 reduced_epoch_metrics = dl_metrics[0].__class__.reduce_on_epoch_end(dl_metrics)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py"", line 433, in reduce_on_epoch_end
 recursive_stack(result)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py"", line 552, in recursive_stack
 result[k] = collate_tensors(v)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/step_result.py"", line 574, in collate_tensors
 return torch.stack(items)
 RuntimeError: All input tensors must be on the same device. Received cuda:3 and cuda:1
 Exception ignored in: <function tqdm.__del__ at 0x7fcf54050a60>
 Traceback (most recent call last):
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1087, in __del__
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1294, in close
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1472, in display
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1090, in __repr__
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1434, in format_dict
 TypeError: cannot unpack non-iterable NoneType object
 </denchmark-code>
 
 Specifically note the line saying
 <denchmark-code>RuntimeError: All input tensors must be on the same device. Received cuda:3 and cuda:1
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version (e.g., 1.0): 1.6.0
 OS (e.g., Linux): Ubuntu 18.04
 How you installed PyTorch (conda, pip, source): conda
 Build command you used (if compiling from source): N/A
 Python version: 3.8.5
 CUDA/cuDNN version: 11.0
 GPU models and configuration: 8 GPU (RTX 2080Ti)
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 This works on v0.9.0:
 import torch
 import pytorch_lightning as ptl
 from pytorch_lightning import LightningModule
 from torch.utils.data import Dataset
 
 
 class RandomDictDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         a = self.data[index]
         b = a + 2
         return {""a"": a, ""b"": b}
 
     def __len__(self):
         return self.len
 
 
 class RandomDictStringDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return {""id"": str(index), ""x"": self.data[index]}
 
     def __len__(self):
         return self.len
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
     def __init__(self):
         """"""
         Testing PL Module
         Use as follows:
         - subclass
         - modify the behavior for what you want
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
         or:
         model = BaseTestModel()
         model.training_epoch_end = None
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.cross_entropy(
             prediction,
             torch.ones(len(prediction), dtype=torch.long, device=prediction.device),
         )
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""loss"": loss}
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""val_loss"": loss}
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""test_loss"": loss}
 
     def configure_optimizers(self):
         optimizer = torch.optim.SGD(self.layer.parameters(), lr=0.1)
         lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)
         return [optimizer], [lr_scheduler]
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def val_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def test_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
 
 def main():
     model = BoringModel()
     trainer = ptl.Trainer(
         distributed_backend=""dp"",
         gpus=4,
         # log_every_n_steps=5,
         # flush_logs_every_n_steps=20,
         # benchmark=True,
         # gradient_clip_val=20,
     )
     trainer.fit(model)
 
 
 if __name__ == ""__main__"":
     main()
 but causes this error under v1.0.0rc4
 <denchmark-code>GPU available: True, used: True
 TPU available: False, using: 0 TPU cores
 LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
 
 | Name  | Type   | Params
 ---------------------------------
 0 | layer | Linear | 66    
 /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 /home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 104 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
 warnings.warn(*args, **kwargs)
 Epoch 0:   0%|                                                                                                                                                                                                          | 0/8 [00:00<?, ?it/s]Traceback (most recent call last):
 File ""dp_bug.py"", line 116, in <module>
 main()
 File ""dp_bug.py"", line 112, in main
 trainer.fit(model)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 440, in fit
 results = self.accelerator_backend.train()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/dp_accelerator.py"", line 97, in train
 results = self.train_or_test()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py"", line 53, in train_or_test
 results = self.trainer.train()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py"", line 483, in train
 self.train_loop.run_training_epoch()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 529, in run_training_epoch
 batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 661, in run_training_batch
 opt_closure_result = self.training_step_and_backward(
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 753, in training_step_and_backward
 self.backward(result, optimizer, opt_idx)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py"", line 767, in backward
 result.closure_loss = self.trainer.accelerator_backend.backward(
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py"", line 83, in backward
 model.backward(closure_loss, optimizer, opt_idx)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py"", line 1077, in backward
 loss.backward()
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/tensor.py"", line 185, in backward
 torch.autograd.backward(self, gradient, retain_graph, create_graph)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 121, in backward
 grad_tensors = _make_grads(tensors, grad_tensors)
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/torch/autograd/__init__.py"", line 47, in _make_grads
 raise RuntimeError(""grad can be implicitly created only for scalar outputs"")
 RuntimeError: grad can be implicitly created only for scalar outputs
 Exception ignored in: <function tqdm.__del__ at 0x7fed7b0c1a60>
 Traceback (most recent call last):
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1087, in __del__
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1294, in close
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1472, in display
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1090, in __repr__
 File ""/home/user/.conda/envs/env/lib/python3.8/site-packages/tqdm/std.py"", line 1434, in format_dict
 TypeError: cannot unpack non-iterable NoneType object
 
 </denchmark-code>
 
 	",f23f5e56480d1a4784fc7829d014590fe4ca1454,Justus Schock,2020-12-04 19:10:07+01:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,106,,1.0,willprice,2020-10-12T11:08:58Z,"
 		import torch
 
 # this is what is happening in will's code:
 
 prediction = torch.rand(8, 2, requires_grad=True)
 
 # device 0 computes:
 x = torch.nn.functional.cross_entropy(
     prediction,
     torch.ones(len(prediction), dtype=torch.long, device=prediction.device),
 )
 
 # devices 1 computes:
 y = torch.nn.functional.cross_entropy(
     prediction,
     torch.ones(len(prediction), dtype=torch.long, device=prediction.device),
 )
 
 # dp backend calls backward on stacked tensor
 l = torch.stack((x, y))
 l.backward()  # backward on a non-scalar
 Here is the pytorch code that shows the problem. Gives the same error as reported by <denchmark-link:https://github.com/willprice>@willprice</denchmark-link>
 
 <denchmark-code>  File ""/home/adrian/repositories/imagenet-optical-flow/asdf.py"", line 19, in <module>
     l.backward()
   File ""/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/tensor.py"", line 185, in backward
     torch.autograd.backward(self, gradient, retain_graph, create_graph)
   File ""/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 121, in backward
     grad_tensors = _make_grads(tensors, grad_tensors)
   File ""/home/adrian/bin/anaconda3/envs/lightning-0.7.1/lib/python3.7/site-packages/torch/autograd/__init__.py"", line 47, in _make_grads
     raise RuntimeError(""grad can be implicitly created only for scalar outputs"")
 RuntimeError: grad can be implicitly created only for scalar outputs
 </denchmark-code>
 
 Conclusion: Somewhere in the dp backend the losses get stacked and backward is called on a non-scalar tensor.
 Have limited time rn, so dropping this info here for now to pick it up later
 		",2.0,willprice,2020-10-14T09:20:31Z,"
 		<denchmark-link:https://github.com/willprice>@willprice</denchmark-link>
  can you check the fix fro <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4138>#4138</denchmark-link>
  ? For me this worked on the reproduction script.
 		",3.0,willprice,2020-11-12T15:37:25Z,"
 		<denchmark-link:https://github.com/willprice>@willprice</denchmark-link>
  friendly ping :)
 		",4.0,willprice,2020-11-12T17:32:45Z,"
 		Hey <denchmark-link:https://github.com/edenlightning>@edenlightning</denchmark-link>
 , I can confirm that this is fixed for me on the reproduction script and my own codebase. Although I did run into <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/2350>this issue</denchmark-link>
  when testing out <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4138>#4138</denchmark-link>
  on my codebase (I have  set on my ).
 		",MODIFY,2.0,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"403,404,407","403,404,407",to,"self,args,kwargs",403,407,MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,1.0,"407,408",,MODIFY,3.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,"391,392,393,394","391,392,393,394,395",log_train_epoch_end_metrics,"self,epoch_output,checkpoint_accumulator,early_stopping_accumulator,num_optimizers",391,395,MODIFY,11.0,tests\trainer\logging\test_logger_connector.py,tests\trainer\logging\test_logger_connector.py,1.0,"422,423,424,425",,test_epoch_results_cache_dp.test_step,"self,args,kwargs",422,425,5.0,willprice,2020-12-03T10:44:11Z,"
 		I think I rediscovered this bug in our examples:
 
 I will try to help with the PR <denchmark-link:https://github.com/justusschock>@justusschock</denchmark-link>
  has still open so we can also finish <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4764>#4764</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,cache_result,self,372,418,,,,,1.0,"405,406,407",,test_epoch_results_cache_dp.training_step_end,"self,training_step_outputs",405,407,1.0,"418,419,420",,test_epoch_results_cache_dp.validation_epoch_end,"self,outputs",418,420,,,,,,,,,,,,,,,,,,,,,,,1.0,292,"292,293,294,295",_track_callback_metrics,"self,eval_results,using_eval_result",291,331,,,,,,,,,,,,,,,,,,,,,,1.0,"409,410,411",,cpu,self,409,411,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"388,389,390,391,392,393","391,392,393",log_train_epoch_end_metrics,"self,epoch_output,checkpoint_accumulator,early_stopping_accumulator,num_optimizers",388,393,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"434,435",,test_epoch_results_cache_dp.val_dataloader,self,434,435,1.0,"437,438",,test_epoch_results_cache_dp.test_dataloader,self,437,438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"427,428,429",,test_epoch_results_cache_dp.test_epoch_end,"self,outputs",427,429,1.0,"394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450",,test_epoch_results_cache_dp,tmpdir,394,450,1.0,"409,410,411",,test_epoch_results_cache_dp.training_epoch_end,"self,outputs",409,411,1.0,"431,432",,test_epoch_results_cache_dp.train_dataloader,self,431,432,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"413,414,415,416",,test_epoch_results_cache_dp.validation_step,"self,args,kwargs",413,416,1.0,"400,401,402,403",,test_epoch_results_cache_dp.training_step,"self,args,kwargs",400,403,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4141,ChanKaHou,2020-10-14T09:57:48Z,2020-10-15T13:12:05Z,the self.log problem in validation_step(),"
 as doc say we should use self.log in last version,
 but the loged data are strange if we change EvalResult() to self.log(on_epoch=True)
 Then we check the data in tensorboard, the self.log() will only log the result of last batch each epoch, instead of the mean of them.
 That is quite unreliable about this issue, it must be turned back to EvalResult() for correct experiments.
 	",45d05ff68dbf3db300a782af97ea54cab70a3ff9,William Falcon,2020-10-15 09:12:05-04:00,MODIFY,5,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"305,306,307",,1.0,ChanKaHou,2020-10-14T09:59:45Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,ChanKaHou,2020-10-14T10:38:11Z,"
 		I am having the same issue... Using PL 1.0.0. The progress bar does get the correct values for validation loss, on the other hand.
 		",3.0,ChanKaHou,2020-10-14T11:18:05Z,"
 		I found something wrong when saving the model. This is a serious bug.
 		",4.0,ChanKaHou,2020-10-14T12:20:00Z,"
 		To speed things up: here is a <denchmark-link:https://colab.research.google.com/drive/149CNaNZPHWzIBI2Ea0k6PSzVePsygYue?usp=sharing>boring model</denchmark-link>
  demonstrating the error. In the TB output, observe that:
 
 the value for val_loss and val_loss_epoch are simply equal to the value of val_loss_step for the last step (same as val_loss_step in progress bar).
 the values for val_loss and val_loss_epoch in TB are different from their equivalents in the progress bar.
 
 So what's happening is that when logging to loggers, reduction is not applied.
 This occurs in version 0.10.0 as well as 1.0.0
 I don't understand how a bug this serious could've made it through all the tests from 0.10.0, through all the 1.0 rcs and all they way to the final version...
 		",MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector.py,1.0,"192,193,194,195,200,201","196,197",_log_on_evaluation_epoch_end_metrics,"self,epoch_logs",137,206,MODIFY,1.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"221,222,223",,MODIFY,2.0,tests\core\test_metric_result_integration.py,tests\core\test_metric_result_integration.py,1.0,138,139,test_result_metric_integration,,104,142,MODIFY,7.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,"270,271,272,273",,test_eval_logging_auto_reduce.validation_epoch_end,"self,outputs",270,273,5.0,ChanKaHou,2020-10-14T13:05:28Z,"
 		The Issue is still present in version 1.0.1.
 I also found out that mean function used to calculate value at the and of the epoch (the value that is present in the progress bar) does not return the correct average value.
 <denchmark-code>def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss
 
 def validation_epoch_end(self, outputs: List[Any]) -> None:
     super().validation_epoch_end(outputs)
     print(""batch_idxs mean"", sum(self.batch_idxs) / len(self.batch_idxs))
     print(""batch_idxs torch mean"", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
     self.batch_idxs = []
 </denchmark-code>
 
 logged value = 31
 progress bar value = 15.1
 mean value = 15.5
 torch mean value = 15.5
 		",6.0,ChanKaHou,2020-10-14T15:44:42Z,"
 		Here 
 
 
 pytorch-lightning/pytorch_lightning/trainer/evaluation_loop.py
 
 
          Line 199
       in
       09c2020
 
 
 
 
 
 
  epoch_logs = self.trainer.get_model()._results 
 
 
 
 
  this private variable (_results) keeps last step_log. Maybe this is the problem, but I don’t know how to fix it yet.
 		",7.0,ChanKaHou,2020-10-15T08:59:47Z,"
 		I also find this problem, then I log it in validation_epoch_end and it seems ok
 <denchmark-code>     def validation_step(self,...):
         return {'val_loss': loss}
 
     def validation_epoch_end(self, outputs):
         val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
 
 
         dist.all_reduce(val_loss, op=dist.ReduceOp.SUM)
         val_loss = val_loss / self.trainer.world_size
         self.log('val_loss', val_loss, on_epoch=True, sync_dist=True)
 
         return {'val_loss': val_loss,}
 </denchmark-code>
 
 		",8.0,ChanKaHou,2020-10-15T10:41:36Z,"
 		ok. good catch. fixing this now!
 		",9.0,ChanKaHou,2020-10-15T10:53:47Z,"
 		Running into the same problem with the logger only reporting the last value for the epoch rather than the average across the epoch. I was wondering why I was getting funky test scores 😅 .
 		",10.0,ChanKaHou,2020-10-15T11:16:03Z,"
 		Is this what you mean?
 <denchmark-link:https://user-images.githubusercontent.com/3640001/96116314-46d2f800-0eb6-11eb-8819-4f3a9ba63e80.png></denchmark-link>
 
 val_loss_epoch and val_loss are the same? but instead, val_loss should be the same as val_loss_step?
 Just wrote a test and it looks like everything is correct, except that the val_loss gets overwritten by the val_loss_epoch by mistake. So:
 
 val_loss_step is correct
 val_loss_epoch is correct
 val_loss is incorrect
 
 		",11.0,ChanKaHou,2020-10-15T11:19:37Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Yes, that's correct. Maybe even val_loss_epoch is incorrect, per <denchmark-link:https://github.com/Alek96>@Alek96</denchmark-link>
 's comment. Although the bigger issue is wrong values being sent to the (tensorboard of anything non pbar) logger.
 		",12.0,ChanKaHou,2020-10-15T11:22:19Z,"
 		
 def validation_step(self, batch, batch_idx):
 loss = self.step(batch)
 self.log('batch_idx', batch_idx, prog_bar=True)
 self.batch_idxs.append(batch_idx)
 return loss
 def validation_epoch_end(self, outputs: List[Any]) -> None:
 super().validation_epoch_end(outputs)
 print(""batch_idxs mean"", sum(self.batch_idxs) / len(self.batch_idxs))
 print(""batch_idxs torch mean"", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
 self.batch_idxs = []
 
 Well... there is a bug in this code:
 def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss
 
 def validation_epoch_end(self, outputs: List[Any]) -> None:
     super().validation_epoch_end(outputs)
     print(""batch_idxs mean"", sum(self.batch_idxs) / len(self.batch_idxs))
     print(""batch_idxs torch mean"", torch.mean(torch.tensor(self.batch_idxs, dtype=torch.float)))
     self.batch_idxs = []
 You return the loss but then compare the batch_idxs...
 outputs has losses, not batch_idxs.
 ie:
 def validation_step(self, batch, batch_idx):
     loss = self.step(batch)
     self.log('batch_idx', batch_idx, prog_bar=True)
     self.batch_idxs.append(batch_idx)
     return loss # <---------- causes outputs to be losses not indexes
 
 def validation_epoch_end(self, outputs: List[Any]) -> None:
     outputs = outputs # <------------- losses not indexes!
 		",13.0,ChanKaHou,2020-10-15T11:31:56Z,"
 		ok, i think i found it. posting a fix now. Looks like the calculations are correct, but the wrong value got logged
 		",14.0,ChanKaHou,2020-10-15T13:12:19Z,"
 		Thanks for the fast fix.
 To explain my intention:
 
 You return the loss but then compare the batch_idxs...
 outputs has losses, not batch_idxs.
 
 In my understanding it should not matter if I returned loss value and then compared the batch_idxs. self.log method should work the same regardless of the value you pass in. That is way I called a method with a value that easily shows if the calculated mean is correct.
 self.log('batch_idx', batch_idx, prog_bar=True)
 The ""bug"" was not in the code, but in my understating of mean function. PyTorch lightning is using weighted_mean that is also taking in the account the size of each batch.
 
 
 
 pytorch-lightning/pytorch_lightning/core/step_result.py
 
 
         Lines 446 to 456
       in
       f064682
 
 
 
 
 
 
  if option['on_epoch']: 
 
 
 
  fx = option['reduce_fx'] 
 
 
 
  if fx == torch.mean: 
 
 
 
  try: 
 
 
 
  reduced_val = weighted_mean(result[k], batch_sizes) 
 
 
 
  except Exception as e: 
 
 
 
  reduced_val = torch.mean(result[k]) 
 
 
 
  else: 
 
 
 
  reduced_val = fx(result[k]) 
 
 
 
  
 
 
 
  result[k] = reduced_val 
 
 
 
 
 
 <denchmark-link:https://colab.research.google.com/drive/1oa6TKF59k744HTX19597QHGY9gJV0A8I?usp=sharing>Boring_model</denchmark-link>
  that shows the behavior.
 		",15.0,ChanKaHou,2020-10-15T13:14:16Z,"
 		all good!
 Please try on master now. Let me know if the errors are fixed.
 For reference, here is the new test for this:
 
 
 
 pytorch-lightning/tests/trainer/logging/test_eval_loop_logging_1_0.py
 
 
         Lines 247 to 313
       in
       45d05ff
 
 
 
 
 
 
  def test_eval_logging_auto_reduce(tmpdir): 
 
 
 
  """""" 
 
 
 
      Tests that only training_step can be used 
 
 
 
      """""" 
 
 
 
  seed_everything(1234) 
 
 
 
  
 
 
 
  os.environ['PL_DEV_DEBUG'] = '1' 
 
 
 
  
 
 
 
  class TestModel(BoringModel): 
 
 
 
  def on_pretrain_routine_end(self) -> None: 
 
 
 
  self.seen_vals = [] 
 
 
 
  self.manual_epoch_end_mean = None 
 
 
 
  
 
 
 
  def on_validation_epoch_start(self) -> None: 
 
 
 
  self.seen_vals = [] 
 
 
 
  
 
 
 
  def validation_step(self, batch, batch_idx): 
 
 
 
  output = self.layer(batch) 
 
 
 
  loss = self.loss(batch, output) 
 
 
 
  self.seen_vals.append(loss) 
 
 
 
  self.log('val_loss', loss, on_epoch=True, on_step=True, prog_bar=True) 
 
 
 
  return {""x"": loss} 
 
 
 
  
 
 
 
  def validation_epoch_end(self, outputs) -> None: 
 
 
 
  for passed_in, manually_tracked in zip(outputs, self.seen_vals): 
 
 
 
  assert passed_in['x'] == manually_tracked 
 
 
 
  self.manual_epoch_end_mean = torch.stack([x['x'] for x in outputs]).mean() 
 
 
 
  
 
 
 
  model = TestModel() 
 
 
 
  
 
 
 
  trainer = Trainer( 
 
 
 
  default_root_dir=tmpdir, 
 
 
 
  limit_train_batches=3, 
 
 
 
  limit_val_batches=3, 
 
 
 
  max_epochs=1, 
 
 
 
  log_every_n_steps=1, 
 
 
 
  weights_summary=None, 
 
 
 
  checkpoint_callback=callbacks.ModelCheckpoint('val_loss') 
 
 
 
      ) 
 
 
 
  trainer.fit(model) 
 
 
 
  
 
 
 
  # make sure all the metrics are available for callbacks 
 
 
 
  manual_mean = model.manual_epoch_end_mean 
 
 
 
  callback_metrics = set(trainer.callback_metrics.keys()) 
 
 
 
  assert callback_metrics == {'debug_epoch', 'val_loss', 'val_loss_epoch'} 
 
 
 
  
 
 
 
  # make sure values are correct 
 
 
 
  assert trainer.logged_metrics['val_loss_epoch'] == manual_mean 
 
 
 
  assert trainer.callback_metrics['val_loss'] == trainer.logged_metrics['val_loss_step/epoch_0'] 
 
 
 
  
 
 
 
  # make sure correct values were logged 
 
 
 
  logged_val = trainer.dev_debugger.logged_metrics 
 
 
 
  
 
 
 
  # sanity check 
 
 
 
  assert logged_val[0]['global_step'] == 0 
 
 
 
  assert logged_val[1]['global_step'] == 0 
 
 
 
  
 
 
 
  # 3 val batches 
 
 
 
  assert logged_val[2]['val_loss_step/epoch_0'] == model.seen_vals[0] 
 
 
 
  assert logged_val[3]['val_loss_step/epoch_0'] == model.seen_vals[1] 
 
 
 
  assert logged_val[4]['val_loss_step/epoch_0'] == model.seen_vals[2] 
 
 
 
  
 
 
 
  # epoch mean 
 
 
 
  assert logged_val[5]['val_loss_epoch'] == model.manual_epoch_end_mean 
 
 
 
  
 
 
 
  # only those logged 
 
 
 
  assert len(logged_val) == 6 
 
 
 
 
 
 		",__run_eval_epoch_end,"self,num_dataloaders,using_eval_result",218,263,get_epoch_pbar_metrics,self,294,318,1.0,"263,264,265,266,267,268",,test_eval_logging_auto_reduce.validation_step,"self,batch,batch_idx",263,268,1.0,"260,261",,test_eval_logging_auto_reduce.on_validation_epoch_start,self,260,261,MODIFY,1.0,tests\trainer\logging\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,1.0,"398,399","398,399",test_different_batch_types_for_sizing,tmpdir,362,403,,,,,,,,,,,,1.0,,86,_ddp_test_fn,"rank,worldsize",46,92,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"279,280,281",,get_epoch_log_metrics,self,268,292,1.0,"468,469,470,471,472",,reduce_on_epoch_end,"cls,outputs",450,488,1.0,"561,562,563,564,565,566",,choose_last,x,561,566,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"256,257,258",,test_eval_logging_auto_reduce.on_pretrain_routine_end,self,256,258,1.0,,71,test__validation_step__log,tmpdir,26,85,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,ChanKaHou,2020-10-15T13:30:38Z,"
 		as a bonus, got rid of the duplicate metric, metric_step chart without losing support for callbacks
 <denchmark-link:https://user-images.githubusercontent.com/3640001/96133953-1183d580-0ec9-11eb-8850-dc50f5b78fea.png></denchmark-link>
 
 <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
   FYI
 		",17.0,ChanKaHou,2020-10-15T14:49:35Z,"
 		ok, mind verifying that this worked for you guys?
 this is a critical bug, so, we'll do a minor release now to fix it for everyone.
 Here's the new colab with master showing it's fixed:
 <denchmark-link:https://colab.research.google.com/drive/1lEZms9QjRZ7kPosu_m7Gbr_sdBZ7exzg?usp=sharing>https://colab.research.google.com/drive/1lEZms9QjRZ7kPosu_m7Gbr_sdBZ7exzg?usp=sharing</denchmark-link>
 
 		",18.0,ChanKaHou,2020-10-15T14:57:05Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  it works now, thanks a lot.
 		",19.0,ChanKaHou,2020-10-15T14:58:38Z,"
 		Works here too :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"320,321,322,323,324,325,326,327,328,329,330,331,332,333,334",,get_forked_metrics,self,320,334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"145,150",test__validation_step__step_end__epoch_end__log,tmpdir,88,166,1.0,"247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313",,test_eval_logging_auto_reduce,tmpdir,247,313,,,,,,,,,,,,,,,20.0,ChanKaHou,2020-10-15T16:46:44Z,"
 		Just testing and it works for me too 💪
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4188,justusschock,2020-10-16T07:46:47Z,2020-10-21T18:34:30Z,To many backwards with LBFGS,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When using LBFGS we have one backward step to much, because we call backward before the optimiser step (also for gradient accumulation), but the optimizer step get's a closure and therefore calls backward again.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-code>import torch
 import pytorch_lightning as ptl
 from pytorch_lightning import LightningModule
 from torch.utils.data import Dataset
 
 
 class RandomDictDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         a = self.data[index]
         b = a + 2
         return {""a"": a, ""b"": b}
 
     def __len__(self):
         return self.len
 
 
 class RandomDictStringDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return {""id"": str(index), ""x"": self.data[index]}
 
     def __len__(self):
         return self.len
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
     def __init__(self):
         """"""
         Testing PL Module
         Use as follows:
         - subclass
         - modify the behavior for what you want
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
         or:
         model = BaseTestModel()
         model.training_epoch_end = None
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         # An arbitrary loss to have a loss that updates the model weights during `Trainer.fit` calls
         return torch.nn.functional.cross_entropy(
             prediction,
             torch.ones(len(prediction), dtype=torch.long, device=prediction.device),
         )
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log(""loss"", loss)
         return loss
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         self.log(""loss"", loss)
         return loss
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return loss
 
     def configure_optimizers(self):
         optimizer = torch.optim.LBFGS(self.parameters())
         return optimizer
 
     def train_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def val_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
     def test_dataloader(self):
         return torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=16)
 
 
 def main():
     model = BoringModel()
     trainer = ptl.Trainer(
         distributed_backend=""dp"",
         gpus=1,
     )
     trainer.fit(model)
 
 
 if __name__ == ""__main__"":
     main()```
 
 ### Environment
 
 Please copy and paste the output from our
 [environment collection script](https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py)
 (or fill out the checklist below manually).
 
 You can get the script and run it with:
 </denchmark-code>
 
 wget <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py</denchmark-link>
 
 <denchmark-h:h1>For security purposes, please check the contents of collect_env_details.py before running it.</denchmark-h>
 
 python collect_env_details.py
 <denchmark-code>
 * CUDA:
         - GPU:
                 - GeForce RTX 2080 Ti
                 - GeForce RTX 2080 Ti
         - available:         True
         - version:           10.2
 * Packages:
         - numpy:             1.19.2
         - pyTorch_debug:     False
         - pyTorch_version:   1.6.0
         - pytorch-lightning: 20201015
         - tqdm:              4.50.2
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - ELF
         - processor:         x86_64
         - python:            3.8.5
         - version:           #52~18.04.1-Ubuntu SMP PREEMPT Thu Sep 10 13:34:23 UTC 2020
 
 ### Additional context
 
 Probably we can fix this. by passing a closure to all optimisers (to be more consistent).
 </denchmark-code>
 
 	",0ec410769744843726a140b7efabbeeb1c4e2929,Justus Schock,2020-10-21 19:34:29+01:00,MODIFY,1,pytorch_lightning\accelerators\accelerator.py,pytorch_lightning\accelerators\accelerator.py,1.0,"94,95","94,95,96,97,98",1.0,justusschock,2020-10-16T08:40:06Z,"
 		cc <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 		",,,,,,,,,,,,,MODIFY,5.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1084,1103,1104","1084,1085,1086,1087",backward,"self,Tensor,Optimizer,int,args,kwargs",1084,1104,MODIFY,31.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,185,182,MODIFY,1.0,tests\base\deterministic_model.py,tests\base\deterministic_model.py,1.0,495,495,backward,"self,loss,optimizer,optimizer_idx",488,495,MODIFY,8.0,tests\trainer\data_flow\test_eval_loop_flow_1_0.py,tests\trainer\data_flow\test_eval_loop_flow_1_0.py,1.0,47,46,test__eval_step__flow,tmpdir,25,66,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_train_end,self,172,202,backward,"self,closure_loss,optimizer,opt_idx,args,kwargs",87,99,1.0,97,96,test__eval_step__eval_step_end__flow.backward,"self,loss,optimizer,optimizer_idx",96,97,1.0,154,153,test__eval_step__epoch_end__flow.backward,"self,loss,optimizer,optimizer_idx",153,154,MODIFY,8.0,tests\trainer\data_flow\test_train_loop_flow_dict_1_0.py,tests\trainer\data_flow\test_train_loop_flow_dict_1_0.py,1.0,182,181,test__training_step__step_end__epoch_end__flow_dict,tmpdir,149,200,1.0,128,127,test__training_step__epoch_end__flow_dict.backward,"self,loss,optimizer,optimizer_idx",127,128,MODIFY,8.0,tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py,tests\trainer\data_flow\test_train_loop_flow_scalar_1_0.py,,,,,,,,1.0,40,39,test__training_step__flow_scalar.backward,"self,loss,optimizer,optimizer_idx",39,40,1.0,40,39,test__training_step__flow_scalar,tmpdir,26,58,MODIFY,2.0,tests\trainer\legacy_deprecate_flow_log_tests\test_eval_loop_dict_return.py,tests\trainer\legacy_deprecate_flow_log_tests\test_eval_loop_dict_return.py,1.0,"26,27,28,29,30",25,1.0,1132,1134,optimizer_step,"self,int,int,Optimizer,int,None,bool,bool,bool",1128,1137,1.0,"1103,1104","1087,1106",backward,"self,Tensor,Optimizer,int",1087,1106,,,,,,,,,,,,,,,,,,,,,,1.0,"50,51",,on_trainer_init,"self,max_epochs,min_epochs,max_steps,min_steps,num_sanity_val_steps,automatic_optimization",50,51,1.0,"440,441,442","440,441",_process_result,"self,training_step_output,split_batch",420,447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,38,37,test__training_step__flow_dict.backward,"self,loss,optimizer,optimizer_idx",37,38,test_validation_step_no_return,tmpdir,22,52,1.0,"28,29",,test_validation_step_no_return.backward,"self,loss,optimizer,optimizer_idx",28,29,MODIFY,4.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,121,120,test__validation_step__step_end__epoch_end__log.backward,"self,loss,optimizer,optimizer_idx",120,121,1.0,50,49,test__validation_step__log,tmpdir,27,84,1.0,50,49,test__validation_step__log.backward,"self,loss,optimizer,optimizer_idx",49,50,1.0,121,120,test__validation_step__step_end__epoch_end__log,tmpdir,87,162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,4.0,tests\trainer\logging\test_train_loop_logging_1_0.py,tests\trainer\logging\test_train_loop_logging_1_0.py,1.0,72,71,test__training_step__log,tmpdir,28,121,1.0,72,71,test__training_step__log.backward,"self,loss,optimizer,optimizer_idx",71,72,1.0,145,144,test__training_step__epoch_end__log,tmpdir,124,183,1.0,145,144,test__training_step__epoch_end__log.backward,"self,loss,optimizer,optimizer_idx",144,145,ADD,0.0,None,tests\trainer\optimization\test_backward_calls.py,,,,MODIFY,38.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,"1114,1115,1116,1117",1114,test_trainer_subclassing.__init__,"self,custom_arg,args,custom_kwarg,kwargs",1114,1117,1.0,"761,763,764,772,774,775","740,742,743,744,745,753,755,756,757,758,762,770",test_disabled_validation,tmpdir,729,775,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"450,452,453,454","449,451,452",optimizer_step,"self,optimizer,opt_idx,batch_idx,train_step_and_backward_closure",449,454,1.0,830,827,update_train_loop_lr_schedulers,"self,monitor_metrics",824,830,1.0,"463,469",467,track_and_norm_grad,"self,optimizer",463,469,1.0,154,153,test__eval_step__epoch_end__flow,tmpdir,118,174,1.0,219,218,test__validation_step__step_end__epoch_end__flow,tmpdir,177,238,1.0,184,183,test__training_step__step_end__epoch_end__flow_scalar.backward,"self,loss,optimizer,optimizer_idx",183,184,1.0,82,81,test__training_step__tr_step_end__flow_scalar,tmpdir,61,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,1084,"1084,1085",manual_backward,"self,Tensor,Optimizer,args,kwargs",1061,1085,1.0,1132,1134,optimizer_step,"self,int,int,Optimizer,int,None,bool,bool,bool",1126,1135,,,,,,,,1.0,184,183,test__training_step__step_end__epoch_end__flow_scalar,tmpdir,151,202,1.0,130,129,test__training_step__epoch_end__flow_scalar.backward,"self,loss,optimizer,optimizer_idx",129,130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"878,879",878,save_loggers_on_train_batch_end,self,875,882,1.0,"641,646,650,651,652,653,673,674,675,676,677,678,679,680,681,682,683,684,685,687,688,689,690,692,694,696,697,698,699,700,701,702,703,704,706,707,709,710,712,713,714,716,717,718,719,720,721,723,724,732,733,734,735,736,754","642,647,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,686,687,689,690,692,693,694,696,697,698,700,701,703,704,706,707,708,709,710,711,712,713,714,716,717,718,719,721,722,730,731,732,733,751",run_training_batch,"self,batch,batch_idx,dataloader_idx",620,756,1.0,"395,396,397,402","395,396,397,402",_process_training_step_output_1_0,"self,training_step_output,split_batch",387,418,1.0,"758,759",,_process_closure_result,"self,list,list,list,int",758,759,1.0,96,93,setup_fit,"self,model,train_dataloader,val_dataloaders,datamodule",91,103,,,,,,,,,,,,,,,,,,,,,,1.0,"886,887,888,889,890,891,892,893,894,895,896,897,898,899,900","878,893,894,895,896,897,898",test_gradient_clipping,tmpdir,872,904,1.0,"368,374,377","366,369",test_loading_yaml,tmpdir,361,377,1.0,"925,926,927,928,929,930,931,932,933,934,935,936,937,938,939","909,926,930,931,932,933,934,935,936,937,938,939,940,941,942",test_gradient_clipping_fp16,tmpdir,909,942,1.0,"193,194,212,226,227,229,230,231,232,233,234,235,236,237,238","206,221,222,223,273,274,275,276,277,278,279,280,281,282",test_gradient_accumulation_scheduling,"tmpdir,schedule,expected",190,282,1.0,"506,540,542,556,560,567,570","525,527,541,545,552,555",test_resume_from_checkpoint_epoch_restored,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",503,571,1.0,"894,895,896,897","894,895,896,897",test_gradient_clipping_fp16._optimizer_step,"args,kwargs",894,897,1.0,786,784,test_nan_loss_detection.training_step,"self,batch,batch_idx,optimizer_idx",782,789,1.0,1013,"1007,1010,1018,1019,1020",test_num_sanity_val_steps,"tmpdir,limit_val_batches",995,1020,1.0,"391,393,394","383,384,385,386,387,388,390,391,392,393,394",test_dp_output_reduce,,380,394,1.0,"229,230,231,232,233,234,235,236,237",,test_gradient_accumulation_scheduling._optimizer_step,"epoch,batch_idx,optimizer,optimizer_idx,second_order_closure,on_tpu,using_native_amp,using_lbfgs",229,237,1.0,"1307,1308",,test_trainer_subclassing.__init__,"self,kwargs",1306,1309,1.0,"46,62,65,70,74,75,76,77,78,79","43,47,63,66,71,75,76,84",test_no_val_module,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",43,84,1.0,"1291,1296,1300,1307,1308,1311,1314,1319",,test_trainer_subclassing,,1286,1319,1.0,"1046,1049","1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054",test_num_sanity_val_steps_neg_one,"tmpdir,limit_val_batches",1032,1054,1.0,1291,,test_trainer_subclassing.__init__,"self,custom_arg,args,custom_kwarg,kwargs",1291,1294,1.0,"786,800","784,793",test_nan_loss_detection,tmpdir,778,805,1.0,"1357,1360,1361,1364,1365",,test_trainer_setup_call,tmpdir,1343,1365,1.0,956,"945,952,953,954,955,956,957",test_gpu_choice,tmpdir,945,957,1.0,"91,108,111,116,117,118,119,120,121","88,105,108,113,114,122,126",test_no_val_end_module,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",88,126,1.0,"457,458,464,469,475,476,482,486,487,490","460,461,467,471,472,475,487,491",test_model_checkpoint_only_weights,tmpdir,455,491,1.0,"888,889,890,891,892,893,894,895,896,897,898","893,894,895,896,897,898",test_gradient_clipping.training_step_and_backward,"split_batch,batch_idx,opt_idx,optimizer,hiddens",888,898,1.0,426,426,test_model_checkpoint_options.mock_save_function,"filepath,args",425,426,1.0,611,"596,616",test_trainer_max_steps_and_epochs,tmpdir,589,621,1.0,"631,638,646,647,648,651,659,660,661","631,632,635,643,644",test_trainer_min_steps_and_epochs,tmpdir,624,661,1.0,"133,155,160,161,162,163,164,165","148,153,154,155,183,184,185,186,187,188",test_strict_model_load,"monkeypatch,tmpdir,tmpdir_server,url_ckpt",130,189,1.0,315,321,test_gradient_accumulation_scheduling_last_batch.on_train_batch_end,"self,outputs,batch,batch_idx,dataloader_idx",314,321,1.0,824,809,test_nan_params_detection,tmpdir,808,830,1.0,,"865,866,867,868",test_gradient_clipping._optimizer_step,"args,kwargs",865,868,,,,,,,,1.0,"814,821",817,backward,"self,result,optimizer,opt_idx,args,kwargs",813,822,1.0,219,218,test__validation_step__step_end__epoch_end__flow.backward,"self,loss,optimizer,optimizer_idx",218,219,1.0,47,46,test__eval_step__flow.backward,"self,loss,optimizer,optimizer_idx",46,47,1.0,97,96,test__eval_step__eval_step_end__flow,tmpdir,69,115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,82,81,test__training_step__tr_step_end__flow_scalar.backward,"self,loss,optimizer,optimizer_idx",81,82,1.0,130,129,test__training_step__epoch_end__flow_scalar,tmpdir,103,148,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"301,303,306,309","300,302,305,308,309",training_step,"self,split_batch,batch_idx,opt_idx,hiddens",297,341,1.0,"249,250","248,249",on_train_batch_end,"self,epoch_output,epoch_end_outputs,batch,batch_idx,dataloader_idx",244,250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,128,127,test__training_step__epoch_end__flow_dict,tmpdir,101,146,1.0,182,181,test__training_step__step_end__epoch_end__flow_dict.backward,"self,loss,optimizer,optimizer_idx",181,182,1.0,38,37,test__training_step__flow_dict,tmpdir,24,56,1.0,80,79,test__training_step__tr_step_end__flow_dict,tmpdir,59,98,1.0,80,79,test__training_step__tr_step_end__flow_dict.backward,"self,loss,optimizer,optimizer_idx",79,80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,209,206,check_checkpoint_callback,"self,should_save,is_last",204,211,1.0,"554,604,611","552,602,603,604,605,612",run_training_epoch,self,522,618,1.0,"85,89","82,86",on_train_start,self,80,89,1.0,850,"845,846",should_check_val_fx,"self,batch_idx,is_last_batch",844,853,1.0,"50,51,52","49,68",on_trainer_init,"self,max_epochs,min_epochs,max_steps,min_steps,num_sanity_val_steps,automatic_optimization",49,70,1.0,"793,796,801,807,808,809,810","787,788,797,800,801",training_step_and_backward,"self,split_batch,batch_idx,opt_idx,optimizer,hiddens",787,811,1.0,363,363,_process_training_step_output,"self,training_step_output,split_batch",343,385,1.0,518,516,tbptt_split_batch,"self,batch",514,520,1.0,"901,902,904,905,911",,process_train_step_outputs,"self,all_train_step_outputs,early_stopping_accumulator,checkpoint_accumulator",884,914,1.0,"860,865","868,869,871,872",build_train_args,"self,batch,batch_idx,opt_idx,hiddens",855,873,1.0,463,"461,467",on_before_backward,"self,batch_idx,optimizer",461,467,1.0,"47,48",38,__init__,"self,trainer",38,48,1.0,292,291,on_after_backward,"self,training_step_output,batch_idx,untouched_loss",283,295,1.0,"696,697,698,699,700,701,702,703,704","696,697,698,700,701,703,704",run_training_batch.train_step_and_backward_closure,,696,704,1.0,"833,834",832,run_on_epoch_end_hook,"self,epoch_output",832,834,1.0,"140,150,169","137,147,166",setup_training,"self,LightningModule",105,170,1.0,"234,241,242","231,232,233,240,241",on_train_epoch_start,"self,epoch",213,242,1.0,"698,701,704,718,720,721,722,723,724","701,703",test_test_checkpoint_path,"tmpdir,ckpt_path,save_top_k",690,726,1.0,"315,329","307,321",test_gradient_accumulation_scheduling_last_batch,"tmpdir,accumulate_grad_batches,limit_train_batches",304,332,1.0,"426,431,432,433,441,446,447,448","426,431,432,433,442,443,449",test_model_checkpoint_options,"tmpdir,save_top_k,save_last,file_prefix,expected_files",422,452,1.0,,"221,222,223",test_gradient_accumulation_scheduling._optimizer_step,"epoch,batch_idx,optimizer,optimizer_idx,second_order_closure,on_tpu,using_native_amp,using_lbfgs",221,223,1.0,"927,928,929,930,931,932,933,934,935,936,937","930,931,932,933,934,935,936,937",test_gradient_clipping_fp16.training_step_and_backward,"split_batch,batch_idx,opt_idx,optimizer,hiddens",927,937,1.0,"212,226,227,229,230,231,232,233,234,235,236,237,238,288,289,290","206,221,222,223,273,274,275,276,277,278,279,280,281,282,286",test_gradient_accumulation_scheduling,"tmpdir,schedule,expected",196,290,1.0,978,977,test_tpu_choice,"tmpdir,tpu_cores,expected_tpu_id,error_expected",976,982,1.0,"343,353","335,345",test_loading_meta_tags,tmpdir,335,358,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4208,asrafulashiq,2020-10-17T17:12:40Z,2020-10-18T14:13:57Z,AttributeError: 'Trainer' object has no attribute 'hpc_save',"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Getting the following error in slurm cluster:
 <denchmark-code>AttributeError: 'Trainer' object has no attribute 'hpc_save'
 
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 pytorch
 	",66e58f5afb6ae8702b29ada52f7b022bbf201f9e,Espen Haugsdal,2020-10-18 10:13:56-04:00,MODIFY,1,pytorch_lightning\trainer\connectors\slurm_connector.py,pytorch_lightning\trainer\connectors\slurm_connector.py,1.0,88,88,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,sig_handler,"self,signum,frame",84,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4229,Vozf,2020-10-19T10:49:14Z,2020-10-27T14:30:57Z,Comet logger overrides COMET_EXPERIMENT_KEY env variable,"
 After <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/2553>#2553</denchmark-link>
   there is a changed logger behavior. It starts using . But it doesn't respect it if it is set already.
 So the bug is in the following.
 I already set this variable
 Then logger overwrites my value here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L189>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L189</denchmark-link>
 
 Then it deletes this variable at all here <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L215>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/loggers/comet.py#L215</denchmark-link>
 
 This way it ignores my variable and deletes it at all later
 Moreover in version function it also ignores my set variable
 I will create a pull request to fix it
 	",4106e2f11292979022c437b9c49b0b3348f4682f,Alexander,2020-10-27 14:30:56+00:00,MODIFY,2,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,1.0,"282,283,284",,,,,,,,,,,,,,,,,,MODIFY,2.0,tests\loggers\test_comet.py,tests\loggers\test_comet.py,1.0,"111,112,113,114,115",,test_comet_logger_manual_experiment_key.save_os_environ,"args,kwargs",111,115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,version,self,274,291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130",,test_comet_logger_manual_experiment_key,comet,103,130,,,,,,,,1.0,"214,215,216","190,215",experiment,self,175,221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4234,pbmstrk,2020-10-19T14:34:45Z,2020-10-20T16:33:19Z,Values logged in test_epoch_end not returned when calling test(),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When calling test(), if values are logged only in the test_epoch_end method they are not returned. This leads to the following somewhat inconsistent behaviour:
 
 Values logged only in step method -> appear in list returned by test().
 Values logged in step and epoch_end -> both appear in list returned by test().
 Values logged only in epoch_end -> values do not appear.
 
 <denchmark-h:h2>Please reproduce using the BoringModel and post here</denchmark-h>
 
 <denchmark-link:https://colab.research.google.com/drive/1C1UiT815EAP6CGZcumhQ_JP8RMVBdFpX?usp=sharing>https://colab.research.google.com/drive/1C1UiT815EAP6CGZcumhQ_JP8RMVBdFpX?usp=sharing</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 If values are logged only in epoch_end they should be returned by test().
 <denchmark-h:h3>Environment</denchmark-h>
 
 PL version: 1.0.2
 see colab.
 	",c33688195964f7582011d10464cced0bc43fbe55,Sean Naren,2020-10-20 18:33:18+02:00,MODIFY,1,pytorch_lightning\trainer\connectors\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector.py,1.0,"176,177,178",,1.0,pbmstrk,2020-10-19T14:54:35Z,"
 		<denchmark-link:https://github.com/SeanNaren>@SeanNaren</denchmark-link>
  mind have look? :]
 		",2.0,pbmstrk,2020-10-19T16:25:32Z,"
 		Thanks for the report <denchmark-link:https://github.com/pbmstrk>@pbmstrk</denchmark-link>
 ! Fairly simple but with a couple of ways to fix this.
 The issue arises because when we log metrics, we do not include any custom logged epoch end metrics. The reason why this is included when something happens within the  function is because we actually complete the loop <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector.py#L174>here</denchmark-link>
  which reduces step metrics per dataloader.
 At <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pytorch_lightning/trainer/connectors/logger_connector.py#L197>this</denchmark-link>
  line we add everything in callback_metrics which includes our custom logged metric in .
 Solution 1
 Add the below lines in the step metrics calculation in logger_connector.py, replacing the simple continue operation:
 if len(self.callback_metrics) > 0:
     self.eval_loop_results.append(self.callback_metrics)
 continue
 The issue with this solution is that if we had multiple data-loaders, our custom metric will be added to every result for each data-loader. This is currently already the case if there is another metric already that is being reduced per dataloader.
 will return with multi dl:
 [
     {
         'custom_metric': 0.4
     },
     {
         'custom_metric': 0.4
     }
 ]
 Solution 2
 Add the below lines after the step metrics calculation, creating a separate entry in the return Results List for custom metrics. I.e add after the dataloader metrics loop:
                 if len(self.callback_metrics) > 0:
                     self.eval_loop_results.append(self.callback_metrics)
 will return:
 [
     {
         'y/dl1': 1.4e-4,
     },
     {
         'x/dl2': 1.8e-3,
     },
     {
         'custom_metric': 0.4
     }
 ]
 cc <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 		",3.0,pbmstrk,2020-10-20T09:25:13Z,"
 		Will implement solution 1, as this reflects what we see if there are step logging.
 		",,,,,MODIFY,2.0,tests\trainer\logging\test_eval_loop_logging_1_0.py,tests\trainer\logging\test_eval_loop_logging_1_0.py,1.0,"322,323,324",,test_eval_epoch_only_logging.test_epoch_end,"self,outputs",322,324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_log_on_evaluation_epoch_end_metrics,"self,epoch_logs",137,209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344",,test_eval_epoch_only_logging,"tmpdir,batches,log_interval,max_epochs",315,344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4268,david-waterworth,2020-10-21T00:06:22Z,2020-10-21T18:14:13Z,"EarlyStopping mode auto is unknown, fallback to auto mode.","
 In the process of refactoring as I upgraded lighting and it looks like there's been a slight change to the callback interface.
 This code:
 early_stop_callback = pl.callbacks.EarlyStopping(verbose=True)
 Results in the message:
 
 EarlyStopping mode auto is unknown, fallback to auto mode.
 EarlyStopping mode set to min for monitoring early_stop_on.
 
 The default for mode is 'auto' so the first message doesn't make sense
 	",2ffad4c89fcd19800a7532a8f7821578770451c6,Dusan Drevicky,2020-10-21 23:32:13+05:18,MODIFY,0,pytorch_lightning\callbacks\early_stopping.py,pytorch_lightning\callbacks\early_stopping.py,0.0,89,89,1.0,david-waterworth,2020-10-21T07:43:33Z,"
 		Hey <denchmark-link:https://github.com/david-waterworth>@david-waterworth</denchmark-link>
 ,
 Thanks for using Pytorch Lightning and finding this bug !
 I think it should be this way.
 <denchmark-code>EarlyStopping mode not provided, fallback to auto mode.
 ...
 EarlyStopping mode set to min for monitoring early_stop_on.
 </denchmark-code>
 
 Best regards,
 Thomas Chaton.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
427,ssaru,2019-10-24T17:39:22Z,2020-05-17T13:24:18Z,save_weights_only parameter in ModelCheckpoint class look like doesn't work,"
 <denchmark-h:h3>Common bugs:</denchmark-h>
 
 
 Tensorboard not showing in Jupyter-notebook see issue 79.
 PyTorch 1.1.0 vs 1.2.0 support see FAQ
 
 Describe the bug
 save_weights_only parameter in ModelCheckpoint class look like doesn't work
 document describe save_weight_only like that
 save_weights_only: if True, then only the model's weights will be saved (model.save_weights(filepath)), else the full model is saved (model.save(filepath)).
 but save_weight_only parameter doesn't save model differently each different options
 To Reproduce
 Steps to reproduce the behavior:
 
 I used sample script in official document
 
 <denchmark-code>import os
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 from torchvision.datasets import MNIST
 import torchvision.transforms as transforms
 
 import pytorch_lightning as pl
 
 class CoolSystem(pl.LightningModule):
 
     def __init__(self):
         super(CoolSystem, self).__init__()
         # not the best model...
         self.l1 = torch.nn.Linear(28 * 28, 10)
 
     def forward(self, x):
         return torch.relu(self.l1(x.view(x.size(0), -1)))
 
     def training_step(self, batch, batch_nb):
         # REQUIRED
         x, y = batch
         y_hat = self.forward(x)
         loss = F.cross_entropy(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def validation_step(self, batch, batch_nb):
         # OPTIONAL
         x, y = batch
         y_hat = self.forward(x)
         return {'val_loss': F.cross_entropy(y_hat, y)}
 
     def validation_end(self, outputs):
         # OPTIONAL
         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
         tensorboard_logs = {'val_loss': avg_loss}
         return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         # REQUIRED
         # can return multiple optimizers and learning_rate schedulers
         # (LBFGS it is automatically supported, no need for closure function)
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
     @pl.data_loader
     def train_dataloader(self):
         # REQUIRED
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def val_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
     @pl.data_loader
     def test_dataloader(self):
         # OPTIONAL
         return DataLoader(MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor()), batch_size=32)
 
 if __name__ == ""__main__"":
     from pytorch_lightning import Trainer
     from pytorch_lightning.callbacks import ModelCheckpoint
 
     weight_path = os.path.join(os.getcwd(), 'checkpoint')
     if not os.path.exists(weight_path):
         os.mkdir(weight_path)
 
     checkpoint_callback = ModelCheckpoint(
         filepath=weight_path,
         save_best_only=False,
         verbose=True,
         monitor='val_loss',
         mode='min',
         prefix='',
         save_weights_only=False
     )
 
     gpus = torch.cuda.device_count()
     device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
     model = CoolSystem()
     model.to(device)
     trainer = Trainer(checkpoint_callback=checkpoint_callback,
                       max_nb_epochs=1, train_percent_check=0.1)
     trainer.fit(model)
 </denchmark-code>
 
 
 i just changed save_weights_only parameter & weight directory for saving. because if i don't model try restore weight
 
 for example i save model in ""checkpoint"" directory.
 after trained i move ckpt file other directory like called test
 so, test directory have two model file save_weight_only_True, save_weight_only_False
 
 
 i was checking what they have some different using torch.load
 (PATH). but not different...
 
 
 two file has same parameter like this
 save_weights_only_False
 
 
 <denchmark-code>{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4830682784: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4830682928: {'step': 187, 'exp_avg': tensor([ 2.8793e-12, -2.0038e-03,  1.6457e-03,  9.9219e-12, -1.4364e-02,
          7.6137e-03,  9.4487e-12,  1.0773e-11,  9.9046e-03, -3.7999e-03]), 'exp_avg_sq': tensor([7.2368e-08, 7.3899e-05, 1.2118e-04, 8.5934e-07, 1.5810e-04, 9.1419e-05,
         7.7932e-07, 1.0131e-06, 1.5449e-04, 2.0831e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4830682784, 4830682928]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[ 0.0198, -0.0171, -0.0024,  ..., -0.0154,  0.0243, -0.0241],
         [ 0.0349, -0.0040, -0.0051,  ...,  0.0159,  0.0295,  0.0129],
         [ 0.0302, -0.0218, -0.0171,  ...,  0.0104,  0.0179, -0.0167],
         ...,
         [-0.0284,  0.0056,  0.0178,  ...,  0.0211, -0.0075,  0.0163],
         [ 0.0145,  0.0204,  0.0121,  ..., -0.0013, -0.0103,  0.0043],
         [ 0.0059, -0.0060,  0.0017,  ..., -0.0214,  0.0273, -0.0288]])), ('l1.bias', tensor([-0.1228,  0.3341, -0.0163, -0.1302,  0.1738,  0.3422, -0.1155, -0.1122,
         -0.5156, -0.1902]))])}
 </denchmark-code>
 
 save_weights_only_True
 <denchmark-code>{'epoch': 0, 'global_step': 187, 'checkpoint_callback_best': inf, 'optimizer_states': [{'state': {4877819552: {'step': 187, 'exp_avg': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]]), 'exp_avg_sq': tensor([[0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         ...,
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.],
         [0., 0., 0.,  ..., 0., 0., 0.]])}, 4877819696: {'step': 187, 'exp_avg': tensor([ 1.4936e-11, -1.6439e-03, -6.1600e-04, -4.5436e-03, -8.8385e-03,
          4.9613e-12,  1.6526e-11,  4.4405e-03,  1.8937e-12, -6.5171e-04]), 'exp_avg_sq': tensor([1.9473e-06, 8.8430e-05, 1.4157e-04, 1.1038e-04, 1.3500e-04, 2.1486e-07,
         2.3841e-06, 1.1693e-04, 3.1302e-08, 2.3050e-04])}}, 'param_groups': [{'lr': 0.02, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [4877819552, 4877819696]}]}], 'lr_schedulers': [], 'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],
         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],
         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],
         ...,
         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],
         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],
         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,
         -0.1239, -0.0374]))])}
 </denchmark-code>
 
 Expected behavior
 if save_weights_only: if True
 expected value like this
 <denchmark-code>{'state_dict': OrderedDict([('l1.weight', tensor([[-0.0299,  0.0281, -0.0246,  ...,  0.0143,  0.0314,  0.0167],
         [ 0.0066, -0.0160,  0.0200,  ..., -0.0266,  0.0097,  0.0138],
         [ 0.0257,  0.0134, -0.0111,  ..., -0.0201,  0.0199, -0.0146],
         ...,
         [ 0.0260, -0.0082, -0.0049,  ...,  0.0277, -0.0070,  0.0275],
         [ 0.0089, -0.0003, -0.0051,  ..., -0.0086,  0.0285, -0.0252],
         [-0.0202,  0.0252,  0.0083,  ..., -0.0144, -0.0181,  0.0105]])), ('l1.bias', tensor([-0.1031,  0.2837,  0.0276, -0.1406,  0.2007, -0.1086, -0.1267,  0.2557,
         -0.1239, -0.0374]))])}
 </denchmark-code>
 
 Screenshots
 If applicable, add screenshots to help explain your problem.
 Desktop (please complete the following information):
 
 OS: MacBook Pro (15-inch, 2018); Mojave
 Browser: chrome
 Version:  pytorch-lightning==0.5.2.1, torch==1.3.0.post2, torchvision==0.4.1.post2, test-tube==0.7.3
 
 Additional context
 I try to find some reason, why save_weights_only parameter doesn't work
 i found <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/master/pytorch_lightning/trainer/trainer_io.py>TrainerIOMixin class</denchmark-link>
  inside PyTorch lightning. and I feel save_weights_only parameter not was implemented in PyTorch lightning
 	",8c4c7b105e16fbe255e4715f54af2fa5d2a12fad,Fabio Natanael Kepler,2020-05-17 09:24:17-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"74,75",,1.0,ssaru,2020-02-22T02:06:33Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",2.0,ssaru,2020-03-01T10:10:33Z,"
 		<denchmark-link:https://github.com/ssaru>@ssaru</denchmark-link>
  could you check it with the latest version on master?
 		",3.0,ssaru,2020-03-03T15:13:22Z,"
 		Appreciate your reply.
 I will check it
 		",4.0,ssaru,2020-03-26T13:58:08Z,"
 		<denchmark-link:https://github.com/ssaru>@ssaru</denchmark-link>
  I assume that it was fixed, if not pls feel free to reopen... 
 		",MODIFY,1.0,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,142,142,_save_model,"self,filepath",136,144,MODIFY,5.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339","309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344",MODIFY,4.0,tests\trainer\test_trainer.py,tests\trainer\test_trainer.py,1.0,273,273,test_model_checkpoint_options.mock_save_function,filepath,273,274,,,,,,,,,,,,5.0,ssaru,2020-04-29T19:41:49Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  It's not fixed yet.
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/42d5cfc3b056b4c82a77a7cdcb8eafc63a812b67/pytorch_lightning/trainer/training_io.py#L291>dump_checkpoint</denchmark-link>
  still returns everything regardless of the  parameter.
 		",6.0,ssaru,2020-05-11T07:14:25Z,"
 		I just ran into this bug myself. I can work on it, but probably not for another week or so. I took a quick look through the code and it doesn't seem too difficult to fix, but as I'm not familiar with the entire code base there might be some distant issue I haven't seen, but I think dump_checkpoint would need an argument and then it could save nothing but the state_dict; plus there'd have to be corresponding checks for the extra checkpoint keys added to restore and restore_training_state.
 		",7.0,ssaru,2020-05-11T08:14:22Z,"
 		<denchmark-link:https://github.com/rightaditya>@rightaditya</denchmark-link>
  mind draft a PR and we can help to finish it fast... :]
 		",8.0,ssaru,2020-05-11T14:04:53Z,"
 		I also just run into this and went ahead and created a draft PR. Saving only the weights is working. However, I haven't changed any logic regarding loading.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,dump_checkpoint,self,309,366,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,273,273,test_model_checkpoint_options.mock_save_function,"filepath,args",273,274,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"395,396,397,398,399,400",,restore_training_state,"self,checkpoint",388,438,1.0,"259,260","259,260",save_checkpoint,"self,filepath,bool",259,272,1.0,"299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334",,test_model_checkpoint_only_weights,tmpdir,299,334,1.0,273,273,test_model_checkpoint_options,"tmpdir,save_top_k,file_prefix,expected_files",270,296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"309,315,316,317,318,319,320,321,323,324,325,326,328,330,331,332,333,335,337,338,339","309,315,316,318,319,320,322,323,324,325,327,329,330,331,332,334,341,342,343,344",dump_checkpoint,"self,bool",309,367,1.0,"259,260","259,260",save_checkpoint,"self,filepath",259,272,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4275,tchaton,2020-10-21T07:54:54Z,2020-10-21T14:06:43Z,[HOT-BUG] Checkpoint in callbacks list fails,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 <denchmark-h:h2>Please reproduce using the BoringModel and post here</denchmark-h>
 
 The ModelCheckpoint is not properly setup when provided through the list of callbacks.
 <denchmark-code>def test_checkpoint_within_callbacks_list(tmpdir):
     """"""
     This test validates that the checkpoint can be called when provided to callacks list
     """"""
 
     os.environ['PL_DEV_DEBUG'] = '1'
 
     checkpoint_callback = ModelCheckpoint(monitor='val_loss', filepath=osp.join(tmpdir, ""{epoch:02d}""))
 
     class ExtendedBoringModel(BoringModel):
 
         def validation_step(self, batch, batch_idx):
             output = self.layer(batch)
             loss = self.loss(batch, output)
             return {""val_loss"": loss}
 
     model = ExtendedBoringModel()
     model.validation_step_end = None
     model.validation_epoch_end = None
     trainer = pl.Trainer(max_epochs=1, 
                          limit_train_batches=2, 
                          limit_val_batches=2, 
                          limit_test_batches=2, 
                          callbacks=[checkpoint_callback])
 
     trainer.fit(model)
     assert os.listdir(tmpdir) == ['epoch=00.ckpt]']
 </denchmark-code>
 
 <denchmark-code>
 tests/checkpointing/test_model_checkpoint.py:590: 
 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 pytorch_lightning/trainer/trainer.py:439: in fit
     results = self.accelerator_backend.train()
 pytorch_lightning/accelerators/cpu_accelerator.py:48: in train
     results = self.train_or_test()
 pytorch_lightning/accelerators/accelerator.py:66: in train_or_test
     results = self.trainer.train()
 pytorch_lightning/trainer/trainer.py:482: in train
     self.train_loop.run_training_epoch()
 pytorch_lightning/trainer/training_loop.py:569: in run_training_epoch
     self.trainer.run_evaluation(test_mode=False)
 pytorch_lightning/trainer/trainer.py:609: in run_evaluation
     self.evaluation_loop.on_evaluation_end()
 pytorch_lightning/trainer/evaluation_loop.py:109: in on_evaluation_end
     self.trainer.call_hook('on_validation_end', *args, **kwargs)
 pytorch_lightning/trainer/trainer.py:822: in call_hook
     trainer_hook(*args, **kwargs)
 pytorch_lightning/trainer/callback_hook.py:177: in on_validation_end
     callback.on_validation_end(self, self.get_model())
 pytorch_lightning/callbacks/model_checkpoint.py:167: in on_validation_end
     self.save_checkpoint(trainer, pl_module)
 pytorch_lightning/callbacks/model_checkpoint.py:213: in save_checkpoint
     self._save_top_k_checkpoints(monitor_candidates, trainer, pl_module, epoch, filepath)
 pytorch_lightning/callbacks/model_checkpoint.py:494: in _save_top_k_checkpoints
     self._update_best_and_save(filepath, current, epoch, trainer, pl_module)
 pytorch_lightning/callbacks/model_checkpoint.py:543: in _update_best_and_save
     self._save_model(filepath, trainer, pl_module)
 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
 
 self = <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x13a775b80>
 filepath = '/private/var/folders/q9/1sc07dt91w9581rl1vt6vd4w0000gn/T/pytest-of-thomas/pytest-1058/test_checkpoint_within_callbac0/epoch=00.ckpt'
 trainer = <pytorch_lightning.trainer.trainer.Trainer object at 0x13a7756a0>, pl_module = ExtendedBoringModel(
   (layer): Linear(in_features=32, out_features=2, bias=True)
 )
 
     def _save_model(self, filepath: str, trainer, pl_module):
         # in debugging, track when we save checkpoints
         trainer.dev_debugger.track_checkpointing_history(filepath)
     
         # make paths
         if trainer.is_global_zero:
             self._fs.makedirs(os.path.dirname(filepath), exist_ok=True)
     
         # delegate the saving to the trainer
         if self.save_function is not None:
             self.save_function(filepath, self.save_weights_only)
         else:
 >           raise ValueError("".save_function() not set"")
 E           ValueError: .save_function() not set
 
 pytorch_lightning/callbacks/model_checkpoint.py:297: ValueError
 ----------------------------------------------------------------------------------------- Captured log call ------------------------------------------------------------------------------------------
 INFO     lightning:distributed.py:49 GPU available: False, used: False
 INFO     lightning:distributed.py:49 TPU available: False, using: 0 TPU cores
 INFO     lightning:lightning.py:1290 
   | Name  | Type   | Params
 ---------------------------------
 0 | layer | Linear | 66
 ========================================================================================== warnings summary ==========================================================================================
 .venv/lib/python3.8/site-packages/comet_ml/monkey_patching.py:19
   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/comet_ml/monkey_patching.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
     import imp
 
 .venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120
   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.
     warnings.warn(msg)
 
 .venv/lib/python3.8/site-packages/wandb/util.py:35
   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/wandb/util.py:35: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
     from collections import namedtuple, Mapping, Sequence
 
 .venv/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55
   /Users/thomas/Documents/projects/pytorch-lightning/.venv/lib/python3.8/site-packages/wandb/vendor/graphql-core-1.1/graphql/type/directives.py:55: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.9 it will stop working
     assert isinstance(locations, collections.Iterable), 'Must provide locations for directive.'
 
 tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list
   /Users/thomas/Documents/projects/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
     warnings.warn(*args, **kwargs)
 
 tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list
   /Users/thomas/Documents/projects/pytorch-lightning/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
     warnings.warn(*args, **kwargs)
 
 -- Docs: https://docs.pytest.org/en/stable/warnings.html
 ====================================================================================== short test summary info =======================================================================================
 FAILED tests/checkpointing/test_model_checkpoint.py::test_checkpoint_within_callbacks_list - ValueError: .save_function() not set
 </denchmark-code>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 Please copy and paste the output from our
 <denchmark-link:https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py>environment collection script</denchmark-link>
 
 (or fill out the checklist below manually).
 You can get the script and run it with:
 <denchmark-code>wget https://raw.githubusercontent.com/PyTorchLightning/pytorch-lightning/master/tests/collect_env_details.py
 # For security purposes, please check the contents of collect_env_details.py before running it.
 python collect_env_details.py
 </denchmark-code>
 
 
 PyTorch Version (e.g., 1.0):
 OS (e.g., Linux):
 How you installed PyTorch (conda, pip, source):
 Build command you used (if compiling from source):
 Python version:
 CUDA/cuDNN version:
 GPU models and configuration:
 Any other relevant information:
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",8a20d6af51d13adec37593c1356ce08ef380e828,William Falcon,2020-10-21 10:06:42-04:00,MODIFY,1,pytorch_lightning\callbacks\model_checkpoint.py,pytorch_lightning\callbacks\model_checkpoint.py,1.0,162,,1.0,tchaton,2020-10-21T08:05:18Z,"
 		isn't passing ModelCheckpoint in callbacks flag WIP?
 here is the PR: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3990>#3990</denchmark-link>
 
 <denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 		",2.0,tchaton,2020-10-21T12:23:10Z,"
 		Yes exactly. ModelCheckpoint (currently) is special, meaning there can only be one and you have to pass it to Trainer(checkpoint_callback=), passing to callbacks argument is not allowed.
 In my PR I try to remove this limitation partially, and it requires several stages of changes and multiple PRs to get this done.
 		",3.0,tchaton,2020-10-21T12:42:35Z,"
 		the solution here is to move the save function inside the callback. no need to have these separated anymore
 		",4.0,tchaton,2020-10-21T13:03:32Z,"
 		That's exactly what I'm doing here:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/3990>#3990</denchmark-link>
 
 		",MODIFY,1.0,pytorch_lightning\trainer\connectors\callback_connector.py,pytorch_lightning\trainer\connectors\callback_connector.py,1.0,,"64,65",init_default_checkpoint_callback,"self,checkpoint_callback",59,67,MODIFY,2.0,tests\checkpointing\test_model_checkpoint.py,tests\checkpointing\test_model_checkpoint.py,1.0,"529,530,531,532",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_checkpoint_within_callbacks_list.validation_step,"self,batch,batch_idx",529,532,on_pretrain_routine_start,"self,trainer,pl_module",157,162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546",,test_checkpoint_within_callbacks_list,tmpdir,518,546,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4276,neergaard,2020-10-21T08:16:15Z,2020-10-26T11:57:04Z,WandbLogger fails in 1.0.2 due to non-JSON serializable object,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 After updating to PL 1.0.2, the WandbLogger fails with the following TypeError:
 <denchmark-code>Traceback (most recent call last):
   File ""wandblogger_issue.py"", line 12, in <module>
     wandb_logger.log_hyperparams(vars(args))
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py"", line 35, in wrapped_fn
     return fn(*args, **kwargs)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py"", line 138, in log_hyperparams
     self.experiment.config.update(params, allow_val_change=True)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/sdk/wandb_config.py"", line 87, in update
     self._callback(data=self._as_dict())
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/sdk/wandb_run.py"", line 587, in _config_callback
     self._backend.interface.publish_config(data)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/interface/interface.py"", line 496, in publish_config
     cfg = self._make_config(config_dict)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/interface/interface.py"", line 232, in _make_config
     update.value_json = json_dumps_safer(json_friendly(v)[0])
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/util.py"", line 524, in json_dumps_safer
     return json.dumps(obj, cls=WandBJSONEncoder, **kwargs)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/__init__.py"", line 238, in dumps
     **kw).encode(obj)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py"", line 199, in encode
     chunks = self.iterencode(o, _one_shot=True)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py"", line 257, in iterencode
     return _iterencode(o, 0)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/site-packages/wandb/util.py"", line 480, in default
     return json.JSONEncoder.default(self, obj)
   File ""/home/groups/mignot/miniconda3/envs/pl/lib/python3.7/json/encoder.py"", line 179, in default
     raise TypeError(f'Object of type {o.__class__.__name__} '
 TypeError: Object of type function is not JSON serializable
 </denchmark-code>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Run the following code snippet to reproduce:
 <denchmark-code>from argparse import ArgumentParser
 from pprint import pprint
 
 from pytorch_lightning import Trainer
 from pytorch_lightning.loggers import WandbLogger
 
 
 if __name__ == ""__main__"":
 
     parser = ArgumentParser()
     parser = Trainer.add_argparse_args(parent_parser=parser)
     args = parser.parse_args()
     pprint(vars(args))
     wandb_logger = WandbLogger()
     wandb_logger.log_hyperparams(vars(args))
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Hyperparams are logged as usual without any TypeError.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>* CUDA:
         - GPU:
         - available:         False
         - version:           10.2
 * Packages:
         - numpy:             1.19.1
         - pyTorch_debug:     False
         - pyTorch_version:   1.6.0
         - pytorch-lightning: 1.0.2
         - tensorboard:       2.3.0
         - tqdm:              4.50.2
 * System:
         - OS:                Linux
         - architecture:
                 - 64bit
                 - 
         - processor:         x86_64
         - python:            3.7.9
         - version:           #1 SMP Mon Jul 29 17:46:05 UTC 2019
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 Pretty printing the arguments gives the following clue about the error:
 <denchmark-code>{'accelerator': None,
  'accumulate_grad_batches': 1,
  'amp_backend': 'native',
  'amp_level': 'O2',
  'auto_lr_find': False,
  'auto_scale_batch_size': False,
  'auto_select_gpus': False,
  'automatic_optimization': True,
  'benchmark': False,
  'check_val_every_n_epoch': 1,
  'checkpoint_callback': True,
  'default_root_dir': None,
  'deterministic': False,
  'distributed_backend': None,
  'fast_dev_run': False,
  'flush_logs_every_n_steps': 100,
  'gpus': <function _gpus_arg_default at 0x7f26b7788f80>,
  'gradient_clip_val': 0,
  'limit_test_batches': 1.0,
  'limit_train_batches': 1.0,
  'limit_val_batches': 1.0,
  'log_every_n_steps': 50,
  'log_gpu_memory': None,
  'logger': True,
  'max_epochs': 1000,
  'max_steps': None,
  'min_epochs': 1,
  'min_steps': None,
  'num_nodes': 1,
  'num_processes': 1,
  'num_sanity_val_steps': 2,
  'overfit_batches': 0.0,
  'precision': 32,
  'prepare_data_per_node': True,
  'process_position': 0,
  'profiler': None,
  'progress_bar_refresh_rate': 1,
  'reload_dataloaders_every_epoch': False,
  'replace_sampler_ddp': True,
  'resume_from_checkpoint': None,
  'sync_batchnorm': False,
  'terminate_on_nan': False,
  'tpu_cores': <function _gpus_arg_default at 0x7f26b7788f80>,
  'track_grad_norm': -1,
  'truncated_bptt_steps': None,
  'val_check_interval': 1.0,
  'weights_save_path': None,
  'weights_summary': 'top'}
 </denchmark-code>
 
 I assume the issue comes from the gpus and tpu_cores values, which are function calls, when not explicitly supplied as arguments.
 	",f07ee33db679a4b4bdcb4a2a221aa5cbb05d7b34,chaton,2020-10-26 11:57:03+00:00,MODIFY,0,.gitignore,.gitignore,0.0,141,,1.0,neergaard,2020-10-21T12:19:55Z,"
 		Trainer.add_argparse_args adds some functions to the args Namespace which are not JSON serializable, so an error is thrown when WandbLogger tries to save the hyperparameters of the run. I temporarily got around the issue by removing functions before calling save_hyperparameters, but this definitely needs a fix.
 <denchmark-code>class MyModel(LightningModule):
     def __init__(self, hparams, *args, **kwargs):
         super().__init__()
         self.save_hyperparameters({k:v for (k,v) in vars(hparams).items() if not callable(v)})
 </denchmark-code>
 
 		",2.0,neergaard,2020-10-22T07:45:31Z,"
 		<denchmark-link:https://github.com/ursulean>@ursulean</denchmark-link>
  As a temporary solution, this works great thanks!
 But definitely, it needs a proper fix.
 I am not sure what the changes between 0.9.0 and 1.0 are since both of them sets the default arg to  (see here for <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/b40de5464a953ff5866a255f4670d318bd8fd65a/pytorch_lightning/trainer/trainer.py#L770>0.9.0</denchmark-link>
  and <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/5c153c224442c8315a2b8ddc0b64a24dc6798aa3/pytorch_lightning/utilities/argparse_utils.py#L188>1.0.0</denchmark-link>
 ), but I might have overlooked something obvious.
 		",3.0,neergaard,2020-10-23T07:56:50Z,"
 		Hey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>
 ,
 Would you mind creating a test to reproduce this bug (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py</denchmark-link>
 ).
 Best regards,
 T.C
 		",4.0,neergaard,2020-10-23T08:13:37Z,"
 		Hi <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  sure, do you want me to just do it in a Colab notebook or do you want a gist with the script you linked to?
 		",MODIFY,0.0,CHANGELOG.md,CHANGELOG.md,0.0,"14,17,20,23,26,29,31,32,35,36,37,38,41,44,47,51",,,,,,MODIFY,2.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,"182,183,184,185,186,187,188,189,190,191,192",,MODIFY,1.0,pytorch_lightning\loggers\wandb.py,pytorch_lightning\loggers\wandb.py,1.0,138,,log_hyperparams,"self,str",135,139,MODIFY,3.0,tests\loggers\test_wandb.py,tests\loggers\test_wandb.py,1.0,"130,131",,test_wandb_sanitize_callable_params.wrapper_something,,130,131,5.0,neergaard,2020-10-23T08:20:37Z,"
 		Hey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>
 , I would prefer the gist :) Easier to integrated in our tests :)
 		",6.0,neergaard,2020-10-23T08:21:15Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  For know here's a gist with the test (<denchmark-link:https://gist.github.com/neergaard/ed0620ab9405b79d420b99db3e43605a>https://gist.github.com/neergaard/ed0620ab9405b79d420b99db3e43605a</denchmark-link>
 ). I've basically inserted the code snippet I supplied in my orig post without deleting anything of the bug report code, but it should run and return the TypeError still.
 		",7.0,neergaard,2020-10-23T08:40:15Z,"
 		Hey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>
 ,
 Without the provided arguments, I can't use the gist :)
 <denchmark-link:https://user-images.githubusercontent.com/12861981/96976415-bcbd0c00-1513-11eb-8f34-24673a6851fc.png></denchmark-link>
 
 		",8.0,neergaard,2020-10-23T08:42:50Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  That's why I asked if you preferred a gist or a colab notebook, as the script as such does not function in a notebook, but the gist works using the command line.
 I don't know how to get the default arguments from the Trainer in a notebook?
 		",9.0,neergaard,2020-10-23T08:46:01Z,"
 		Hey, you can provide the command line as a string :)
 <denchmark-code>opt = ""--name_1 arg_1 .... --name_n arg_n"".split("" "")
 parser = ArgumentParser()
 parser = Trainer.add_argparse_args(parent_parser=parser)
 args = parser.parse_args(opt)
 </denchmark-code>
 
 		",10.0,neergaard,2020-10-23T08:51:35Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  D'oh! Simple solution works great, thanks!
 I've updated the gist, can you try it now?
 		",11.0,neergaard,2020-10-23T08:54:02Z,"
 		Hey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>
  ,
 I will also update bug_report to add this trick :) Thanks for asking about it :)
 Best regards
 		",12.0,neergaard,2020-10-23T08:59:25Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  thanks for helping out!
 I can add that I investigated the issue more, and it doesn't seem to be a problem in WandB version 0.10.8, but it is an issue in version 0.10.7.
 edit: what I mean is, using wandb==0.10.8 does not result in a TypeError, but I still think the _gpus_arg_default default value should be handled properly in Pytorch Lightning.
 		",13.0,neergaard,2020-10-23T09:02:16Z,"
 		I will look into this afternoon or tomorrow. Feel free to investigate and submit a PR if you find the bug :)
 		",14.0,neergaard,2020-10-23T11:06:11Z,"
 		Hey <denchmark-link:https://github.com/neergaard>@neergaard</denchmark-link>
 ,
 Feel free to have a look at the PR.
 Best,
 T.C
 		",,,,,_sanitize_callable_params._sanitize_callable,val,182,192,,,,,1.0,"116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140",,test_wandb_sanitize_callable_params,tmpdir,116,140,1.0,"126,127",,test_wandb_sanitize_callable_params.return_something,,126,127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194",,_sanitize_callable_params,str,172,194,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4304,jopo666,2020-10-22T12:12:49Z,2020-11-25T19:44:06Z,TensorBoardLogger not working as expected with accumulate_grad_batches&gt;1,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When logging inside training step to TensorBoard and using accumulate_grad_batches > 1 inside pl.Trainer() the behavior is not as expected.
 With  everything looks good.
 <denchmark-link:https://user-images.githubusercontent.com/49716607/96869625-65b62900-1478-11eb-85fd-c3c5d219c65e.png></denchmark-link>
 
 With accumulate_grad_batches = 8 the values are reported on the same step.
 <denchmark-link:https://user-images.githubusercontent.com/49716607/96869632-66e75600-1478-11eb-981b-399af7dd173c.png></denchmark-link>
 
 <denchmark-h:h3>To Reproduce (sorry for not using colab)</denchmark-h>
 
 <denchmark-code>import os
 
 import torch
 from torch.nn import functional as F
 from torch.utils.data import DataLoader, random_split
 
 import pytorch_lightning as pl
 from pytorch_lightning.loggers import TensorBoardLogger
 from torchvision.datasets.mnist import MNIST
 from torchvision import transforms
 
 class LitClassifier(pl.LightningModule):
     def __init__(self, hidden_dim=128, learning_rate=1e-3):
         super().__init__()
         self.save_hyperparameters()
 
         self.l1 = torch.nn.Linear(28 * 28, self.hparams.hidden_dim)
         self.l2 = torch.nn.Linear(self.hparams.hidden_dim, 10)
 
     def forward(self, x):
         x = x.view(x.size(0), -1)
         x = torch.relu(self.l1(x))
         x = torch.relu(self.l2(x))
         return x
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
         self.log(""train_loss"",loss)
         return loss
 
     def validation_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self(x)
         loss = F.cross_entropy(y_hat, y)
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
 
 def run_test(accumulate_grad_batches,batch_size,num_workers):
     dataset = MNIST('', train=True, download=True, transform=transforms.ToTensor())
     mnist_train, mnist_val = random_split(dataset, [55000, 5000])
     train_loader = DataLoader(mnist_train,batch_size)
     val_loader = DataLoader(mnist_val,batch_size)
 
     model = LitClassifier()
 
     trainer = pl.Trainer(
         logger=TensorBoardLogger(os.getcwd()',name='bug'),
         accumulate_grad_batches=accumulate_grad_batches,
         max_epochs=2
         )
     trainer.fit(model, train_loader, val_loader)
 
 run_test(1,32)
 run_test(8,32)
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Take a mean of the values logged at same step?
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version: 1.6
 OS: Linux
 How you installed PyTorch: conda
 Python version: 3.8
 Tensorboard: 2.3.0
 
 	",204a0a2d03ce7dfb014e347f1d34a49ef5e86902,chaton,2020-11-25 19:44:05+00:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"103,104",,1.0,jopo666,2020-10-22T12:13:31Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,jopo666,2020-10-23T13:46:38Z,"
 		To add something to the issue: when using DataParallel () it might cause even worse results, i.e in W&B it looks like this:
 <denchmark-link:https://user-images.githubusercontent.com/6958772/97011285-c2801500-1546-11eb-8362-37f2e5e1814e.png></denchmark-link>
 
 I'm also logging from training_step.
 		",3.0,jopo666,2020-10-23T21:20:19Z,"
 		The first one looks like an issue with the limitations of  since  is updated after every accumulated step, and since x-axis is the  so for every  it's logging 8 values(num accumulated batches). I tried this on Wandb the it's looking good there.
 <denchmark-link:https://user-images.githubusercontent.com/30778939/97055069-a199dd00-15a3-11eb-9872-b9c4ebeb6348.png></denchmark-link>
 
 		",4.0,jopo666,2020-10-24T14:01:49Z,"
 		<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  have you checked that with or without DataParallel?
 		",MODIFY,4.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,627,622,log_train_step_metrics,"self,batch_output",620,628,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"24,25,26,27,32,34,38,39,40,41,43,44,45,46,47,48,49,51,55,57,59,61,65,388","28,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,388",MODIFY,1.0,tests\loggers\test_all.py,tests\loggers\test_all.py,1.0,"127,135","127,135",_test_loggers_fit_test,"tmpdir,logger_class",81,139,MODIFY,4.0,tests\loggers\test_tensorboard.py,tests\loggers\test_tensorboard.py,1.0,"231,232,233,234,235",,test_tensorboard_with_accummulated_gradients.validation_step,"self,batch,batch_idx",231,235,5.0,jopo666,2020-10-24T14:11:24Z,"
 		<denchmark-link:https://github.com/marrrcin>@marrrcin</denchmark-link>
  don't have GPUs , can't check it.
 		",6.0,jopo666,2020-10-24T18:13:55Z,"
 		So what I've found out is the following:
 
 Logging with on_step=False and on_epoch=True in __step_end yields fine charts on the epoch level (for any kind of step).
 Logging with on_step=True and on_epoch=False in training_step_end results in messed up charts that are dependant on the accumulate_grad_batches parameter of the Trainer as in the picture:
 
 Each red box contains number of points equal to accumulate_grad_batches.
 
 It seems to me like the problem is that for the same Trainer's global_step, there are accumulate_grad_batches-calls to both train_step and backward and when you log from there, the charts are invalid. IMHO when accumulate_grad_batches is > 1 then there should be some callback / on_* event just before the optimizer step happens that will have access to the losses calculated during the accumulation step.
 My current workaround to this is to store every training loss's value during the same  and only call  when global step increments - then the chart looks normal:
 <denchmark-link:https://user-images.githubusercontent.com/6958772/97088721-27189e00-1633-11eb-8e25-337ec66d7175.png></denchmark-link>
 
 It's a nasty workaround, I hope <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  you will be able to fix it in more robust manner.
 		",7.0,jopo666,2020-10-28T14:45:38Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  any update on this?
 		",8.0,jopo666,2020-11-10T09:37:21Z,"
 		Ping <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 
 		",9.0,jopo666,2020-11-17T18:19:57Z,"
 		defining the behavior only for accumulate_grad_batches,
 avoid making general logging changes
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"219,220,221,222,223,224,225,226,227,228,229",,test_tensorboard_with_accummulated_gradients.training_step,"self,batch,batch_idx",219,229,1.0,"211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264",,test_tensorboard_with_accummulated_gradients,"mock_log_metrics,expected,tmpdir",211,264,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"98,99",98,on_trainer_init,"self,logger,int,int,bool",98,99,1.0,"184,193,194,212,213,214,215,216","184,210,211",log_metrics,"self,metrics,grad_norm_dic,step",184,221,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"237,238,239,240",,test_tensorboard_with_accummulated_gradients.configure_optimizers,self,237,240,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"184,193,194,212,213,214,215,216","184,210,211",log_metrics,"self,metrics,grad_norm_dic,step,log_train_step_metrics",184,226,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4486,ET-Chan,2020-11-02T19:19:18Z,2020-11-27T12:57:26Z,TrainerDataLoadingMixin.replace_sampler ignores multiprocessing_context,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 replace_sampler in TrainerDataLoadingMixin ignores multiprocessing_context
 <denchmark-h:h2>Please reproduce using [the BoringModel and post here]</denchmark-h>
 
 <denchmark-link:https://colab.research.google.com/drive/1IRCGh1HbutELn3ADsZ9jAwDjc6zoyB_R?usp=sharing>https://colab.research.google.com/drive/1IRCGh1HbutELn3ADsZ9jAwDjc6zoyB_R?usp=sharing</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 It should return a new data loader with the replaced sampler, and the same multiprocessing_context
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 Tesla T4
 
 
 available:         True
 version:           10.1
 
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     False
 pyTorch_version:   1.6.0+cu101
 pytorch-lightning: 0.10.0
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
 
 
 
 	",dee968f20b89db7d6cbdd6069e5ed307980cbc86,chaton,2020-11-27 12:57:25+00:00,MODIFY,1,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"134,136",,1.0,ET-Chan,2020-11-02T19:20:03Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",,,,,,,,,,,,,MODIFY,2.0,tests\trainer\test_dataloaders.py,tests\trainer\test_dataloaders.py,1.0,"1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139",,test_replace_sampler_with_multiprocessing_context,tmpdir,1119,1139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,replace_sampler,"self,dataloader,sampler",125,137,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"1129,1130",,test_replace_sampler_with_multiprocessing_context.train_dataloader,self,1129,1130,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4556,Vozf,2020-11-06T19:52:04Z,2020-11-10T21:13:43Z,Gpu memory leak with self.log on_epoch=True,"
 pl 1.0.5
 Using new logging api I want to log a metric in LightningModule
 <denchmark-code>self.log("";;;;;;;;;;;;;;;;;;;"", 1, on_step=False, on_epoch=True)
 </denchmark-code>
 
 This is a dummy example but it is sufficient to add to LightningModule's training_step to cause a memory leak on gpu.
 What could go wrong? We want to log a metric which is not even a cuda tensor. How could it lead to a gpu memory leak?
 Well thanks to the magic of metric epoch aggregation stuff
 Let's dig in and take a look at here
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 550 to 569
       in
       b3db197
 
 
 
 
 
 
  # ------------------------------------ 
 
 
 
  # TRAINING_STEP + TRAINING_STEP_END 
 
 
 
  # ------------------------------------ 
 
 
 
  batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx) 
 
 
 
  
 
 
 
  # when returning -1 from train_step, we end epoch early 
 
 
 
  if batch_output.signal == -1: 
 
 
 
  break 
 
 
 
  
 
 
 
  # only track outputs when user implements training_epoch_end 
 
 
 
  # otherwise we will build up unnecessary memory 
 
 
 
  epoch_end_outputs = self.process_train_step_outputs( 
 
 
 
  batch_output.training_step_output_for_epoch_end, 
 
 
 
  self.early_stopping_accumulator, 
 
 
 
  self.checkpoint_accumulator, 
 
 
 
  ) 
 
 
 
  
 
 
 
  # hook 
 
 
 
  # TODO: add outputs to batches 
 
 
 
  self.on_train_batch_end(epoch_output, epoch_end_outputs, batch, batch_idx, dataloader_idx) 
 
 
 
 
 
 Here we run batch, convert batch_output to epoch_end_outputs if on_epoch was set and append epoch_end_outputs to epoch_output inside on_train_batch_end
 epoch_output is defined here
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 540
       in
       b3db197
 
 
 
 
 
 
  epoch_output = [[] for _ in range(self.num_optimizers)] 
 
 
 
 
 
 Everything seems normal, but there is a problem inside  there is a surprise - loss value stored on gpu.
 <denchmark-link:https://user-images.githubusercontent.com/22998537/98406840-ba17f600-207f-11eb-9661-1535d90612a1.png></denchmark-link>
 
 I think you can guess by now what could go wrong if we store a lot of separate cuda tensors in a long long 
 Yeah the gpu memory is going to end and you'll get a famous
 <denchmark-code>RuntimeError: CUDA out of memory. Tried to allocate 114.00 MiB (GPU 1; 10.92 GiB total capacity; 9.39 GiB already allocated; 27.38 MiB free; 10.24 GiB reserved in total by PyTorch)
 </denchmark-code>
 
 Where is the loss appended to output? Here
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
         Lines 396 to 427
       in
       b3db197
 
 
 
 
 
 
  def _process_training_step_output_1_0(self, training_step_output, split_batch): 
 
 
 
  result = self.trainer.get_model()._results 
 
 
 
  
 
 
 
  loss = None 
 
 
 
  hiddens = None 
 
 
 
  
 
 
 
  # handle dict return 
 
 
 
  if isinstance(training_step_output, dict): 
 
 
 
  loss = training_step_output.pop(""loss"", None) 
 
 
 
  hiddens = training_step_output.pop(""hiddens"", None) 
 
 
 
  result[""extra""] = training_step_output 
 
 
 
  
 
 
 
  # handle scalar return 
 
 
 
  elif isinstance(training_step_output, torch.Tensor): 
 
 
 
  loss = training_step_output 
 
 
 
  result[""extra""] = {} 
 
 
 
  
 
 
 
  # map to results under the hood 
 
 
 
  result.minimize = loss 
 
 
 
  result.hiddens = hiddens 
 
 
 
  
 
 
 
  # track batch for manual reduction with result 
 
 
 
  result.track_batch_size(len(split_batch)) 
 
 
 
  
 
 
 
  # track metrics without grads for epoch reduction 
 
 
 
  training_step_output_for_epoch_end = copy(result) 
 
 
 
  training_step_output_for_epoch_end.detach() 
 
 
 
  
 
 
 
  # what flows back into the system 
 
 
 
  training_step_output = result 
 
 
 
  
 
 
 
  return training_step_output_for_epoch_end, training_step_output 
 
 
 
 
 
 In the first line we get a pretty result without the loss in it, and in line 414 the loss get appended and we start our memory leak chain of events
 How is it affecting the training? It can lead to error only on the first epoch of training. If you've got enough memory to hold a list of gpu losses during the 1st epoch there won't be any exceptions as subsequent epochs will have the same list of losses, if not you'll get it somewhere in the middle of 1st epoch. And of course the more steps you have in an epoch the more memory this list of gpu losses will require as one loss is stored per step
 Here is the comparison for my task. My gpu could hold 2k steps before memory error
 With 
 <denchmark-link:https://user-images.githubusercontent.com/22998537/98408278-f51b2900-2081-11eb-92ae-ceeb80693753.png></denchmark-link>
 
 Without 
 <denchmark-link:https://user-images.githubusercontent.com/22998537/98408336-10863400-2082-11eb-97d8-9d00f13c70ca.png></denchmark-link>
 
 You can see how there is a rapid growth in the first minute in both as the model is loaded and feeded the 1st batch.
 The difference is in subsequent minutes where in the former case the list of losses eats 7gb of gpu memory and leads to crash, and in the latter nothing happens and training goes on
 Pretty cool how one  could eat 2 times more gpu memory more than actual training process
 	",514cb22bd719e6ca056cacce730c8de875c9dbf6,chaton,2020-11-10 21:13:41+00:00,MODIFY,1,pytorch_lightning\core\step_result.py,pytorch_lightning\core\step_result.py,1.0,"398,399,400,401,402",,1.0,Vozf,2020-11-09T06:02:01Z,"
 		Same problem! However, I use self.log(""log name"", (scalar tensor).item()) to avoid that OOM problem. Maybe you can log the data in the tensor instead of the tensor itself.
 		",2.0,Vozf,2020-11-09T06:36:43Z,"
 		I'm logging just a python1 not a tensor as you can see from the example
 		",3.0,Vozf,2020-11-09T11:19:02Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 , <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
   Any thoughts on this?
 		",4.0,Vozf,2020-11-09T12:14:25Z,"
 		For anyone having the same problem, I monkeypatched like this to avoid setting loss
 <denchmark-code>    from pytorch_lightning.trainer.training_loop import TrainLoop
 
     old_process_training_step_outputs = TrainLoop.process_train_step_outputs
 
     def process_train_step_outputs_delete_loss(*args, **kwargs):
         results = old_process_training_step_outputs(*args, **kwargs)
         for result in results:
             for res in result:
                 res.minimize = None
         return results
 
     TrainLoop.process_train_step_outputs = process_train_step_outputs_delete_loss
 </denchmark-code>
 
 		",MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,1.0,"395,396,397,398",,cache_result,self,368,408,MODIFY,2.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,96,96,MODIFY,3.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"616,617,620","606,607,610,611",run_evaluation,"self,bool,max_batches",548,644,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"437,438",,_process_training_step_output_1_0,"self,training_step_output,split_batch",410,443,5.0,Vozf,2020-11-09T15:39:33Z,"
 		Validation loop has the same issue cuda tensors are stored in a list, but they are detached compared to non-detached train loop so overhead isn't big, but it's still there. This can be fixed by loss.cpu() before returning it in validation_step or not returning anything at all
 		",6.0,Vozf,2020-11-09T17:44:31Z,"
 		Hey <denchmark-link:https://github.com/Vozf>@Vozf</denchmark-link>
  and <denchmark-link:https://github.com/AristoYU>@AristoYU</denchmark-link>
 ,
 I deeply apologise for this bug. Let me work on it in priority !
 Best regards,
 Thomas Chaton.
 		",7.0,Vozf,2020-11-10T10:07:20Z,"
 		I also had this issue
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,on_trainer_init,"self,logger,flush_logs_every_n_steps,log_every_n_steps",96,102,cpu,self,398,402,,,,,,,,,,,,,,,MODIFY,2.0,pytorch_lightning\utilities\memory.py,pytorch_lightning\utilities\memory.py,1.0,"20,29,39,40,41","20,38",recursive_detach,dict,20,41,1.0,"20,29,39,40,41,42,43","20,38",recursive_detach,"dict,bool",20,46,,,,,1.0,"646,647,648,649,650,651,652,653,654,655,656,657",,track_output_for_epoch_end,"self,outputs,output",646,657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"96,104",96,on_trainer_init,"self,logger,int,int,bool",96,104,,,,,,,,1.0,139,,__init__,"self,LightningLoggerBase,True,bool,None,None,float,int,int,int,str,None,bool,str,None,None,int,int,0,int,float,1,int,bool,int,int,1,int,int,None,None,int,0,int,0,int,0,int,0,int,int,str,None,bool,int,None,int,None,None,BaseProfiler,bool,None,bool,bool,bool,bool,False,bool,bool,str,False,bool,None,str,str,None,bool,bool",87,139,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4681,annikabrundyn,2020-11-15T01:41:54Z,2020-11-15T15:41:34Z,self.log on validation_step is broken on pre 1.1 [nightly],"
 <denchmark-link:https://colab.research.google.com/drive/1tSphAIaCdy3tC9Lzhe1GEK_fH_0a6oYj?usp=sharing>https://colab.research.google.com/drive/1tSphAIaCdy3tC9Lzhe1GEK_fH_0a6oYj?usp=sharing</denchmark-link>
 
 	",867eef0e4c34e92887faee7040779e8cde00b20f,chaton,2020-11-15 10:41:33-05:00,MODIFY,1,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,1.0,,"448,449,450",1.0,annikabrundyn,2020-11-15T01:42:39Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,annikabrundyn,2020-11-15T10:01:59Z,"
 		<denchmark-link:https://github.com/annikabrundyn>@annikabrundyn</denchmark-link>
  mind write here a bit more description as it will be useful for others who have the same problem in the future as the linked colab won't be available always... pls copy-paste the model and TB PrintScreen 
 		",3.0,annikabrundyn,2020-11-15T10:28:05Z,"
 		just tested and it appears only on master, the latest 1.0.7rc0 is fine...
 		",4.0,annikabrundyn,2020-11-16T15:52:18Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  thank you for looking at this so quickly! will remember to paste context in addition to colab link next time 
 		",MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,"263,264,265,266,267",,get_evaluate_epoch_results,"self,test_mode",262,283,MODIFY,5.0,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,1.0,"790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_validation_step_log_with_tensorboard,"mock_log_metrics,tmpdir",790,880,update_logger_connector,"self,str",411,464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"799,800,801,802,803",,test_validation_step_log_with_tensorboard.training_step,"self,batch,batch_idx",799,803,1.0,"805,806,807,808,809,810,811,812,813",,test_validation_step_log_with_tensorboard.validation_step,"self,batch,batch_idx",805,813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"845,846,847,848,849,850",,test_validation_step_log_with_tensorboard.get_metrics_at_idx,idx,845,850,1.0,"815,816,817,818,819",,test_validation_step_log_with_tensorboard.test_step,"self,batch,batch_idx",815,819,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4781,junwen-austin,2020-11-20T03:55:48Z,2020-12-05T14:49:46Z,Potential bug in metric when updated with a slice of tensor in DDP,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 when a metric is updated with a slice of tensor as one of the inputs (either pred or target) with multiple GPU with DDP, it throws out an error:
 <denchmark-code>RuntimeError: Tensors must be non-overlapping and dense
 </denchmark-code>
 
 Once the slice of the tensor is clone and detach, then it works.
 <denchmark-h:h2>Please reproduce using [the BoringModel and post here]</denchmark-h>
 
 The issue can be reproduced below:
 <denchmark-link:https://colab.research.google.com/drive/1yuqwb8BHhmDATp2IUNRNjSXOg3kV73Wp?usp=sharing>https://colab.research.google.com/drive/1yuqwb8BHhmDATp2IUNRNjSXOg3kV73Wp?usp=sharing</denchmark-link>
 
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 the metric update should work with a slice of tensor
 <denchmark-h:h3>Environment</denchmark-h>
 
 CUDA:
 - GPU:
 - Tesla P100-PCIE-16GB
 - available:         True
 - version:           10.1
 
 Packages:
 
 numpy:             1.18.5
 pyTorch_debug:     True
 pyTorch_version:   1.7.0+cu101
 pytorch-lightning: 1.0.7
 tqdm:              4.41.1
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.6.9
 version:           #1 SMP Thu Jul 23 08:00:38 PDT 2020
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",1b40a4053d4b2116de02938b747946446443c54a,Nicki Skafte,2020-12-05 15:49:45+01:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"123,124,125",,1.0,junwen-austin,2020-11-20T05:02:45Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  I thought we solved something like this?
 		",2.0,junwen-austin,2020-11-20T07:37:18Z,"
 		I also though we already fixed this.
 <denchmark-link:https://github.com/junwen-austin>@junwen-austin</denchmark-link>
  when I try to run the colab you linked to, I cannot reproduce. Is this only happening in ddp mode?
 		",3.0,junwen-austin,2020-11-20T14:09:53Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
  Could you try the notebook again with the ddp (which I just updated). The issue was initially identified with ddp but you are right the issue does not replicate without ddp. I'll also update the post once you confirm this issue exists in the ddp model even with 1 GPU. Thanks.
 		",4.0,junwen-austin,2020-11-22T10:25:13Z,"
 		So I can confirm that this happens when running in ddp mode, however I am not sure if this is a bug. It had nothing to do with detach bug we had some weeks ago.
 This happens because all_gather requires all tensors to be contiguous (data is stored in an uninterrupted block of memory). When you take a slice of a tensor, you don´t alter the underlying memory location of data, just how you read it (basically changing the stride). If you try this in your code, you can see for yourself
 <denchmark-code>print('output', output.is_contiguous()) # True
 print('output slice', output[:,0].is_contiguous()) # False
 </denchmark-code>
 
 the reason why calling output[:,0].detach().clone() works is that clone takes a contiguous copy of the data (the detach() is not nessesary). The more correct solution would be to call:
 <denchmark-code>self.metric.update(output[:, 0].contiguous(),  
                    output[:, 1].contiguous())
 </denchmark-code>
 
 We could easily implement calling  inside the lightning metric on all tensors, however I at least want <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>
  opinion on this before moving forward.
 		",MODIFY,1.0,pytorch_lightning\utilities\distributed.py,pytorch_lightning\utilities\distributed.py,1.0,"92,93,94",,gather_all_tensors,None,76,103,MODIFY,5.0,tests\metrics\test_ddp.py,tests\metrics\test_ddp.py,1.0,"49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65",,,,,,,,,,,,,,,,,,,,,,,,5.0,junwen-austin,2020-11-22T13:45:10Z,"
 		<denchmark-link:https://github.com/SkafteNicki>@SkafteNicki</denchmark-link>
   thanks for the follow-up and good to know the more correct solution :)
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_test_non_contiguous_tensors,"rank,worldsize",49,65,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"69,70,71",,test_non_contiguous_tensors,,69,71,1.0,"60,61,62",,_test_non_contiguous_tensors.compute,self,60,62,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"53,54,55",,_test_non_contiguous_tensors.__init__,self,53,55,1.0,"57,58",,_test_non_contiguous_tensors.update,"self,x",57,58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4857,YannDubs,2020-11-25T16:44:23Z,2020-12-06T13:01:44Z,Logging not working in `on_train_batch_end` of a callback.,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 logging in on_train_batch_end of a callback doesn't work but for on_train_validation_batch_end it does.
 I.e. below only eval will be logged not train
 class TestCallback(Callback):
   
     def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
         pl_module.log(""train"", 0)
 
     def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):
         pl_module.log(""eval"", 0)
 reproduce:  <denchmark-link:https://colab.research.google.com/drive/1Utnf7uqQTPid68_baS9yMdApZaoMP6Lq?usp=sharing>https://colab.research.google.com/drive/1Utnf7uqQTPid68_baS9yMdApZaoMP6Lq?usp=sharing</denchmark-link>
 
 context: I found that error because logging of training in pl_bolts.callbacks.ssl_online.SSLOnlineEvaluator does not work.
 	",2e838e6dd8803f40da3a1d4111669bd69ae7dd0f,chaton,2020-12-06 13:01:43+00:00,MODIFY,0,docs\source\logging.rst,docs\source\logging.rst,0.0,"9,60,62,63,64,100,101,102,152,153,246","9,60,62,147,148,241",1.0,YannDubs,2020-11-25T16:45:05Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,YannDubs,2020-11-25T18:25:17Z,"
 		can you try with master??
 		",3.0,YannDubs,2020-11-25T19:45:56Z,"
 		works! thanks
 		",4.0,YannDubs,2020-12-03T00:44:22Z,"
 		<denchmark-link:https://github.com/rohitgr7>@rohitgr7</denchmark-link>
  now it doesn't work even on master (see  <denchmark-link:https://colab.research.google.com/drive/1Utnf7uqQTPid68_baS9yMdApZaoMP6Lq?usp=sharing>https://colab.research.google.com/drive/1Utnf7uqQTPid68_baS9yMdApZaoMP6Lq?usp=sharing</denchmark-link>
  )
 		",MODIFY,2.0,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py,1.0,"360,361,362,368,399,400","358,367,369",update_logger_connector,self,348,400,MODIFY,1.0,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,pytorch_lightning\trainer\connectors\logger_connector\logger_connector.py,1.0,"590,595,596,597,598,599","593,595,596,597",MODIFY,3.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"349,350","349,350,351,352",__log_result_step_metrics,"self,step_log_metrics,step_pbar_metrics,batch_idx",348,364,MODIFY,3.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,"858,859",,_reset_result_and_set_hook_fx_name,"self,hook_name",857,865,5.0,YannDubs,2020-12-03T14:54:16Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 , mind check this?
 		",6.0,YannDubs,2020-12-04T08:30:07Z,"
 		Hey <denchmark-link:https://github.com/YannDubs>@YannDubs</denchmark-link>
 ,
 Yes, I deactivated to optimize logging.
 <denchmark-code>******************************
 Logging from a LightningModule
 ******************************
 
 Lightning offers automatic log functionalities for logging scalars, or manual logging for anything else.
 
 Automatic logging
 =================
 Use the :func:`~~pytorch_lightning.core.lightning.LightningModule.log` method to log from :ref:`lightning_module` and :ref:`callbacks`
 Currently, it is supported in the following functions:
     - training_step
     - evaluation_step
     - test_step
     - training_epoch_end
     - validation_epoch_end
     - test_epoch_end
     - on_train_epoch_end
     - on_validation_epoch_end
     - on_validation_end
     - on_test_epoch_end
     - on_test_end
     - on_epoch_end
 </denchmark-code>
 
 However, I have an idea for an Hybrid version which might work out.
 I will update you when finished.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,log_train_step_metrics,"self,batch_output",589,599,,,,,1.0,"873,875,898","871,873,874,897",call_hook,"self,hook_name,args,capture,kwargs",871,899,1.0,"873,875,898","873,874,897",call_hook,"self,hook_name,args,kwargs",873,900,MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"828,829","828,829",run_on_epoch_end_hook,"self,epoch_output",824,829,,,,,,,,MODIFY,0.0,tests\test_deprecated.py,tests\test_deprecated.py,1.0,"109,111","109,111",on_evaluation_end,"self,args,kwargs",107,111,0.0,"1,2,3,4,5,6,7,8,9,10,11,12,13",,,,,,,,,,,,,MODIFY,3.0,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,tests\trainer\logging_tests\test_eval_loop_logging_1_0.py,1.0,"673,674","678,679,680",1.0,"342,343,344",342,cache_result,self,307,346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"332,334","332,334",on_evaluation_epoch_end,"self,args,kwargs",329,334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_log_works_in_test_callback.on_test_batch_end,"self,trainer,pl_module,outputs,batch,batch_idx,dataloader_idx",673,680,1.0,"673,674,722,726","665,678,679,680,681,682,693,731,735,736,737",test_log_works_in_test_callback,tmpdir,602,782,MODIFY,1.0,tests\trainer\logging_tests\test_train_loop_logging_1_0.py,tests\trainer\logging_tests\test_train_loop_logging_1_0.py,1.0,561,"561,574,575,576,577,578,579,580,581,582,595,632,636,637,641,642",test_log_works_in_train_callback,tmpdir,504,674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"487,495,543,547","475,476,513,514,544,546,550,553",test_log_works_in_val_callback,tmpdir,427,598,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4928,Borda,2020-12-01T08:19:47Z,2020-12-01T17:19:45Z,update min dependencies,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 after pip install strategy changes we have several packages which are not feasible to install together
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 see all min CI config <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/runs/1478280229>https://github.com/PyTorchLightning/pytorch-lightning/runs/1478280229</denchmark-link>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 update some libs to be installable, probably go one by one or list dependencies of our dependencies and find a minimal intersection
 	",563f9214fa4add3e984de993c231ac0ff2f4fca6,Jeff Yang,2020-12-01 17:19:44+00:00,MODIFY,0,.github\workflows\ci_test-full.yml,.github\workflows\ci_test-full.yml,0.0,"39,40,60,61,68",39,1.0,Borda,2020-12-01T13:34:45Z,"
 		Let me take this
 		",,,,,,,,,,,,,MODIFY,0.0,requirements.txt,requirements.txt,0.0,3,3,,,,,MODIFY,0.0,requirements\extra.txt,requirements\extra.txt,0.0,5,5,MODIFY,0.0,requirements\test.txt,requirements\test.txt,0.0,"4,5,15","4,5,15",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4953,rakhimovv,2020-12-02T20:09:05Z,2020-12-07T19:31:56Z,manual_optimization does not work with ddp,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Can't run ddp with manual optimization. Fails on the second batch with a error:
 RuntimeError: Expected to mark a variable ready only once. This error is caused by one of the following reasons: 1) Use of a module parameter outside the forwardfunction. Please make sure model parameters are not shared across multiple concurrent forward-backward passes2) Reused parameters in multiple reentrant backward passes. For example, if you use multiplecheckpoint functions to wrap the same part of your model, it would result in the same set of parameters been used by different reentrant backward passes multiple times, and hence marking a variable ready multiple times. DDP does not support such use cases yet.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Change optimization to manual in basic gan bolt.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Do not fail when n_gpus > 1
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 CUDA:
 
 GPU:
 
 Tesla V100-SXM2-16GB
 Tesla V100-SXM2-16GB
 Tesla V100-SXM2-16GB
 Tesla V100-SXM2-16GB
 
 
 available:         True
 version:           10.2
 
 
 Packages:
 
 numpy:             1.19.4
 pyTorch_debug:     True
 pyTorch_version:   1.7.0
 pytorch-lightning: 1.0.8
 tqdm:              4.54.0
 
 
 System:
 
 OS:                Linux
 architecture:
 
 64bit
 
 
 
 processor:         x86_64
 python:            3.7.9
 version:           #1 SMP Tue Sep 10 10:50:19 EDT 2019
 
 
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 To have manual optimization working with GANs in multi-gpu regime is very useful applicaiton.
 	",239347435029c0a02b305201ebbfa39d62746ca8,chaton,2020-12-07 19:31:54+00:00,MODIFY,1,benchmarks\test_sharded_parity.py,benchmarks\test_sharded_parity.py,1.0,"184,192,193,194","185,193,194",1.0,rakhimovv,2020-12-02T20:09:55Z,"
 		Hi! thanks for your contribution!, great first issue!
 		",2.0,rakhimovv,2020-12-02T21:36:43Z,"
 		possibly related <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/4917>#4917</denchmark-link>
 
 		",3.0,rakhimovv,2020-12-03T10:17:15Z,"
 		hey <denchmark-link:https://github.com/rakhimovv>@rakhimovv</denchmark-link>
 ! We're seeing incorrect behaviour with DDP/manual optimization because backward DDP hooks are not being called correctly within the training step, thus meaning gradients are not being reduced per process.
 Our current short term solution is to assert to prevent users from using DDP/manual optimization by enforcing an assert. It will probably take some time for us to come up with a elegant fix for this!
 		",4.0,rakhimovv,2020-12-03T12:24:44Z,"
 		Heu <denchmark-link:https://github.com/rakhimovv>@rakhimovv</denchmark-link>
 ,
 I noticed this bug too with pytorch 1.7.
 Can you try 1.6 ?
 Best,
 T.C
 		",MODIFY,2.0,pytorch_lightning\accelerators\accelerator.py,pytorch_lightning\accelerators\accelerator.py,1.0,"226,227,228,229,230,231,232,233",,block_ddp_plugin_sync_behaviour,self,226,233,MODIFY,3.0,pytorch_lightning\overrides\data_parallel.py,pytorch_lightning\overrides\data_parallel.py,1.0,"218,219",,MODIFY,3.0,pytorch_lightning\plugins\ddp_plugin.py,pytorch_lightning\plugins\ddp_plugin.py,1.0,"146,147",,on_before_manual_backward,"self,LightningDistributedDataParallel,Any",146,147,MODIFY,2.0,pytorch_lightning\plugins\sharded_plugin.py,pytorch_lightning\plugins\sharded_plugin.py,1.0,"101,102",,on_after_manual_backward,self,101,102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,reducer_reset_hooks,self,218,219,training_step,"self,batch,batch_idx,optimizer_idx",178,196,1.0,"98,99",,on_before_manual_backward,"self,Any",98,99,,,,,,,,MODIFY,2.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"682,683,685,686,687,688,689,690","682,684,746,747",run_training_batch,"self,batch,batch_idx,dataloader_idx",640,748,1.0,"752,753,754,755,756,757,758,759,760,761,762,763,764,765,767",,block_ddp_sync_behaviour,self,751,767,MODIFY,0.0,tests\special_tests.sh,tests\special_tests.sh,1.0,"149,150",,on_after_manual_backward,"self,LightningDistributedDataParallel",149,150,0.0,"15,17","14,15",,,,,,,,,,,,MODIFY,13.0,tests\trainer\optimization\test_manual_optimization.py,tests\trainer\optimization\test_manual_optimization.py,1.0,"912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013",,1.0,"90,91,92,93,94,95,107,108,109,110",,backward,"self,closure_loss,optimizer,opt_idx,args,kwargs",89,111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"205,206","208,209,210",reducer_prepare_for_backwards,"self,output",205,216,1.0,"164,198,199,200,201,202,203",,forward,"self,inputs,kwargs",162,203,1.0,"138,139,140,141,142,143,144",,block_backward_sync,"self,LightningDistributedDataParallel",138,144,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_step_with_optimizer_closure_with_different_frequencies_ddp,"mock_sgd_step,mock_adam_step,tmpdir",912,1013,1.0,"928,929,930",,test_step_with_optimizer_closure_with_different_frequencies_ddp.manual_sync_grad,self,928,930,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977",,test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step,"self,batch,batch_idx,optimizer_idx",932,977,1.0,"964,965,966,967",,test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.dis_closure,,964,967,1.0,"920,921,922",,test_step_with_optimizer_closure_with_different_frequencies_ddp.loss_ones,"self,batch,prediction",920,922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"924,925,926",,test_step_with_optimizer_closure_with_different_frequencies_ddp.loss_zeros,"self,batch,prediction",924,926,1.0,"943,944,945,946,947,948,949,950",,test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.compute_loss,,943,950,1.0,"959,960,961,962",,test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.gen_closure,,959,962,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"983,984,985,986",,test_step_with_optimizer_closure_with_different_frequencies_ddp.configure_optimizers,self,983,986,1.0,866,865,test_step_with_optimizer_closure_with_different_frequencies,"mock_sgd_step,mock_adam_step,tmpdir",832,905,1.0,"979,980,981",,test_step_with_optimizer_closure_with_different_frequencies_ddp.training_epoch_end,"self,outputs",979,981,1.0,"952,953,954,955,956,957",,test_step_with_optimizer_closure_with_different_frequencies_ddp.test_step_with_optimizer_closure_with_different_frequencies_ddp.training_step.make_manual_backward,"loss,opt,retain_graph",952,957,1.0,866,865,test_step_with_optimizer_closure_with_different_frequencies.training_step,"self,batch,batch_idx,optimizer_idx",839,873,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4974,yikuanli,2020-12-04T15:02:39Z,2020-12-11T13:51:46Z,AttributeError: 'LightningOptimizer' object has no attribute 'state',"
 I am using pytorch lightning with Adam optmiser to train a BYOL model, the model and pipeline works fine while training, when I stop and resume from checkpoint, it raise this error. I didn't do anything just resume from previous checkpoint, have no idea why there is an error like this, I upgrade my bolts and pytorch lightning to the mast (I think its the latest version)
 <denchmark-link:https://user-images.githubusercontent.com/40010984/101178978-8b227f00-3641-11eb-8d72-a3b3ca8c1173.png></denchmark-link>
 
 my opmizer is like this, and Adam is the original Adam provided by pytorch
 <denchmark-link:https://user-images.githubusercontent.com/40010984/101179109-be650e00-3641-11eb-951c-0717e217b532.png></denchmark-link>
 
 	",7755572b4f37b811b83f6a933329b01af4735e66,chaton,2020-12-11 14:51:45+01:00,MODIFY,2,pytorch_lightning\core\optimizer.py,pytorch_lightning\core\optimizer.py,1.0,,76,1.0,yikuanli,2020-12-04T16:04:57Z,"
 		Thanks for the report, it would be really really useful (and get this fixed faster) if you could reproduce this via the bug report model:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py>https://github.com/PyTorchLightning/pytorch-lightning/blob/master/pl_examples/bug_report_model.py</denchmark-link>
 
 cc <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 
 		",2.0,yikuanli,2020-12-04T16:14:19Z,"
 		Hey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>
 ,
 Thanks a lot for testing the latest version of Lightning. I am going to work on this asap.
 If you could reproduce the bug with BoringModel, it will help a lot.
 While waiting for fix, you can use Trainer(enable_pl_optimizer=False).
 Best regards,
 T.C
 		",3.0,yikuanli,2020-12-04T16:27:52Z,"
 		Thanks for the comments, I used BoringModel to test, and seems it doesn't have this problem, I am wondering if the wrapper and LWCA LR is the problem.
 		",4.0,yikuanli,2020-12-04T16:38:46Z,"
 		<denchmark-code>import os
 import torch
 from torch.utils.data import Dataset
 from pytorch_lightning import Trainer, LightningModule
 from pytorch_lightning.callbacks import ModelCheckpoint
 import pytorch_lightning as pl
 from pl_bolts.optimizers.lars_scheduling import LARSWrapper
 from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR
 
 class CheckpointEveryNSteps(pl.Callback):
     """"""
     Save a checkpoint every N steps, instead of Lightning's default that checkpoints
     based on validation loss.
     """"""
 
     def __init__(
         self,
         save_step_frequency,
         prefix=""latest-Checkpoint"",
         use_modelcheckpoint_filename=False,
     ):
         """"""
         Args:
             save_step_frequency: how often to save in steps
             prefix: add a prefix to the name, only used if
                 use_modelcheckpoint_filename=False
             use_modelcheckpoint_filename: just use the ModelCheckpoint callback's
                 default filename, don't use ours.
         """"""
         self.save_step_frequency = save_step_frequency
         self.prefix = prefix
         self.use_modelcheckpoint_filename = use_modelcheckpoint_filename
 
     def on_batch_end(self, trainer: pl.Trainer, _):
         """""" Check if we should save a checkpoint after every train batch """"""
         global_step = trainer.global_step
         if global_step % self.save_step_frequency == 0:
             if self.use_modelcheckpoint_filename:
                 filename = trainer.checkpoint_callback.filename
             else:
                 filename = ""{}.ckpt"".format(self.prefix)
             ckpt_path = os.path.join(trainer.checkpoint_callback.dirpath, filename)
             trainer.save_checkpoint(ckpt_path)
 
 
 class RandomDataset(Dataset):
     def __init__(self, size, length):
         self.len = length
         self.data = torch.randn(length, size)
 
     def __getitem__(self, index):
         return self.data[index]
 
     def __len__(self):
         return self.len
 
 
 class BoringModel(LightningModule):
 
     def __init__(self):
         """"""
         Testing PL Module
         Use as follows:
         - subclass
         - modify the behavior for what you want
         class TestModel(BaseTestModel):
             def training_step(...):
                 # do your own thing
         or:
         model = BaseTestModel()
         model.training_epoch_end = None
         """"""
         super().__init__()
         self.layer = torch.nn.Linear(32, 2)
 
     def forward(self, x):
         return self.layer(x)
 
     def loss(self, batch, prediction):
         return torch.nn.functional.mse_loss(prediction, torch.ones_like(prediction))
 
     def step(self, x):
         x = self.layer(x)
         out = torch.nn.functional.mse_loss(x, torch.ones_like(x))
         return out
 
     def training_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""loss"": loss}
 
     def training_step_end(self, training_step_outputs):
         return training_step_outputs
 
     def training_epoch_end(self, outputs) -> None:
         torch.stack([x[""loss""] for x in outputs]).mean()
 
     def validation_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""x"": loss}
 
     def validation_epoch_end(self, outputs) -> None:
         torch.stack([x['x'] for x in outputs]).mean()
 
     def test_step(self, batch, batch_idx):
         output = self.layer(batch)
         loss = self.loss(batch, output)
         return {""y"": loss}
 
     def test_epoch_end(self, outputs) -> None:
         torch.stack([x[""y""] for x in outputs]).mean()
 
     def configure_optimizers(self):
         optimizer = torch.optim.Adam(self.parameters(), lr=0.1)
 
         optimizer = LARSWrapper(optimizer)
         scheduler = LinearWarmupCosineAnnealingLR(
             optimizer,
             warmup_epochs= 1, 
             max_epochs= 20
         )
         return [optimizer], [scheduler]
 
 
 def run_test():
 
     class TestModel(BoringModel):
 
         def on_train_epoch_start(self) -> None:
             print('override any method to prove your bug')
 
 
     train_data = torch.utils.data.DataLoader(RandomDataset(32, 64), batch_size=1)
     val_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)
     test_data = torch.utils.data.DataLoader(RandomDataset(32, 64),batch_size=1)
 
 
     checkpoint_callback = ModelCheckpoint(monitor='loss', mode= 'min', filepath='./checkpoint')
     model = TestModel()
     trainer = Trainer(
         default_root_dir=os.getcwd(),
         resume_from_checkpoint='./latest-Checkpoint.ckpt',
         max_epochs=10,
         weights_summary=None,
         accelerator= 'ddp',
         log_every_n_steps=1,
         gpus=1,
         checkpoint_callback= checkpoint_callback,
         callbacks=[CheckpointEveryNSteps(1)]
     )
     trainer.fit(model, train_data, val_data)
 
 
 if __name__ == '__main__':
     run_test()
 </denchmark-code>
 
 		",MODIFY,0.0,pytorch_lightning\utilities\__init__.py,pytorch_lightning\utilities\__init__.py,0.0,57,,,,,,MODIFY,0.0,requirements\extra.txt,requirements\extra.txt,0.0,10,10,MODIFY,2.0,tests\core\test_lightning_module.py,tests\core\test_lightning_module.py,1.0,"58,59,60,61",,test_automatic_optimization_num_calls.training_step,"self,batch,batch_idx,optimizer_idx",58,61,MODIFY,13.0,tests\core\test_lightning_optimizer.py,tests\core\test_lightning_optimizer.py,1.0,"154,160","152,158",test_lightning_optimizer_manual_optimization_and_accumulated_gradients.training_step,"self,batch,batch_idx,optimizer_idx",146,160,5.0,yikuanli,2020-12-04T16:39:22Z,"
 		I reproduce the error, it is because of the ddp, if I train with ddp, this error occurs
 		",6.0,yikuanli,2020-12-04T16:57:47Z,"
 		Hey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>
 ,
 Let me prioritise this :)
 Best,
 T.C
 		",7.0,yikuanli,2020-12-04T20:08:57Z,"
 		Hey <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>
 ,
 Would you mind trying this branch: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981>#4981</denchmark-link>
 .
 I wasn't able to reproduce your bug, but another one. When resolved, it seems to train fine.
 I would be interested to see what you get your bug. Please, have a look at my test if I missed something.
 Best regards,
 T.C
 		",8.0,yikuanli,2020-12-07T10:13:46Z,"
 		Hi, <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
 , I am sorry, need to ask how to pull this branch you mentioned in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4981>#4981</denchmark-link>
 . I am not very familiar with those management in GitHub,
 BTW, the way I got that problem is firstly run the boring model train with for example 5 epoch with ddp, and not resume from check point.
 after ran it, I got a checkpoint and resume from that checkpoint, but change to 10 epoch for example, because check point already save the first 5, and I need to increase the number to let it load the model and kept training.
 thanks for helping out, plz let me know how to test that branch ,and I will do it.
 		",9.0,yikuanli,2020-12-07T11:54:23Z,"
 		<denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  <denchmark-link:https://github.com/yikuanli>@yikuanli</denchmark-link>
   LARSWrapper should not be passed to a Scheduler, since it is not an optimizer object. We have updated out SimCLR and SwAV code accordingly. I will update the BYOL with this soon.
 To see how LR schedule is set for LARS, refer to:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330>https://github.com/PyTorchLightning/pytorch-lightning-bolts/blob/master/pl_bolts/models/self_supervised/swav/swav_module.py#L330</denchmark-link>
 
 <denchmark-link:https://github.com/tchaton>@tchaton</denchmark-link>
  LARSWrapper's step method takes in a closure and the class itself sets the param_group and state property. The reason we would prefer to keep LARSWrapper as a wrapper class for an Optimizer, instead of an Optimizer itself, is because any Optimizer passed to this wrapper can get the layer-wise LR scaling property of LARS.
 		",10.0,yikuanli,2020-12-09T11:39:50Z,"
 		Hey <denchmark-link:https://github.com/ananyahjha93>@ananyahjha93</denchmark-link>
 ,
 Not sure to understand why Lars can't be an optimizer. I found several implementations making this cleaner: <denchmark-link:https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11>https://github.com/kakaobrain/torchlars/blob/3b3d7e9c7bd35a31b2c2fa6f213beb0bf6881892/torchlars/lars.py#L11</denchmark-link>
 
 I will look into this deeper.
 Best,
 T.C
 		",,,,,,,,,,,,,,,,,,,,,,,,,_on_trainer_init,"self,trainer",74,80,1.0,"200,201","198,199,200",test_state,tmpdir,192,207,1.0,"227,228",,test_lightning_optimizer_with_wrong_optimizer_interface.state,self,227,228,MODIFY,4.0,tests\trainer\optimization\test_manual_optimization.py,tests\trainer\optimization\test_manual_optimization.py,1.0,"905,936,938","905,936,938,939",test_step_with_optimizer_closure_with_different_frequencies,"mock_sgd_step,mock_adam_step,tmpdir",864,939,1.0,905,905,test_step_with_optimizer_closure_with_different_frequencies.training_step,"self,batch,batch_idx,optimizer_idx",871,905,,,,,1.0,"58,59,60,61,62",,test_automatic_optimization_num_calls,"enable_pl_optimizer,tmpdir",49,105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"114,115,116,117,118",114,__optimizer_step,"self,args,None,str,kwargs",100,138,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,828,828,test_step_with_optimizer_closure_and_extra_arguments.training_step,"self,batch,batch_idx",813,828,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"238,239,240",,test_lightning_optimizer_with_wrong_optimizer_interface.step,self,238,240,1.0,"101,107","99,105",test_lightning_optimizer_manual_optimization.training_step,"self,batch,batch_idx,optimizer_idx",93,107,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"244,245,246,247",,test_lightning_optimizer_with_wrong_optimizer_interface.configure_optimizers,self,244,247,1.0,"231,232",,test_lightning_optimizer_with_wrong_optimizer_interface.param_groups,self,231,232,1.0,"101,107","99,105",test_lightning_optimizer_manual_optimization,"mock_sgd_step,mock_adam_step,tmpdir",87,135,1.0,"210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256",,test_lightning_optimizer_with_wrong_optimizer_interface,tmpdir,210,256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"828,858","828,858",test_step_with_optimizer_closure_and_extra_arguments,"step_mock,tmpdir",806,859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"235,236",,test_lightning_optimizer_with_wrong_optimizer_interface.param_groups,"self,value",235,236,1.0,"212,213,214,215,216,217,218,219,220",,test_lightning_optimizer_with_wrong_optimizer_interface.__init__,"self,optimizer",212,220,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"154,160","152,158",test_lightning_optimizer_manual_optimization_and_accumulated_gradients,"mock_sgd_step,mock_adam_step,tmpdir",140,189,1.0,"223,224",,test_lightning_optimizer_with_wrong_optimizer_interface.__class__,self,223,224,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4978,edenlightning,2020-12-04T16:21:42Z,2020-12-09T09:59:45Z,Fix pipy badges and images not rendering,"
 it seems that it did not work for <denchmark-link:https://dustingram.com/articles/2018/03/16/markdown-descriptions-on-pypi/>PyPI page</denchmark-link>
 
 see our RC - <denchmark-link:https://pypi.org/project/pytorch-lightning/1.1.0rc1>https://pypi.org/project/pytorch-lightning/1.1.0rc1</denchmark-link>
 
 when I open the PKG-INFO I still see the original paths...
 Downloaded implemented in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4250>#4250</denchmark-link>
 
 	",e2c404bad2eaf90d77d2faf3d2802f783f8dc4f7,Jirka Borovec,2020-12-09 10:59:44+01:00,MODIFY,0,.github\workflows\release-pypi.yml,.github\workflows\release-pypi.yml,0.0,"31,32,33,34,35,36,37,38,39,40",,1.0,edenlightning,2020-12-04T16:31:48Z,"
 		ref: <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/4250#issuecomment-737552478>#4250 (comment)</denchmark-link>
 
 		",2.0,edenlightning,2020-12-07T09:41:10Z,"
 		If you still  need some help, I can look into it.
 		",3.0,edenlightning,2020-12-07T09:55:36Z,"
 		
 If you still need some help, I can look into it.
 
 it would be nice, I am about to take it today too, do you have any similar experience?
 		",4.0,edenlightning,2020-12-07T10:00:33Z,"
 		No similar experience but happy to dive in and learn. I might take some time getting up to speed (just like the checkpoint issue) so if it is very urgent, feel free to skip over me :]
 		",MODIFY,0.0,MANIFEST.in,MANIFEST.in,0.0,,45,,,,,MODIFY,0.0,README.md,README.md,0.0,"47,94,95,96,97,98,99","47,94,95,96,97,98,99",MODIFY,3.0,pytorch_lightning\setup_tools.py,pytorch_lightning\setup_tools.py,1.0,"58,59,60,61,62",62,_parse_for_badge,"str,str,str,Iterable",58,62,,,,,,,,,,,,5.0,edenlightning,2020-12-07T12:30:22Z,"
 		seems we would need to upload the images somewhere on a cloud... <denchmark-link:https://stackoverflow.com/a/46875147/4521646>https://stackoverflow.com/a/46875147/4521646</denchmark-link>
 
 		",6.0,edenlightning,2020-12-07T12:35:32Z,"
 		
 seems we would need to upload the images somewhere on a cloud... https://stackoverflow.com/a/46875147/4521646
 
 Or as an asset of this repo and just use the raw github link
 		",7.0,edenlightning,2020-12-07T12:37:36Z,"
 		yes, but as we download them during package build time there are not in any git version (just on the fly) so we would need to upload them to some free public domain and refer them there...
 so, yes, we can put a hard link to the images but not budges... :/ <denchmark-link:https://github.com/pypa/warehouse/issues/5246#issuecomment-739895844>pypa/warehouse#5246 (comment)</denchmark-link>
 
 well, let's try to upload it as assets and refer from the given path :]
 <denchmark-link:https://github.com/actions/upload-release-asset>https://github.com/actions/upload-release-asset</denchmark-link>
 
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"161,162,163,165,167,168,169,170,171,172,173,174,175,176,177,178,179,180","155,156,159,160,161,163,164,165,166,167",_load_long_description,str,155,181,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"62,63,67,68,70,87,88,89,90,91","62,66,67,69,86,89",_parse_for_badge,"str,str,Iterable",62,91,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
498,ChenghaoMou,2019-11-12T18:37:49Z,2019-11-13T11:03:39Z,Escaping % in add_default_args,"
 Describe the bug
 In utilities/arg_parse.py, a percentage symbol is not escaped and would cause an error when printing help information.
 parser.add_argument('--overfit', default=-1, type=float,
                         help='% of dataset to use with this option. float, or -1 for none')
 To Reproduce
 Steps to reproduce the behavior:
 <denchmark-code>import os
 import random
 import sys
 from pytorch_lightning.utilities.arg_parse import add_default_args
 from test_tube import HyperOptArgumentParser, Experiment
 
 if __name__ == ""__main__"":
     root_dir = os.path.split(os.path.dirname(sys.modules['__main__'].__file__))[0]
     parent_parser = HyperOptArgumentParser(strategy='random_search', add_help=True)
     add_default_args(parent_parser, root_dir)
     hyperparams = parent_parser.parse_args()
 </denchmark-code>
 
 Execute the file with --help
 <denchmark-code>python temp.py --help
 </denchmark-code>
 
 Throws an error:
 <denchmark-code>WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.
 Traceback (most recent call last):
   File ""/Users/chenghaomou/Code/ai2/temp.py"", line 11, in <module>
     hyperparams = parent_parser.parse_args()
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/site-packages/test_tube/argparse_hopt.py"", line 238, in parse_args
     results = self.__parse_args(args, namespace)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/site-packages/test_tube/argparse_hopt.py"", line 157, in __parse_args
     args, argv = self.parse_known_args(args, namespace)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 1782, in parse_known_args
     namespace, args = self._parse_known_args(args, namespace)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 1988, in _parse_known_args
     start_index = consume_optional(start_index)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 1928, in consume_optional
     take_action(action, args, option_string)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 1856, in take_action
     action(self, namespace, argument_values, option_string)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 1038, in __call__
     parser.print_help()
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 2475, in print_help
     self._print_message(self.format_help(), file)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 2459, in format_help
     return formatter.format_help()
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 284, in format_help
     help = self._root_section.format_help()
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 215, in format_help
     item_help = join([func(*args) for func, args in self.items])
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 215, in <listcomp>
     item_help = join([func(*args) for func, args in self.items])
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 215, in format_help
     item_help = join([func(*args) for func, args in self.items])
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 215, in <listcomp>
     item_help = join([func(*args) for func, args in self.items])
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 525, in _format_action
     help_text = self._expand_help(action)
   File ""/Users/chenghaomou/Anaconda/envs/Elisa/lib/python3.7/argparse.py"", line 615, in _expand_help
     return self._get_help_string(action) % params
 TypeError: %o format: an integer is required, not dict
 </denchmark-code>
 
 Expected behavior
 Escape the percentage sign and help can be printed.
 Desktop (please complete the following information):
 
 OS: macOS 10.15
 Browser Chrome
 Version 78.0.3904.87
 
 Additional context
 Add any other context about the problem here.
 	",89f7a82157297f32ca12283c0badbc5b50bb5224,Chenghao MOU,2019-11-13 06:03:38-05:00,MODIFY,1,pytorch_lightning\utilities\arg_parse.py,pytorch_lightning\utilities\arg_parse.py,1.0,84,84,1.0,ChenghaoMou,2019-11-13T07:00:39Z,"
 		the sample code has several bugs it really fails...
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,add_default_args,"parser,root_dir,rand_seed,possible_model_names",9,98,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
529,alumae,2019-11-20T19:38:00Z,2019-11-21T18:27:40Z,Training with DDP could fail at startup due to FileExistsError,"
 In multi-GPU mode with DDP, starting the training can fail with the following error:
 <denchmark-code>Traceback (most recent call last):
   File ""train.py"", line 109, in <module>
     main(hyperparams)
   File ""train.py"", line 62, in main
     trainer.fit(model)
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 343, in fit
     mp.spawn(self.ddp_train, nprocs=self.num_gpus, args=(model,))
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 171, in spawn
     while not spawn_context.join():
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 118, in join
     raise Exception(msg)
 Exception: 
 
 -- Process 1 terminated with the following error:
 Traceback (most recent call last):
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
     fn(i, *args)
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/ddp_mixin.py"", line 181, in ddp_train
     self.run_pretrain_routine(model)
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py"", line 415, in run_pretrain_routine
     self.configure_checkpoint_callback()
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_config_mixin.py"", line 28, in configure_checkpoint_callback
     filepath=ckpt_path
   File ""/home/tanel/miniconda3/lib/python3.7/site-packages/pytorch_lightning/callbacks/pt_callbacks.py"", line 202, in __init__
     os.makedirs(filepath)
   File ""/home/tanel/miniconda3/lib/python3.7/os.py"", line 221, in makedirs
     mkdir(name, mode)
 FileExistsError: [Errno 17] File exists: '/home/tanel/devel/torch-xvectors/lightning_logs/version_151/checkpoints'
 
 </denchmark-code>
 
 It happens quite rarely.
 Desktop (please complete the following information):
 
 OS: Linux
 Version 0.5.3.2 (git as of 2019-11-20)
 
 Fix is easy, I'll create a PR.
 	",539d7bcb4476883771f0492b3a3774048de5163d,Tanel Alumäe,2019-11-21 13:27:39-05:00,MODIFY,0,pytorch_lightning\callbacks\pt_callbacks.py,pytorch_lightning\callbacks\pt_callbacks.py,0.0,201,"201,202",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
537,VSJMilewski,2019-11-21T15:44:20Z,2019-12-09T12:42:07Z,Summary not working for model on GPU with multiple inputs,"
 Describe the bug
 When you want a summary for a model that requires multiple input parameters for forward, then this doesn't work. You can set self.example_input_array to be a tuple and there is some code for passing this to the forward method. However, if the model is on cuda, it tries to pass to move this input directly to cuda without a check whether it is a tuple or list.
 the line with the error is here:
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/7324dd902b8d071f4889ab1274a4d4dc09de9a78/pytorch_lightning/root_module/memory.py#L53>pytorch-lightning/blob/master/pytorch_lightning/root_module/memory.py#L53</denchmark-link>
 
 example of how it should be checked:
 <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/7324dd902b8d071f4889ab1274a4d4dc09de9a78/pytorch_lightning/root_module/memory.py#L61>pytorch-lightning/blob/master/pytorch_lightning/root_module/memory.py#L61</denchmark-link>
 
 To Reproduce
 Steps to reproduce the behavior:
 
 create a model that requires multiple inputs in the forward method.
 set self.example_input_array to be a tuple
 run the model on GPU
 
 Expected behavior
 a list with all layers and the input and output shapes of these layers.
 
 <denchmark-link:https://user-images.githubusercontent.com/6348139/69352469-4bc70180-0c7d-11ea-88c6-8056ab531c80.png></denchmark-link>
 
 Desktop (please complete the following information):
 
 OS: Linux Mint 19.2
 Browser chrome
 Version 8.0.3904.97 (Official Build) (64-bit)
 
 	",d562172b4cd363b86f9d670120932cd333b03cf5,VSJMilewski,2019-12-09 04:42:07-08:00,MODIFY,1,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,"53,54,55,56,57,58,59,62,63,64,65,66,67,72,77,89","53,56,61,66,78",1.0,VSJMilewski,2019-11-25T11:26:29Z,"
 		@victormilewski1994 good point. I guess we didn't specifically design the summarizer to take in multiple inputs. Would you be interested in submitting a PR?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get_variable_sizes,self,45,100,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
566,iamsimha,2019-12-02T11:24:36Z,2019-12-04T12:04:59Z,Using print_nan_grads in the Trainer results in an error,"
 Describe the bug
 When using
 <denchmark-code>print_nan_grads=True
 </denchmark-code>
 
 in the Trainer, I am getting the error below.
 trainer.fit(lstm_model)
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 364, in fit
 self.run_pretrain_routine(model)
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py"", line 471, in run_pretrain_routine
 self.train()
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py"", line 60, in train
 self.run_training_epoch()
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py"", line 99, in run_training_epoch
 output = self.run_training_batch(batch, batch_nb)
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py"", line 219, in run_training_batch
 self.print_nan_gradients()
 File ""/Users/anaconda3/envs/snorkel/lib/python3.6/site-packages/pytorch_lightning/trainer/training_tricks_mixin.py"", line 16, in print_nan_gradients
 if torch.isnan(param.grad.float()).any():
 AttributeError: 'NoneType' object has no attribute 'float'
 To Reproduce
 Steps to reproduce the behavior:
 If some param object, does not have .grad, then that object should not be checked for nans
 	",d4571d1d6f524b0b9284e84ea8f95bb8eb656c86,Ir1dXD,2019-12-04 07:04:58-05:00,MODIFY,1,pytorch_lightning\trainer\training_tricks_mixin.py,pytorch_lightning\trainer\training_tricks_mixin.py,1.0,16,16,1.0,iamsimha,2019-12-03T06:40:52Z,"
 		sounds like an easy fix: iterate only over params where grad is not None. Would that solve the issue?
 		",2.0,iamsimha,2019-12-03T12:57:28Z,"
 		exactly. Anyone want to submit the PR?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,print_nan_gradients,self,13,17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
604,Borda,2019-12-07T22:15:17Z,2020-02-11T17:44:13Z,failing Docs build,"
 <denchmark-h:h3>Description</denchmark-h>
 
 Not sure what is happening but it seems that the documentation fails... <denchmark-link:https://pytorch-lightning.readthedocs.io/en/latest></denchmark-link>
 
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  could you pass details about documentation build?
 	",ea59a99426c050cf301783f411c41a732af8c752,Jirka Borovec,2020-01-20 14:50:31-05:00,MODIFY,0,.github\BECOMING_A_CORE_CONTRIBUTOR.md,.github\BECOMING_A_CORE_CONTRIBUTOR.md,0.0,9,9,1.0,Borda,2019-12-08T05:06:08Z,"
 		we no longer have the old docs... so the deployed version is out of sync and the build fails.
 The solution is to get the new docs in place
 		",2.0,Borda,2019-12-08T09:26:36Z,"
 		What will be the new mapping of the top link/shortcuts in new docs?
 <denchmark-code>  'github': 'https://github.com/williamFalcon/pytorch-lightning',
   'github_issues': 'https://github.com/williamFalcon/pytorch-lightning/issues',
   'contributing': 'https://github.com/williamFalcon/pytorch-lightning/blob/master/CONTRIBUTING.md',
   'docs': 'https://williamfalcon.github.io/pytorch-lightning/documentation.html',
   'twitter': 'https://twitter.com/PyTorchLightnin',
   'discuss': 'https://join.slack.com/t/pytorch-lightning/',
   'tutorials': 'https://williamfalcon.github.io/pytorch-lightning/examples.html',
   'previous_pytorch_versions': 'https://github.com/williamFalcon/pytorch-lightning/releases',
   'home': 'https://williamfalcon.github.io/pytorch-lightning/',
   'get_started': 'https://williamfalcon.github.io/pytorch-lightning/new-project.html',
   'features': 'https://williamfalcon.github.io/pytorch-lightning/',
   'blog': 'https://medium.com/@_willfalcon',
   'resources': 'https://williamfalcon.github.io/pytorch-lightning/',
   'support': 'https://williamfalcon.github.io/pytorch-lightning/',
 </denchmark-code>
 
 		",3.0,Borda,2020-01-15T14:10:08Z,"
 		I assume you know that the docs cannot be fully used from the web site
 		",4.0,Borda,2020-01-15T15:06:44Z,"
 		<denchmark-link:https://github.com/sneiman>@sneiman</denchmark-link>
  what do you mean?
 		",MODIFY,0.0,.github\ISSUE_TEMPLATE\bug_report.md,.github\ISSUE_TEMPLATE\bug_report.md,0.0,"11,12","11,12",,,,,MODIFY,0.0,.github\PULL_REQUEST_TEMPLATE.md,.github\PULL_REQUEST_TEMPLATE.md,0.0,4,4,MODIFY,0.0,.gitignore,.gitignore,0.0,"8,9,10,11,12,13,14,15,16,68,69,70,88,110,128,129","3,5,6,7,11,12,13,14,16,17,24,26,35,47,90",,,,,MODIFY,0.0,.readthedocs.yml,.readthedocs.yml,0.0,24,23,,,,,5.0,Borda,2020-01-15T19:36:43Z,"
 		I cannot get to the documents online. If I go to:
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning>https://github.com/PyTorchLightning/pytorch-lightning</denchmark-link>
 
 and press the read the docs button, it attempts to go to
 <denchmark-link:https://williamfalcon.github.io/pytorch-lightning/>https://williamfalcon.github.io/pytorch-lightning/</denchmark-link>
 
 and displays a 404 error. I can get to some of the examples and things through other paths, but nothing seems to get me to main docs.
 		",6.0,Borda,2020-01-15T19:44:38Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  ^^
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,MANIFEST.in,MANIFEST.in,0.0,,4,,,,,,,,,,,,MODIFY,0.0,README.md,README.md,,,,,,,,0.0,"35,87,92,168,280,283,284,285,286,287,288,289,290,291,292,293,312,316,317,318,323,373,375,382","35,87,92,168,280,282,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,369,373,378,428,430,437",,,,,,,,,,,,ADD,0.0,None,docs\source\_static\images\lightning_icon.svg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,docs\source\_static\images\lightning_logo-large.svg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,docs\source\_static\images\lightning_logo-name.svg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,docs\source\_static\images\lightning_logo.png,docs\source\_static\images\lightning_logo.png,0.0,,,ADD,0.0,None,docs\source\_static\images\lightning_logo.svg,,,,,,,,,,,,,,,DELETE,0.0,docs\source\_static\images\lightning_logo_medium.png,docs\source\_static\images\lightning_logo_medium.png,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,DELETE,0.0,docs\source\_static\images\lightning_logo_small.png,docs\source\_static\images\lightning_logo_small.png,,,,MODIFY,0.0,docs\source\_templates\theme_variables.jinja,docs\source\_templates\theme_variables.jinja,0.0,"5,8,9,10,11,12,13,14,15","5,8,9,10,11,12,13,14,15",,,,,,,,,,,,,,,,MODIFY,0.0,docs\source\conf.py,docs\source\conf.py,MODIFY,0.0,pl_examples\__init__.py,pl_examples\__init__.py,MODIFY,0.0,pl_examples\basic_examples\lightning_module_template.py,pl_examples\basic_examples\lightning_module_template.py,0.0,5,4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,"153,306","153,306",,,,,,,,,,,,0.0,"6,12","6,12",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pl_examples\full_examples\imagenet\imagenet_example.py,pl_examples\full_examples\imagenet\imagenet_example.py,0.0,"12,18,19","11,17,18,19,24",,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\__init__.py,pytorch_lightning\__init__.py,MODIFY,0.0,pytorch_lightning\callbacks\pt_callbacks.py,pytorch_lightning\callbacks\pt_callbacks.py,0.0,"3,4,8,12,168","3,9,166,167,168",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,"8,24","8,24",,,,,MODIFY,0.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,0.0,"3,4,8,11,16","1,2,3,4,7,13,17",,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,0.0,6,12,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\core\root_module.py,pytorch_lightning\core\root_module.py,0.0,,"9,10",,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\testing\model_base.py,pytorch_lightning\testing\model_base.py,0.0,12,,,,,,MODIFY,0.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,0.0,5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\distrib_data_parallel.py,pytorch_lightning\trainer\distrib_data_parallel.py,MODIFY,0.0,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,MODIFY,0.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,0.0,126,129,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,0.0,"3,27","6,26",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,116,118,0.0,279,279,MODIFY,1.0,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,273,,restore,"self,checkpoint_path,on_gpu",272,289,MODIFY,0.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,0.0,"154,157","155,156",MODIFY,0.0,pytorch_lightning\trainer\training_tricks.py,pytorch_lightning\trainer\training_tricks.py,0.0,"1,5",4,MODIFY,0.0,setup.py,setup.py,0.0,47,47,MODIFY,0.0,tests\README.md,tests\README.md,0.0,13,13,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
606,awaelchli,2019-12-08T00:30:41Z,2019-12-09T18:32:50Z,Early Stopping kicks in at min_epochs + 2 instead of min_epochs,"
 <denchmark-h:h3>Describe the bug</denchmark-h>
 
 I was working on a fix for <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/issues/524>#524</denchmark-link>
  and found that early stopping starts to kick in at epoch 3 despite min_epochs = 1.
 <denchmark-h:h4>To Reproduce</denchmark-h>
 
 run basic_examples/gpu_template.py and log the callback calls every epoch.
 <denchmark-h:h4>Expected behavior</denchmark-h>
 
 When setting min_epochs=n (counting from 1), we should evaluate early stopping at the end of epoch n.
 <denchmark-h:h4>Proposed fix:</denchmark-h>
 
 I propose to change <denchmark-link:https://github.com/williamFalcon/pytorch-lightning/blob/58cc6e13b9999161a526b83063c983750e6553b1/pytorch_lightning/trainer/training_loop.py#L322>this</denchmark-link>
  line in the training loop:
 
 to
 
 
 
 Why the ""-1""? The epoch variable in the training loop starts at 0, but the Trainer argument min_epochs starts counting at 1.
 
 
 Why the "">=""? The early stop check is done at the end of each epoch, hence the epoch counter will be = to min_epochs after min_epochs have passed.
 
 
 Desktop (please complete the following information):
 
 OS: Linux
 Version: master
 
 	",e2ee4ddbdb75a91132394208fabc8c62ca39f3e9,Adrian Wälchli,2019-12-09 10:32:49-08:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,345,342,1.0,awaelchli,2019-12-09T12:49:31Z,"
 		submit the PR for this?
 		",2.0,awaelchli,2019-12-09T13:20:42Z,"
 		yep. was waiting for an approval.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,train,self,282,360,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
618,jaypmorgan,2019-12-10T08:43:52Z,2019-12-11T00:24:23Z,Comet PAPI Depreciated,"
 Use of the Comet API logger reports an unecessary depreciation warning relating to the use of comet_ml.papi, rather than the newer comet_ml.api.
 Example:
 COMET WARNING: You have imported comet_ml.papi; this interface is deprecated. Please use comet_ml.api instead. For more information, see: https://www.comet.ml/docs/python-sdk/releases/#release-300
 	",d1633aac112fb35878aae83deb8b57259acea75b,Jay Morgan,2019-12-10 16:24:21-08:00,MODIFY,0,pytorch_lightning\logging\comet.py,pytorch_lightning\logging\comet.py,0.0,"55,56,57,58,59",55,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
638,kwanUm,2019-12-19T12:52:41Z,2020-04-07T10:30:47Z,"Pytorch lightning spawns processes after each epoch of training, causing training script to crash unexpectedly","
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When running my training script with PTL, I noticed that after the script is running process spawning is happening at the end of every epoch, which reloads the script. This unintended behavior often causes the training to crash if I'm in the middle of changing the code, and essentially locks the script running from any edits to it.
 A possible fix for this is to create the Data Loader object only once <denchmark-link:https://fburl.com/1x51d9ul>here</denchmark-link>
 .
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Train with PTL using DDP and num_workers > 1, and place a breakpoint at the top of the main file of your project.
 	",3c2fd560aa4d31b4f48ee225b83361deec53d9c7,Adrian Wälchli,2020-03-12 12:47:23-04:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,11,,1.0,kwanUm,2020-01-21T02:35:16Z,"
 		That link seems to be FB-internal, so I can't see what you're linking to, but you may be able to solve your problem by defining the dataloader as an attribute, then simply returning the existing dataloader in the val_dataloader method.
 Early versions of Lightning actually cached the dataloader so that it didn't get re-created every epoch, but there are some use cases that need to create a new DL every time. This way, you have the flexibility to cache it yourself.
 		",2.0,kwanUm,2020-01-21T06:50:30Z,"
 		Thanks for your reply Nic, I'll go ahead and try that!
 <denchmark-link:#>…</denchmark-link>
 
 
 On Tue, Jan 21, 2020 at 4:35 AM Nic Eggert ***@***.***> wrote:
  That link seems to be FB-internal, so I can't see what you're linking to,
  but you may be able to solve your problem by defining the dataloader as an
  attribute, then simply returning the existing dataloader in the
  val_dataloader method.
 
  Early versions of Lightning actually cached the dataloader so that it
  didn't get re-created every epoch, but there are some use cases that need
  to create a new DL every time. This way, you have the flexibility to cache
  it yourself.
 
  —
  You are receiving this because you authored the thread.
  Reply to this email directly, view it on GitHub
  <#638?email_source=notifications&email_token=AB5EBDELPYKS3JKJX6U4VG3Q6ZNOJA5CNFSM4J5FOHPKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJOJLCA#issuecomment-576492936>,
  or unsubscribe
  <https://github.com/notifications/unsubscribe-auth/AB5EBDAKGCFLHKW3J6ID2K3Q6ZNOJANCNFSM4J5FOHPA>
  .
 
 
 
 		",3.0,kwanUm,2020-01-21T12:34:54Z,"
 		<denchmark-link:https://github.com/tullie>@tullie</denchmark-link>
  want to look at this?
 		",4.0,kwanUm,2020-01-21T12:44:20Z,"
 		Yep. Will take a look this week
 		",MODIFY,2.0,pytorch_lightning\core\grads.py,pytorch_lightning\core\grads.py,1.0,11,10,grad_norm,"self,norm_type",10,29,MODIFY,14.0,pytorch_lightning\core\hooks.py,pytorch_lightning\core\hooks.py,1.0,,"52,56",MODIFY,37.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"1183,1202","1193,1195,1213",test_dataloader,self,1183,1223,MODIFY,17.0,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,"234,235,236,237,238,239","233,234,235,236,237",get_memory_profile,mode,233,248,5.0,kwanUm,2020-01-31T02:37:46Z,"
 		<denchmark-link:https://github.com/kwanUm>@kwanUm</denchmark-link>
  i've been unable to reproduce the problem.
 I'm wondering if you're not tagging your test_dataloader with @data_loader. This will ensure that the dataloader is only constructed once like you suggested. See 
 
 
 pytorch-lightning/pytorch_lightning/core/decorators.py
 
 
          Line 5
       in
       06242c2
 
 
 
 
 
 
  def data_loader(fn): 
 
 
 
 
 
 If that doesn't help, can you please send me some more code of your setup so I can try and reproduce?
 		",6.0,kwanUm,2020-02-01T21:16:21Z,"
 		Thanks for trying to check this <denchmark-link:https://github.com/tullie>@tullie</denchmark-link>
 .
 I haven't defined a test_dataloader method in my PTL Trainer - only overriden train_dataloader().
 I'm sorry for not being able to share the code for you to reproduce it yourself, but here's my train_dataloader code (it's annotated correctly):
 <denchmark-code>@pl.data_loader
 def train_dataloader(self):
     train_dataset = GANTrainingDataset(...)
     if self.distributed_backend == ""ddp"":
         data_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,
                                                                        num_replicas=self.num_of_gpus_running,
                                                                        rank=self.logger.rank)
     else:
         data_sampler = torch.utils.data.RandomSampler(train_dataset)
     return DataLoader(
         train_dataset,
         batch_size=self.batch_size,
         sampler=data_sampler,
         num_workers=self.num_workers,
         drop_last=True,
         pin_memory=True,
     )
 </denchmark-code>
 
 		",7.0,kwanUm,2020-02-03T22:04:15Z,"
 		<denchmark-link:https://github.com/kwanUm>@kwanUm</denchmark-link>
  thanks - I was able to reproduce the problem.
 My understanding is that you want DataLoader worker processes to be reused after each epoch. There's an issue and suggested work around for that in the pytorch repo here: <denchmark-link:https://github.com/pytorch/pytorch/issues/15849#issuecomment-573921048>pytorch/pytorch#15849 (comment)</denchmark-link>
 
 Another suggestion, and what I do, is copy the code base to a separate folder before running distributed training. This ensures any local changes you make won't affect the run.
 Let me know if this helps!
 		",8.0,kwanUm,2020-04-03T22:22:33Z,"
 		This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
 		",9.0,kwanUm,2020-04-07T08:07:25Z,"
 		Copying the code to another location for each run did the trick for me. Thanks!
 		",10.0,kwanUm,2020-04-07T10:30:47Z,"
 		awesome!
 		",,,,,,,,,,,,,,,,,,,,,on_batch_start,"self,batch",52,57,,,,,1.0,"21,22",,__init__,"self,str",21,28,1.0,208,,print_mem_stack,,208,214,MODIFY,12.0,pytorch_lightning\core\saving.py,pytorch_lightning\core\saving.py,1.0,38,39,load_hparams_from_tags_csv,str,38,47,1.0,27,"26,29",on_hpc_save,"self,checkpoint",26,30,,,,,1.0,"1049,1057,1058","1016,1024,1025",tbptt_split_batch,"self,batch,split_size",1016,1076,,,,,,,,,,,,,,,,,,,,,,1.0,11,,grad_norm,"self,float",11,30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,62,,on_epoch_start,self,62,63,1.0,51,52,on_batch_start,"self,Any",51,55,1.0,"901,902",,configure_optimizers,self,901,902,1.0,"745,746,747,748",,configure_ddp,self,745,748,1.0,523,593,test_step,"self,args,kwargs",523,602,1.0,"745,746,747,748,749,759,760","731,741,742",configure_ddp,"self,model,device_ids",731,766,1.0,1111,,prepare_data,self,1111,1134,1.0,1393,1364,_load_model_state,"cls,checkpoint",1364,1396,1.0,"143,144",,training_step,"self,args,kwargs",143,144,1.0,76,,print,"self,args,kwargs",76,92,1.0,"1461,1467","1452,1459",on_save_checkpoint,"self,checkpoint",1452,1472,1.0,32,"32,35",on_hpc_load,"self,str",32,35,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,66,,on_epoch_end,self,66,67,1.0,40,41,on_train_start,self,40,42,1.0,"94,95","97,98,99,100,102",on_after_backward,self,94,111,1.0,"253,256,257,258","254,255,256,257,258",get_gpu_memory_map,,253,273,1.0,"138,139,143","141,150",make_summary,self,138,151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,45,"45,48",on_train_end,self,45,48,1.0,58,,on_batch_end,self,58,59,1.0,70,,on_pre_performance_check,self,70,71,1.0,,"119,126",backward,"self,trainer,loss,optimizer,optimizer_idx",119,150,1.0,113,"119,126",backward,"self,trainer,Tensor,Optimizer,int",113,143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"78,90",80,on_before_zero_grad,"self,Optimizer",78,91,1.0,276,276,get_human_readable_count,number,276,300,1.0,276,276,get_human_readable_count,int,276,300,1.0,"21,22","17,18,19,20",__init__,"self,model,mode",17,26,1.0,"103,104",115,get_layer_names,self,103,115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"231,232",,training_step_end,"self,args,kwargs",231,232,1.0,,1398,summarize,"self,mode",1398,1400,1.0,1136,"1139,1152",train_dataloader,self,1136,1170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"90,94","80,92,93",on_before_zero_grad,"self,optimizer",80,94,1.0,74,,on_post_performance_check,self,74,75,1.0,"971,972,973,974,975,976,977,978,986,987,988,989,990","945,953,954,955,956,957",optimizer_step,"self,epoch,batch_idx,optimizer,optimizer_idx,second_order_closure",945,1014,1.0,382,438,validation_step_end,"self,args,kwargs",382,438,1.0,"864,865,866,867,868,869",,configure_apex,"self,object,str",864,869,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,27,29,on_hpc_save,"self,str",27,30,1.0,50,51,convert,str,50,64,1.0,,39,load_hparams_from_tags_csv,tags_csv,39,48,1.0,18,17,on_save_checkpoint,"self,checkpoint",17,21,1.0,10,,on_load_checkpoint,"self,str",10,16,1.0,32,"32,35",on_hpc_load,"self,checkpoint",32,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,153,,summarize,self,153,160,1.0,36,45,named_modules,self,36,45,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"47,48",101,get_variable_sizes,self,47,101,1.0,217,,count_mem_items,,217,231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"786,794,795","768,776,777",init_ddp_connection,"self,proc_rank,world_size",768,844,1.0,1447,"1432,1438",on_load_checkpoint,"self,checkpoint",1432,1450,1.0,"1481,1488",,on_save_checkpoint,"self,str",1481,1501,1.0,1447,"1452,1459",unfreeze,self,1447,1459,1.0,604,657,test_step_end,"self,args,kwargs",604,660,1.0,"446,447,448,449",438,validation_epoch_end,"self,list",438,510,1.0,"864,865,866,867,868,869,870","846,852,853,854,855",configure_apex,"self,amp,model,optimizers,amp_level",846,875,1.0,1431,"1432,1438",freeze,self,1431,1445,1.0,"786,794,795","846,852,853,854,855",init_ddp_connection,"self,int,int",786,862,1.0,"446,447,448",,validation_epoch_end,"self,str,str",446,448,1.0,"1461,1467",1474,on_load_checkpoint,"self,str",1461,1479,1.0,"971,972,973,974,975,976,977",,optimizer_step,"self,int,int,Optimizer,int,None",971,977,1.0,1393,"1398,1402,1418",_load_model_state,"cls,str",1393,1425,1.0,"668,669,670",,test_epoch_end,"self,str,str",668,670,1.0,"1049,1057,1058","1078,1102,1104",tbptt_split_batch,"self,Tensor,int",1049,1109,1.0,1427,,summarize,"self,str",1427,1429,1.0,"1225,1243",1260,val_dataloader,self,1225,1289,1.0,294,374,validation_step,"self,args,kwargs",294,380,1.0,1503,,get_tqdm_dict,self,1503,1520,1.0,"668,669,670,671,686","657,672",test_epoch_end,"self,outputs",657,729,1.0,"163,164,168","163,167",_format_summary_table,cols,163,205,1.0,"128,129",136,get_parameter_nums,self,128,136,1.0,"234,235,236,237,238,239","234,235,236,237",get_memory_profile,str,234,250,1.0,"117,118",126,get_parameter_sizes,self,117,126,1.0,,51,convert,val,51,65,1.0,10,9,on_load_checkpoint,"self,checkpoint",9,15,1.0,18,,on_save_checkpoint,"self,str",18,22,,,,,,,,,,,,,,,,,,,,,,,,,,,,
675,onkyo14taro,2020-01-09T13:16:11Z,2020-02-05T10:15:51Z,Mismatch of displayed 'epoch',"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 The display of epoch's number mismatches between the progress bar and the checkpoint indicator. I wonder this mismatch could confuse users.
 
 progress bar: The number of epochs starts from 1.
 checkpoint indicator: The number of epochs starts from 0.
 metrics.csv also starts from 0.
 
 I think that to change checkpoint and metrics.csv causes a serious problem.
 So progress bar should be changed in my opinion.
 What do you think about it?
 <denchmark-code>Epoch 32: 100%|██████████| 331/331 [00:05<00:00, 88.73batch/s, batch_idx=17, loss=1.148, train_batch_loss=1.02, v_num=0, val_loss=1.05]
 AINFO:root:
 Epoch 00031: val_loss reached 1.04545 (best 1.04545), saving model to /dummy/version_0/checkpoints/_ckpt_epoch_31.ckpt as top 1
 {'loss': 1.022357702255249, 'train_batch_loss': 1.022357702255249, 'val_loss': 1.0454469919204712}
 Epoch 33:   5%|▌         | 18/331 [00:00<00:05, 61.06batch/s, batch_idx=17, loss=1.073, train_batch_loss=1.31, v_num=0, val_loss=1.05]
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version : 1.3.1
 OS : macOS 10.14.6
 How you installed PyTorch : pip install git+https://github.com/williamFalcon/pytorch-lightning.git@master --upgrade
 Python version : 3.7.3
 use CPU
 
 	",734b28ed2dcd0feb23b44744a3d3d40de0b20a08,Shunsuke Hidaka,2020-02-05 05:15:51-05:00,MODIFY,3,pytorch_lightning\callbacks\pt_callbacks.py,pytorch_lightning\callbacks\pt_callbacks.py,1.0,"400,401",407,1.0,onkyo14taro,2020-01-19T13:24:02Z,"
 		Indeed, the inconsistency of zero-based/one-based epoch is very confusing.
 I think we should use zero-based only.
 		",2.0,onkyo14taro,2020-01-21T12:28:20Z,"
 		<denchmark-link:https://github.com/matthew-z>@matthew-z</denchmark-link>
  i  agree it should be zero based.  want to submit a PR?
 <denchmark-link:https://github.com/onkyo14taro>@onkyo14taro</denchmark-link>
  <denchmark-link:https://github.com/matthew-z>@matthew-z</denchmark-link>
 
 		",3.0,onkyo14taro,2020-01-21T12:31:08Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  I'll try this weekend.
 		",4.0,onkyo14taro,2020-01-26T08:40:45Z,"
 		<denchmark-link:https://github.com/matthew-z>@matthew-z</denchmark-link>
  <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
 
 I found a 'one epoch'-based API in GradientAccumulationScheduler while I was fixing the code.
 If we force the API and display to be zero-based, it will break backward compatibility.
 However, I think that mixing 0 and 1 will confuse users than changing the API, it should be zero based.
 I think it would be better we warn with FutureWarning in version 0.6.x, and then change the API and display to be zero-based in version 0.7.x.
 What do you think about it?
 		",MODIFY,1.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"284,285",,train,self,283,368,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,onkyo14taro,2020-01-27T09:34:46Z,"
 		I can't quite wrap my head around this. If you make it zero based, then when the progress bar shows
 Epoch 1: 50/100%
 it will no longer mean that the first epoch is in progress, but actually it is the second epoch 50% completed? Is this really what you want? Why should it be this way?
 		",6.0,onkyo14taro,2020-01-27T17:39:49Z,"
 		<denchmark-link:https://github.com/awaelchli>@awaelchli</denchmark-link>
 
 I agree with you, but I think zero-based would be better than one-based.
 Zero-based implementations are:
 
 Progress bar (console display)
 GradientAccumulationScheduler (console display, API)
 EarlyStopping (console display)
 
 One-based implementations are:
 
 metrics.csv (in the file)
 _ckpt_epoch_{0-based epoch number}.ckpt (filename)
 ModelCheckpoint (console display)
 
 I think what of most influensive in these items are ""metrics.csv"" and ""_ckpt_epoch_{0-based epoch number}.ckpt.""
 That's because we usually use the result file rather than console display.
 So I think that zero-based implementations have less confusion than one-based when breaking backward compatibility in order to unify representation of epoch numbers.
 		",7.0,onkyo14taro,2020-01-27T17:52:14Z,"
 		agreed. let’s do zero-based
 		",8.0,onkyo14taro,2020-01-28T11:48:54Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Is it ok that  is thrown when Progress bar,  or  are used in version 0.6.x , and then the API and display renew in version 0.7.0?
 		",9.0,onkyo14taro,2020-01-28T13:18:31Z,"
 		we are using DeprecatedWarning :]
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,__init__,"self,dict",391,409,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"412,413,414",,on_epoch_begin,"self,epoch,trainer",411,418,1.0,"177,178",,on_train_end,"self,logs",175,179,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
688,fgerzer,2020-01-15T15:42:14Z,2020-01-20T19:51:45Z,Checkpoint saving isn't atomic,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Saving checkpoints happens non-atomically. In some cases, this causes an incomplete write of a checkpoint (for example when receiving a SIGKILL during writing), causing any subsequent loading to fail with
 RuntimeError: unexpected EOF, expected 8 more bytes. The file might be corrupted.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 This is difficult to reproduce, since it relies on timing outside of code. For me, it happens with fast-running models that run at ~1-4 seconds per epoch.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Checkpointing should be resistant to such issues, and instead simply continue as-is.
 	",9aad69d85635a8a65e1f0ee995516c0f8183c0f3,Frederik Diehl,2020-01-20 14:51:44-05:00,MODIFY,3,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275","265,270",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,_atomic_save,"self,checkpoint,filepath",260,275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"435,440","418,423",hpc_save,"self,folderpath,logger",413,442,1.0,"282,287",,save_checkpoint,"self,filepath",277,287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
694,Borda,2020-01-16T08:40:58Z,2020-02-11T03:47:19Z,JIT problem with `torchvision` 0.5,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 There are some JIT problems with newly released  0.5
 in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/687>#687</denchmark-link>
  we freeze version to <0.5 but in future, we want to support all s
 Maybe it is just a temporal bug in  and they will handle it...
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/687#issuecomment-574913237>#687 (comment)</denchmark-link>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-link:https://app.circleci.com/jobs/github/Borda/pytorch-lightning/839>https://app.circleci.com/jobs/github/Borda/pytorch-lightning/839</denchmark-link>
 
 	",af445830506061680bd41744926c8bee1cca1104,Jirka Borovec,2020-02-10 22:47:18-05:00,MODIFY,0,MANIFEST.in,MANIFEST.in,0.0,18,"18,19,20",1.0,Borda,2020-02-05T11:58:54Z,"
 		Based on <denchmark-link:https://github.com/pytorch/vision/issues/1383#issuecomment-536539662>pytorch/vision#1383 (comment)</denchmark-link>
   now requires ""recent enough"" version of PyTorch. All tests are green with  and .
 The problem is when  is installed,  installs  which is incompatible. Ah, I  dependency issues...
 		",2.0,Borda,2020-02-05T12:07:00Z,"
 		Then the problem is torchvision, it shall enforce torch >= 1.4.0
 		",3.0,Borda,2020-02-05T13:39:18Z,"
 		But it does (starting from 0.4.0):
 <denchmark-code># pkginfo -f requires_dist torchvision-0.5.0+cpu-cp37-cp37m-linux_x86_64.whl
 requires_dist: ['numpy', 'six', 'torch (==1.4.0)', 'pillow (>=4.1.1)', ""scipy ; extra == 'scipy'""]
 # pkginfo -f requires_dist torchvision-0.4.2+cpu-cp37-cp37m-linux_x86_64.whl
 requires_dist: ['numpy', 'six', 'torch (==1.3.1)', 'pillow (>=4.1.1)', ""scipy ; extra == 'scipy'""]
 # pkginfo -f requires_dist torchvision-0.4.1+cpu-cp37-cp37m-linux_x86_64.whl
 requires_dist: ['numpy', 'six', 'torch (==1.3.0)', 'pillow (>=4.1.1)', ""scipy ; extra == 'scipy'""]
 # pkginfo -f requires_dist torchvision-0.4.0+cu92-cp37-cp37m-manylinux1_x86_64.whl
 requires_dist: ['numpy', 'six', 'torch (==1.2.0)', 'pillow (>=4.1.1)', ""scipy ; extra == 'scipy'""]
 </denchmark-code>
 
 My problem is I cannot freeze dependencies via pip-tools with torch==1.4.0 since pytorch-lightning limits torchvision version.
 Dirty solution is to manipulate torch version through torchvision, i.e. omit torch dependency in requirements.txt completely and set torchvision>=0.4.0.
 		",4.0,Borda,2020-02-07T06:22:21Z,"
 		Do we need the torchvision dependency at all? IIRC it is only used for tests and examples.
 		",ADD,0.0,None,pl_examples\requirements.txt,,,,,,,,DELETE,0.0,pytorch_lightning\testing\__init__.py,None,,,,DELETE,0.0,pytorch_lightning\testing\model.py,None,,,,,,,,MODIFY,0.0,requirements.txt,requirements.txt,0.0,,5,,,,,5.0,Borda,2020-02-07T09:06:39Z,"
 		
 idk why we would limit the version
 not needed. We do need to add to the examples that people need to have torchvision and torch installed.
 
 		",6.0,Borda,2020-02-07T09:44:55Z,"
 		we are using MNIST dataset in tests from torchvision.datasets import MNIST and transforms from torchvision import transforms
 		",7.0,Borda,2020-02-07T09:55:51Z,"
 		so add torchvision to the tests requirements
 		",8.0,Borda,2020-02-07T10:11:16Z,"
 		but it is in package pytorch_lightning.testing.base_model so we would need to move it also out of the package or make own MNIT...
 		",9.0,Borda,2020-04-02T19:41:53Z,"
 		I have torch version 0.4.1, and torchvision->inception.py says that there is no module named call Optional in torch.jit.annotations....I want to use torch version 0.4.1 because I want to use it along with cuda 9.2 for my program. And as per my understanding, torchvision requires torch 1.4. How do I solve this dependency problem?
 		",10.0,Borda,2020-04-02T20:36:15Z,"
 		are you talking about torch==0.4.1 or torchvision=0.4.1 this is not related to lightning, you shall ask at torch/vision
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ADD,0.0,None,tests\models\__init__.py,,,,,,,,,,,,,,,RENAME,0.0,pytorch_lightning\testing\model_base.py,tests\models\base.py,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\debug.py,tests\models\debug.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,pytorch_lightning\testing\model_mixins.py,tests\models\mixins.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,RENAME,0.0,tests\utils.py,tests\models\utils.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\requirements.txt,tests\requirements.txt,0.0,1,,MODIFY,0.0,tests\test_amp.py,tests\test_amp.py,0.0,"5,7","5,7",,,,,,,,,,,,MODIFY,0.0,tests\test_cpu_models.py,tests\test_cpu_models.py,0.0,"5,10","5,10",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,MODIFY,0.0,tests\test_gpu_models.py,tests\test_gpu_models.py,0.0,"6,12","6,12",MODIFY,0.0,tests\test_logging.py,tests\test_logging.py,0.0,"7,18","7,18",,,,,,,,,,,,,,,,MODIFY,0.0,tests\test_restore_models.py,tests\test_restore_models.py,MODIFY,0.0,tests\test_trainer.py,tests\test_trainer.py,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,"6,9","6,9",,,,,,,,,,,,0.0,"6,11","6,11",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
703,goodok,2020-01-17T13:59:28Z,2020-01-20T19:50:57Z,Fitting with log_gpu_memory=True fails in python3.6.,"
 <denchmark-h:h2>Bug</denchmark-h>
 
 Fitting with log_gpu_memory=True in the Trainer fails in python3.6 version.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 
 Use python3.6 version
 Create any trainer with log_gpu_memory=True option.
 Then fit it.
 See error:
 
 <denchmark-code>/a/pytorch-lightning/pytorch_lightning/core/memory.py in get_gpu_memory_map()
     237         encoding='utf-8',
     238         capture_output=True,
 --> 239         check=True)
     240     # Convert lines into a dictionary
     241     gpu_memory = [int(x) for x in result.stdout.strip().split(os.linesep)]
 
 /usr/lib/python3.6/subprocess.py in run(input, timeout, check, *popenargs, **kwargs)
     421         kwargs['stdin'] = PIPE
     422 
 --> 423     with Popen(*popenargs, **kwargs) as process:
     424         try:
     425             stdout, stderr = process.communicate(input, timeout=timeout)
 
 TypeError: __init__() got an unexpected keyword argument 'capture_output'
 
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>trainer = Trainer(
     log_gpu_memory=True,
    # ....
 )
 trainer.fit()
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 For the same code there is no errors for python3.7
 <denchmark-h:h3>Environment</denchmark-h>
 
 pytorch:          1.2.0
 Ubuntu 18.04
 pytorch-lightning:
 - installed to pip environment
 - commit <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/commit/7a1df80f4e98fca82478dbfde2ba91f677218691>7a1df80</denchmark-link>
 
 - python setup.py develop
 - version 0.6.0
 python:         3.6.8
 cuda: 10.0, V10.0.130
 cudnn:          7.6.2
 GPU: RTX 2080 TI
 <denchmark-h:h3>Additional context</denchmark-h>
 
 In the setup.py
 python_requires='>=3.6',
 But   is used in   calling, which is valid only for python3.7
 See also workaround to maintain python3.6:
 <denchmark-link:https://stackoverflow.com/questions/53209127/>https://stackoverflow.com/questions/53209127/</denchmark-link>
 
 	",06242c200a318a37d1f882c786e60354ec04533f,Alexey U. Gudchenko,2020-01-20 14:50:57-05:00,MODIFY,1,pytorch_lightning\core\memory.py,pytorch_lightning\core\memory.py,1.0,"239,240",238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,get_gpu_memory_map,,223,245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
704,matthew-z,2020-01-17T16:17:56Z,2020-01-18T12:17:54Z,TensorBoardLogger and ModelCheckpoint are not using the same folder by default,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 (master branch)
 By default, the TensorBoardLogger writes logs into lightning_logs/0 but ModelCheckpoint writes checkpoint into lightning_logs/version_0.
 	",de2ccc03a8df997b8841f33ae70050498960f08c,Z ZH,2020-01-18 07:17:53-05:00,MODIFY,2,pytorch_lightning\logging\tensorboard.py,pytorch_lightning\logging\tensorboard.py,1.0,66,66,,,,,,,,,,,,,,,,,MODIFY,2.0,tests\test_logging.py,tests\test_logging.py,1.0,"299,300","299,300",test_tensorboard_automatic_versioning,tmpdir,295,304,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,experiment,self,51,68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"311,312,313","311,312,313",test_tensorboard_manual_versioning,tmpdir,307,317,,,,,,,,1.0,"134,135,136,137,138","134,135,136",_get_next_version,self,132,142,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
708,felixkreuk,2020-01-18T12:57:14Z,2020-02-19T11:37:36Z,LR Schedulers shouldn't get `epoch` argument in `step` function,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 PyTorch LR schedulers now shouldn't get any arguments in  function, see <denchmark-link:https://github.com/pytorch/pytorch/blob/9e9bfbfd8d893cdf8205db67f220551214787d7f/torch/optim/lr_scheduler.py#L13-L20>here</denchmark-link>
  and <denchmark-link:https://github.com/pytorch/pytorch/issues/31828>here</denchmark-link>
 .
 Looks like the calls in PytorchLightning are not in line with the new interface, see <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/dac59bb8d354f28b919d08aa369a7db6ab31bfa6/pytorch_lightning/trainer/training_loop.py#L334-L337>here</denchmark-link>
 .
 This results in unexpected LR changes. Removing the epoch argument from step call solves the issue for me.
 <denchmark-h:h2>Environment</denchmark-h>
 
 PyTorch 1.4
 PyTorchLightning 0.5.3.2
 	",c58aab0b0024c36a9bd4d5a0a472c84b80edc61d,Nicki Skafte,2020-02-19 06:37:35-05:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"365,373","365,373",1.0,felixkreuk,2020-01-18T12:58:31Z,"
 		good catch. we need to keep it backward compatible though
 		",2.0,felixkreuk,2020-01-18T15:40:33Z,"
 		We shall catch it with a test...
 		",3.0,felixkreuk,2020-01-21T12:21:23Z,"
 		<denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  want to submit PR for this?
 		",4.0,felixkreuk,2020-02-11T12:59:59Z,"
 		<denchmark-link:https://github.com/felixkreuk>@felixkreuk</denchmark-link>
  want  to submit a PR?
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,train,self,307,401,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
709,borisdayma,2020-01-19T02:35:32Z,2020-01-24T10:04:37Z,imagenet_example cannot run,"
 🐛 Bug
 imagenet_example cannot be executed
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Steps to reproduce the behavior:
 
 Download imagenet
 cd pl_examples/full_examples/imagenet
 python imagenet_example.py --data-path imagenet_path
 
 Error:
 <denchmark-code>Traceback (most recent call last):
   File ""imagenet_example.py"", line 248, in <module>
     main(get_args())
   File ""imagenet_example.py"", line 229, in main
     model = ImageNetLightningModel(hparams)
 TypeError: Can't instantiate abstract class ImageNetLightningModel with abstract methods forward
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 The example should run.
 <denchmark-h:h3>Environment</denchmark-h>
 
 PyTorch version: 1.3.1
 Is debug build: No
 CUDA used to build PyTorch: 10.1.243
 OS: Ubuntu 18.04.3 LTS
 GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
 CMake version: Could not collect
 Python version: 3.7
 Is CUDA available: Yes
 CUDA runtime version: Could not collect
 GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
 Nvidia driver version: 440.44
 cuDNN version: Could not collect
 Versions of relevant libraries:
 [pip3] numpy==1.18.1
 [pip3] pytorch-lightning==0.6.0
 [pip3] torch==1.3.1
 [pip3] torchvision==0.4.2
 [conda] blas                      1.0                         mkl
 [conda] mkl                       2019.4                      243
 [conda] mkl-service               2.3.0            py37he904b0f_0
 [conda] mkl_fft                   1.0.14           py37ha843d7b_0
 [conda] mkl_random                1.1.0            py37hd6b4f25_0
 <denchmark-h:h3>Additional context</denchmark-h>
 
 I suspect that there was breaking changes in the API and that the example is obsolete as the method forward is now expected.
 	",eeb48ceb965f0cc140728676a3bb77e7271d9e35,Harsh Sharma,2020-01-21 16:35:42-05:00,MODIFY,7,pl_examples\full_examples\imagenet\imagenet_example.py,pl_examples\full_examples\imagenet\imagenet_example.py,1.0,138,135,1.0,borisdayma,2020-01-21T12:20:49Z,"
 		<denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  let's remove forward from being abstract?
 		",2.0,borisdayma,2020-01-21T17:42:27Z,"
 		I did a local fix for now by updating line 39 from output = self.model(images) to output = self.forward(images) and my forward function is just calling self.model
 <denchmark-code>def forward(self, x):
         return self.model(x)
 </denchmark-code>
 
 Also following issues also exist
 
 Argument 'seed' must not be None as it breaks tensorboard writer.
 Argument 'data' might have been updated to 'data_path' recently but there are references to hparams.data in the dataloaders.
 
 After fixing all three, I was able to run the example.
 I can create a PR if required.
 		",3.0,borisdayma,2020-01-21T17:53:03Z,"
 		PR please!
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,train_dataloader,self,132,160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,191,188,add_model_specific_args,parent_parser,183,207,1.0,"37,38",,forward,"self,x",37,38,1.0,65,,validation_step,"self,batch,batch_idx",63,81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,42,,training_step,"self,batch,batch_idx",40,61,1.0,220,217,get_args,,210,226,1.0,168,165,val_dataloader,self,163,180,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
712,colehurwitz,2020-01-19T20:18:08Z,2020-01-21T13:09:28Z,Trainer is setting parameters with requires_grad=False to requires_grad=True (bug),"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When training a model that has some parameters where requires_grad=False,  the Trainer  is actually setting requires_grad=True for these parameters and changing them. The bug appears to originate in the TrainerTrainLoopMixin code.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 <denchmark-h:h4>Steps to reproduce the behavior:</denchmark-h>
 
 
 Create a model with some parameters which have requires_grad=False
 Fit the model using the Trainer
 Check to see if the parameters which were set with `requires_grad=False' have changed.
 
 <denchmark-h:h4>Code sample (to reproduce the bug)</denchmark-h>
 
 <denchmark-code>import torch
 import numpy as np
 import os
 from torch.nn import functional as F
 from torch.utils.data import DataLoader
 import pytorch_lightning as pl
 
 # Make toy dataset
 features = torch.from_numpy(np.asarray([[0],[0],[0],[1],[1],[1]])).float()
 targets = torch.from_numpy(np.asarray([0,0,0,1,1,1]))
 train = torch.utils.data.TensorDataset(features, targets)
 train_loader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)
 
 
 #Define lightning model
 class CoolSystem(pl.LightningModule):
 
     def __init__(self):
         super(CoolSystem, self).__init__()
         self.l1 = torch.nn.Linear(1, 10)
         self.l2 = torch.nn.Linear(10, 2)
         for param in self.l2.parameters():
             param.requires_grad = False
         self.loss_func = torch.nn.CrossEntropyLoss()
    
     def forward(self, x):
         return self.l2(torch.relu(self.l1(x)))
 
     def training_step(self, batch, batch_idx):
         x, y = batch
         y_hat = self.forward(x)
         loss = self.loss_func(y_hat, y)
         tensorboard_logs = {'train_loss': loss}
         return {'loss': loss, 'log': tensorboard_logs}
 
     def configure_optimizers(self):
         return torch.optim.Adam(self.parameters(), lr=0.02)
 
     @pl.data_loader
     def train_dataloader(self):
         return train_loader
 
 # Run the lightning model (check parameter before and after training)
 
 coolsystem = CoolSystem()
 print(list(coolsystem.parameters())[3])
 trainer = pl.Trainer(min_epochs=10, max_epochs=10, logger=False)    
 trainer.fit(coolsystem)
 list(coolsystem.parameters())[3]
 
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 <denchmark-h:h4>Expected</denchmark-h>
 
 The parameters with requires_grad == False should not change during training.
 <denchmark-h:h4>Actual</denchmark-h>
 
 The printed out parameter before training has requires_grad == False, but after training with the Trainer, the parameter now has requires_grad == True and has changed values.
 <denchmark-h:h3>Environment</denchmark-h>
 
 
 PyTorch Version 1.3.1
 Linux
 PyTorch installed with pip
 Python 3.7.1
 pytorch-lightning 0.6.0
 
 <denchmark-h:h3>Where I think the issue is!</denchmark-h>
 
 Here is the code snippet from training_loop.py that I think is causing the issue:
 <denchmark-code>class TrainerTrainLoopMixin(ABC):
             .
             .
             .
     def run_training_batch(self, batch, batch_idx):
             .
             .
             .
             # call training_step once per optimizer
             for opt_idx, optimizer in enumerate(self.optimizers):
                 # make sure only the gradients of the current optimizer's paramaters are calculated
                 # in the training step to prevent dangling gradients in multiple-optimizer setup.
                 for param in self.get_model().parameters():
                     param.requires_grad = False
                 for group in optimizer.param_groups:
                     for param in group['params']:
                         param.requires_grad = True
 </denchmark-code>
 
 As you can see, the params in the model are all set to  param.requires_grad = True during each training batch!
 	",a2b20b46bca5101627ed392aec17611ac0e97133,Ayberk Aydın,2020-01-21 08:09:27-05:00,MODIFY,1,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"463,464,465,466,467,468","463,464,465,466,467",1.0,colehurwitz,2020-01-21T03:57:13Z,"
 		This behavior was introduced in <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603>#603</denchmark-link>
 . Looks like we may want to revisit some of the changes that PR made. In the meantime, you can work around the problem by not passing parameters with  to your optimizer.
 		",2.0,colehurwitz,2020-01-21T03:58:36Z,"
 		@aybberk <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>
  ^
 		",3.0,colehurwitz,2020-01-21T12:02:35Z,"
 		I just disabled that loop for now since I am not working with GANs. thanks for responding and good luck fixing it!
 		",4.0,colehurwitz,2020-01-21T12:16:59Z,"
 		@aybberk <denchmark-link:https://github.com/jeffling>@jeffling</denchmark-link>
  yeah. we need to revert this change or make a change to address
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,colehurwitz,2020-01-21T12:23:57Z,"
 		I think <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/603#discussion_r358626166>#603 (comment)</denchmark-link>
  would also fix that when not working with GANs.
 		",6.0,colehurwitz,2020-01-21T12:35:55Z,"
 		Unless I am misunderstanding, the solution would be to only do this if there is more than one optimizer. However, normal networks can have more than one optimizer as well and not want this behavior right?
 		",7.0,colehurwitz,2020-01-21T12:42:56Z,"
 		You are probably correct, but I can't think of a such niche situation right now.
 		",8.0,colehurwitz,2020-01-21T12:47:12Z,"
 		If you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.
 		",9.0,colehurwitz,2020-01-21T12:48:21Z,"
 		i suggest we do this only if 2 optimizes are present. PR? <denchmark-link:https://github.com/colehurwitz>@colehurwitz</denchmark-link>
  or @aybberk
 		",10.0,colehurwitz,2020-01-21T12:49:59Z,"
 		
 i suggest we do this only if 2 optimizes are present. PR? @colehurwitz or @aybberk
 
 PR is ready with this suggestion.
 		",11.0,colehurwitz,2020-01-21T12:53:54Z,"
 		Ok, this fixes my immediate issue at least. Thanks!
 		",12.0,colehurwitz,2020-01-21T12:54:02Z,"
 		
 If you want different learning rates for some parameters than you would use different optimizers, right? For example, if you are making a VAE with a deep encoder and a parametric model for the decoder than you may want different optimizers for the neural network and the learned decoder params. Maybe it is niche, but I have used it before.
 
 You are right, but it would still be implemented in Lightning with iteration between optimizers and this change would not affect the gradients in that setup. Also, I need to mention I think it would not be the best way to implement the situation in Lightning since it automatically iterates between optimizers for same training loop.
 edit: You mentioned you used that setup, did you do it in Lightning or something else?
 		",13.0,colehurwitz,2020-01-21T13:02:23Z,"
 		I did this in regular Pytorch for a recent paper. You bring up some good points though, I did not freeze any parameters so that example may not apply. I do think that if this is specifically a GAN issue though, maybe there is a GAN specific solution? Maybe not though.
 Anyways, I appreciate the quick response and the fix should be appropriate for my current work!
 		",14.0,colehurwitz,2020-01-21T13:04:17Z,"
 		The most structured way to fix it would be, as <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
   mentioned, to save states before regular freeze/unfreeze operation and gradient calculation and load it after gradients are computed. I can work on it but I'm not really sure if it messes up with the distributed training settings since I do not know anything about ddp internals.
 		",15.0,colehurwitz,2020-01-21T13:08:51Z,"
 		Would it be possible to only freeze and unfreeze variables that have requires_grad=True or is that too expensive to search for every loop?
 		",,,,,run_training_batch,"self,batch,batch_idx",429,553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16.0,colehurwitz,2020-01-21T13:08:52Z,"
 		it shouldn't mess up the internals @aybberk. and I would do it how <denchmark-link:https://github.com/colehurwitz>@colehurwitz</denchmark-link>
  mentioned
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
760,fdelrio89,2020-01-28T15:36:34Z,2020-02-26T23:34:53Z,Test metrics not logging to Comet after training,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When testing a model with Trainer.test metrics are not logged to Comet if the model was previously trained using Trainer.fit. While training metrics are logged correctly.
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>    comet_logger = CometLogger()
     trainer = Trainer(logger=comet_logger)
     model = get_model()
 
     trainer.fit(model) # Metrics are logged to Comet
     trainer.test(model) # No metrics are logged to Comet
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 Test metrics should also be logged in to Comet.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>- PyTorch version: 1.3.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1.243
 
 OS: Ubuntu 18.04.3 LTS
 GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
 CMake version: version 3.10.2
 
 Python version: 3.7
 Is CUDA available: Yes
 CUDA runtime version: 10.1.168
 GPU models and configuration:
 GPU 0: GeForce GTX 1080 Ti
 GPU 1: GeForce GTX 1080 Ti
 GPU 2: GeForce GTX 1080 Ti
 GPU 3: GeForce GTX 1080 Ti
 GPU 4: GeForce GTX 1080 Ti
 GPU 5: GeForce GTX 1080 Ti
 GPU 6: GeForce GTX 1080 Ti
 GPU 7: GeForce GTX 1080 Ti
 
 Nvidia driver version: 418.67
 cuDNN version: /usr/local/cuda-10.1/targets/x86_64-linux/lib/libcudnn.so.7.6.1
 
 Versions of relevant libraries:
 [pip3] numpy==1.16.4
 [pip3] pytorch-lightning==0.6.0
 [pip3] torch==1.3.0
 [pip3] torchvision==0.4.1
 [conda] Could not collect
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 I believe the issue is caused because at the <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/blob/deffbaba7ffb16ff57b56fe65f62df761f25fbd6/pytorch_lightning/trainer/training_loop.py#L366>end of the training routine</denchmark-link>
 ,  is called. This in turn calls  inside the logger and the  object doesn't expect to send more information after this.
 An alternative is to create another Trainer object, with another logger but this means that the metrics will be logged into a different Comet experiment from the original. This issue can be solved using the ExistingExperiment object form the Comet SDK, but the solution seems a little hacky and the CometLogger currently doesn't support this kind of experiment.
 	",4ac9925dad71edda26db486795341f0b0ba4ed38,fdelrio89,2020-02-21 20:47:48-05:00,MODIFY,5,pytorch_lightning\loggers\comet.py,pytorch_lightning\loggers\comet.py,1.0,"176,177",,1.0,fdelrio89,2020-02-11T17:36:13Z,"
 		Did you find a solution?
 Mind submitting a PR?
 <denchmark-link:https://github.com/fdelrio89>@fdelrio89</denchmark-link>
 
 		",2.0,fdelrio89,2020-02-13T21:36:24Z,"
 		I did solve the issue but in a kind of hacky way. It's not that elegant but it works for me, and I haven't had the time to think of a better solution.
 I solved it by getting the experiment key and creating another logger and trainer with it.
 <denchmark-code>    comet_logger = CometLogger()
     trainer = Trainer(logger=comet_logger)
     model = get_model()
 
     trainer.fit(model)
 
     experiment_key = comet_logger.experiment.get_key()
     comet_logger = CometLogger(experiment_key=experiment_key)
     trainer = Trainer(logger=comet_logger)
 
     trainer.test(model)
 </denchmark-code>
 
 For this to work, I had to modify the CometLogger class to accept the experiment_key and create a CometExistingExperiment from the Comet SDK when this param is present.
 <denchmark-code>class CometLogger(LightningLoggerBase):
      ...
 
     @property
     def experiment(self):
         ...
 
         if self.mode == ""online"":
             if self.experiment_key is None:
                 self._experiment = CometExperiment(
                     api_key=self.api_key,
                     workspace=self.workspace,
                     project_name=self.project_name,
                     **self._kwargs
                 )
             else:
                 self._experiment = CometExistingExperiment(
                     api_key=self.api_key,
                     workspace=self.workspace,
                     project_name=self.project_name,
                     previous_experiment=self.experiment_key,
                     **self._kwargs
                 )
         else:
             ...
 
         return self._experiment
 </denchmark-code>
 
 I can happily do the PR if this solution is acceptable for you guys, but I think a better solution can be achieved I haven't had the time to think about it <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
 .
 		",3.0,fdelrio89,2020-02-17T11:18:50Z,"
 		<denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  Any progress on this Issue? I am facing the same problem.
 		",4.0,fdelrio89,2020-02-17T11:21:16Z,"
 		<denchmark-link:https://github.com/fdelrio89>@fdelrio89</denchmark-link>
  Since the logger object is available for the lifetime of the trainer, maybe you can refactor to store the  directly in the logger object itself, instead of having to re-instantiate the logger.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5.0,fdelrio89,2020-02-18T21:27:32Z,"
 		<denchmark-link:https://github.com/xssChauhan>@xssChauhan</denchmark-link>
  good idea, I just submitted a PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/892>#892</denchmark-link>
 ) considering this. Thanks!
 		",6.0,fdelrio89,2020-02-26T23:34:53Z,"
 		I assume that it was fixed by <denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/892>#892</denchmark-link>
 
 if you have some other problems feel free to reopen or create a new... 
 		",7.0,fdelrio89,2020-04-19T06:51:52Z,"
 		Actually I'm still facing the problem.
 		",8.0,fdelrio89,2020-04-19T09:11:27Z,"
 		<denchmark-link:https://github.com/dvirginz>@dvirginz</denchmark-link>
  are you using the latest master? may you provide a minimal example?
 		",9.0,fdelrio89,2020-04-19T09:21:55Z,"
 		
 @dvirginz are you using the latest master? may you provide a minimal example?
 
 You are right, sorry.
 After building from source it works.
 		",10.0,fdelrio89,2020-07-14T13:43:08Z,"
 		I should probably open a new issue, but it happens with Weights & Biases logger too. I haven't had the time to delve deep into it yet.
 		",,,,,,,,,,,,,,,,,,,,,,,,,reset_experiment,self,176,177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"37,38",36,__init__,"self,api_key,save_dir,workspace,rest_api_key,project_name,experiment_name,experiment_key,kwargs",36,38,1.0,"137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152","134,135,136,137,138,139",experiment,self,123,161,1.0,,36,__init__,"self,api_key,save_dir,workspace,rest_api_key,project_name,experiment_name,kwargs",35,36,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"181,182,183,184,185,186,187,188,190",,finalize,"self,status",180,190,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
796,williamFalcon,2020-02-07T09:36:51Z,2020-02-09T22:48:38Z,new profiler has failing tests,"
 <denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
 
 Tests fail on OSX
 <denchmark-code>tests/test_profiler.py::test_advanced_profiler FAILED 
 </denchmark-code>
 
 	",fc0ad03008f5b725814a9091dc6a874950f49b42,Jirka Borovec,2020-02-09 17:48:37-05:00,MODIFY,4,tests\test_profiler.py,tests\test_profiler.py,1.0,,28,1.0,williamFalcon,2020-02-07T09:43:10Z,"
 		<denchmark-link:https://github.com/jeremyjordan>@jeremyjordan</denchmark-link>
  <denchmark-link:https://travis-ci.org/PyTorchLightning/pytorch-lightning/jobs/647158609>https://travis-ci.org/PyTorchLightning/pytorch-lightning/jobs/647158609</denchmark-link>
 
 		",2.0,williamFalcon,2020-02-07T12:47:11Z,"
 		 that environment has really inconsistent s (eg. see discussion <denchmark-link:https://www.reddit.com/r/Python/comments/8eqwsr/unexpected_behavior_of_timesleep_seems_to_add/?utm_source=amp&utm_medium=&utm_content=post_body>here</denchmark-link>
 ). i could relax the precision requirement on the test even further.
 perhaps i can also profile a no-op and require the recorded duration to be less than X to ensure our profilers don’t add much overhead.
 		",3.0,williamFalcon,2020-02-07T22:26:06Z,"
 		maybe some larger tolerance on the value?
 <denchmark-code>        a_duration = get_duration(p.profiled_actions[""a""])
 >       np.testing.assert_almost_equal(a_duration, [4], decimal=1)
 E       AssertionError: 
 E       Arrays are not almost equal to 1 decimals
 E       
 E       Mismatch: 100%
 E       Max absolute difference: 0.150021
 E       Max relative difference: 0.03750525
 E        x: array(4.2)
 E        y: array([4])
 tests/test_profiler.py:46: AssertionError
 </denchmark-code>
 
 for the record, restartd job/build helps but it is not very nice hack which need manual work...
 		",4.0,williamFalcon,2020-02-07T23:20:52Z,"
 		yeah a larger tolerance will ensure it passes the tests, we should also include a new test which does something like
 <denchmark-code>with profiler.profile(""no-op""):
     pass
 </denchmark-code>
 
 and ensure the ""no-op"" duration is less than some desired overhead threshold. this ensures that we won't introduce performance regressions in future updates.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_advanced_profiler.get_duration,profile,28,29,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"24,25,26,27","22,23,24",test_simple_profiler,,8,27,1.0,"31,48,49,50,51,52,53,54,55","45,46,47,48,49,50",test_advanced_profiler,,30,55,1.0,31,,test_advanced_profiler._get_duration,profile,31,32,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
850,hjalmarlucius,2020-02-15T01:26:14Z,2020-02-22T01:27:20Z,Epoch end checkpoint restarts previous epoch,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 If restarting the training and reloading the model, the epoch that the checkpoint had just completed is restarted rather than beginning the next.
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 When a checkpoint upon epoch end is saved, restarting it should resume its state and start the next epoch.
 	",6e7dc9c2363779a12e8122ee1e2d470a0b0f013e,Matt Painter,2020-02-21 20:27:19-05:00,MODIFY,3,pytorch_lightning\trainer\training_io.py,pytorch_lightning\trainer\training_io.py,1.0,"311,312","310,311",1.0,hjalmarlucius,2020-02-16T11:25:27Z,"
 		
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_io.py
 
 
          Line 389
       in
       edd4a87
 
 
 
 
 
 
  self.current_epoch = checkpoint['epoch'] 
 
 
 
 
 
 This seems as simple as replacing the line above with self.current_epoch = checkpoint['epoch'] + 1 since the checkpointers save at the end of validation and the main loop runs from the current epoch.
 We should probably also increase the global step by 1 since this happens after saving the checkpoint.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_io.py
 
 
          Line 388
       in
       edd4a87
 
 
 
 
 
 
  self.global_step = checkpoint['global_step'] 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 411
       in
       edd4a87
 
 
 
 
 
 
  self.run_evaluation(test=self.testing) 
 
 
 
 
 
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/training_loop.py
 
 
          Line 430
       in
       edd4a87
 
 
 
 
 
 
  self.global_step += 1 
 
 
 
 
 
 I'll add a test in whilst I do it.
 <denchmark-link:https://github.com/williamFalcon>@williamFalcon</denchmark-link>
  <denchmark-link:https://github.com/Borda>@Borda</denchmark-link>
  Any other thoughts if I put a PR in with this? The test should presumably go in ?
 		",2.0,hjalmarlucius,2020-02-16T15:47:36Z,"
 		That works when saving at epoch end but there's many cases of saving during an epoch as well (e.g. for a very large dataset). Both the epoch number and the global step are technically correct upon saving mid or end epoch but when resuming, the loop starts at the beginning. The best solution is to make the resume reliably restart precisely where in the loop it left off. I'm not that familiar with this code but guess one should then also save the batch_idx.
 		",3.0,hjalmarlucius,2020-02-16T17:03:20Z,"
 		I can think of at least one way to do it, although it's not ideal:
 
 Use the global step as stored and when resuming, load this into a (hidden?) trainer variable which is checked every training batch.
 Skip each batch until the batch number is above this saved variable, at which point we can set it to None or something.
 Run batches as normal
 
 Two Problems I see:
 
 When the data set is shuffled there would be no guarantee of seeing only new samples after the reload without somehow 'resuming' the dataloaders.
 Any calls that are usually made during the batch wouldn't happen. For example, the tqdm update calls would need to be faked so it didn't appear to end the epoch early in the progress bars.
 
 		",4.0,hjalmarlucius,2020-02-17T13:11:56Z,"
 		Currently I've put in a PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/866>#866</denchmark-link>
 )  that deals with the off by one when loading a checkpoint from epoch end. I've added a warning when loading a mid-epoch checkpoint that says resuming training is not reliable and to consider loading an end of epoch checkpoint.
 If it's preferable I can also rerun the previous epoch when we detect mid-epoch checkpoints, but this technically means you run for more epochs than you would expect/report, so I'm not sure if this is a good idea.
 I'd suggest a new issue and discussion on how to resume mid epoch checkpoints, since we have no way of ensure data set states are preserved, and close this issue with the PR I have up.
 		",MODIFY,2.0,tests\test_restore_models.py,tests\test_restore_models.py,1.0,"285,286",285,test_cpu_restore_training,tmpdir,262,321,MODIFY,4.0,tests\test_trainer.py,tests\test_trainer.py,1.0,"424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440",,,,,,,,,,,,,,,,,,,,,,,,5.0,hjalmarlucius,2020-02-18T17:20:24Z,"
 		Great, a good compromise for now
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,test_resume_from_checkpoint_epoch_restored.new_model,,424,440,dump_checkpoint,self,309,350,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"217,218","217,218",test_dp_resume,tmpdir,184,259,,,,,,,,1.0,"392,393,394,395,396,397,398,399,400,401,402",,restore_training_state,"self,checkpoint",375,419,1.0,"128,129",,__init__,self,112,129,,,,,,,,1.0,"433,434",,test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_batch,"self,_",433,434,1.0,"416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481",,test_resume_from_checkpoint_epoch_restored,tmpdir,416,481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"430,431",,test_resume_from_checkpoint_epoch_restored.test_resume_from_checkpoint_epoch_restored.new_model.increment_epoch,self,430,431,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
922,srush,2020-02-23T20:09:13Z,2020-02-25T03:23:26Z,Init'ing Dataloader calls get_train_dataloader,"
 It seems like the code of initializing the dataloader calls into getting the dataloader.
 
 
 
 pytorch-lightning/pytorch_lightning/trainer/data_loading.py
 
 
          Line 69
       in
       c00a8a1
 
 
 
 
 
 
  if EXIST_ITER_DATASET and isinstance(self.get_train_dataloader().dataset, IterableDataset): 
 
 
 
 
 
 This means that all the effort for wrapping get_dataloader to sync through barriers for multi-gpu / tpu is not used on this first call (and results in a crash).
 	",1015a0050621828c9e8af2c934e19c5c68d61a5e,William Falcon,2020-02-24 22:23:25-05:00,MODIFY,0,CHANGELOG.md,CHANGELOG.md,0.0,"11,12,13,28",,1.0,srush,2020-02-23T20:14:29Z,"
 		ummm good point. I think we need to simplify the get_XXX_dataloader() calls. The lazy loading decorator does make it harder for people to debug their data loading issues.
 <denchmark-link:https://github.com/neggert>@neggert</denchmark-link>
  do you remember the original reason we added the decorator? maybe it's time to remove it and simplify this logic?
 		",2.0,srush,2020-02-23T20:35:20Z,"
 		<denchmark-link:https://github.com/ethanwharris>@ethanwharris</denchmark-link>
  <denchmark-link:https://github.com/jakubczakon>@jakubczakon</denchmark-link>
  <denchmark-link:https://github.com/MattPainter01>@MattPainter01</denchmark-link>
 
 any thoughts?
 		",3.0,srush,2020-02-23T20:55:31Z,"
 		I always assumed the decorator was to stop multiple instantiation - there was some old bug where data loading threads would hang around after each epoch because new data loaders were created and the old threads just carried on - having said that I can't find the issue anywhere
 The IterableDataset stuff at the moment is a bit fragile (mostly hard coded type checks), there might be better ways to deal with it that simplify the above
 If there's some way we can remove the decorator but still only create the dataloader once then that would be a big usability improvement :)
 		",4.0,srush,2020-02-23T21:13:36Z,"
 		agreed. that was the original reason. basically we could refactor to make sure we only call it at time of the epoch beginning.
 i think we needed it before to determine length and some other reasons
 		",MODIFY,0.0,docs\source\hooks.rst,docs\source\hooks.rst,0.0,"11,12,13","11,12,13",,,,,MODIFY,2.0,pl_examples\basic_examples\lightning_module_template.py,pl_examples\basic_examples\lightning_module_template.py,1.0,"208,209,210,211,212,213,214","208,209",MODIFY,3.0,pytorch_lightning\core\decorators.py,pytorch_lightning\core\decorators.py,1.0,"15,16,17","14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32",data_loader._get_data_loader,self,14,32,MODIFY,2.0,pytorch_lightning\core\lightning.py,pytorch_lightning\core\lightning.py,1.0,"880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905",,prepare_data,self,880,905,5.0,srush,2020-02-23T22:02:02Z,"
 		I'm stuck on a couple issues here actually that I can't unwind.
 Main Issue: I don't really understand the semantics of train_dataloader in ddp / tpu training. Is it supposed to be called by only (a) with rank 0 or (b) with all ranks. I would prefer (a) but I need to know so I don't call barriers internally. If it is (b) then I do need to do that, but the semantics are more clear. My assumption had been (b) which works for DDP for me (TPU I'm still stuck).
 Side Issue: It's very difficult to determine ordering. I had been calling training_data() from configure_optimizer, but doing that seems to preempt everything and lead to strange behavior.
 A related issue to this is that loading the data set 8x times on TPU blows up the limited amount of RAM to Colab allows for. It would be nice to avoid this issue.
 		",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,prepare_data,self,208,214,,,,,1.0,"808,809,810",801,optimizer_step,"self,epoch,batch_idx,optimizer,optimizer_idx,second_order_closure",751,816,,,,,,,,MODIFY,11.0,pytorch_lightning\trainer\data_loading.py,pytorch_lightning\trainer\data_loading.py,1.0,"215,220,221,223,224,225,226,227,228,229,230,231,232,235,238","215,216,217,219,220,222,223,224,225,227,228,229,230,231,232,233,235,238,239,240",reset_test_dataloader,"self,model",215,240,1.0,"92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136","93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,122,127,130,133,134,135,136",auto_add_sampler,"self,dataloader,train",92,136,MODIFY,6.0,pytorch_lightning\trainer\evaluation_loop.py,pytorch_lightning\trainer\evaluation_loop.py,1.0,"15,16","15,16",data_loader.inner_fx,self,15,16,1.0,"215,216,217",,reset_val_dataloader,"self,model",215,217,1.0,"273,274,275,276,277,278","261,262,263,264,265,291,292",evaluate,"self,model,dataloaders,max_batches,test",219,300,MODIFY,2.0,pytorch_lightning\trainer\model_hooks.py,pytorch_lightning\trainer\model_hooks.py,1.0,"14,15,16","14,15",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,195,"195,198,201,202,203,204",__dataloader,"self,train",190,206,,,,,,,,1.0,"12,13,15,16,17","11,12,13,14,15,16,17",data_loader,fn,6,17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"176,177,178,179,180,181,182,183,184,185,186,187,188,194,195,196,197,198,199,202,203,204,205","161,167,170,173,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205",init_test_dataloader,"self,model",161,205,is_overriden,"self,f_name",14,20,1.0,"14,15,16","14,15",is_overriden,"self,f_name,model",14,21,MODIFY,9.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,,"1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153",_set_dataloader,"model,dataloader,attribute",1110,1153,1.0,"950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981",,__set_fit_dataloaders,"self,model,train_dataloader,val_dataloaders,test_dataloaders",950,981,1.0,"978,979",,__set_fit_dataloaders.patch_test_dataloader,,978,979,1.0,"858,859",,fit,"self,LightningModule,None,None,None",854,859,1.0,"1064,1065,1066,1069,1070,1074,1075,1076,1077,1090,1097","1013,1014,1015,1048,1055,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119",run_pretrain_routine,"self,LightningModule",1009,1119,1.0,"968,969",,__set_fit_dataloaders.patch_val_dataloader,,968,969,1.0,"82,113",,__init__,"self,LightningLoggerBase,True,ModelCheckpoint,True,EarlyStopping,None,None,float,gradient_clip,int,nb_gpu_nodes,int,str,None,None,None,bool,int,float,int,int,bool,int,int,1,max_nb_epochs,min_nb_epochs,int,int,None,None,float,float,float,0,int,int,add_row_log_interval,None,use_amp,int,bool,str,None,str,nb_sanity_val_steps,int,None,None,None,bool",67,113,1.0,,"852,853",fit,"self,LightningModule,None,None,None",848,853,MODIFY,5.0,pytorch_lightning\trainer\training_loop.py,pytorch_lightning\trainer\training_loop.py,1.0,"227,228,229",229,__init__,self,183,229,1.0,"410,411,412,417,418,422","408,409,414,415,419",run_training_epoch,self,403,488,1.0,"597,598,612,613,614","594,595,596,597,598,612,613",run_training_batch,"self,batch,batch_idx",490,622,1.0,"318,319,320,398","317,318,319",train,self,309,401,MODIFY,0.0,tests\models\__init__.py,tests\models\__init__.py,0.0,"15,16,17,18",,MODIFY,2.0,tests\models\base.py,tests\models\base.py,1.0,"153,154,155,156,157,158,159",158,prepare_data,self,153,159,1.0,166,"161,164,165,166,167,168,169,170,171,175,176",_dataloader,"self,train",161,176,MODIFY,4.0,tests\models\mixins.py,tests\models\mixins.py,1.0,"484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531",,validation_step,"self,batch,batch_idx,dataloader_idx",484,531,1.0,"388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435",,test_step,"self,batch,batch_idx,dataloader_idx",388,435,1.0,"343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384",,test_step,"self,batch,batch_idx",343,384,1.0,"439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480",,validation_step,"self,batch,batch_idx",439,480,,,,,,,,MODIFY,2.0,tests\models\utils.py,tests\models\utils.py,1.0,"76,77,78,79,80",72,MODIFY,2.0,tests\test_cpu_models.py,tests\test_cpu_models.py,1.0,,307,test_tbptt_cpu_model,tmpdir,266,337,1.0,49,49,test_lbfgs_cpu_model,tmpdir,43,58,run_model_test,"trainer_options,model,on_gpu",52,89,MODIFY,2.0,tests\test_gpu_models.py,tests\test_gpu_models.py,MODIFY,6.0,tests\test_restore_models.py,tests\test_restore_models.py,MODIFY,7.0,tests\test_trainer.py,tests\test_trainer.py,1.0,532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"168,169,171,172","168,169",__init__,self,145,172,1.0,"379,382",,evaluation_forward,"self,model,batch,batch_idx,dataloader_idx,test",375,410,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,"214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237",test_ddp_sampler_error,tmpdir,214,237,1.0,"127,128,129,130,131",127,test_cpu_slurm_save_load,tmpdir,100,170,1.0,"56,57,58,59,60",56,test_running_test_pretrained_model_ddp,tmpdir,12,61,1.0,"352,353,354,355,356",348,test_model_saving_loading,tmpdir,328,381,1.0,318,314,test_cpu_restore_training.assert_good_acc,,311,319,test_train_dataloaders_passed_to_fit,tmpdir,527,550,1.0,"558,559,577,578,580,581","570,572,573,581",test_train_val_dataloaders_passed_to_fit,tmpdir,553,581,1.0,"665,666,667,684,689,690,692,693,694,695,696,697","666,671,672,674,675,676,677",test_mixing_of_dataloader_options,tmpdir,660,697,1.0,"412,416","408,412",test_multiple_val_dataloader,tmpdir,383,417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"210,211,212",,reset_test_dataloader,"self,model",210,212,1.0,"304,305,315,316,317,318,322,323,324,325","302,306,360,363",run_evaluation,"self,test",302,373,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"627,628,629,647,648,649,650,652,654,655,656,657","632,633,634,635,638,639,640,641,649",test_multiple_dataloaders_passed_to_fit,tmpdir,622,657,1.0,"589,590,591,609,610,611,614,615,616,617,618,619","599,600,603,604,605,606,614",test_all_dataloaders_passed_to_fit,tmpdir,584,619,1.0,"513,514,516,520","510,514",test_multiple_test_dataloader,tmpdir,488,524,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"958,959",,__set_fit_dataloaders.patch_train_dataloader,,958,959,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"305,306,307",,reset_train_dataloader,"self,model",305,307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"138,144,145,146,147,148,149,152,157,176,177,178,179,180,181,182,183,184,185,186","138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,167,170,173,177,178,179,180,181,182,183,184,185,186",reset_train_dataloader,"self,model",138,186,1.0,"209,212,215,220,221,223,224,225,226,227,228,229,230,231,232,235,238,242,244,245,246","207,209,210,213,214,215,216,217,219,220,222,223,224,225,227,228,229,230,231,232,233,235,238,239,240,241,242,243,244,245,246,247",get_dataloaders,"self,model",207,247,1.0,"116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,144,145,146,147,148,149,152,157","116,122,127,130,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159",init_val_dataloader,"self,model",116,159,1.0,"242,244,245,246,249,251,253,254,256,257,258,260,263,264,265,266,267","242,243,244,245,246,247",request_data_loader,"self,data_loader_fx",242,267,1.0,"188,194,195,196,197,198,199,202,203,204,205,206,209,212","188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,209,210,213",reset_val_dataloader,"self,model",188,213,1.0,"50,51,52,53,54,55,56",,__init__,self,40,56,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,"67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90","69,74",call_prepare_data,"self,model",67,90,1.0,"67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114","60,66,69,74,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114",init_train_dataloader,"self,model",60,114,,,,,,,,1.0,"39,40,41,42,43",39,run_model_test_no_loggers,"trainer_options,model,min_acc",23,49,1.0,251,247,test_dp_resume.assert_good_acc,,243,252,1.0,318,314,test_cpu_restore_training,tmpdir,266,325,1.0,251,247,test_dp_resume,tmpdir,188,263
939,helldragger,2020-02-25T11:51:32Z,2020-02-27T20:54:07Z,logger is NoneType hence doesn't have any experiment or other functionality in a lightning module,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 When trying to use the logging abilities of lightning, I hit a wall, the default and tensorboard loggers both seem to stay uninitialized when calling trainer.fit(model), resulting in crashes everytime I try to log something.
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 Create a lightning module as such
 <denchmark-code>class SimpleRegressor(pl.LightningModule):
     ...
 </denchmark-code>
 
 Use the logger anywhere to get this kind of stacktrace:
 <denchmark-code>d:\Documents\projects\MetaWatch\MetaWatch\notebooks\audio-video-interest\simple_regressor.py in configure_optimizers(self)
     105         #see https://pytorch-lightning.readthedocs.io/en/latest/pytorch_lightning.core.lightning.html#pytorch_lightning.core.lightning.LightningModule.configure_optimizers
     106         # REQUIRED
 --> 107         self.logger.experiment.add_hparams({'hidden_layer_size':self.hidden_layer_size,
     108                                             'linear_layer_size':self.linear_layer_size,
     109                                             'lstm_layers':self.lstm_layers})
 
 AttributeError: 'NoneType' object has no attribute 'experiment'
 </denchmark-code>
 
 <denchmark-h:h4>Code sample</denchmark-h>
 
 <denchmark-code>import pytorch_lightning as pl
 
 class SimpleRegressor(pl.LightningModule):
     def __init__(self, cuda=False):
         super(SimpleRegressor, self).__init__()
         self.logger.experiment.add_hparams({'hidden_layer_size':1})
 </denchmark-code>
 
 <denchmark-h:h3>Expected behavior</denchmark-h>
 
 To log as described in the documentation.
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Microsoft Windows 10 Pro    
 GCC version: Could not collect  
 CMake version: Could not collect
 
 Python version: 3.7
 Is CUDA available: Yes
 CUDA runtime version: Could not collect
 GPU models and configuration: GPU 0: GeForce GTX 970
 Nvidia driver version: 441.12
 cuDNN version: Could not collect
 
 Versions of relevant libraries:
 [pip3] numpy==1.18.1
 [pip3] pytorch-lightning==0.6.0
 [pip3] tinynumpy==1.2.1
 [pip3] torch==1.4.0
 [pip3] torchvision==0.4.1
 [conda] Could not collect
 </denchmark-code>
 
 <denchmark-h:h3>Additional context</denchmark-h>
 
 	",f5e0df390c6e1eaf11ad488e297aa2d383daa177,Ethan Harris,2020-02-27 15:54:06-05:00,MODIFY,0,docs\source\experiment_logging.rst,docs\source\experiment_logging.rst,0.0,"25,31,55,61,79,85,105,111,130,136,154,160,162,163,165,166,167","25,31,55,61,79,85,105,111,130,136,154,160",1.0,helldragger,2020-02-25T11:52:17Z,"
 		Hey, thanks for your contribution! Great first issue!
 		",2.0,helldragger,2020-02-25T19:04:09Z,"
 		Thanks for the issue! The intended way to acheive this is through a hook. When __init__ is called on the LightningModule, the loggers won't have been created yet. I don't think there's any way to change that so we should update the docs to use a hook instead of __init__.
 <denchmark-link:https://github.com/orgs/PyTorchLightning/teams/core-contributors>@PyTorchLightning/core-contributors</denchmark-link>
  any other thoughts on this?
 		",3.0,helldragger,2020-02-26T21:00:09Z,"
 		it also doesn't work in other functions, I tried in the training step, in the configure_optimizers too
 		",4.0,helldragger,2020-02-27T09:09:05Z,"
 		Ok, most of these work on master (i.e. if you install from github) for me - except configure_optimizers.
 I've opened a PR (<denchmark-link:https://github.com/PyTorchLightning/pytorch-lightning/pull/964>#964</denchmark-link>
 ) which fixes that and cleans up the docs a bit.
 		",MODIFY,0.0,pytorch_lightning\loggers\__init__.py,pytorch_lightning\loggers\__init__.py,0.0,"3,4,18,19,20,21,22,23,24,30,61,67,78,79,88","3,17,18,24,55,61,80",,,,,MODIFY,1.0,pytorch_lightning\loggers\base.py,pytorch_lightning\loggers\base.py,1.0,"103,104",,MODIFY,1.0,pytorch_lightning\trainer\trainer.py,pytorch_lightning\trainer\trainer.py,1.0,1071,"1068,1070,1071",run_pretrain_routine,"self,LightningModule",1055,1159,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,__getitem__,"self,int",103,104,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
997,bkkaggle,2020-03-02T02:05:28Z,2020-03-03T02:51:06Z,Precision=16 with TPUs bug,"
 <denchmark-h:h2>🐛 Bug</denchmark-h>
 
 Setting precision=16 when training with a TPU throws an error
 <denchmark-h:h3>To Reproduce</denchmark-h>
 
 see colab: <denchmark-link:https://colab.research.google.com/drive/1s-ZDIqzgKQ1Byf-Lw58RZ8LGgmdB6qjB>https://colab.research.google.com/drive/1s-ZDIqzgKQ1Byf-Lw58RZ8LGgmdB6qjB</denchmark-link>
 
 Relavent stack trace:
 <denchmark-code>Exception in device=TPU:0: str expected, not int
 Traceback (most recent call last):
   File ""/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py"", line 119, in _start_fn
     fn(gindex, *args)
   File ""/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/distrib_parts.py"", line 492, in tpu_train
     os.environ['XLA_USE_BF16'] = 1
   File ""/usr/lib/python3.6/os.py"", line 674, in __setitem__
     value = self.encodevalue(value)
   File ""/usr/lib/python3.6/os.py"", line 744, in encode
     raise TypeError(""str expected, not %s"" % type(value).__name__)
 TypeError: str expected, not int
 </denchmark-code>
 
 To fix this all that should be needed should be casting 1 to a string
 <denchmark-code>    os.environ['XLA_USE_BF16'] = str(1)
 </denchmark-code>
 
 <denchmark-h:h3>Environment</denchmark-h>
 
 <denchmark-code>Collecting environment information...
 PyTorch version: 1.4.0
 Is debug build: No
 CUDA used to build PyTorch: 10.1
 
 OS: Ubuntu 18.04.3 LTS
 GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
 CMake version: version 3.12.0
 
 Python version: 3.6
 Is CUDA available: No
 CUDA runtime version: 10.1.243
 GPU models and configuration: Could not collect
 Nvidia driver version: Could not collect
 cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
 
 Versions of relevant libraries:
 [pip3] numpy==1.17.5
 [pip3] torch==1.4.0
 [pip3] torchsummary==1.5.1
 [pip3] torchtext==0.3.1
 [pip3] torchvision==0.5.0
 [conda] Could not collect
 </denchmark-code>
 
 	",29cbc9e7230dd4c5e5e8e8ff789b838ed4a79e20,Bilal Khan,2020-03-02 21:51:05-05:00,MODIFY,1,pytorch_lightning\trainer\distrib_parts.py,pytorch_lightning\trainer\distrib_parts.py,1.0,492,492,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,tpu_train,"self,tpu_core_idx,model",471,499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
