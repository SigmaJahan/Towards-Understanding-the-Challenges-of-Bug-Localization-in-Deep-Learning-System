BR_id,BRsummary,BRdescription,commit_id,file_new_name
1092,tqdm fails in notebook for versions tqdm&lt; 4.41.0,  üêõ bug    tqdm notebook doesn t have a  method until version          training fails in notebooks if you have a lower version of tqdm     to reproduce    train any model with tqdm       in a colab notebook   expected behavior    attributeerror   tqdm notebook  object has no attribute  reset        ,36274bed49cf40c0d9bdabb3058674879badf1e4,requirements.txt
1114,ReduceLROnPlateau scheduler type check,  üêõ bug    incorrect type check for scheduler of class reducelronplateau         pytorch lightning pytorch lightning trainer trainer py               line          in                        isinstance              i believe  this check   isinstance   must look like this   isinstance   to reproduce    steps to reproduce the behavior     create a scheduler of type optim lr scheduler reducelronplateau in the configure optimizers method of a lightningmodule class   return an optimizer and scheduler from this method  place them in lists  return  optimizer    scheduler    execute the trainer fit    put a break point here        pytorch lightning pytorch lightning trainer trainer py               line          in                        scheduler  reduce on plateau                   make sure that the condition is never true      ,384e124490f7a629dc677fc5b658b69afade0a04,CHANGELOG.md
1116,Wandb logger doesn't upload saved model checkpoint for final epoch,  üêõ bug    when training a model on the tpu and using the wandb logger  the checkpoint for the last epoch trained doesn t get uploaded to wandb   to reproduce    colab notebook     jowtt     ,a707d4bea1a78a98265fd1ea5b7a7a6cadc37fb9,CHANGELOG.md
1119,Checkpoint fails in single node multi-GPU mode using  DDP,  üêõ bug    checkpoint fails in single node multi gpu mode using  ddp   to reproduce    python pl examples basic examples gpu template py   distributed backend ddp   gpus    epoch                  s  l home xz   envs x lib     multiprocessing semaphore tracker py    userwarning  semaphore tracker  there appear to be   leaked semaphores to clean up at shutdown                                                                                                                                                                                                                     len    traceback       file  gpu template py   line    in       main     file  gpu template py   line    in main      trainer fit     file   home xz   envs x lib     site packages pytorch lightning trainer trainer py   line    in fit      mp spawn      file   home xz   envs x lib     site packages torch multiprocessing spawn py   line    in spawn      while not spawn context join      file   home xz   envs x lib     site pa,b4d4e489bf413ebf3288d29c5905d2292ce18d58,pytorch_lightning\callbacks\model_checkpoint.py
1131,Better message when DataLoader is wrong,  on the verge between bug and improvement   there was a bug in my validation dataloader and was returning irrelevant staff  accidentally the length was     probably an edge case combination  the error i was getting during the validation sanity check was quite cryptic   traceback       file  unet waveprop py   line    in       trainer fit     file   mnt rds home code pytorch lightning pytorch lightning trainer trainer py   line    in fit      self run pretrain routine     file   mnt rds home code pytorch lightning pytorch lightning trainer trainer py   line    in run pretrain routine      false     file   mnt rds home code pytorch lightning pytorch lightning trainer evaluation loop py   line    in evaluate      eval results   model validation epoch end     file  unet waveprop py   line    in validation epoch end      avg loss   torch stack  mean   runtimeerror  stack expects a non empty tensorlist      i had to go through the code of pytorch lightning for few hours to understand what w,2ccc7456ca421afcdfba0c4482635c99e2593f70,CHANGELOG.md
1139,Can't cast Trainer automatically generated args to their required types,  ‚ùì questions and help    what is your question     i m not sure  that this is a bug  so i put it like a question   the problem is  if i want to add the trainer arguments to my custom argumentparser object  i call the add argparse args trainer classmethod  but this method doesn t cast the trainer arguments to their required types   it forces me to cast the arguments by myself  like so   trainer args update                           accumulate grad batches   int                 train percent check   float                 val percent check   float                 val check interval   int                 track grad norm   int                 max epochs   int                 precision   int                 gradient clip val   float                  and after that  i can pass updated arguments to the trainer   trainer   pytorch lightning trainer             trainer args     and i can t find a central place  where the trainer handles an automatically generated arguments     what have you tri,ced662fc2790058f5a55ca20244b31003c970ee5,CHANGELOG.md
1143,`use_amp` is broken in 0.7.0,  i see that use amp is deprecated but since the trainer still accepts it as an argument  i believe this is still a bug   the issue is that the hook that deals with this   specifically checks if   however  the trainer does not set  if    proposed solution   if use amp is passed in  set precision     and raise a deprecation warning    ,4ed3027309fe1882554e9b7ffe33f1aa92c88106,CHANGELOG.md
1155,No validation checks when overfit_pct is set,  üêõ bug    when setting the overfit pct to any value between   and     in trainer  the validation checks are disabled   to reproduce    i have worked on a minimal example to reproduce the bug   import pytorch lightning as pl  import torch    class dataset          def   init              super    init             self input dim   input dim          self output dim   output dim        def   getitem              x   torch rand           y   torch randint            return x  y        def   len              return      class model          def   init              super    init             self layer   torch nn linear           self dataset   dataset         def forward            yhat   torch softmax                return f nll loss         def train dataloader            return torch utils data dataloader         def configure optimizers            return torch optim adam   lr             def training step            loss   self forward           return   loss   loss   log     loss   los,d735055e6fb6225ad11c566e3711888c0cb4a21e,pytorch_lightning\trainer\__init__.py
1156,ReduceLROnPlateau does not recognise val_loss despite progress_bar dict,  üêõ bug    when training my model  i get the following message     file  c  users luc   envs pytorch lib site packages pytorch lightning trainer training loop py   line    in train      raise misconfigurationexception   pytorch lightning utilities debugging misconfigurationexception  reducelronplateau conditioned on metric val loss which is not available  available metrics are  loss      ihis is similar to    instance  but i definitely return a progress bar dict with a val loss key in it     code sample      def training step           z  y true   batch         y pred   self forward          loss val   self loss function          return   loss   loss val sqrt         def validation step           z  y true   batch         lr   torch tensor          y pred   self forward          loss val   self loss function          return   val loss   loss val sqrt    lr   lr        def validation epoch end           val loss mean   torch stack  mean          lr   outputs      lr           logs     v,711892a0a293f7c7f951eba0907e1c0ccd2b37d8,docs\source\optimizers.rst
1161,multi-gpu ddp calls validation and testing loops too many times,  when using ddp with multiple gpus  each validation and test loop is called with the entire validation dataset for each gpu   expected behavior is that the dataset is divided appropriately across the gpus   i am using current master    ubuntu      cuda      python        pytorch      venv environment   the problem appears to be in auto add sampler  in data loading py  it does not create a distributedsampler for validation or test datasets    ,6dfe9951e132bdc9896926557138e7a21d4cd000,pytorch_lightning\trainer\data_loading.py
1181,Additional dataloader created and discarded when training with reload_dataloaders_every_epoch,  üêõ bug    i am training with reload dataloaders every epoch and i ve noticed it instantiates an extra dataloader before training for which nothing is run  this is an issue for me as i am training with chunks that get loaded every epoch and it is messing with the order i load them in especially if i reload a checkpoint  it would be an issue for people that generate a new dataset every epoch as they waste computation  the tqdm bar also keeps the information of the first  discarded dataloader        to reproduce    run the code sample below  which runs for one epoch and displays a message every time a dataloader is created   a dataloader gets instantiated a first time line   in training loop py outside of the epoch loop  that s the usual time it gets instantiated when not reloading every epoch  then when using reload dataloaders every epoch another one is created at the start of every epoch line    inside the loop  so for the first epoch there s an extra one   code sample    import torch,04935ea7184a50d535af96dd85a58fdc43a659b8,CHANGELOG.md
1201,Early stopping not working on 0.7.1,  üêõ bug    early stopping does not work anymore  when i downgrade from       or the current dev version to       early stopping works again  with the same code   code sample    def main        if hparams early stopping     yes            early stopping   earlystopping               monitor  batch mean absolute loss                min delta hparams min delta               patience hparams patience               mode  min                  else           early stopping   false        model   memorytest       trainer   pl trainer           val percent check             early stop callback early stopping           default save path src settings log dir           max epochs hparams epochs               trainer fit       class memorytest          main testing unit for experiments on recurrent cells      def   init              super    init             self predict col   hp predict col          self n datasamples   hp n datasamples          self dataset   hp dataset          if self dataset i,1aba411da96ed95419d13ec1f86a0d38a232f73e,CHANGELOG.md
1213,Testing in dp mode uses only one of the GPUs,  üêõ bug    to reproduce    steps to reproduce the behavior   run a test without training      code sample    modified from the conference seed repo  trainer   trainer               gpus                    distributed backend  dp               trainer test   expected behavior    environment      pl version         pytorch version         os    ubuntu  how you installed pytorch    pip  build command you used     python version       cuda cudnn version       gpu models and configuration   any other relevant information     additional context     ,c869dd8b8f6301f3726df84535a3da4e9acf04ec,CHANGELOG.md
122,Fix appveyor build,  windows support is not a priority  if the badge can be fixed today we ll keep appveyor  otherwise we ll drop it from the project    borda     ,6f1d2c45fe72d7a8a637290c2e78973deb1637e0,appveyor.yml
1223,gan.py multi-gpu running problems,  running gan py   example with trainer  causes two types of error     if trainer     exception has occurred  attributeerror   nonetype  object has no attribute  detach     file   home user gan py   line    in training step      self discriminator    fake         if trainer       in   lightling logs one run creates two folders    and    exception caused   file   opt   envs ctln gan lib     site packages pytorch lightning callbacks model checkpoint py   line    in  del model  os remove   filenotfounderror   errno    no such file or directory    home user pyproj dcgan lightning logs   checkpoints epoch   ckpt     it seems that each subprocess tries to create its own checkpoints and delete not ctrated one   environment version     python        pytorch        pytorch lightning         ,55fdfe384537e4d43e7397306ba001ffc3474322,pl_examples\domain_templates\generative_adversarial_net.py
1236,AdvancedProfiler error,  hi  as others have pointed out  the profiler doesn t seem to work    and trying out the advancedprofiler as in   lightning readthedocs io en latest profiler html   like   from pytorch lightning profiler import advancedprofiler      profiler   advancedprofiler       trainer   trainer       gives me the following error   validation sanity check              s              traceback       file   users sdumitre work style training py   line    in       main     file   users sdumitre work style training py   line    in main      trainer fit     file   users sdumitre virtual   lib     site packages pytorch lightning trainer trainer py   line    in fit      self run pretrain routine     file   users sdumitre virtual   lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine               callback metrics      self process output     file   users sdumitre virtual   lib     site packages pytorch lightning trainer logging py   line    in process output      ,54507f417eaf3317a798b3303c398152a4e35a18,pytorch_lightning\__init__.py
1262,incorrect run on the test set with overwritten validation_end and test_epoch_end,  üêõ bug    if i override validation end and test epoch end  trainerevaluationloopmixin evaluate works incorrectly on the test set  suppose we override  and   but not  and      suppose i run   consider lines     in evaluation loop py    then we have   so the first  block is executed  that is   but look at the second  and its   we have   hence the  of the second  will also be executed  that is   and we will have validation results recorder as test results  which is a mistake   this problem is present in the commit      and the inverse problem  which happens if we override only  and  is present in          ,ebd9fc9530242e1c9b5f3093dc62ceb4185735b0,CHANGELOG.md
1264,Multiple undesired checkpoints created during single epoch,  üêõ bug    thanks for the great project  when i sent custom modelcheckpoint to trainer and hoping to get one checkpoint each epoch  the trainer eventually produced a lot of versioned checkpoints within a single epoch  wasting lots of disk space and were causing confusion  an example is shown as below   checkpoints   ‚îî‚îÄ‚îÄ gan      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      ‚îú‚îÄ‚îÄ   ckpt      the minimal code i used for training     checkpoint callback   modelcheckpoint           filepath os path join                gan                experiment name                      save top k              period                     trainer   trainer           checkpoint callback checkpoint callback                 to reproduce    steps to reproduce the behavior     clone the repo    git clone   ai git  git checkout      run ,09167efdb59e1be8ffe9ff7010393bff084390be,pl_examples\multi_node_examples\multi_node_ddp_demo.py
1290,bug(logger): wandb fails on sweep,  üêõ bug    when using wandb sweeps for hyperparameters search  i get this error     wandb  error attempted to change value of key  dropout std  from     to        the reason is i ran   wandb logger log hyperparams   which i guess has some problem with floating point numbers in high accuracy    ,f3d139e90f9212813c4f5e6de777bdef9dfe7635,CHANGELOG.md
1306,RuntimeError: Unimplemented backend XLA on TPU,  üêõ bug     raised for  line in  file when running mnist on tpu  i think it was introduced in       to reproduce    steps to reproduce the behavior     go to mnist on tpus  run all  scroll down to trainer  see error    traceback       file   usr local lib     dist packages torch xla distributed xla multiprocessing py   line    in  start fn      fn     file   usr local lib     dist packages torch xla distributed xla multiprocessing py   line    in  start fn      fn     file   usr local lib     dist packages pytorch lightning trainer distrib parts py   line    in tpu train      self run pretrain routine     file   usr local lib     dist packages pytorch lightning trainer distrib parts py   line    in tpu train      self run pretrain routine     file   usr local lib     dist packages pytorch lightning trainer distrib parts py   line    in tpu train      self run pretrain routine     file   usr local lib     dist packages torch xla distributed xla multiprocessing py   line    in  start fn ,b8ff9bc1d242a18f5e7147f34d63f43fcdd0e50a,CHANGELOG.md
1322,Training loop temporarily hangs after every 4 steps,  i am porting some of my code to pytorch lightning  and everything seems to work fine  however  for some reason after every   training steps i see some temporary hanging    which is severely slowing down my overall training time  am i missing some obvious configuration   this is my trainer configuration       trainer   pl trainer           gpus            num nodes             distributed backend  ddp            checkpoint callback false           max epochs             max steps none           progress bar refresh rate             check val every n epoch             val check interval               gradient clip val               log save interval             num sanity val steps             amp level                   ,b18accc64ccd24095c11fdbd64cc924456134592,CHANGELOG.md
1335,Trainer DDP should invoke load_spawn_weights() only in proc_rank == 0,  üêõ bug    trainer ddp load spawn weights should happen only in proc rank      since only in this process   save spawn weights actually saves checkpoint  to reproduce    steps to reproduce the behavior     setup two node cluster   set slurm nodeid on each node      on node   and     on node     run the script python app py on each node   see stdout on the node       traceback       file  app py   line    in       main      pylint  disable no value for parameter    file  app py   line    in main       trainer fit     file   home ubuntu   envs nightly pt lib     site packages pytorch lightning trainer trainer py   line    in fit      self load spawn weights     file   home ubuntu   envs nightly pt lib     site packages pytorch lightning trainer distrib data parallel py   line    in load spawn weights      loaded model   original model   class   load from checkpoint     file   home ubuntu   envs nightly pt lib     site packages pytorch lightning core lightning py   line    in load from ch,9754c5da55059dd89cf0a4fd582fe5df9449bbe5,pytorch_lightning\trainer\distrib_data_parallel.py
1366,ModelCheckpoint tries to remove already removed checkpoint in DDP mode,  üêõ bug    when training in ddp mode with modelcheckpoint callback  the train process fails  when modelcheckpoint callback tries to remove previous checkpoint  i assume that it was already deleted by another process   to reproduce    steps to reproduce the behavior   run training with  ddp  backend and modelcheckpoint callback with save top k  some number     file   home myuser   lib     site packages torch multiprocessing spawn py   line    in  wrap                                                                                                       fn                                                                                     file   home myuser   lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routine                                                                                                             file   home myuser   lib     site packages pytorch lightning trainer trainer py   line    in run pre,58a467dd68b157fdba8824a437dbaf698ad88569,CHANGELOG.md
1375,Tensorboard logger error: lightning_logs directory not exists in multi-node DDP on nodes with rank != 0,  üêõ bug    in multi node ddp train mode on all nodes except rank   errors appears at the start of the training caused by accessing lightning logs directory in tensorboard logger which is not exist at the moment   to reproduce    steps to reproduce the behavior     setup multi node cluster    set environment variables on each node     export master addr   export master port    export rank   export slurm nodeid   export world size         install dependencies     pip install torch torchvision hydra core pytorch lightning        copy app y and conf yaml to each node  run script on each node    python app py        see the error     exception        process   terminated with the following error   traceback       file   home ubuntu   envs nightly pt lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   home ubuntu   envs nightly pt lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routi,495ffbd028ae860528c719544cf0409b41d5ef5a,CHANGELOG.md
138,val_dataloader is not optional in distributed_backend='ddp',  describe the bug  val dataloader function is kept optional but a line in the code does not check for  if self val dataloader is not none   which leads to the following error   file   misc   fergusgroup ananya pyenv     lib     site packages pytorch lightning models trainer py   line    in get dataloaders  for dataloader in self val dataloader    typeerror   nonetype  object is not iterable  file  models trainer py  line    to reproduce  steps to reproduce the behavior       not write the optional function  val dataloader      use the following trainer configuration   trainer   trainer   experiment exp   checkpoint callback checkpoint callback   distributed backend  ddp    gpus args gpu id   amp level       use amp true   max nb epochs args epochs   progress bar true         expected behavior  code should ignore  all  for dataloader in self val dataloader   check if self val dataloader is none   environment     pytorch        cuda      test tube          pytorch lightning           ,5b694c7e0ec5f83bf4ec93860d91a48757351265,pytorch_lightning\models\trainer.py
1388,Use isinstance() instead of type() in trainer.distrib_parts.check_gpus_data_type,  üêõ bug    when instantiating a trainer object  it makes sense to be able to pass a subclass of list   ideally  this would be something even more general like collections abc sequence  but i m not too familiar with lightning s codebase and that change would have a greater likelihood of breaking things   to reproduce    instantiate a trainer with the gpus parameter being a subclass of list   code sample        from pytorch lightning import trainer      class mylist            pass            gpus   mylist       t   trainer   this produces  traceback       file     line    in     file   opt anaconda   envs ai lib     site packages pytorch lightning trainer trainer py   line    in   init        self data parallel device ids   parse gpu ids     file   opt anaconda   envs ai lib     site packages pytorch lightning trainer distrib parts py   line    in parse gpu ids      check gpus data type     file   opt anaconda   envs ai lib     site packages pytorch lightning trainer distrib parts py   ,a22a8142ac65668781a6e6f76d3c4e55ea7c249a,pytorch_lightning\trainer\distrib_parts.py
142,AttributeError: 'xxx' object has no attribute 'tng_dataloader' continued...,  this is the same issue as mentioned in        describe the bug  whenever a  property raises an attributeerror  it will be looked up in   getattr      and then the original error message is lost  and lightingmodule inherits   getattr   from torch nn module   this makes debugging difficult as you lose line number and you get an error message that is misleading   the attribute exists  look at the minimal example in the next section to see what i mean   to reproduce  import torch    class foo         property      def bar            return torch does not exist    raises an attributeerror     foo  bar      the output produced   traceback       file  c  minimal py   line    in       foo  bar    file  c  program files   lib site packages torch nn modules module py   line    in   getattr        type    name    name    attributeerror   foo  object has no attribute  bar       the trace says the  attribute does not exist  but it does  it is the  attribute that is nonexistent  now if you look in,b31539f62e60e1ad370214e05003c214298d2431,pytorch_lightning\root_module\decorators.py
1421,run_training_batch breaks on None batch or -1 response from on_batch_start (in new 0.7.2 release),  üêõ bug    run training batch now is supposed to return a   tuple in        however  there are two places where it still returns a   tuple  which will cause the program to crash  saying  valueerror  not enough values to unpack        if batch is none       return    grad norm dic             if response             return     grad norm dic          vs  the standard return  return    grad norm dic  all log metrics  batch output  to reproduce    just return    from on batch start   ,b2707c9b2ebeac03f19a3939df9432ac8859d894,CHANGELOG.md
1422,Not auto add DistributedSampler for DDP training,  üêõ bug    in      even if we don t set sampler  pytorch lightning will not add  distributedsampler for us   to reproduce    the reason is in pytorch  if we don t set sampler   pytorch will add a sampler for us   in pytorch s dataloader py            if sampler is none     give default samplers              if self  dataset kind     datasetkind iterable                     see note   custom samplers and iterabledataset                    sampler    infiniteconstantsampler               else     map style                  if shuffle                       sampler   randomsampler                   else                       sampler   sequentialsampler       but in pytorch lightning we check whether sampler is none to  decide to add sampler  in  data loading py funciton auto add sampler           no sampler added   dataloader sampler is none      because pytorch have default sampler for us  which is not none  pytorch lighting will not automatically add sampler    ,21a1972921809ea04ab1e4c657e326dfecd5e352,pytorch_lightning\trainer\data_loading.py
1435,Test metrics is not being reported to TensorBoard since 0.7.2,  üêõ bug    to reproduce    steps to reproduce the behavior         code sample    please see the colab above   expected behavior    the test metrics should be reported   environment    the colab environment   cuda    gpu    available            false   version                   packages    numpy                        pytorch debug        false   pytorch version              pytorch lightning            tensorboard                  tqdm                        system    os                   linux   architecture             processor                python                       version                 smp wed feb         pst        additional context    regression from         ,1f685c2882d2bb0755a7ab0ed6819b008780948e,CHANGELOG.md
1442,Failed to configure_optimizers from dictionary without lr_scheduler field presented,  üêõ bug    optimizer is failed to be configured from the dictionary without lr sheduler field   consider an example of the module configure optimizers method           def configure optimizers                config                       optimizer   torch optim sgd   lr                                  return config  then  we run a simple trainer       trainer options   dict       trainer   trainer           trainer fit   and we fail with an error   unboundlocalerror  local variable  lr schedulers  referenced before assignment      i believe  that the reason is that lr schedulers local variable is not determined here         pytorch lightning pytorch lightning trainer optimizers py              lines   to          in                          single dictionary          elif isinstance            optimizer   optim conf  optimizer            lr scheduler   optim conf get           if lr scheduler           lr schedulers   self configure schedulers           return  optimizer   lr scheduler,4c34d16a349bc96a717be5674606c2577fab8946,CHANGELOG.md
1447,"Test results not logged to tensorboard, since 0.7.3, this worked in 0.7.1",  üêõ bug    test results are not logged to tensorboard  with the exact same code  version       logged them flawlessly  also  with the exact same code  validation and train results are logged  so i assumed the issue is with the test   to reproduce    run test  step with a model that has tensorboard logging   logger   tensorboardlogger   code sample    def validation step                   return   val loss   loss     def validation epoch end        avg loss   torch stack  mean       tensorboard logs     val loss   avg loss       return   avg val loss   avg loss   log   tensorboard logs   this works     def test step                              return   test loss   loss     def test epoch end        avg loss   torch stack  mean       tensorboard logs     mse   avg loss       print    avg loss      this works                                return   avg test loss   avg loss   log   tensorboard logs   the issue might be here      expected behavior    the expected behavior is for tensorboar,b3fe17ddeb00fb66db08e5fc7414591662ebd440,.github\workflows\ci-testing.yml
1468,Mixing hparams and arguments in LightningModule.__init__() crashes load_from_checkpoint(),  üêõ bug    right now  if you initialize a lightning module with a mixture of a namespace   as well as additional arguments    load from checkpoint can t recover   to reproduce    create a lightningmodule as follows   class model          def   init                    self hparams   hparams                self train dset  self val dset   train dataset  val dataset                     run training  then try to restore from checkpoint  via   nn   model restore from checkpoint   expected behavior    ideally  you d just be able to pass in the additional arguments   and everything would work    ,3c6f856f232ccd124ca90621cdda8094bae6e332,CHANGELOG.md
1476,Learning rate scheduler should step after each optimizer step,  üêõ bug    i m not sure that this is a bug or if it is a deliberate design decision  but right now the learning rate schedule gets updated at every  step  which actually corresponds to every forward pass  i think a more standard implementation would have the learning rate scheduler  step  interval correspond to being updated every backwards pass  this has caused me a lot of problems with instability as i did not realize that using standard learning rate warmups of say   steps would actually only warm up for   steps if i set accumulate grad batches      ,0203938af8f69a19b7e0264f18e03d543d86e0e9,CHANGELOG.md
1485,wandb logger 'global_step' affects other logger,  üêõ bug    the wandb logger adds a  global step  to the metric dict which appears in all other loggers    only the wandb logger is adding  global step  to metric and i think it is not necessary  another side effect of that is  that  global step  is also added to empty dicts which then are logged and resulting to strange graphs like this       or this      i also wrote a simple logger class to print out metrics  i got this output   step       global step       step       global step              step       global step       step       global step       step       val mse         train mse         global step       step       global step       step       global step              step       global step       step       global step       step       val mse         train mse         global step       step       global step       step       global step            step       global step       step       global step       step       val mse         train mse         global step                ,152a2eb30ce82deefdb738b81fda66a9c218ed76,CHANGELOG.md
1503,Do not configure python logging,  üêõ bug    pytorch lightning right now configures the python logging module  here     this is generally not recommended when writing a library as it makes it difficult for users to modify logging format  see python docs    stack overflow post     i would suggest deleting the configuration line    ,1df0d2dc97e20b9646cbe0f42060a57f99f397fc,pytorch_lightning\__init__.py
1506,0.7.3 breaks reusable dataloaders in DDP,  üêõ bug          breaks reusable dataloaders in ddp  traceback       file   opt conda lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   opt conda lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routine     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in train      self reset train dataloader     file   opt conda lib     site packages pytorch lightning trainer data loading py   line    in reset train dataloader      self train dataloader   self auto add sampler     file   opt conda lib     site packages pytorch lightning trainer data loading py   line    in auto add sampler      dataloader   type      file     main dataset py   line    in   init        super    init     typeerror    init    got an unexp,c71bd73acb5a89bb2a8ff44beab37fd2ceba352b,CHANGELOG.md
1507,After update from 0.5.x to 0.7.3 merge_dicts #1278 sometimes breaks training,  üêõ bug    after i updated from a quite old lightning version to the newest one  i sometimes get a typeerror from merge dicts  i guess it s related to this mr        this type error is deterministic  meaning it always occurs at the same global step during training  it somehow seems to be related to val check interval as well  for some data changing this value leads to no error  but for other datasets this does not work  also this only happens during training step  i suspect the training step after validating   to reproduce    steps to reproduce the behavior   i have no idea   file   home sebastian  cache pypoetry virtualenvs forgerydetection       lib     site packages pytorch lightning trainer training loop py   line    in train      self run training epoch     file   home sebastian  cache pypoetry virtualenvs forgerydetection       lib     site packages pytorch lightning trainer training loop py   line    in run training epoch      self log metrics     file   home sebastian  cache py,edb8d7a23cac91d607ab97c0adcbb815780936ac,CHANGELOG.md
1510,Memory (CPU and GPU) leaks during the 1st epoch,  üêõ bug    hello   this memory leak occurs during the first epoch  if one has a large epoch time    the oom error will come  it s interesting  that in precision   mode  it leaks out on the gpu and the cpu both  if we switch amp optimization off    the leak goes only on the cpu   also  i checked the number of tensors  which are tracked by the garbage collector  and it appeared to be linearly increasing during the first epoch  and then    it falls to the initial value and begins increasing again   let me provide the plots       experiment    amp level      precision                          experiment    amp level none  precision none                        as you can see  both cases have a cpu leak  the  amp  case also has a gpu leak   also  it s clear  that such leaky behavior stops when the   epoch starts   on these plots  the   epoch starts on the    saw claw  of the  num of tensors  plot   also  there is another observation  the speed of tensors number increasing is    and this is m,ae2e14e3ed45e23dbe2868017b630fa7be9e5604,pl_examples\basic_examples\cpu_template.py
1520,"Bug and question about logging -- missing epoch, validation before train?",  üêõ bug    first  the clear bug  in trainerloggingmixin log metrics  the epoch is added to the metrics variable   which is never accessed again  that should be to scalar metrics  shouldn t it   second  a question  i implemented a very primitive logger   and logging to it  i don t get training results when the first epoch ends until after the first epoch validation step  and consequently don t get training metrics from the last epochs  see code and sample output below  does this make sense   to reproduce    add the following code to a lightning module and run a trainer with the following logger   use gpu   int    print logger   printlogger   trainer   trainer   trainer fit   code sample    minimal logging in the lightningmodule       def training epoch end            avg loss   torch stack  mean           avg acc   torch stack  mean           logs   dict           return dict         def validation epoch end            avg loss   torch stack  mean           avg acc   torch stack  mean  ,fe2b6666e0c3a47992860a2200ab40ae1c2ea6c7,pytorch_lightning\trainer\logging.py
1522,Performance drop when activating gradient clipping,  hello all   i experienced a substantial drop in computation time when activating gradient clipping     i noticed that in the current implementation of the clipping gradient method in pytorch lightning trainer training tricks py redundant computations are made by first computing the   norm and second squaring this result  which could be shortened by computing the sum of squares directly  this saves one square root and squaring operation per parameter set   best   jonas  environment    cuda    gpu    available            false   version              none  packages    numpy                        pytorch debug        false   pytorch version              pytorch lightning          dev   tensorboard                  tqdm                        system    os                   darwin   architecture             processor                python                       version              darwin kernel version        wed mar          pst    root xnu                additional context    i trained ,e02146943d3373020b7fa6e8acc31dc18b4201e4,pytorch_lightning\trainer\training_tricks.py
1538,`num_tpu_cores=8` does not work on kaggle,  üêõ bug    when i try to train a model on kaggle tpu s with num tpu cores set to    i receive an error exception  process   terminated with exit code     would be great if this worked on kaggle   to reproduce    steps to reproduce the behavior     run this notebook     on tpu with pytorch lightning                                                                                 exception                                 traceback     in             most basic trainer  uses good defaults            trainer   pl trainer           trainer fit      opt conda lib     site packages pytorch lightning trainer trainer py in fit                               train                    xmp spawn   nprocs self num tpu cores  start method start method                               load weights if not interrupted     opt conda lib     site packages torch xla distributed xla multiprocessing py in spawn                 join join                 daemon daemon                 start method start method      ,831842972f7e2d25ae3a376d5584748c3054f899,CHANGELOG.md
154,transfer_to_batch_gpu returns null when input has primitives,  describe the bug  when passing a batch such as   batch   list     the list of ints won t be returned correctly  additional context  fix should add a return of the item it no condition matches   ,55a804b7cfb9b2376ccaa1253a966dcaa9b6ab07,pytorch_lightning\models\trainer.py
1540,DDP on GPUs invalid ordinal,  üêõ bug    on latest version    training with ddp backend on gpus   results in a cuda error  invalid device ordinal    to reproduce    steps to reproduce the behavior     run any lightning module with ddp backend on more than   gpu with gpu indexes that do not start from    see error    info lightning gpu available  true  used  true  info lightning visible gpus       warning lightning slurm nodeid or node rank environment variable is not defined  set as     warning lightning master addr environment variable is not defined  set as localhost  warning lightning slurm nodeid or node rank environment variable is not defined  set as     warning lightning master addr environment variable is not defined  set as localhost  thcudacheck fail file  pytorch torch csrc cuda module cpp line   error     invalid device ordinal  thcudacheck fail file  pytorch torch csrc cuda module cpp line   error     invalid device ordinal  traceback       file  bin run py   line    in       run     file  bin run py  ,bafdeca42f746aac59b4f0c1103264d7bff556db,pytorch_lightning\trainer\distrib_data_parallel.py
1546,LightningTemplateModel is broken,  üêõ bug     has no implemented  method  see here    to reproduce    just try to run test evaluation on mnist example  expected behavoir    it should not crash   ,210cd657dd0f83069b8c6abc7402508f354668b3,CHANGELOG.md
1547,Metric aggragation is broken for LoggerCollection,  üêõ bug    after changes in      it is now not possible to log testing metrics after traning while using several loggers   to reproduce    say we want to run a minst example   and also want to add a change   log testing metrics after training  for that we define a callback  class testcallback        def on train end              note that it would crash if you don t pass the  pl module           trainer test       and pass it to trainer callbacks argument   we would also like to use several loggers to track all metrics  say mlflowlogger and tensorboardlogger  for this we create instances of these loggers and pass them into trainer in a list   expected behavior    testing metrics should be logged   but they don t as there s no final aggregation when our logger is a loggercollection  additional context    in my opinion  the logic in agg and log metrics  and  finalize agg metrics is hard to follow  so i d be happy if user could choose plain old log metrics which worked nicely    ,458d3e210e2da10482d97a996708731b8b0fabae,CHANGELOG.md
1566,Batch being moved to gpu repeatedly with multiple optimizers and single gpu training,  if you have multiple optimizers  then transfer batch to gpu winds up getting called once per opt idx  and the batch is copied each time via copy copy  in training forward  why copy the batch when there is only a single gpu  by removing the copy copy  my gan model moves from     s to     s  pretty significant speedup    ,41b6cbb3ca8a3a43e091d7f0de4d0184a8870d19,pytorch_lightning\trainer\training_loop.py
157,Recursive device conversion of tuple,  this bug report related to      and        describe the bug  when passing a batch such as   batch      transfer batch to gpu raises the error  typeerror   tuple  object does not support item assignment   i found that above error caused by concatenation of two conditions  isinstance  or isinstance      lightning blob   pytorch lightning models trainer py        expected behavior  a batch  batch      should be  batch      additional context  i already fixed the bug and will submit pr soon    ,4a0b56755c7239e6a85b32a3d3e97cf7f9e39044,pytorch_lightning\models\trainer.py
1570,Trainer.add_argparse_args bool type,  üêõ bug    the boolean arguments added using trainer add argparse args always evaluate to true  this is caused by the following lines of the add argparse args fucntion   if isinstance        def allowed type             return bool    because allowed type is the actual data type and not an instance of bool  isinstance  is equal to false  bool is bool is equal true   ,545b38ec5f1b1de7aaabec7a6cf4f2f4d7893b71,CHANGELOG.md
1588,Named converted to regular tuples when sent to the gpu.,  üêõ bug    named tuples returned from dataset get converted to regular tuples when sent to the gpu   this happens because isinstance  evaluates to true in distrib parts py        pytorch lightning pytorch lightning trainer distrib parts py               line          in                        if isinstance               to reproduce    import pytorch lightning as pl  from collections import namedtuple  import torch  import numpy    namedtupledemoinput   namedtuple     class namedtupledemodataset       def   len              return          def   getitem                  numpy random uniform               numpy random uniform           y               numpy random normal           return namedtupledemoinput     class weightedsum        def   init              super    init             self a   torch nn parameter            self b   torch nn parameter          def forward            return self a       self b        class namedtupledemo          def   init              super    init     ,3eac6cfd4fbbc4d13f4e93f6d90f8ee5302c421e,pytorch_lightning\trainer\distrib_parts.py
1620,horovod cicd tests are failing on ubuntu 18.04 python 3.6 latest,  üêõ bug    the failed job    lightning runs      we see two errors     runtimeerror  failed to determine if nccl support has been built  run again with   verbose for more details   importerror   opt hostedtoolcache python         lib     site packages horovod torch   cpython     linux gnu so  undefined symbol       my hunch is that both are caused by the same horovod compilation issue   another thing to note is that the same tests are passing on ubuntu     python     minimal    tgaddair   maybe you have an idea   to reproduce    run the cicd test suite    ,813e37916d9b17224be3d6c4d1672876bfb88a54,.github\workflows\ci-testing.yml
1628,"Bug in DDP, but not DP modes.",  pytorch      in      pytorch lightning   version                                                                                                                                                                                                                                                                    out                    in dp everything works   in ddp fails with    file   home vladimir   envs solaris lib     multiprocessing popen fork py   line    in   init        self  launch     file   home vladimir   envs solaris lib     multiprocessing popen spawn posix py   line    in  launch      reduction dump     file   home vladimir   envs solaris lib     multiprocessing reduction py   line    in dump      forkingpickler  dump    pickle picklingerror  can t pickle   it s not the same object as torch  c  variablefunctions       ,9604d7bf8994615431af9d86c8de154677237b75,pytorch_lightning\callbacks\model_checkpoint.py
1665,Trainer add args doesn't add default root dir,  üêõ bug      when using parser   trainer add argparse args   it s supposed to put all trainer s arguments in the argparse with default values  though currently it doesn t add default root dir and you get the error      namespace  object has no attribute  default root dir       it does add default save path which is deprecated   to reproduce    code sample    import argparse  from pytorch lightning import trainer    parser   argparse argumentparser   parser   trainer add argparse args   args   parser parse args     print   a similar unit test could also be made  if not there already   environment      cuda             gpu                     geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti                    geforce rtx   ti            available          true            version                   packages  ,9059d21042a5f18fcb18a1792a901e8e62a3b61a,CHANGELOG.md
1683,NeptuneLogger doesn't work with distributed_backend='ddp',  üêõ bug    when using neptunelogger with distributed backend  ddp  and running it on a single node with two gpus  i find an error like this   traceback       file  pl py   line    in       main     file  pl py   line    in main      trainer fit     file   home hirune   envs panda lib     site packages pytorch lightning trainer trainer py   line    in fit      mp spawn      file   home hirune   envs panda lib     site packages torch multiprocessing spawn py   line    in spawn      return start processes     file   home hirune   envs panda lib     site packages torch multiprocessing spawn py   line    in start processes      while not context join      file   home hirune   envs panda lib     site packages torch multiprocessing spawn py   line    in join      raise exception   exception        process   terminated with the following error   traceback       file   home hirune   envs panda lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   home hirune,0cb676746568b6ca3c1ef9d9d2879b913f183179,pytorch_lightning\loggers\neptune.py
1687,name 'IProgress' is not defined,  name  iprogress  is not defined when running from jupyter notebook    ,1a54ed6ad9f1faf8ac58bbded4b71e4dd18246d6,CHANGELOG.md
1697,[Examples] The UNet model has some bugs,  üêõ bug    the unet model definition has some bugs pertaining to bilinear interpolation   code sample                pytorch lightning pl examples models unet py              lines   to          in                        for   in range            layers append   bilinear           feats                   in the code above  there seems to be a typo  the bilinear flag should be passed to the function up   it has instead been passed to the  append  method of the list               pytorch lightning pl examples models unet py              lines   to          in                        if bilinear           self upsample   nn upsample           else           self upsample   nn                the number of channels once the input passes through either one of these layers is different  for  bilinear   the number of channels remains the same  whereas they decrease to half if a   is used  this gives an error in the network s  forward  method   i wanted to directly use the model for some other a,cf2d32d0a6c757aad39c36b621a646ed3a24619a,pl_examples\domain_templates\semantic_segmentation.py
1721,instable GitHub action cache,  üêõ bug    there is some issue with gh action and caching as it is randomly failing with using horovod  to reproduce             ,281a73ccf7a22cdf004755f1f7b4aead40b12d84,.github\workflows\ci-testing.yml
1751,Early Stopping behavior,  hi there   thanks for the great library    i am not following the bug report template as i m not sure this is indeed a bug  or simply i cannot understand how early stopping is implemented  my code looks as follows       early stop callback   earlystopping           monitor  val acc            min delta               patience             verbose true           mode self mode               trainer   trainer           early stop callback early stop callback           auto select gpus true           max epochs             terminate on nan true           show progress bar true           fast dev run false           gpus               as i understand it  the model should perform early stopping after at least   epochs have passed without improvement on the validation accuracy  however  in my case  early stopping happened at epoch    is this how it should be   as i said  i am not sure this is actually a bug or a choice    if it is indeed a bug  i will work a reproducible example  thank you  ,3af4994d5a84bc80738b50983b4b42c3eb946433,CHANGELOG.md
1829,Allow boolean flags to work without passing True,  we tried to fix this but it s still broken  this fails when adding args to argparse automatically       auto lr find      instead we have to do     auto lr find true      which is not great   ,bee0392c372936567b2bbe6e7ed5828cb3078354,pytorch_lightning\trainer\trainer.py
1850,lr_find doesn't return the correct suggestion if some losses are nan,  üêõ bug    lr finder doesn t return the correct suggestion if some losses are nan  the returned loss is the one corresponding to the nan value  which is very big in my case       to reproduce    this depends on the dataset  please see the code sample   code sample    i believe this is caused by numpy  the related code should be       pytorch lightning pytorch lightning trainer lr finder py               line          in                        min grad       argmin              example losses                                   print  argmin    example losses                                  float    print  argmin    output             expected behavior    return the correct suggested loss   environment      cuda     gpu   available          false  version                     packages     numpy                     pytorch debug      false  pytorch version           pytorch lightning         tensorboard               tqdm                          system     os                 linux  archit,ac76dfcf62a672c84f843f2e3158e4c6262776da,CHANGELOG.md
1857,Can not use Trainer.test() if train and val dataloaders are not defined,  üêõ bug    when the model does not define train dataloader and no val dataloader  we can not use trainer test    the configuration checks fail with a misconfigurationexception   code sample    model          a model with no  train dataloader    val dataloader  defined  test dl         a dataloader  trainer   pl trainer   trainer test       expected behavior    we expect the testing loop to execute    ,1a797bdad5df6d4e7ccc586ddeb93dccb2a9648a,tests\trainer\test_checks.py
1878,prepare_data called multiple times per node for slurm and elastic training,  üêõ bug    slurm and elastic training create the training processes per node outside of the lightning context  this means that when the fit function calls prepare data  the assumption that it s only being called on proc   is broken and it gets called for each process   this is an issue computational reasons   and for training stability if the data preparation process isn t deterministic   see calling code here         pytorch lightning pytorch lightning trainer trainer py               line          in                        model prepare data              to reproduce    steps to reproduce the behavior     add print statements to prepare data  train a lightning model with either slurm or elastic training  see that it s being called multiple times     expected behavior    expected prepare data to only be called once per node    ,5fd01b0e68a6087908ac0bcefd4edaeddfb0e248,pytorch_lightning\callbacks\model_checkpoint.py
1889,trainer.scale_batch_size() throws exception due to LRScheduler,  üêõ bug    i tried finding the biggest possible batch size for my training  but pl raises a misconfigurationexception saying that my lrscheduler   is conditioned on a metric that is only available after validation epoch end  the available metrics are  loss  val loss   i assume the lrscheduler requires a metric from the training loop for this to work  why is this neccessary   to reproduce    steps to reproduce the behavior     have a model with a metric that only exists in validation epoch end  have a lrscheduler which monitors that metric  use trainer scale batch size  see error    file  c  programdata   envs ml lib site packages pytorch lightning trainer training loop py   line    in update learning rates      raise misconfigurationexception   pytorch lightning utilities exceptions misconfigurationexception  reducelronplateau conditioned on metric meaniou which is not available  available metrics are  loss train loss  condition can be set using  monitor  key in lr scheduler dict      ,3459a546672303204a4ae6efcc2613a90f003903,pytorch_lightning\trainer\training_loop.py
189,Logging of GPU memory utilization can significantly slow down training,  when training using gpus pytorch lightning automatically logs the gpu memory utilization during training  this is a useful feature  but can severely impact performance dependent on the speed of the nvidia smi call   on our particular cluster    this leads to a performance decrease of almost   fold when training on gpu vs  cpu   describe the bug  logging of gpu memory can have a severe impact on training performance     remove gpu memory logging by commenting out the lines   to   in pytorch lightning models trainer py  see here    expected behavior  logging of gpu memory utilization should not impede performance   or it should at least be possible to deactivate it in case performance issues arise   desktop       os  ubuntu linux      nvidia geforce gtx       ,dac41030d48acbfecdf7c083b8e7b00f3fd9be06,docs\Trainer\Logging.md
1898,batch size finder does not recognize flag and seems to download often,  the   auto scale batch size flag requires a string  but it should also just work with set true    ie  support this case       auto scale batch size        it seems to trigger data downloads more frequently     files already downloaded and verified  files already downloaded and verified  files already downloaded and verified  files already downloaded and verified  files already downloaded and verified  files already downloaded and verified  batch size   succeeded  trying batch size    files already downloaded and verified       ,a34eb9e169622fe91fdf4d98560b65b2f2b5c8d0,CHANGELOG.md
1899,Incorrect number of batches when multiple test loaders are used and test_percent_check is specified,  üêõ bug    when there are multiple test dataloaders and test percent check is specified  the estimated total batches are incorrect and progress bar doesn t show properly   for example  when i specify two dataloaders each of which has   batches and test percent check      the expected total batches are          but actually    batches are run   at this line  num batches is the global number of batches and will be assigned to self num test batches        pytorch lightning pytorch lightning trainer data loading py               line          in                        num batches   int              while in the evaluation loop  max batches is regarded as the number of batches for one data loader         pytorch lightning pytorch lightning trainer evaluation loop py               line          in                        if batch idx    max batches              to reproduce    steps to reproduce the behavior     return multiple dataloaders from test dataloaders   specify test percent check   ,e085e93dd303e80af2e9a5fe4aa392055c831114,CHANGELOG.md
1916,Trainer.parse_argparser does not yield sensible default for default_root_dir,  üêõ bug    using trainer parse argparser returns true for default root dir  however  a string is expected   to reproduce    steps to reproduce the behavior       from pytorch lightning import trainer      from argparse import argumentparser  namespace      parser   argumentparser       parser   trainer add argparse args       args   trainer parse argparser       args  namespace        ,b3ebfec863df8513f42e7211a29f857139e8ede4,pytorch_lightning\trainer\trainer.py
1937,"TODO list for ""replace Hparams by init args"" PR",  üöÄ todo  follow up work on module arguments rework in                 make clear the multiple ways args can and cannot be passed in   example    class litmodel        def   init                 trainer add argparse args    litmodel add model specific args    litmodel      this will fail  this won t work since the list of arguments in constructor is a fixed size   we can fix it in two ways     add   kwargs to the init signature to catch any unnecessary args    split the parsers to separate model args from trainer args              make it clear which types we save to the checkpoints and which not    the name  module arguments  maybe misleading to believe all args are saved           some old code was left commented  including tests  as mentioned by                the model checkpointing has changed  we should thoroughly test that the correct args are loaded             test case for positional args            fix for when super  is not called or called after other local vars were added,4234992302608e1999c00b4faffac591fb537a34,CHANGELOG.md
195,Non plain-tensor batches,  hello   i have been trying to use lightning to train a graph neural network built with torch geometric   package using a gpu   this is the error i get when i try to fit the model       envs pyg lib     site packages torch geometric nn conv gcn conv py in forward              def forward                                          x   torch matmul                           if self cached and self cached result is not none     runtimeerror  expected object of backend cpu but got backend cuda for argument         i investigated the code of lightning for a probable cause and found that transfer batch to gpu causes this error   the current behavior of this function considers a batch to be either a  plain   or some simple collection of such objects    the problem is that torch geometric uses a custom aggregate type   docs    which itself implements  method     it would be nice if you could make the code more flexible to process this case   correctly  i believe  the best solution is to replace,34b824a9d3d0fdd377da675e0398c66ab5e16e7b,pytorch_lightning\models\trainer.py
2027,Support DictConfig,  we need to add dictconfig support for omegaconf  borda   to the auto hparam save   ,d2967d9305b42c9260f821f2b7fb43fbf19ca1aa,CHANGELOG.md
2058,Hydra MLFlow Clash,  üêõ bug    when using the mlflow logger with hydra  because the parameters passed to the lightningmodule is a dictconfig  the condition in the logger base py is not met         pytorch lightning pytorch lightning loggers base py               line          in                        if isinstance               to reproduce    use hydra and mlflow together   traceback       file   home siavash kronikare   kwae ma models pl train segmentation model py   line    in       main     file   home siavash   envs kwae ma lib     site packages hydra main py   line    in decorated main      strict strict     file   home siavash   envs kwae ma lib     site packages hydra  internal utils py   line    in run hydra      overrides args overrides     file   home siavash   envs kwae ma lib     site packages hydra  internal hydra py   line    in run      job subdir key none     file   home siavash   envs kwae ma lib     site packages hydra plugins common utils py   line    in run job      ret return value ,44385bb582467acaa35cd4da553b2343a7860598,pytorch_lightning\loggers\base.py
2092,[ddp] New ddp implementation doesn't work in notebooks / using scripts,  the using  spawn  to spin off subprocesses ddp in had a few problems     everything needs to be picklable   it doesn t work well with num workers on dataloaders because of spawn  fit  trains the model in a subprocess  so the original model is not updated   those are not limitations of lightning  but of pytorch and python     as a result  we removed  spawn and instead call the script under the hood   this approach solves all problems above  but it assumes you can call your model like  python train py     and does not support other ways of calling the script   we should decide how to support ddp on jupyter notebooks    ,3260e59b2723a0f5d666c6779486717aa3a9373d,docs\source\multi_gpu.rst
2131,wandLogger().name is None in DDP mode,  if you remove line      everything works   when you re not in ddp mode  this is not an issue    ,145670f893f43ff70866668cf087d82fe51a22a6,pytorch_lightning\callbacks\model_checkpoint.py
2143,Fix checkpoint warning for floats,       added this warning when saving checkpoints        is there a reason why native python floats aren t supported  i think it s actually quite common especially when libraries e g  scipy are involved when computing the metrics   it says checkpoint not saved  but actually checkpoints are still saved  this check doesn t actually change the saving logic besides giving that warning      ,97e62b38cfa0c9ce14050b603ec3e735ba760a71,pytorch_lightning\core\step_result.py
2167,"The docker image tagged with Pytorch 1.5 and Python 3.8, has Pytorch 1.4 installed and is running Python 3.7",  üêõ bug    the docker image tagged   with pytorch      eg   has torch     installed in it  as seen via pip list  also  it is running python     instead of python      as the tag indicates   to reproduce    steps to reproduce the behavior     pull docker image  docker pull pytorchlightning pytorch lightning                run the container  docker run   rm  it   init pytorchlightning pytorch lightning                check version of python and pytorch     containeruser      python   version  python          containeruser      pip list   grep torch  pytorch lightning                           torch                            expected behavior    since the tag of the docker image is                i expect to see it running python     and pytorch       environment    n a   ,2f739f5977640bb8580b82a11f322f81f1b90d09,.github\workflows\docker-builds.yml
2180,Global Gradient calculation is turned off during validation step.,  if an error occurs during the validation step  the tradition calculation is turned off for the runtime  you have to either specifically enable it or restart runtime    ,25c7465591371f8fe4b4244ccc996706f4136cea,pytorch_lightning\trainer\training_loop.py
2188,[hparams] save_hyperparameters doesn't save kwargs,  ‚ùì questions and help    when i use hyperparemeters like docs   class litmnist          def   init              super    init               call this to save   to the checkpoint          self save hyperparameters         model checkpoint doesn t save args in kwargs  but kwargs is important  args such as num frames  img size  img std     must be used in creating dataloader  but it will be tedious if writes them in   init   explicitly    it can make code clean if hides them in kwargs   before i use hparams  it s ok  but now it s not recommended to use hparams   is there any good idea to  deal with this problem    ,6ae9a97b09fd8e3239219c2882c6f3cc31a2ccf8,pytorch_lightning\core\lightning.py
2205,[metrics] Accuracy Metric: Tensors must be CUDA and dense,  i try the new accuracy metric  but it throws error   traceback       file  main py   line    in       main     file  main py   line    in main      trainer fit     file   mnt lustre     lib     site packages pytorch lightning trainer trainer py   line    in fit      self ddp train     file   mnt lustre     lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routine     file   mnt lustre     lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      false     file   mnt lustre     lib     site packages pytorch lightning trainer evaluation loop py   line    in  evaluate      output   self evaluation forward     file   mnt lustre     lib     site packages pytorch lightning trainer evaluation loop py   line    in evaluation forward      output   model     file   mnt lustre     lib     site packages torch nn modules module py   line    in   call        result   self forward     file  ,17d87731062691f4510c75f12f2ce63b5dde0a43,CHANGELOG.md
224,set_epoch for DistributedSampler,  describe the bug  pytorch example suggests the use set epoch function for distributedsampler class before each epoch start  i could not find this function call in lightning s trainer module        line      as can be seen from the distributedsampler class code        the set epoch function is required to set the seed for each  function call   can you confirm if this function has been called on distributedsampler   at some point in lightning s trainer module    ,c0f3b6b035f955fc371dec412d3816712f3fc1dd,pytorch_lightning\trainer\trainer.py
2254,"Single node DDP: ""Default process group is not initialized""",  üêõ bug    unable to start single node ddp training on        to reproduce    was going to run the gpu template but        both methods of running the template result in the same error    python  m pl examples basic examples gpu template   gpus     distributed backend ddp spawn    python  m pl examples basic examples gpu template   gpus     distributed backend ddp      gpu available  true  used  true  tpu available  false  using    tpu cores  cuda visible devices             traceback       file   opt conda lib     runpy py   line    in  run module as main         main     mod spec     file   opt conda lib     runpy py   line    in  run code      exec     file   opt conda lib     site packages pl examples basic examples gpu template py   line    in       main     file   opt conda lib     site packages pl examples basic examples gpu template py   line    in main      trainer fit     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in fit      self ba,57d5f6e74a3bcd8f5c73211ba3a4e2480fcc1114,docs\source\trainer.rst
2281,RuntimeError: OrderedDict mutated during iteration,  üêõ bug    i was getting runtimeerror  ordereddict mutated during iteration   it seems like using the same lightningmodule object with modelsummary and trainer causes this error   to reproduce    from pytorch lightning core memory import modelsummary  model   cifarnet    any pl module would work here  modelsummary   trainer   trainer   trainer fit   steps to reproduce the behavior     view model summary using modelsummary class  call trainer fit with same object     stacktrace    runtimeerror                              traceback     in              checking for errors          trainer   trainer           trainer fit     frames   usr local lib     dist packages pytorch lightning trainer trainer py in fit                         elif self single gpu                     self single gpu train                         elif self use tpu     pragma  no cover   usr local lib     dist packages pytorch lightning trainer distrib parts py in single gpu train                     self reinit schedu,f972ab3a828eae1847a793da0b2c25c6074647a4,CHANGELOG.md
2286,example_input_array dtype,  currently assumed that example input array dtype to be equal to model dtype  this is not necessarily correct   e g  if input is a vector of int         pytorch lightning pytorch lightning core memory py               line          in                        input    apply to collection                ,6bfcfa8671c4bf54b34290171f191db65fa27d8c,CHANGELOG.md
2299,DDP Bug with Model Checkpoint parsing,  üêõ bug    my script works with cpu  single gpu and dp   i need ddp to do   bit training  also even on a single machine ddp is faster   here is my modelcheckpoint code   def setup model checkpoint        kwargs   config  model checkpoint kwargs        metrics   kwargs pop       if isinstance            metrics    metrics         fp    checkpoints  epoch        for metric in metrics           fp                  fp    str           fp                  return modelcheckpoint       in my case it would generate the checkpoint  checkpoints epoch           for example   although i even tried it with just checkpoints and it s the same issue   the issue is the following                    file   home user miniconda envs   lib     site packages pytorch lightning trainer trainer py   line    in fit                   file   home user miniconda envs   lib     site packages pytorch lightning trainer trainer py   line    in fit                   file   home user miniconda envs   lib     site package,90f641af0d509645ecd679d00f1213f68d4a44ad,pl_examples\models\lightning_template.py
2311,overfit_batches doesn't work,  when i try to use overfit batches     lightning readthedocs io en latest debugging html make model overfit on subset of data     trainer   trainer       my code fails with      trainer fit     file   home andriy   envs patchy discs model lib     site packages pytorch lightning trainer trainer py   line    in fit      self single gpu train     file   home andriy   envs patchy discs model lib     site packages pytorch lightning trainer distrib parts py   line    in single gpu train      self run pretrain routine     file   home andriy   envs patchy discs model lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self reset val dataloader     file   home andriy   envs patchy discs model lib     site packages pytorch lightning trainer data loading py   line    in reset val dataloader      self  reset eval dataloader     file   home andriy   envs patchy discs model lib     site packages pytorch lightning trainer data loading py   line    in  re,e6c7548b306055e41552e23d57f0057e7f441256,CHANGELOG.md
2314,Breaking compatibility with custom datatypes implementing `.to`,  üöÄ feature    bring back compatibility for custom datatypes in collections implementing  to for transferring data   motivation    i am using pytorch lightning together with pytorch geometric  pytorch geometric implements several custom datatypes and dataloaders which is really useful for geometric deep learning  everything worked well with pytorch lightning        as the custom datatypes implement a  to method for transferring the data to different devices   however  with the recent       update  this is no longer possible and i had to scour the documentation to be able to implement a fix using transfer batch to device   this is in my opinion not very pretty  as my batch looks like this    data   pytorch geometric batch object   id   tensor            i e  it is just a dictionary of types that all implement the  to method   pitch      make it possible for classes implementing the  to method to be transferred automatically  if part of the batch could not be transferred automatically ou,aab9e77d2d4ac601a08ca6365dd846a88b83517f,CHANGELOG.md
2315,Bug in average_precision Metric,  üêõ bug    hi everyone  i encountered a bug when using the average precision metric    it yields incorrect results     there seems to be a missing parenthesis in the code here      lightning blob master pytorch lightning metrics functional classification py      it works when corrected as    return  torch sum    precision   in order to reproduce negative results    import torch  import pytorch lightning metrics functional classification as m    torch manual seed   truth            pred   torch rand     m average precision       i did not find an issue on this topic yet  if needed i can submit a pr   thanks ‚ò∫Ô∏è   ,92f122e0df7e233f3a8b7873c7294155afbbf852,CHANGELOG.md
2330,`use_amp` and multiple optimizers bug,  üêõ bug    faced this issue when tried to use mixed precision with my two head model which has two pairs of optimizer   scheduler  without use amp everything works fine   with it enabled  i get   typeerror   cosineannealinglr  object is not subscriptable      my investigation has ended up here       def reinit scheduler properties              reinitialize optimizer step properties added by schedulers          for scheduler in schedulers               for optimizer in optimizers                   scheduler   scheduler  scheduler             this place                                         if scheduler optimizer    optimizer                              obviously  next optimizer will get scheduler as an actual non dict object as it was reassigned on the first iteration     to reproduce    steps to reproduce the behavior     build any lightningmodule  which configure optimizers  method outputs lists of two optimizers and two schedulers  in my case it s something like     def configure ,c275e1fc91df4d351799b633e9df08e010094bfe,pytorch_lightning\trainer\optimizers.py
2333,AttributeError: 'LightningDataParallel' object has no attribute 'teardown',  üêõ bug    to reproduce    steps to reproduce the behavior   trainer   pytorch lightning trainer       gpus         distributed backend  dp      model   basemodel load from checkpoint   trainer test       traceback     file  run kitti py   line    in   trainer test   file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in test  self model teardown   file   opt conda lib     site packages torch nn modules module py   line    in getattr  type  name  name    attributeerror   lightningdataparallel  object has no attribute  teardown   code sample    expected behavior    environment      cuda     gpu     geforce gtx   ti  geforce gtx   ti      available          true  version                     packages     numpy                     pytorch debug      false  pytorch version           pytorch lightning         tensorboard               tqdm                          system     os                 linux  architecture              processor             python    ,d22181714ac3f201ea7a35b7fd06d85db82c0465,pytorch_lightning\trainer\trainer.py
2334,LightningModule.load_from_checkpoint not working with .ckpt from 0.7.6,  üêõ bug    trying to use an old experiment  ckpt   in        results in an error when trying to load   to reproduce    steps to reproduce the behavior     train something with       and save checkpoint with the checkpoint callback  try to load checkpoint with          trying to load  ckpts         ckpt with pl version                                                                                      typeerror                                 traceback     in           selected ckpt   glob              print          litclassifier load from checkpoint       frames   usr local lib     dist packages pytorch lightning core saving py in load from checkpoint                 checkpoint cls checkpoint hyper params key  update                         model   cls  load model state                 return model             usr local lib     dist packages pytorch lightning core saving py in  load model state                             if cls checkpoint hyper params type in checkpoint             ,861a73be12ef17214bb0ed49aabc9f48a80fde16,CHANGELOG.md
2359,Problem with loading checkpoint of a model with embeddings,  üêõ bug    unable to load from checkpoint for model with embeddings  code sample    model arch  class model          def   init                  super    init                 m   get base               self enc    nn sequential         nn flatten                    nc   list       in features              self head   nn sequential  mish                                    nn     nn dropout  nn linear                self embs   nn modulelist  for c s in emb szs                def forward                      e  for i e in enumerate                     torch cat                x img   self enc                x   torch cat                     x   torch cat                return self head         checkpoint callback   modelcheckpoint                filepath os path join    model dir                        save top k true                verbose true                monitor  val loss                 mode  min                 prefix                          trainer   trainer max epochs         ,51711c265a9e234f2b4164f1a2fab73373707d61,.github\PULL_REQUEST_TEMPLATE.md
2371,hparams are not logged in tensorboard,  üêõ bug    i m using the latest version of pl  my problem is when i launch an experiment  everything is logged correctly to tensorboard  except the hparams       i tried to launch the cpu template on a fresh virtual environement  but it didn t work  however  when i launch the pytorch example of logging hparams to tensorboard    torch utils tensorboard writer summarywriter add hparams     it worked   to reproduce    i succeeded to reproduce this using the official mnist collab       scrollto       by skipping the  simplest example  and excuting directly     lightningmodule   environment      pytorch version         os    ubuntu      how you installed pytorch    pip  build command you used     python version             cuda cudnn version       gpu models and configuration  geforce gtx    any other relevant information      ,11069c87845ea9a14e6fe807094313a67f9946dc,pytorch_lightning\core\decorators.py
2372,training_epoch_end's outputs doesn't have 'loss' key,  pytorch lightning  build from master  traceback       file  main py   line    in       main     file  main py   line    in main      trainer fit     file   mnt lustre     lib     site packages pytorch lightning trainer trainer py   line    in fit      self ddp train     file   mnt lustre     lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routine     file   mnt lustre     lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   mnt lustre     lib     site packages pytorch lightning trainer training loop py   line    in train      self run training epoch     file   mnt lustre     lib     site packages pytorch lightning trainer training loop py   line    in run training epoch      self run training epoch end     file   mnt lustre     lib     site packages pytorch lightning trainer training loop py   line    in run training epoch end      epoch output   m,a42a0e16ddd75dd7199ecefe4d10c2941c17ba76,pytorch_lightning\trainer\training_loop.py
241,"In Multi GPU DDP, pytorch-lightning creates several tfevents files",  describe the bug  right now pytorch lightning seems to create several tfevent files in the multi gpu ddp way   e g  for   gpus    rw rw r      sam sam     sep       events out tfevents             rw rw r      sam sam   sep       events out tfevents             rw rw r      sam sam     sep       events out tfevents                i suppose the first one is created by the main process and the next   are created by the   ddp processes    unfortunately  the actual events are not logged in the last created one  and that confuses tensorboard  cf tensorflow tensorboard      i have to restart tensorboard if i want to see the new data   a clear and concise description of what the bug is   to reproduce  launch any training on multi gpu ddp   expected behavior  only one tfevent file is created  from the master gpu    ,614cb3c03bd0894238b3197f3b7f904656f284f4,docs\Trainer\Logging.md
2411,0.8.2 calls backward on '_GeneratorContextManager',  üêõ bug          calls backward on   generatorcontextmanager  and crashes training         works correctly  my training step returns   loss  loss   log    learn rate  self lr    traceback       file   opt conda lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   opt conda lib     site packages pytorch lightning trainer distrib data parallel py   line    in ddp train      self run pretrain routine     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in train      self run training epoch     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in run training epoch      batch output   self run training batch     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in run training batch      self hiddens,e8bb4165b76496089d24c74891f2167350e594be,pytorch_lightning\trainer\training_loop.py
2429,Batched iterative dataloading disables validation,  üêõ bug    setting the batch size parameter for torch utils data dataloader to a number greater than    prevents validation step and validation epoch end from being called   to reproduce    steps to reproduce the behavior     run python main py with bs      observe exception raised in validation step  run python main py after changing to bs      observe the model train successfully    code sample    import pytorch lightning as pl  import torch  import torch nn as nn  import torch nn functional as f  from torch utils data import dataloader  iterabledataset    class dataset        def   init              super    init           def   iter              for   in range                yield torch randn         def   len              return      class model        def   init              super    init             self fst   nn linear           self snd   nn linear         def forward            x   self fst           x   f relu           x   self snd           return x        def training ste,927f305f7e556828b5cdd45e3977c67f3c54b8fc,CHANGELOG.md
2436,Fix horovod tests that try to access filepath on global rank &gt; 0,  üêõ bug    we had to skip two tests in       namely    test horovod cpu  test horovod cpu implicit    problem is that since they run in ddp and the test tries to access the trainer internal variable for the checkpoint path  it gets a nonetype error when trying to os join  none paths   to reproduce    steps to reproduce the behavior   run these two tests     test horovod cpu  test horovod cpu implicit     ,78db847e42457ce3dcd89a2a5eccc8e79f60e731,tests\base\develop_pipelines.py
2438,`validation_epoch_end` and `test_epoch_end` can't return nothing,  üêõ bug    if validation epoch end or test epoch end returns nothing    an error occurs      to reproduce    steps to reproduce the behavior   overwrite test epoch end and remove return  same for validation epoch end  file    conda envs ppi env lib     site packages pytorch lightning trainer logging py   line    in process output      for k  v in output items    attributeerror   nonetype  object has no attribute  items       code sample    import os    import torch  from torch nn import functional as f  from torch utils data import dataloader  from torchvision datasets import mnist  from torchvision import transforms  from pytorch lightning core lightning import lightningmodule  from pytorch lightning import trainer  seed everything    class litmodel          def   init              super    init             self     torch nn linear         def forward            return torch relu                def training step            x  y   batch          y hat   self           loss   f cross en,325852c6df93f749bb843bff1a3cdba41698722c,docs\source\bolts.rst
2442,validation_epoch_end needs to return CUDA tensors,  üêõ bug    i m not sure if this is expected behaviour or not  but upgrading to the latest version   caused my validation epoch end to break  it appears that a cuda tensor is expected for the metric where before the tensor was device agnostic   this was using sklearn s roc auc score  i haven t yet got around to testing pl s new metrics   feel free to close if this is expected behaviour   to reproduce    this is my validation epoch end  uncommenting  to  allows this to run with the dev version of pl   this was run with ddp  precision   using apex amp   def validation epoch end            avg loss   torch stack  mean           y pred   torch cat  for x in outputs   cpu  numpy           y true   torch cat  for x in outputs   cpu  numpy             metric   torch tensor       to             tensorboard logs                   loss validation   avg loss                auc   metric                      return   val loss   avg loss   log   tensorboard logs   auc   metric       the error message,a5538af3558cf544dffd92b1b8bab3a5793f0ba0,CHANGELOG.md
2444,self.hparam silently removes params that are not serializable,  üêõ bug    following the approach found under hyperparameters in the docs    step    i passed a dict with parameters to my    in the  printing  shows all contents i passed   however  in the function   some hparams are gone   this might be related to that not all param values are yaml serializable  and therefore automatically removed  because the   removed params are  criterion   torch nn bceloss  and  optimizer   partial    to reproduce    steps to reproduce the behavior     run the following script     from functools import partial  import torch  import torch optim as optim  from torch utils data import dataset  import pytorch lightning as pl  from pytorch lightning import trainer      partial to give all params  except the data  hparams           criterion   torch nn bceloss      f cross entropy      loss function       optimizer   partial                 learning rate              filters            layers           class emptydataset        def   init              pass            d,d5254ff9dfb67fba388de224a320f3a562561a80,pytorch_lightning\utilities\__init__.py
2456,multi-gpu training triggers CUDA out of memory error,  hi    i am running into issues when going from single to multi gpu training   specifically  if i switch the line  pl trainer   to  pl trainer   i get the dreaded cuda out of memory error   is there any reason why the parallelism causes the gpu to receive more data    ,afdfba1dc6061c5e1ee6eaf215500d6a56e95482,pytorch_lightning\trainer\evaluation_loop.py
2458,Wandb Flatten Dict,  wandb logger should flatten the dictionary of parameters before logging  every other logger has the bellow pattern of code    params   self  convert params    params   self  flatten dict       üêõ bug    wandb logger does not flatten parameters resulting in dictionaries being logged to wandb  which are not searchable causing for some loss of features in wandb   to reproduce    run the cpu template with wandb logger  and log a nested dictionary   expected behavior    solution  just call   params   self  flatten dict  this in the wandb logger   environment      cuda     gpu   available          false  version            none      packages     numpy                     pytorch debug      false  pytorch version           pytorch lightning         tensorboard               tqdm                          system     os                 darwin  architecture              processor             python                    version            darwin kernel version        wed mar          pst    root xn,899cd74044505eb308e624f15a4cb65c57973bbb,CHANGELOG.md
2472,DDP breaks when `python` does not refer to the correct interpreter,  üêõ bug    if using the ddp distributed backend  the program breaks if python refers to   or does not exist   to reproduce    steps to reproduce the behavior     make sure the python command does not link to    such as on ubuntu       run trainer  fit with distributed backed  ddp     additional context    the problem lies at       pytorch lightning pytorch lightning trainer distrib data parallel py               line          in                        command     python     command            the python command is hardcoded here  on many systems  python is a symlink to    or does not exist    ,fc61c200c085f78fa2af4850aa8dc8e832fb80d0,CHANGELOG.md
2476,Model and Input not on same GPU when training with native AMP and DP,  üêõ bug    training with distributed backend  dp  and precision   result in the error that some input output of the model is not on the same gpu   to reproduce    from pytorch lightning import trainer  seed everything  from pl examples models lightning template import lightningtemplatemodel  seed everything     def main          model   lightningtemplatemodel        model   lightningtemplatemodel         trainer   trainer           gpus             num nodes             distributed backend  dp            precision                     trainer fit     if   name         main          main       run the above example code will result in following error   traceback       file      py   line    in       main     file      py   line    in main      trainer fit     file   home ubuntu   envs trfm lib     site packages pytorch lightning trainer trainer py   line    in fit      self dp train     file   home ubuntu   envs trfm lib     site packages pytorch lightning trainer distrib parts py   line,031274c25dedc92e383d2715e283a55a2b102d29,pl_examples\README.md
2479,init_slurm_connection causing hostname errors,  problem    can you update this function to support checking if master addr and master port are already in os environ   running into some weird errors where this code adds host to master addr and crashes the code         pytorch lightning pytorch lightning core lightning py               line          in                        def  init slurm connection     none              solution    do you prefer a pull request   here s the check i put it  def  init slurm connection     none                        sets up environment variables necessary for pytorch distributed communications          based on slurm environment                          if  master port  not in os environ                 use slurm job id for the port number                guarantees unique ports across jobs from same grid search              try                     use the last   numbers in the job id as the id                  default port   os environ  slurm job id                    default port   default port    ,c6df63a58817b6414f8a3ae28edd9e6552be3914,pytorch_lightning\trainer\__init__.py
2480,For versions &gt;0.8.2 learning rate is zero for last epoch (potentially a logging bug),  üêõ bug    version       and above changed the behavior of either my learning rate scheduler or the wandblogger logger  i am using a linear warmup and decay scheduler  however  the learning rate graph produced by the learningratelogger is as shown below ever since version             the period where the learning rate is zero corresponds to the last epoch of training as you can see below       this graph raises another issue  the first epoch appears to take twice as many steps as the second and third epoch  i specified max epochs    during training  each epoch takes the same amount of time  so this seems like a logging issue   note that the above graphs are for a model that had its training stopped early  so the last epoch is slightly shorter than the second to last  this is not the issue   both of these issues   do not exist in version        and both graphs look as they should   these issues could be caused by the logger or they might actually occur and be logged correctly  i have lo,992a7e2a414d052754f3579e173620baf740308a,pytorch_lightning\trainer\training_tricks.py
2484,Trainer.scale_batch_size requires model.batch_size instead of model.hparams.batch_size,  üêõ bug    trainer scale batch size only works if a model has the batch size property and does not work with model hparams batch size even though all documentation points to the reverse   to reproduce    all of my hyperparameters are available as model hparams like suggested in the documentation   hyperparameters  option       this means that my  is available as    this should be fully compatible with the documented example code   of  since that code also uses  instead of    however  when i put my model in trainer scale batch size  i get the following error   pytorch lightning utilities exceptions misconfigurationexception  field batch size not found in  model hparams       example code    class litmodel        def   init              super    init             self hparams   args    model   litmodel   trainer   trainer   trainer scale batch size       expected behavior    either  should work with  or the error message  linked documentation examples and docstrings should all change  i e,7b917de94642f63eedaffde79fb973705d2288dd,CHANGELOG.md
249,"UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars",  describe the bug  not sure if this is a bug  it shows me this warning at the beginning of training    home adrian research envs research lib     site packages torch nn parallel  functions py    userwarning  was asked to gather along dimension    but all input tensors were scalars  will instead unsqueeze and return a vector   to reproduce  the minimal mnist example from the docs has this problem when trained on multiple gpus  attached the python script   import os  import torch  from torch nn import functional as f  from torch utils data import dataloader  from torchvision datasets import mnist  import torchvision transforms as transforms    import pytorch lightning as pl      class coolmodel          def   init              super    init               not the best model             self     torch nn linear         def forward            return torch relu                def training step              required          x  y   batch          y hat   self forward           return   loss ,8b2a2aeda3066fe30cc496a58368a523ef90ad9b,pytorch_lightning\trainer\ignored_warnings.py
2495,`precision=16` displaying wrong loss in progress bar,  üêõ bug    when training on a gpu   and setting precision    the loss displayed by the progress bar is some crazy large number   stopping the example bellow in the middle of epoch   gives a loss of       if i train with precision    this loss is the true value of        the loss tensor is ok  if i add a print statement in the training loop it displays normal values   code sample    import torch  from torch nn import functional as f  from torch import nn    from pytorch lightning core lightning import lightningmodule  from pytorch lightning import trainer    from torch utils data import dataloader  random split  from torchvision datasets import mnist    import os  from torchvision import datasets  transforms    class litmnist        def   init              super    init             self     torch nn linear           self     torch nn linear           self     torch nn linear         def forward            batch size  channels  width  height   x size           x   x view           x   se,9924c76faa7789294811a27c392ba6b33e07f3f1,pl_examples\models\lightning_template.py
2498,TPU hangs when using only a train loop (ie: no val loop),  i think it s somehow related to checkpointing   easiest way to debug is to get on colab    ,0fe933e23d026fce6fd065f87e66c2637693e963,.circleci\config.yml
2531,IndexError with multiple validation loaders and fast_dev_run,  üêõ bug    an indexerror when using multiple validation datasets and fast dev run true  to reproduce    steps to reproduce the behavior     use multiple val dataloaders  use fast dev run true    code sample         usp sharing    traceback    traceback       file   home luca repositories set operations src run experiment py   line    in       trainer fit     file   home luca  cache pypoetry virtualenvs set operations       lib     site packages pytorch lightning trainer trainer py   line    in fit      self single gpu train     file   home luca  cache pypoetry virtualenvs set operations       lib     site packages pytorch lightning trainer distrib parts py   line    in single gpu train      self run pretrain routine     file   home luca  cache pypoetry virtualenvs set operations       lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   home luca  cache pypoetry virtualenvs set operations       lib     site packages pyt,84c507c4df5f5c336deb19ce7f70fa02329f39f6,CHANGELOG.md
2532,TypeError with multiple validation loaders and overfit_batches,  üêõ bug    a typeerror when using multiple validation datasets and  overfit batches       to reproduce    steps to reproduce the behavior     use multiple val dataloaders  use overfit batches       e g  overfit batches        code sample         yfb usp sharing    traceback    typeerror                                 traceback       in            trainer   pl trainer           trainer fit       frames     usr local lib     dist packages pytorch lightning trainer trainer py in fit                    self optimizers  self lr schedulers  self optimizer frequencies   self init optimizers                           self run pretrain routine                         callbacks     usr local lib     dist packages pytorch lightning trainer trainer py in run pretrain routine                                                  self val dataloaders                                                  max batches                                                  false                             allow no re,d787208e768085b608198f3e0313e2be28d4cbfe,pytorch_lightning\trainer\data_loading.py
2539,TPU fp16 requires apex installed,  when i tried to use precision   on tpu  pytorch lightning is trying to find amp  which is unnecessary   the backtrace is  gpu available  false  used  false  tpu available  true  using    tpu cores  traceback       file  bert ner light   py   line    in       trainer   pl trainer     file     envs torch xla     lib     site packages pytorch lightning trainer trainer py   line    in   init        self init amp     file     envs torch xla     lib     site packages pytorch lightning trainer auto mix precision py   line    in init amp       you set  use amp true  but do not have apex installed    modulenotfounderror  you set  use amp true  but do not have apex installed install apex first using this guide and rerun with use amp true   linux his run will not use   bit precision      to reproduce    steps to reproduce the behavior   build a whatever trainer in tpu and use    code sample    import pytorch lightning as pl    trainer   pl trainer       expected behavior    should have nothing ,e068af9ea8c86df8ed5eb20e57a36fbb38c70462,pytorch_lightning\core\memory.py
2551,TrainerEvaluationLoopMixin activates model.train() at the end,  üêõ bug    according to the example on fine tuning    it is important to set the frozen sub modules to eval mode  this is sensitive because when in training mode  batchnorm and dropout change state   however  at the end of trainerevaluationloopmixin  evaluate there is following code     enable train mode again  model train   so after the first validation run  the model is again completely in training mode and the freezing is partially undone      ,f58c7604093fc37c765ac88e46aaf52b403332fe,pytorch_lightning\core\hooks.py
2555,apex amp state dict,  üêõ bug          pytorch lightning pytorch lightning trainer training io py               line          in                        if self use amp and native amp avalaible and  native amp scaling state  in checkpoint              it seems for native amp support  the scalar state dict is saved  but for non native amp  the amp state dict is not saved    ,bef27c58eda4c4425c8aa750d38e16522bfcbe39,pytorch_lightning\trainer\training_io.py
2574,horovod mode increase lr,  not really a üêõ bug      lightning blob master pytorch lightning trainer distrib parts py      under horovod mode  the learning rate will automatically be increased by hvd size    this behavior is different from ddp  so it may confuse the users    ,1369012bc71f257dcf7423ec65146d055ddc1cc7,CHANGELOG.md
2600,Trainer flag overfit_batches does not overwrite train dataloaders shuffle flag,  üêõ bug    setting the trainer flag overfit batches   does not overwrite the shuffle flag set in the training dataloader  even though the warning reads   userwarning  you requested to overfit but enabled training dataloader shuffling  we are turning it off for you   to reproduce    steps to reproduce the behavior     create lightning module with method train dataloader with flag shuffle true        def train dataloader     loading dataloader           dataset   prostatex           batch transforms  gpu transforms  sample transforms   self get transformations           dataloader   loading dataloader dataset                                           batch size self hparams tr batch size                                           batch transforms batch transforms                                           shuffle true                                           sample transforms  sample transforms                                           gpu transforms gpu transforms                        ,b5dc6998ae80b026bb6adc4040980a153390307a,CHANGELOG.md
2622,set_epoch isn't called for TPU training,  üêõ bug    this line   lightning blob master pytorch lightning trainer training loop py     doesn t call  when training on tpus unless using  as  is false    ,2cc60c625ed6593aea01d237fa047ad1863dc79c,pytorch_lightning\trainer\training_loop.py
2635,Loss value in the progress bar is wrong when `accumulate_grad_batches &gt; 1`,  üêõ bug    the loss value reported in the progress bar is the correct loss value   accumulate grad batches  so this value is wrong when accumulate grad batches       this is happening because here   the loss is divided by   then here   the running loss is the  of these losses   to fix this  either remove the first line   or replace mean with sum in the second line   to reproduce      train any model with accumulate grad batches   and note the loss value reported in the progress bar  train the same model with accumulate grad batches   and half the batch size   now the loss value in the progress bar will be half the value from step       expected behaviour    the loss in steps   and   should be the same  environment    pytorch lightning          ,c047676fae8cdbfe77189c218cfde73d863acc91,pytorch_lightning\trainer\training_loop.py
2636,nan metric breaking ModelCheckpoint,  üêõ bug    comparing any numbers to float  is false in python so as a result if a non loss metric score is nan initially in training  then callback cannot checkpoint any scores after   expected behavior    ignore a nan metric score  this is orthogonal to when grad or weights become nan   environment      pytorch version           os    linux  how you installed pytorch    pip  build command you used     python version       cuda cudnn version       gpu models and configuration  tesla      additional context    previous issue wasn t addressed completely        ,6ac0958166c66ed599c96737b587232b7a33d89e,pytorch_lightning\callbacks\model_checkpoint.py
2637,to() got an unexpected keyword argument 'non_blocking' for DGLGraph,  üêõ bug    to reproduce    i use dgl library to make a gnn and batch the dglgraph   no problem during training  but in test  i got a typeerror  to  got an unexpected keyword argument  non blocking     to  function has no keyword argument  non blocking   code sample    expected behavior    environment      os  linux  cuda       python version       pytorch version         dgl version         pytorch lightning version           additional context       file     src main py   line    in       run     file     src main py   line    in run      trainer test     file   home jiangyize   envs galixir lib     site packages pytorch lightning trainer trainer py   line    in test      results   self   test given model     file   home jiangyize   envs galixir lib     site packages pytorch lightning trainer trainer py   line    in   test given model      results   self fit     file   home jiangyize   envs galixir lib     site packages pytorch lightning trainer trainer py   line    in fit      result,69d241c82e10cf40e5787fb39bb808687d693b57,CHANGELOG.md
2653,Checkpoints cannot be loaded in non-pl env,     üöÄ feature  add an option to save only state dict for modelcheckpoint callbacks  üêõ bug    pl checkpoints cannot be loaded in non pl envs  motivation    to be able to move trained models and weights into pytorch only environments  additional context    currently  when you do torch load  on a pl generated checkpoint in an environment without pl  there is a pickling error  for my current use case i have to load the checkpoints in my training environment and save them again with only state dict for the weights   see reply below   for more info   ,65e6687c54937db0f9bdbdb089ac9d288457d6f8,pytorch_lightning\trainer\training_io.py
2669,--gpus flag with add_argparse_args bug,  üêõ bug    when using  parser   trainer add argparse args   args   parser parse args     trainer   trainer from argparse args       if the user does not provide the   gpus flag  the code allocates some memory on the gpu and reports  gpu available  true  used  true  but does not use this gpu  this issue has been discovered in bolts     pytorchlightning pytorch lightning bolts    pytorchlightning pytorch lightning bolts      to reproduce    steps to reproduce the behavior   use the following method to create the trainer object  parser   trainer add argparse args   args   parser parse args     trainer   trainer from argparse args       and don t pass the   gpus flag   expected behavior    if   gpus flag is not provided in the script call  then lightning should report  gpu available  true  used  false  with a warning  and not allocate any memory on the gpu  this way  the user can set the   gpus flag if they have missed out on it    ,6780214b27e6ebace9cf38b6f5701224204e28ad,pytorch_lightning\trainer\distrib_data_parallel.py
2678,training_epoch_end seems to fail when returning nothing,  i m trying to log weight histograms to tensorboard at the end of each training epoch  i have the following code       def training epoch end            self log hists       this is in line with the documentation   if you don t need to display anything  don t return anything   however when this function runs i get the following error        file   venv lib     site packages pytorch lightning trainer training loop py   line    in run training epoch  ‚îÇ    self run training epoch end   ‚îÇ  file   venv lib     site packages pytorch lightning trainer training loop py   line    in run training epoch end  ‚îÇ     processed outputs   self process output   ‚îÇ  file   venv lib     site packages pytorch lightning trainer logging py   line    in process output  ‚îÇ    for k  v in output items    ‚îÇattributeerror   nonetype  object has no attribute  items   ‚îÇexception ignored in    ‚îÇtraceback     ‚îÇ  file   venv lib     site packages tqdm std py   line    in   del    ‚îÇ  file   venv lib     site packages t,b014223f72ee457285fa3eb336d1d4039cedb651,pytorch_lightning\trainer\training_loop.py
2680,Checkpoint saving order,  the last model save action  here    should be after saving the top k model because the best model and best score could have changed  swapping the order allows resuming the training from the last checkpoint  with the last checkpoint having the latest information about the best model path score    ,f798cffd02a0b6cbdc3033c981501c1a0c4677bd,CHANGELOG.md
2688,Training on GPU failed with Torchtext when using include_lengths=True in torchtext.data.Field,  üêõ bug    the issues raises in pytorch lightning utilities apply func py which assumes that the attributes of a batch from trochtext are tensors  however if torchtext data field is configured to include a length tensor   the field is a tuple   a bugfix is prepared and a pr can be submitted soon   to reproduce    steps to reproduce the behavior     use torchtext field with include lengths true on a gpu machine and fit model   training works on cpu but fails on gpu with  typeerror  cannot unpack non iterable nonetype object    full error message    traceback      file  debug torchtext py   line    in     trainer fit    file     thschaaf   envs   lib     site packages pytorch lightning trainer trainer py   line    in fit    results   self single gpu train    file     thschaaf   envs   lib     site packages pytorch lightning trainer distrib parts py   line    in single gpu train    results   self run pretrain routine    file     thschaaf   envs   lib     site packages pytorch lightning tr,a6719f09f0a383034f4285d65cba880208a03ae4,CHANGELOG.md
2691,Subprocess launched in ddp have the wrong cwd when using hydra.,  üêõ bug    details          i ve talked to  omry   about the issue and i will send out a fix soon   to reproduce    please see the comment i posted above   expected behavior    the cwd for subprocesses should be the same as that of the parent  and relative paths should work    ,b7f613ba6da32941bc86e9188629b572b10db4ad,pytorch_lightning\trainer\distrib_data_parallel.py
272,Trainer track_grad_norm always results in 0,  describe the bug  the trainer has a flag track grad norm which allows us to log the gradient norms to tensorboard  this flag is checked in the run tng epoch function after the training step and validation step  however  the training step   calls model optimizer step   which  in the default implementation  calls optimizer zero grad   this results in the tracked gradient norms to be always zero   moreover  there is an optional   run evaluation call in the validatin step  this results in a call to model zero grad   which i assume will also result in zero gradient norms   to reproduce  steps to reproduce the behavior     train a model with the track grad norm flag set to true and tensorboard logging enabled  go to tensorboard  check the gradient norms    expected behavior  the gradient norms should not always be zero   desktop       os  windows  version           ,41236c7bbbe2a22714c19b625bdf557854d747e8,pytorch_lightning\trainer\trainer.py
2724,Issues with Confusion Matrix normalization and DDP computation,  üêõ bug    i started using the confusionmatrix metric to compute normalized confusion matrices within a mult gpu ddp environment   however  i found the following issues     the normalization divisor is computed correctly in a row wise manner  however  the division is applied column wise   the normalization does not protect against divide by zero if there is no data in a particular row   while this is not a usual case for a well designed validation set  it is possible when you have a large number of unbalanced classes and limit val batches is small     there is no way to specify the number of classes for the confusion matrix   this is critical when performing ddp reduction as it is possible that the automatic computation of the number of classes could produce different answers for each process   i encountered this possibility when using a large number of unbalanced classes such that one of the ddp processes did not see any true data or declarations of the last class  causing its number ,a552d4a2d5056705c68f2eed570a83ee3160b3bc,pytorch_lightning\metrics\functional\classification.py
2742,[DataModule] `prepare_data()` and `setup()` not called,  üêõ bug    it seems that when using datamodule to separate training logic and data loading   of the five methods   that should be called that are         and    only the last three are actually used  witch is problematic since the datasets used by the data loaders should be assigned in the    to reproduce    steps to reproduce the behavior   run this   code sample    import torch  from pytorch lightning import lightningdatamodule  from pytorch lightning core lightning import lightningmodule  from pytorch lightning trainer import trainer  from torch nn import    linear  from torch optim import sgd  from torch utils data import dataloader      class mydatamodule          def   init              super    init           def prepare data            print  in prepare data                    this should be called before train dataloader  but is not           def setup            print  in setup                    this should be called before train dataloader  but is not             self train,036bcea4992865e8a82a5939f0d374530e17b778,docs\source\datamodules.rst
2769,Bug in `LightningModule.load_from_checkpoint`,  suppose you have a class model    and as parameters it takes  params  argparse namespace    along with other keyword arguments which might take default arguments  loading from a checkpoint  model   model          saves a checkpoint  model   model load from checkpoint       same behavior arises if params is passed as positional argument  will throw the error typeerror    init    got multiple values for argument  params    this comes from the fact that in line   in pytorch lightning core saving py  model   cls   when inspected  cls args is   and cls kwargs is  of course    params   params    this stems from the fact in lines                     if args name     kwargs                      in case the class cannot take any extra argument filter only the possible                  cls kwargs update               elif args name                   if args name in cls init args name                       cls kwargs update               else                   cls args       cls args  the else ,cea5f1f53876399dfaa0d37accdc527af7ca39af,pytorch_lightning\core\saving.py
2782,Use of shell=True could lead to shell injection,  file  pytorch lightning trainer training io py  line number       relevant code                  find job id  job id   os environ  slurm job id    cmd    scontrol requeue     format             requeue job          log info           result   call        from here     executing shell commands that incorporate unsanitized input from an untrusted source makes a program vulnerable to shell injection  a serious security flaw which can result in arbitrary command execution  for this reason  the use of shell true is strongly discouraged in cases where the command string is constructed from external input   shell false disables all shell based features  but does not suffer from this vulnerability   meaning anything that can set the slurm job id environment variable can perform code execution   the documentation also describes why you might need want shell true   this can be useful if you are using python primarily for the enhanced control flow it offers over most system shells and still wan,96eb6ebacd5b8bba2dea4741355f576e8f1c6a16,pytorch_lightning\trainer\training_io.py
2844,Tensorboard logger fails to save model OmegaConf hparams,  üêõ bug    the tensorboard logger fails to log module hyperparameters configured with omegaconf  this happens when updating the logger hparams here     the trainer calls the logger s log hyperparams here   inside log hyperparams the logger s hparams are updated here  this causes the hparams type to now be dict instead of dictconfig  as a result  this branch in  save hparams to yaml   is never triggered    this is the stacktrace when logging hyperparams        to reproduce    code sample    a hacky fix would be something like changing the hparams update to use this inside the tensorboard logger   if isinstance       self hparams   omegaconf merge   else       self hparams update       expected behavior    environment    please copy and paste the output from our  environment collection script        you can get the script and run it with   wget   lightning master tests collect env details py    for security purposes  please check the contents of collect env details py before running it  ,b39f4798a6859d2237b48b29b39a2390164612c1,CHANGELOG.md
2859,Failing docker-Conda build,  üêõ bug    there seems to be some connection issue while creating conda env  to reproduce      lightning runs      additional context     ,ad956b5ed9add7e601dfbe57e96ea586305127d0,.github\workflows\docker-builds.yml
286,Double check that fast_dev_run works correctly,  it should run a single validation and training batch and cover the full loop   suggested by  adefazio     ,608a90a490798a743410768a832309fe40b6ab7b,pytorch_lightning\trainer\trainer.py
2862,"Metrics error due to inplace operation, ""computation has been modified by an inplace operation"".",  hey   williamfalcon    i got a new error since i upgraded the library today   i used the accuracy metric  but got an error   code sample       in lightning module  def training step            x  y   batch          y hat   self           loss   f cross entropy           acc   accuracy         from the functional metric classification          tensorboard logs     train loss   loss           return   loss   loss   log   tensorboard logs       error msg     runtimeerror  one of the variables needed for gradient computation has been modified by an inplace operation   torch cuda longtensor      is at version    expected version   instead  hint  enable anomaly detection to find the operation that failed to compute its gradient  with torch autograd set detect anomaly        can be solved using  clone  method     however  when i clone the y before feeding to the accuracy function  no error was shown   acc   accuracy        but  it s inconvenience if user has to do it manually  isn t it   ac,d9d7e91a3b68fb7bbb966c73745a932ea95a2e6b,pytorch_lightning\metrics\functional\classification.py
2868,Throw warning for changing val_loss,  add warning to user that when changing val loss to another keyword it will break checkpointing  early stopping  and other features relying on it    ,51de6802edd6c050ba3f2803724298eb059dc5ad,pytorch_lightning\callbacks\early_stopping.py
2891,The total number of batches shows by the progress bar of the sanity check is wrong,  üêõ bug    the total of the sanity check progress bar is set by        pytorch lightning pytorch lightning callbacks progress py               line          in                        self val progress bar total   convert inf               the progress bar will always show trainer num sanity val steps even if  the length of the validation dataloader is less than trainer num sanity val steps   maybe the total could be computed by  from pytorch lightning trainer import data loading    num full val dataloader batches          len  if data loading  has len  else float       for dataloader in trainer val dataloaders     self val progress bar total   convert inf       sum               for num batches in num full val dataloader batches    we use the private function data loading  has len to check if dataloader has   len    maybe we could make data loading  has len public   or we could make num full val dataloader batches   a member variable of trainer and update the value in pytorch lightning,a628d181ee662a77b708a12c51477f912ce02f63,CHANGELOG.md
2916,ModelCheckpoint with custom filepath don't support training on multiple nodes,  üêõ bug    when training on multiple nodes using  with custom   it will raise  caused by the following line of code  model checkpoint py       maybe a try except block is needed    ,56396abe9839fa075bcc087c32f098145b0bdc9f,pytorch_lightning\callbacks\model_checkpoint.py
2936,"Trainer ""optimizers"" attribute is None when saving checkpoint and callbacks list is not empty",  üêõ bug    i m training a gan and i m running a few custom callbacks as well  when the model attempts to save at the end of the first epoch  it crashes  here s the very strange thing  i have the exact same code in a jupyter notebook and the error doesn t occur   to reproduce    steps to reproduce the behavior   the bug does not occur when the callbacks list passed into the trainer is empty  none of the callbacks i m using have anything to do with saving checkpoints  they re all for logging certain things about the model  enabling any one of them causes the error  running the exact same code in jupyter results in no crashes   stack trace   traceback   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     loss        v num     d loss        g loss           file  mnist dense gan convergence py   line    in       main     file  mnist dense gan convergence py   line    in main      trainer fit     file   users robbie  conda envs ganresearch lib     site packages pytorch lightning trainer,cb2a3265e5eb329a48fb44df6ab8fd74df62b85a,tests\trainer\test_optimizers.py
2943,Issue with pl.Trainer.from_argparse_args(...),  üêõ bug    to reproduce    steps to reproduce the behavior     use parser   pl trainer add argparse args   run python main py   overfit batches    the training runs over the whole dataset instead of running on a single batch        code sample    expected behavior    only one batch should have run   environment      cuda     gpu     tesla   pcie        available          true  version                     packages     numpy                     pytorch debug      false  pytorch version             pytorch lightning         tensorboard               tqdm                          system     os                 linux  architecture              processor             python                    version               smp thu jul         pdt          additional context     ,48f658fbb551e5f65a32938224dd782dd4605820,pytorch_lightning\core\datamodule.py
2955,Using IterableDatasets without __len__ for Training,  calling fit  internally calls enforce datamodule dataloader override  this function  has the if statement if   and datamodule         pytorch lightning pytorch lightning trainer configuration validator py               line          in                        if   and datamodule              this is similar to the pr       the problem is that the  translates to   but there s no dataloader  so bool  uses dataloader       but    dataloader  uses iterabledataset  for iterabledatasets for which  is undefined   the fix is also the same  the if dl should be replaced by if dl is not none   i will open a pr fixing this    ,88bfed371e9597e813384b3d951b0e5280be71bd,pytorch_lightning\trainer\configuration_validator.py
2956,'NoneType' object has no attribute 'lower'  while training on TPU,  üêõ bug    distributed backend is not set to  tpu  which breaks it here  line       pytorch lightning pytorch lightning trainer distrib data parallel py               line          in                        if self distributed backend lower  not in   ddp spawn    ddp cpu    tpu                distributed backend has to be explicitly specified in trainer params to have it working which is misleading from the docs    lightning readthedocs io en latest tpu html distributed backend with tpu    to reproduce    steps to reproduce the behavior     mnist tpu    expected behavior    should automatically set distributed backend and train successfully   additional context    related to                     ,cfd06a083b47b6c5c619f5362441197cb4e93e9e,pytorch_lightning\trainer\distrib_data_parallel.py
2961,AttributeError: 'NoneType' object has no attribute 'best_model_path' when `checkpoint_callback` = False,  üêõ bug    model does not complete training on tpu and error s out with error attributeerror   nonetype  object has no attribute  best model path  when checkpoint callback false   to reproduce    steps to reproduce the behavior   trainer trainer   trainer fit                                                                                  attributeerror                            traceback     in           trainer   trainer           trainer fit      opt conda lib     site packages pytorch lightning trainer states py in wrapped fn                      if entering is not none                          self state   entering                     result   fn                                 the interrupted state can be set inside the run function  to indicate that run was interrupted     opt conda lib     site packages pytorch lightning trainer trainer py in fit                    self accelerator backend setup                    self accelerator backend train                    self accelera,8be002ccc7c2e8371ab426ea07c953f72747269e,pytorch_lightning\accelerators\tpu_backend.py
3000,valdation_epoch_end won't log if no logging is done in validation_step,  üêõ bug     edenlightning   looks like setting both logger false and prog bar false won t do anything  if this is intended  maybe we should add a warning or something   also saw another issue  if i don t log anything in validation step then logged values in validation epoch end won t be logged too even if we set logger true  updated the notebook attatched above for the same to verify   code sample       scrollto      environment    pl  master  env  colab   ,3453bba898a8cab7e0a6fd73e988291740f295d0,pytorch_lightning\callbacks\model_checkpoint.py
3001,ModelCheckpoint does not create full path,  üêõ bug    to reproduce    run checkpoint callback   modelcheckpoint   only my folder is created   i think this line   discard the last trailing slash  so the directories are not created as intended when the paths are getting split   expected behavior    path should be fully created    ,580b04b490d4d6819133a5604ea0ef82e2a21727,CHANGELOG.md
3005,`type_as` bug in the doc of LightningModule,  üêõ bug    when i run this line of code   in the doc  it complains that  shouldn t be given a    to reproduce    code sample    the last line of the following code  x   torch zeros   new x   torch zeros   new x   new x type as    gives this error   typeerror  type as   argument  other    must be tensor  not str      expected behavior    cast new x to the same type as x   a potential fix   new x   new x type as   environment      cuda             gpu                     geforce rtx                      geforce rtx              available          true            version                   packages             numpy                               pytorch debug      false            pytorch version                     pytorch lightning                   tensorboard                         tqdm                        system             os                 linux            architecture                                          elf            processor                       python                ,9f6be96f845cabb114ef0df7a04498af6d5d8874,docs\source\introduction_guide.rst
3019,Results gathering with varying tensor shapes (e.g. last batch),  üêõ bug    results object reduction when batch sizes are different won t work because torch stack get s different input shapes  this can happen if your dataloader returns a smaller batch for the last iteration  for example   def recursive stack        for k  v in result items            if isinstance                recursive stack           if isinstance  and len      and isinstance                v   torch stack               result k    v  context  from slack discussion by  artgor      lightning slack com archives         ,9031dc3b817d46dc9b36007cce1360cfcf99939f,CHANGELOG.md
3032,Epoch counting is one-off in multiple instances,  üêõ bug  two issues occur     the final epoch does not save a checkpoint during training   resuming from a checkpoint n will start the epochs at n       expected behavior      final checkpoint should save a  ckpt file  as usual   should resume from epoch n       environment      cuda      gpu       tesla   dgxs        tesla   dgxs        tesla   dgxs        tesla   dgxs       available          true     version                   packages      numpy                        pytorch debug      false     pytorch version              pytorch lightning            tensorboard                  tqdm                        system      os                 linux     architecture                  processor                python                       version               ubuntu smp thu jul         utc         ,10150fccb001867472e3cbde298591999e321278,pytorch_lightning\callbacks\progress.py
3035,Incorrect Precision/Recall/F1 score compared to sklearn,  üêõ bug    to reproduce    steps to reproduce the behavior     copy the code  run the code from top to bottom  compare print results  see difference between sklearn and lightning    code    import torch  import numpy as np  import pytorch lightning as pl  from sklearn metrics import accuracy score  precision score  recall score       print            generate binary data  pl seed everything   n        number of samples  y   np random choice   y pred   np random choice   y tensor   torch tensor   y pred tensor   torch tensor         accuracy appears alright  print    print    print          results     accuracy from sklearn         accuracy from lightning functional tensor      accuracy from lightning tensor tensor       precision appears to be off  compared to sklearn  print    print    print          precision from sklearn         precision from lightning functional tensor      precision from lightning tensor tensor      recall appears to be off  compared to sklearn  print    print   ,28af34bc5134fddf544425fed9ffe04445b237e3,CHANGELOG.md
3053,load_from_checkpoint() doesn't work when a LightningModule inherits from typing.Generic,  üêõ bug    when a lightningmodule with saved hyperparameters inherits from   hyperparameters saved in the checkpoint file are not loaded automatically  causing an error  when  calls  to gather the list of arguments of the lightningmodule that inherits from    returns  instead of the actual arguments  because  implements an empty   the execution path ends up here       lib inspect py       as a result  pl filters out all the saved hyperparameters from the checkpoint  which results in an error when trying to instantiate the lightningmodule  i d assume this would happen when a lightningmodule inherits from any class that implements  such as    to reproduce    create a lightningmodule that inherits from typing generic with some hyperparameters  fit it  then try to load it from a checkpoint   code sample    import torch  import torch nn functional as f  import pytorch lightning as pl    from typing import generic  typevar  from torch utils data import dataloader    t   typevar       class g,88886ace7232c8e25ece431969c5d8d101f3368d,pytorch_lightning\core\saving.py
3097,IoU metric returns 0 score for classes not present in prediction or target,  üêõ bug    the iou metric implementation always returns a score of   for a class that is not present in either the prediction or the target  this can lead to a deflated score even for perfectly predicted examples   case    one example of an affected case is multi class semantic segmentation of an image that does not contain one of the classes  this can be outlined as follows     we have   possible classes in this dataset     ground truth target for an image consists only of classes   and     model perfectly predicts the target   the iou score should be        but the actual score will be deflated   since there will be an unnecessary penalty for class       case    another example that is a bit more implementation dependent to explain     target contains only   s   prediction perfectly assigns all   s   the iou score should be        but the actual score will be deflated   since there will be an unnecessary penalty for class     this only applies when a higher numbered class is present ,76c4afb840b0ae5fcafee07d527c58e9245d099d,CHANGELOG.md
3104,TPU available: true when there are no TPUs,  üêõ bug    i am using a dgx machine    but on initiating trainer  it logs tpu available  true  this ends up returning missing xla configuration when i run my script   to reproduce    code sample    simply running the following lines on my machine      trainer   pl trainer                                                                                                                    gpu available  true  used  true  tpu available  true  using    tpu cores  expected behavior       trainer   pl trainer                                                                                                                    gpu available  true  used  true  tpu available  false  using    tpu cores  environment      cuda             gpu                     tesla                  available          true            version                   packages             numpy                               pytorch debug      false            pytorch version                     pytorch lightning               ,69833dad5b2a0e7e68ed60a91a5a8c32ae22f707,CHANGELOG.md
3111,Horovod with native 16 precision not working,  üêõ bug    to reproduce    steps to reproduce the behavior     using precision   with distributed backend horovod    traceback       file   workspace main lightning py   line    in       main     file   workspace main lightning py   line    in main      trainer fit     file   usr local lib     dist packages pytorch lightning trainer states py   line    in wrapped fn      result   fn     file   usr local lib     dist packages pytorch lightning trainer trainer py   line    in fit      results   self horovod train     file   usr local lib     dist packages pytorch lightning trainer distrib parts py   line    in horovod train      model  optimizers   model configure apex     file   usr local lib     dist packages pytorch lightning core lightning py   line    in configure apex      model  optimizers   amp initialize       code sample    trainer   trainer           precision             gpus             distributed backend  horovod        environment      pytorch version           how you in,091d37f968b593e7e3b212d53bec6395a8c546de,CHANGELOG.md
3143,Trainer crashed when optimizer frequency is defined.,  üêõ bug    to reproduce    steps to reproduce the behavior   run the following code           traceback       file  pl bug py   line    in       trainer fit      file   opt conda lib     site packages pytorch lightning trainer states py   line    in wrapped fn      result   fn     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in fit      results   self accelerator backend train     file   opt conda lib     site packages pytorch lightning accelerators gpu backend py   line    in train      results   self trainer run pretrain routine     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in train      self run training epoch     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in run training epoch      batch output   self run training ba,a1ea681c47004599ee5a47a05ddd1b4ea12e60d4,CHANGELOG.md
3144,ONNX model does not save on GPU,  üêõ bug    attempting to export on onnx after training model on gpu  throws an error is the input sample or example input array is not a cuda tensor   to reproduce    steps to reproduce the behavior     train a model on gpu  try to export to onnx when  self example input array   torch zeros  or input sample   torch zeros                                                                                  runtimeerror                              traceback     in           filepath    model onnx           model to onnx      opt conda lib     site packages pytorch lightning core lightning py in to onnx                if  example outputs  not in kwargs                    self eval                    kwargs  example outputs     self                       torch onnx export      opt conda lib     site packages torch nn modules module py in   call                       result   self  slow forward                 else                     result   self forward                 for hook in self  forw,d9ea25590e95ca9e70401123a0f1f59de711e2ff,CHANGELOG.md
3162,RMSLE metric appears to be incorrect,  üêõ bug    the usage of mse in the rmsle function   looks wrong to me  it looks like this function currently computes  instead of    expected behavior    i would expect that rmsle looks like this   rmsle   rmse   torch log   reduction reduction    ,888340d17ed91eeee1b576cda36f13f0ef3e5459,CHANGELOG.md
3168,Max line length mismatch,  üêõ bug       yml   has  set to   whereas it is   in pyproject toml    expected behavior    line length should be consistent   ,59fb332677c0f865de1e42f2fc80caa2460915ff,.pep8speaks.yml
3172,"""Unsupported `ReduceOp` for distributed computing"" warning when using Result without distributed",  üêõ bug    step result py imports pytorch lightning metrics converters  converters py raises the following warning if torch distributed reduceop cannot be imported     rank zero warn     i don t use and don t want to use distributed training  but this warning is printed to stdout non stop at one warning per second rate   to reproduce    use result object without a distributed package available   expected behavior    this warning should be printed once at most   if i don t use distributed  i don t need to see this warning at all   environment      cuda     gpu     geforce rtx      available          true    version                 packages     numpy                       pytorch debug      false    pytorch version             pytorch lightning           tensorboard                 tqdm                      system     os                 windows    architecture          windowspe    processor            family   model   stepping    genuineintel    python                      version      ,99f05ed23f818d4f21c2c6925a66e75df606c859,pytorch_lightning\metrics\converters.py
3185,"Value out of range (expected to be in range of [-1, 0], but got 1)",  value out of range      exception in device tpu    torch xla csrc helpers cpp     check failed  min shape dim    dim    dim    max shape dim  following is the stack trace when i am using all   tpu cores on kaggle  the exact same code with    sync dist false    works completely fine on kaggle gpu     value out of range  exception in device tpu    torch xla csrc helpers cpp     check failed  min shape dim    dim    dim    max shape dim      begin stack trace      tensorflow  currentstacktrace   torch xla  xlahelpers  getcanonicaldimensionindex   torch xla  xlahelpers  maketransposepermutation   torch xla  xlatensor  transpose   torch xla  atenxlatype  t      impl  wrap kernel functor unboxed     at  tensor     call   at  t   at  tensor  t  const   pymethoddef rawfastcallkeywords   pymethoddescr fastcallkeywords   pyeval evalframedefault   pyfunction fastcallkeywords   pyeval evalframedefault   pyfunction fastcallkeywords   pyeval evalframedefault   pyfunction fastcallkeywords   pyeval ,3910ad033074367f6abfe0001562db725a75cb73,pytorch_lightning\callbacks\early_stopping.py
3189,Broken [Source] links in docs,  üìö documentation    hello  i was browsing the docs  and out of curiosity  i wanted to check the source for   and so i clicked on the  source  link of the  method  the link  however  sends to   lightning blob pytorch lightning core lightning lightningmodule py    which is not right  as the repo name is  pytorch lightning    i am not familiar with sphinx  but i checked the docs and the conf file  and i think that the reason can be found here         pytorch lightning docs source conf py              lines   to          in                        project    pytorch lightning           copyright   pytorch lightning   copyright            author   pytorch lightning   author                       the short x y version          version   pytorch lightning   version              the full version  including alpha beta rc tags          release   pytorch lightning   version                       options for the linkcode extension                                                        github user ,9be26d0c1b3aa2923475cd83098850d1e438a24e,docs\source\_templates\theme_variables.jinja
3199,Early Stopping + result dictionary + no validation not working.,  üêõ bug    the case where the user does not use validation and returns a dictionary   during training does not work in combination with early stopping   the test case which should check this is here         pytorch lightning tests callbacks test early stopping py              lines   to          in                        def test early stopping no val step               test that early stopping callback falls back to training metrics when no validation defined                       class currentmodel            def training step            output   super  training step           output update     could be anything else          return output                   model   currentmodel           model validation step   none          model val dataloader   none                   stopping   earlystopping           trainer   trainer           default root dir tmpdir           early stop callback stopping           overfit batches               max epochs                            result   trai,197acd535fee5e79dafeeff14cc742095c77bd70,docs\source\results.rst
3233,auto_scale_batch_size not working with datamodule,  üêõ bug    the trainer expects the lightningmodule to have self batch size   in training tricks py   however  if one is using the new lightningdatamodule  that should be the class with self batch size defined   to reproduce    assert hasattr   trainer   trainer   trainer fit   pytorch lightning utilities exceptions misconfigurationexception  field batch size not found in both  model  and  model hparams   expected behavior    auto scale batch size should work using lightningdatamodule  environment      packages      numpy                        pytorch debug      false     pytorch version              pytorch lightning            tensorboard                  tqdm                           ,48c22c8bad9a47141c7160d92f2edc9e2e4ad159,CHANGELOG.md
3253,**gather_all_tensors_if_available**  share the same underlying storage for all GPUs,  üêõ bug    hi  one of new features in         has a   copy bug   and this would lead that tensors in all gpus  are the wrongly same as one gpu   since they share the same storage     lightning blob master pytorch lightning metrics converters py      gathered result   world size    torch zeros like        change into   gathered result    torch zeros like  for   in range         ,d521c1b1787930dd4f6375a3c61a25579ca59ee5,pytorch_lightning\metrics\converters.py
3259,Cap batch size by number of training samples when using auto_scale_batch_size,  üêõ bug    the batch size finder sets an unrealistically high batch size if all samples of the training dataset fit into one batch        batch size   succeeded  trying batch size    batch size   succeeded  trying batch size    batch size   succeeded  trying batch size    finished batch size finder  will continue with full run using batch size        to reproduce    steps to reproduce the behavior     run mnist example with auto scale batch size true       expected behavior    batch size search space should not be larger than number of available training samples    ,e245065fbcc7701da528fbe2568242d50586a0a3,CHANGELOG.md
326,Broken link in Examples readme,  the link to the template at the top of   lightning examples examples    doesn t work  seems like the underlying codebase has shifted and the doc wasn t yet updated    ,c0bd203cffad86cc55fda2d87b7f7e0d51135166,docs\examples\Examples.md
3260,auto_scale_batch_size won't reset current_epoch,  üêõ bug    when auto scale batch size is enabled  the model is initially trained with varying batch sizes  when training begins  trainer current epoch equals   instead of     to reproduce    either observe the progress bar or use a simple callback to track the epoch number  once with  auto scale batch size  enabled and once with  auto scale batch size disabled   from pytorch lightning import callback    class printcallback              def   init              self observed epochs                     def on train epoch start            print           self observed epochs append          ,39b3704285e40a29a5862c4d8145b68d3b35d45e,CHANGELOG.md
3276,Logging non-tensor scalar with result breaks subsequent epoch aggregation,  üêõ bug    logging non tensor scalar with result breaks subsequent epoch tbptt aggregation        process   terminated with the following error   traceback       file   opt conda lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   opt conda lib     site packages pytorch lightning accelerators ddp spawn backend py   line    in ddp train      results   self trainer run pretrain routine     file   opt conda lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine      self train     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in train      self run training epoch     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in run training epoch      self run training epoch end     file   opt conda lib     site packages pytorch lightning trainer training loop py   line    in run training epoch end      epoch log metrics  epoch progres,2d5a7f5e7dc686cfc8172101a81505bf421468af,pytorch_lightning\core\step_result.py
3280,Error in transfer_batch_to_device when None type is in the batch,  üêõ bug    to reproduce    steps to reproduce the behavior     there should be no torchtext pre installed  run the sample code    code sample    from torch utils data import dataloader  import pytorch lightning as pl      def collate fn        return batch      class mydatamodule        def   init              super    init           def prepare data            pass        def setup            self train      input   torch randn    output   none          def train dataloader            return dataloader       class mymodel        def   init              super    init             self linear   torch nn linear         def forward            return self linear         def configure optimizers            return torch optim adam          def training step            x   batch     input            y   batch     output            loss   self           result   pl trainresult           result log           return result      def main          dataset      data module   mydatamodule           m,bd5f53c51994e14c79404c9dcededae53b21b664,pytorch_lightning\utilities\apply_func.py
3303,AUROC metric should throw an error when used for multi-class problems,  üêõ bug    auroc accepts multi class input without throwing an error  instead  it gives a random value  which gives the illusion that it is working   background    lightning auroc value for multi class seems to be completely off compared to sklearn using it wrong        to reproduce    steps to reproduce the behavior     manually create some multi class arrays  use pytorch lightning s auroc  metric  use sklearn s auroc metric  observe values not matching    code sample    import torch  import sklearn metrics  import pytorch lightning as pl  from pytorch lightning metrics classification import auroc    pl seed everything   auroc   auroc     def test auroc sk multiclass        for i in range            target   torch randint                       pred   torch rand  softmax     torch randint            score sk   sklearn metrics roc auc score   pred numpy   multi class  ovo   labels            score pl   auroc           print           assert torch allclose  float   torch tensor  float   ,b1347c956af4752560b53b891d352c48c6050305,CHANGELOG.md
3335,Cannot replicate training results with seed_everything and deterministic flag = True with DDP,  üêõ bug    i noticed this when i was adding more metrics calculation to the lightningmodule  for example  adding the confusion matrix at the end of validation test epoch   before and after i added these functions    i noticed the training results are not the exactly the same   however  once i added these function and re ran again  yes i got the same training results   to reproduce    code sample    expected behavior    the training results should be identical even if some deterministic functions are added  environment    please copy and paste the output from our  environment collection script        you can get the script and run it with   wget   lightning master tests collect env details py    for security purposes  please check the contents of collect env details py before running it   python collect env details py        pytorch version          os     linux  how you installed pytorch    pip  build command you used     python version       cuda cudnn version       gpu models and con,a71d62d8409f4960a4b438b8d19c924d3636c73f,CHANGELOG.md
3393,"MLFlow Logger slows training steps dramatically, despite only setting metrics to be logged on epoch",  üêõ bug    when using the mlflow logger  with a remote server  logging per step introduces latency which slows the training loop   i have tried to configure logging of metrics only per epoch  however it seems this still results in much slower performance  i suspect the logger is still communicating with the mlflow server on each training step   to reproduce      start an mlflow server locally    mlflow ui        run the minimal code example below as is     uncomment out the tracking uri to use the local mlflow server and run the code again  you will see a     times drop in the iterations per second     code sample    import torch  from torch utils data import tensordataset  dataloader  import pytorch lightning as pl    class mymodel        def   init              super    init             self num examples              self num valid              self batch size              self lr                self wd                self num features              self linear   torch nn linear      ,656c1af0df0cd0a8102a69c9c5045e86dc2b6b3a,CHANGELOG.md
3417,CometLogger failing without save_dir,  üêõ bug    cometmllogger with api key and  without save dir results in error   this happens due to this if   lightning blob master pytorch lightning loggers comet py       save dir is not set and later train loop tries to read it and fails   this can be fixed by setting  save dir to none  i will supply pr in a moment  to reproduce    steps to reproduce the behavior       model   lightningmodel       comet logger   cometlogger           api key key           workspace  workspace                trainer   trainer       trainer fit       traceback     trainer fit   file       site packages pytorch lightning trainer states py   line    in wrapped fn  result   fn   file       site packages pytorch lightning trainer trainer py   line    in fit  results   self accelerator backend train   file       site packages pytorch lightning accelerators gpu backend py   line    in train  results   self trainer run pretrain routine   file       site packages pytorch lightning trainer trainer py   line    ,5b4db52851000d5e4eca8c680d851bcdaafc3a80,pytorch_lightning\loggers\comet.py
3424,DataModule with lr_find not supported,  üêõ bug    to reproduce    steps to reproduce the behavior     create a trainer    a lightningmodule   and a datamodule    call trainer lr find   error  typeerror  lr find  got an unexpected keyword argument  datamodule     code sample    import torch  import torch nn as nn  import pytorch lightning as pl  from torch utils data import dataloader  class datamodule        def   init              super    init         def gen set            vals   torch tensor  for i in range             return torch stack   torch cos            def train dataloader            return dataloader        def val dataloader            return dataloader        def test dataloader            return dataloader      class litmodel        def   init              super    init             self in layer   nn linear           self out layer   nn linear           self criterion   nn mseloss           self lr            def forward            inp   torch tanh            return self out layer       def training step    ,e4e60e9b82adc48482db4721ce3e1fdc3ab6d6fe,CHANGELOG.md
3487,Gradient norms are not logged unless row_log_interval==1,  üêõ bug    in version     the guards to calculate the gradient norms and then log the metrics can t be satisfied in the same batch unless the row log interval  is    in most places the guard seems to be     self row log interval      such as here         pytorch lightning pytorch lightning trainer training loop py              lines   to          in                        def save train loop metrics to loggers              when metrics should be logged          should log metrics       self row log interval      or self should stop          if should log metrics or self fast dev run             logs user requested information to logger          metrics   batch output batch log metrics          grad norm dic   batch output grad norm dic          if len      or len                self log metrics              however in run batch backward pass it is batch idx   self row log interval             pytorch lightning pytorch lightning trainer training loop py              lines   to          ,4ed96b2eb471124184144f96d259055e49ac97e7,CHANGELOG.md
3549,Bug in validation_epoch_end,  üêõ bug    in the documentation     is described as running at the end of a validation epoch and does not need to necessarily return anything   when running a slightly modified version of the example   in the docs   seemingly runs once on a single batch of validation data and returns an error if nothing is returned   to reproduce    a mwe   import os  import torch  import torch nn functional as f  from torchvision datasets import mnist  from torchvision import transforms  from torch utils data import dataloader  import pytorch lightning as pl  from torch utils data import random split    class litmodel          def   init              super    init             self     torch nn linear           self     torch nn linear         def forward            x   x view                x   self             x   f relu           x   self             return x        def configure optimizers            optimizer   torch optim adam   lr               return optimizer        def training step          ,9acee67c31c84dac74cc6169561a483d3b9c9f9d,pytorch_lightning\trainer\connectors\logger_connector.py
3578,"Incorrect ""Saving latest checkpoint"" warning",  üêõ bug     saving latest checkpoint     warning appears regardless of whether a modelcheckpoint exists or save last is set to true        pytorch lightning pytorch lightning trainer training loop py              lines   to          in                          save latest checkpoint          rank zero warn           self check checkpoint callback                    pytorch lightning pytorch lightning trainer training loop py              lines   to          in                        def check checkpoint callback            model   self trainer get model                      when no val loop is present or fast dev run still need to call checkpoints            todo bake this logic into the checkpoint callback          should activate   not is overridden  and not should check val          if should activate or force save           checkpoint callbacks    c for c in self trainer callbacks if isinstance                     c on validation end  for c in checkpoint callbacks              this,ed12e422a42472af1acb88f870dba3d43710b31d,pytorch_lightning\trainer\training_loop.py
3597,distributed training: ModelCheckpoint is receiving bad data,  you can reproduce in   minutes on         i tried master and got an unrelated wandb error and gave up trying to reproduce there   you must be on a machine with multiple gpus  git clone git github com huggingface transformers git  cd transformers  pip install  e    pip install  e   examples     installs pytorch lightning         git checkout pl checkpoint bug  cd examples    wget https     amazonaws com datasets huggingface co translation wmt en ro tar gz  tar  xzvf wmt en ro tar gz    export max len    export m sshleifer      python finetune py        learning rate            do train        do predict                 val check interval            data dir wmt en ro        max source length  max len   max target length  max len   val max target length  max len   test max target length  max len        freeze encoder   freeze embeds        train batch size     eval batch size          tokenizer name  m   model name or path  m        warmup steps     sortish sampler   logger name wandb ,2aebf65241ab054df9256cc33d37236651691a48,tests\checkpointing\test_model_checkpoint.py
3600,Infinite hang when running `Trainer.test` after `Trainer.fit` with DDP,  üêõ bug    if i run trainer test after running trainer fit with distributed backend  ddp  then the system hangs   to reproduce    steps to reproduce the behavior   run the following script    main py  import os  from argparse import argumentparser  from pl examples models lightning template import lightningtemplatemodel  from pytorch lightning import trainer  seed everything    seed everything       def main        model   lightningtemplatemodel        trainer   trainer from argparse args       trainer fit        if this is commented out then test will complete  otherwise it hangs      trainer test       def run cli        root dir   os path dirname        parent parser   argumentparser       parser   lightningtemplatemodel add model specific args       parser   trainer add argparse args       parser set defaults       args   parser parse args         main       if   name         main          run cli   with command line arguments    python main py   gpus     hidden dim     max epochs ,d2a3d6aa8e8b69e6f373243bd25165a0963d7a53,docs\source\multi_gpu.rst
3619,ModelCheckpoint period should not always save on the first epoch,  üöÄ feature    period should work so     modelcheckpoint on validation end  if     period         do not save      return      e g     period       save on epoch             period       save on epoch             period       save on epoch               currently  it always runs on the first epoch and then runs every period epochs  e g     period       save on epoch             period       save on epoch             period       save on epoch               this would also allow having period     which would never save  just as save top k      motivation    i want to save a checkpoint every period epochs but current behaviour forces to always save on the first one    ,3b2efe5b2afe664d88c9d5eda127774bba4cff4c,CHANGELOG.md
3652,Creation of many data module instances incurs RecursionError,  üêõ bug    thank you for a nice framework   when i repeated hundreds of experiments  each time with a new instance of a single lightningdatamodule class  recursionerror was raised  i also found that creating data modules and calling setup  were enough to reproduce the issue   to reproduce    please look at the following code sample and error messages   code sample    import pytorch lightning as pl    class dummydm        def setup            pass    if   name         main          max iters          for i in range            try               dm   dummydm               dm setup           except recursionerror               print               raise      error messages    recursionerror occured in the   th iteration   traceback       file  test dm py   line    in       dm setup     file   workspace src  venv lib     site packages pytorch lightning core datamodule py   line    in wrapped fn      return fn     file   workspace src  venv lib     site packages pytorch lightning core datamod,17c8c95fbc7b31f73671761430e86d881f6d6c6d,pytorch_lightning\core\datamodule.py
3668,incorrect batch_sizes when Dataloader returns a dict with multiple tensors.,  üêõ bug    tracked batch sizes in result object are incorrect when a dataloader returns a dict with multiple tensors   to reproduce    create data loader that returns a dict  e g  batch     batcha   tensor a   batchb   tensor b    both entires have batch size n with n        for this example a batch size of   will be logged since len               pytorch lightning pytorch lightning trainer evaluation loop py              lines   to          in                          track batch size for weighted average          is result obj   isinstance           if is result obj           output track batch size                     pytorch lightning pytorch lightning trainer training loop py              lines   to          in                          track batch size for weighted average          if is result obj           training step output track batch size               expected behavior    log correct batch size   i m not sure what can be defined as the  correct  batch size when there are m,b34c7add23553f10f6f0d7caf4177c67ee213f3a,pytorch_lightning\core\step_result.py
367,setting gpus=-1 and gpus='-1' in Trainer give different behaviours,  i discovered this while looking through the code  trainer constructor does not mention that  gpus can be    or       however if such values are passed they are accepted and result in  different behaviour     will result in no gpus used       will use all available gpus   to reproduce    run any model first setting trainer gpus parameter to     no gpus will be used   run same model setting gpus to       all available gpus will be used     being able to set    to indicate that all gpus should be used is and i believe useful behaviour   the issue is in function self   parse gpu ids    where the handling of    when passed as int is not implemented   solution would be to implement equivalent logic for    as for        happy to submit a pr    ,2aba70e228b427b16e547e030ab3bbad736b5b00,docs\Trainer\Distributed training.md
3693,"Missing attribute ""training_step_output_for_epoch_end""",  i used the documentation way of stopping the training    lightning readthedocs io en latest early stopping html enable early stopping using callbacks on epoch end      if on bath start method returns    at the very beginning of an epoch  the titled attributeerror exception   the problem is in training loop py line       code sample    use the method and run your code       def on batch start            return     expected behavior    check batch output value if equals    before running trainin loop py line     the early stopping method achieved the same way the documentation specifies should not throw an exception but rather simply stop the training   environment      cuda     gpu   available          false  version            none      packages     numpy                     pytorch debug      false  pytorch version           pytorch lightning         tqdm                          system     os                 windows  architecture        windowspe      processor            family   ,9942f3ebdf14d0139b1b156dd56662b425f3c777,CHANGELOG.md
3778,training_step log requires that tbptt_reduce_fx is also set,  üêõ bug    training step log requires that tbptt reduce fx is also set   code sample    def training step                 self log       self log            foo            torch tensor            on step false           on epoch true           reduce fx max            tbptt reduce fx max  error when commented             return loss    def validation step          no issues here      self log            bar            torch tensor            on step false           on epoch true           reduce fx max          error   time outputs      tr loss   tensor    foo   tensor    minimize   tensor            classmethod      def reduce across time              auto reduce across time for tbptt          meta   time outputs     meta                    in     the results have  extra   once we deprecate       we may not need this          if  extra  in time outputs                   x pop  for x in time outputs                 result   cls           result   recursive gather           recursive st,89cc12311f5eaa7860d66bce9bfe3d93255f35b6,pytorch_lightning\core\step_result.py
3780,auto_scale_batch_size doesnt use 'binsearch',  i tried to following and it s still using power                               init model                            model   litautoencoder                                 init trainer                          trainer   pl trainer                                 tune                         trainer fit       did we remove support  or is that a bug    ,f745c4a773fa9742a1c3cc9d051af0acbfe411ec,docs\source\training_tricks.rst
3797,Broken ddp_cpu backend,  üêõ bug    broken on current master   to reproduce    def test        import pytorch lightning as pl      trainer   pl trainer           default root dir tmpdir           max epochs             limit train batches             limit val batches             distributed backend  ddp cpu               model   dummymodule    linear layer on mnist      trainer fit   error   e          process   terminated with the following error   e       traceback     e         file   home carmocca projects pylaia venv lib     site packages torch multiprocessing spawn py   line    in  wrap  e           fn   e         file   home carmocca projects pylaia venv src pytorch lightning pytorch lightning accelerators ddp cpu spawn backend py   line    in ddp train  e           results   self train or test   e         file   home carmocca projects pylaia venv src pytorch lightning pytorch lightning accelerators base backend py   line    in train or test  e           results   self trainer train   e         file   ,22efce8f400ab452ad1369e6ec9e8e733cc9a93d,pytorch_lightning\overrides\data_parallel.py
3811,ModelCheckpoint not picking up metrics logged from lightning module,  üêõ bug    the model checkpoint raises a misconfiguration error because metrics logged from validation epoch end are mysteriously unavailable to the callback  to reproduce    from typing import optional  import torch  from pytorch lightning import trainer  lightningmodule  from pytorch lightning callbacks import modelcheckpoint  from torch utils data dataset import dataset  class randomdataset        def   init              self len   length          self data   torch randn       def   getitem              return self data index       def   len              return self len  class testmodule        def   init                 lightningmodule for testing purposes          args               epoch min loss override    pass in an epoch that will be set to the minimum                  validation loss for testing purposes    if none this is ignored  defaults to none                        super    init             self layer   torch nn linear           self epoch min loss override   epoch min,d9bc95f83e163f1ef0e64012ad086d4448410817,pytorch_lightning\callbacks\early_stopping.py
3813,Calling module.log(...) within a callback fails,  üêõ bug    calling pl module log  within a callback fails  even though this is recommended by the documentation here    lightning readthedocs io en latest loggers html logging from a callback    error      file  my callback file py   line xx  in on validation epoch end      pl module log dict     file   home local usherbrooke   opt   envs cav lib     site packages pytorch lightning core lightning py   line    in log dict      self log     file   home local usherbrooke   opt   envs cav lib     site packages pytorch lightning core lightning py   line    in log      self  results log     file   home local usherbrooke   opt   envs cav lib     site packages pytorch lightning core step result py   line    in log      self   set meta     file   home local usherbrooke   opt   envs cav lib     site packages pytorch lightning core step result py   line    in   set meta       internal   self  meta     internal    keyerror    internal   python baseexception      cc  nathanpainchaud    this is happ,3d202f9ecc4137b08cb5b1ac15af276456fcfaaf,CHANGELOG.md
3898,"TypeError: expected str, bytes or os.PathLike object, not NoneType",  üêõ bug    i am summarizing the source of the issue to speedup the fix   after this line of code        pytorch lightning pytorch lightning accelerators ddp backend py               line          in                        env copy  pl global seed     os environ get              i have that env copy  pl global seed   is none and having an environment variable set to none breaks subprocess popen here        pytorch lightning pytorch lightning accelerators ddp backend py               line          in                        proc   subprocess popen              my fix at the moment is to add  if env copy  pl global seed   is none                   del env copy  pl global seed        after        pytorch lightning pytorch lightning accelerators ddp backend py               line          in                        env copy  pl global seed     os environ get              environment      cuda      gpu      available          false     version                   packages      numpy              ,e4a56fa5cfb5b67147c2013ae444ad0cd9a1b63a,pytorch_lightning\accelerators\ddp_backend.py
3906,Infinite recursion when calling `self.log(...)` in validation loop with dataset that returns string in item dict,  üêõ bug    i m not sure if this is a behavior that was intended to be supported in the first place  but pr      introduced a regression on passing strings as part of the data in a batch  now  if we pass a dictionary where one of the values is a string   falls into an infinite recursion loop when trying to log anything during the validation step   to reproduce    see pr      for a test that reproduces the bug on the current master  the test becomes functional when commenting out line       the recursion happens in this specific statement in unpack batch size    elif isinstance        sample   next           size   self unpack batch size   which recurses infinitely when sample is a string   the full stacktrace i get when running the test is the following   test logging py                                                                                              pytorch lightning trainer trainer py    in fit      results   self accelerator backend train         pytorch lightning acceler,c510a7f90077140d60c47adf8e1e73638c2d1017,pytorch_lightning\core\step_result.py
394,ModelCheckpoint wipes out current directory,   williamfalcon   i think this is what you were seeing in       if we let the  create the default  callback and don t use   the prefix ends up being set to the current directory  then  when  tries to clean up previous checkpoints  it wipes out everything in the current directory   relevant bits of code     default save path set to os getcwd     lightning blob master pytorch lightning trainer trainer py    modelcheckpoint falls back to default save path    lightning blob master pytorch lightning trainer trainer py      modelcheckpoint blows away pre existing files in checkpoint directory    lightning blob master pytorch lightning callbacks pt callbacks py      the most obvious fix is to provide a better default checkpoint prefix  but there would still be a lurking footgun for a user who sets default save path incorrectly  should we maybe insist that the checkpoint directory not exist before training starts  or that it be empty    ,9fa28066059c3bda0b022c33921796c7425cd41e,pytorch_lightning\callbacks\pt_callbacks.py
3945,Unexpected signature for validation_step,  üêõ bug    typeerror  validation step  takes   positional arguments but   were given      full stacktrace        this test is passing for me on        but not on master  to reproduce    from typing import optional  import unittest  import torch  from pytorch lightning import lightningmodule  from torch utils data dataset import dataset  class randomdataset        def   init              self len   length          self data   torch randn       def   getitem              return self data index       def   len              return self len  class testmodule        def   init                 lightningmodule for testing purposes          args               epoch min loss override    pass in an epoch that will be set to the minimum                  validation loss for testing purposes    if none this is ignored  defaults to none                        super    init             self layer   torch nn linear           self epoch min loss override   epoch min loss override      def forward       ,6044cf900317ec9542fb1745976c9a96cc70b396,pytorch_lightning\trainer\data_loading.py
3974,[Bug]: Late update of Trainer `current_epoch` property for `LightningDataModule`,  üêõ bug    late update of trainer current epoch property for lightningdatamodule object   to reproduce    the below code reproduces the issue   please check for the print logs for the current epoch number in train dataloader   code sample    import torch  import torch nn as nn  import torch nn functional as f  from torchvision import transforms  from torchvision datasets import mnist  from torch utils data import random split  dataloader    import pytorch lightning as pl      class litmodel          def   init                super    init                 we take in input dimensions as parameters and use those to dynamically build model           self channels   channels          self width   width          self height   height          self num classes   num classes          self hidden size   hidden size          self learning rate   learning rate            self model   nn sequential               nn flatten                nn linear                nn relu                nn dropout   ,fcfa5874923000a4b391c88b6488e065aee4d671,CHANGELOG.md
3993,Mismatch between docstring and code regarding when `on_load_checkpoint` hook is called,  üêõ bug    the docstring of on load checkpoint¬†hook says that it is called before trying to load state dict         pytorch lightning pytorch lightning core saving py              lines   to          in                        def on load checkpoint     none                                do something with the checkpoint                   gives model a chance to load something before   state dict   is restored              however  in lightningmodule load from checkpoint  it is called after load state dict         pytorch lightning pytorch lightning core saving py              lines   to          in                          load the state dict on the model automatically          model load state dict                      give model a chance to load something          model on load checkpoint              additional context    related discussion on slack    lightning slack com archives        i think the docstring is correct and the call to on load checkpoint¬†should be moved right before,a8573b005224bde87eb7a81ccdf5f428620c121f,pytorch_lightning\core\saving.py
4001,on_train_epoch_end and on_epoch_end are out of order,  üêõ bug    consider the following order in which the  hooks are called from          on epoch start  on train epoch start  on validation start  on validation epoch start  on validation epoch end  on validation end  on epoch end  on train epoch end      naturally one would expect the opening and closing scope hooks to match  however  on train epoch end is called after on epoch end  which seems incorrect  it is natural to open the epoch scope before the train epoch scope    in which case the epoch scope should be closed after closing the train epoch scope      pytorch version           os    ubuntu      how you installed pytorch    pip  build command you used     python version         cuda cudnn version  na  gpu models and configuration  na  any other relevant information  na     ,3777988502d1013508455a5fd34dc7d1a7e8e035,pytorch_lightning\utilities\model_utils.py
4011,Broken link in Documentation,  üìö documentation    the module index link at the bottom of the main page of the lightning documentation is broken  this seems to be because the make html command does not create a py modindex html file     if the module index page is not required a solution is to remove    ref  modindex from the index rst file   additionally  below the module index link there is a link to a search page  that is currently empty  seeing as searching is possible in the sidebar  not sure if the page is required  so could remove    ref  search as well   not super familiar with sphinx but think this wouldn t break anything    ,9e919763231d6f210f711515e496c953e6411a57,docs\source\index.rst
4020,"Validation loss Tensor object is print in progress bar, it is expected only value",  üêõ bug    when i add validation loss in progress bar training  tensor object is printed whereas only loss value is expected   for example     epoch       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     s  loss      v num    val loss tensor    validation loss is added with the following command   self log   i tried self log   prog bar true  with no effect   to reproduce    the bug is reproductible with the minimal code example    lightning blob master pl examples bug report model py     see code sample below with validation step overridden    code sample    class testmodel          def validation step            output   self layer           loss   self loss           self log           return   x   loss       expected behavior    it is expected to only obtain value of validation loss in progress bar and not tensor object   environment      cuda     gpu   available          false  version            none      packages     numpy                     pytorch debug      false  pytorch version           pyt,bdbf84602973dc86a16f66d2902b22ee5a4c9f21,CHANGELOG.md
403,Error when model checkpoint and no early stop,    when creating a   if we set a  and  we get an error at this line   here   to reproduce  steps to reproduce the behavior     create a ckpt   modelcheckpoint   create a trainer  setting checkpoint callback   ckpt and early stop callback false  see the error attributeerror   nonetype  object has no attribute  wait     expected behavior  it should be possible to save the model without setting an earlystopping condition  of course one could set an earlystopping with the max integer  but changing the condition from an or to an and solves the problem   desktop    os  ubuntu      browser  firefox quantum  version            ,e7c12d936e30aec96b1bf333ed9dc17c736dcc9e,pytorch_lightning\trainer\trainer_io.py
4073,Data Parallel bug (return outputs not being moved to same device),  üêõ bug    under backend  dp  doesn t handle reduction of the loss across multiple gpus correctly  this is present in             to reproduce    code sample    import torch  import pytorch lightning as ptl  from pytorch lightning import lightningmodule  from torch utils data import dataset      class randomdictdataset        def   init              self len   length          self data   torch randn         def   getitem              a   self data index           b   a              return   a   a   b   b         def   len              return self len      class randomdictstringdataset        def   init              self len   length          self data   torch randn         def   getitem              return   id   str    x   self data index          def   len              return self len      class randomdataset        def   init              self len   length          self data   torch randn         def   getitem              return self data index         def   len              return,f23f5e56480d1a4784fc7829d014590fe4ca1454,CHANGELOG.md
4141,the self.log problem in validation_step(),  as doc say we should use self log in last version   but the loged data are strange if we change evalresult  to self log   then we check the data in tensorboard  the self log  will only log the result of last batch each epoch  instead of the mean of them   that is quite unreliable about this issue  it must be turned back to evalresult  for correct experiments    ,45d05ff68dbf3db300a782af97ea54cab70a3ff9,pytorch_lightning\core\step_result.py
4188,To many backwards with LBFGS,  üêõ bug    when using lbfgs we have one backward step to much  because we call backward before the optimiser step    but the optimizer step get s a closure and therefore calls backward again   to reproduce    import torch  import pytorch lightning as ptl  from pytorch lightning import lightningmodule  from torch utils data import dataset      class randomdictdataset        def   init              self len   length          self data   torch randn         def   getitem              a   self data index           b   a              return   a   a   b   b         def   len              return self len      class randomdictstringdataset        def   init              self len   length          self data   torch randn         def   getitem              return   id   str    x   self data index          def   len              return self len      class randomdataset        def   init              self len   length          self data   torch randn         def   getitem              return self ,0ec410769744843726a140b7efabbeeb1c4e2929,pytorch_lightning\accelerators\accelerator.py
4208,AttributeError: 'Trainer' object has no attribute 'hpc_save',  üêõ bug    getting the following error in slurm cluster   attributeerror   trainer  object has no attribute  hpc save         environment    pytorch   ,66e58f5afb6ae8702b29ada52f7b022bbf201f9e,pytorch_lightning\trainer\connectors\slurm_connector.py
4229,Comet logger overrides COMET_EXPERIMENT_KEY env variable,  after       there is a changed logger behavior  it starts using   but it doesn t respect it if it is set already   so the bug is in the following   i already set this variable  then logger overwrites my value here   lightning blob master pytorch lightning loggers comet py      then it deletes this variable at all here   lightning blob master pytorch lightning loggers comet py      this way it ignores my variable and deletes it at all later  moreover in version function it also ignores my set variable  i will create a pull request to fix it   ,4106e2f11292979022c437b9c49b0b3348f4682f,pytorch_lightning\loggers\comet.py
4234,Values logged in test_epoch_end not returned when calling test(),  üêõ bug    when calling test   if values are logged only in the test epoch end method they are not returned  this leads to the following somewhat inconsistent behaviour     values logged only in step method    appear in list returned by test    values logged in step and epoch end    both appear in list returned by test    values logged only in epoch end    values do not appear     please reproduce using the boringmodel and post here       usp sharing    expected behavior    if values are logged only in epoch end they should be returned by test    environment    pl version         see colab    ,c33688195964f7582011d10464cced0bc43fbe55,pytorch_lightning\trainer\connectors\logger_connector.py
4268,"EarlyStopping mode auto is unknown, fallback to auto mode.",  in the process of refactoring as i upgraded lighting and it looks like there s been a slight change to the callback interface   this code   early stop callback   pl callbacks earlystopping   results in the message     earlystopping mode auto is unknown  fallback to auto mode   earlystopping mode set to min for monitoring early stop on     the default for mode is  auto  so the first message doesn t make sense   ,2ffad4c89fcd19800a7532a8f7821578770451c6,pytorch_lightning\callbacks\early_stopping.py
427,save_weights_only parameter in ModelCheckpoint class look like doesn't work,  common bugs       tensorboard not showing in jupyter notebook see issue     pytorch       vs       support see faq    describe the bug  save weights only parameter in modelcheckpoint class look like doesn t work  document describe save weight only like that  save weights only  if true  then only the model s weights will be saved     else the full model is saved      but save weight only parameter doesn t save model differently each different options  to reproduce  steps to reproduce the behavior     i used sample script in official document    import os  import torch  from torch nn import functional as f  from torch utils data import dataloader  from torchvision datasets import mnist  import torchvision transforms as transforms    import pytorch lightning as pl    class coolsystem          def   init              super    init               not the best model             self     torch nn linear         def forward            return torch relu                def training step        ,8c4c7b105e16fbe255e4715f54af2fa5d2a12fad,CHANGELOG.md
4275,[HOT-BUG] Checkpoint in callbacks list fails,  üêõ bug    please reproduce using the boringmodel and post here    the modelcheckpoint is not properly setup when provided through the list of callbacks   def test checkpoint within callbacks list                 this test validates that the checkpoint can be called when provided to callacks list                 os environ  pl dev debug                checkpoint callback   modelcheckpoint          class extendedboringmodel              def validation step                output   self layer               loss   self loss               return   val loss   loss         model   extendedboringmodel       model validation step end   none      model validation epoch end   none      trainer   pl trainer max epochs                               limit train batches                               limit val batches                               limit test batches                               callbacks          trainer fit       assert os listdir       epoch   ckpt           tests checkpointing tes,8a20d6af51d13adec37593c1356ce08ef380e828,pytorch_lightning\callbacks\model_checkpoint.py
4276,WandbLogger fails in 1.0.2 due to non-JSON serializable object,  üêõ bug    after updating to pl        the wandblogger fails with the following typeerror   traceback       file  wandblogger issue py   line    in       wandb logger log hyperparams      file   home groups mignot   envs pl lib     site packages pytorch lightning utilities distributed py   line    in wrapped fn      return fn     file   home groups mignot   envs pl lib     site packages pytorch lightning loggers wandb py   line    in log hyperparams      self experiment config update     file   home groups mignot   envs pl lib     site packages wandb sdk wandb config py   line    in update      self  callback      file   home groups mignot   envs pl lib     site packages wandb sdk wandb run py   line    in  config callback      self  backend interface publish config     file   home groups mignot   envs pl lib     site packages wandb interface interface py   line    in publish config      cfg   self  make config     file   home groups mignot   envs pl lib     site packages wandb interfa,f07ee33db679a4b4bdcb4a2a221aa5cbb05d7b34,.gitignore
4304,TensorBoardLogger not working as expected with accumulate_grad_batches&gt;1,  üêõ bug    when logging inside training step to tensorboard and using accumulate grad batches     inside pl trainer  the behavior is not as expected   with  everything looks good       with accumulate grad batches     the values are reported on the same step       to reproduce      import os    import torch  from torch nn import functional as f  from torch utils data import dataloader  random split    import pytorch lightning as pl  from pytorch lightning loggers import tensorboardlogger  from torchvision datasets mnist import mnist  from torchvision import transforms    class litclassifier        def   init              super    init             self save hyperparameters             self     torch nn linear           self     torch nn linear         def forward            x   x view                x   torch relu            x   torch relu            return x        def training step            x  y   batch          y hat   self           loss   f cross entropy           self log       ,204a0a2d03ce7dfb014e347f1d34a49ef5e86902,CHANGELOG.md
4486,TrainerDataLoadingMixin.replace_sampler ignores multiprocessing_context,  üêõ bug    replace sampler in trainerdataloadingmixin ignores multiprocessing context  please reproduce using  the boringmodel and post here        usp sharing    expected behavior    it should return a new data loader with the replaced sampler  and the same multiprocessing context  environment      cuda     gpu     tesla        available          true  version                     packages     numpy                     pytorch debug      false  pytorch version             pytorch lightning         tqdm                          system     os                 linux  architecture              processor             python                    version               smp thu jul         pdt           ,dee968f20b89db7d6cbdd6069e5ed307980cbc86,pytorch_lightning\trainer\data_loading.py
4556,Gpu memory leak with self.log on_epoch=True,  pl        using new logging api i want to log a metric in lightningmodule  self log       this is a dummy example but it is sufficient to add to lightningmodule s training step to cause a memory leak on gpu   what could go wrong  we want to log a metric which is not even a cuda tensor  how could it lead to a gpu memory leak   well thanks to the magic of metric epoch aggregation stuff  let s dig in and take a look at here        pytorch lightning pytorch lightning trainer training loop py              lines   to          in                                                                          training step   training step end                                                          batch output   self run training batch                      when returning    from train step  we end epoch early          if batch output signal                 break                     only track outputs when user implements training epoch end            otherwise we will build up unnecessary memory  ,514cb22bd719e6ca056cacce730c8de875c9dbf6,pytorch_lightning\core\step_result.py
4681,self.log on validation_step is broken on pre 1.1 [nightly],     usp sharing     ,867eef0e4c34e92887faee7040779e8cde00b20f,pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py
4781,Potential bug in metric when updated with a slice of tensor in DDP,  üêõ bug    when a metric is updated with a slice of tensor as one of the inputs   with multiple gpu with ddp  it throws out an error   runtimeerror  tensors must be non overlapping and dense      once the slice of the tensor is clone and detach  then it works   please reproduce using  the boringmodel and post here     the issue can be reproduced below      usp sharing    to reproduce    expected behavior    the metric update should work with a slice of tensor  environment    cuda     gpu     tesla   pcie      available          true    version                   packages     numpy                     pytorch debug      true  pytorch version             pytorch lightning         tqdm                          system     os                 linux  architecture              processor             python                    version               smp thu jul         pdt          additional context     ,1b40a4053d4b2116de02938b747946446443c54a,CHANGELOG.md
4857,Logging not working in `on_train_batch_end` of a callback.,  üêõ bug    logging in on train batch end of a callback doesn t work but for on train validation batch end it does   i e  below only eval will be logged not train  class testcallback            def on train batch end            pl module log         def on validation batch end            pl module log   reproduce      usp sharing    context  i found that error because logging of training in pl bolts callbacks ssl online sslonlineevaluator does not work    ,2e838e6dd8803f40da3a1d4111669bd69ae7dd0f,docs\source\logging.rst
4928,update min dependencies,  üêõ bug    after pip install strategy changes we have several packages which are not feasible to install together  to reproduce    see all min ci config   lightning runs      expected behavior    update some libs to be installable  probably go one by one or list dependencies of our dependencies and find a minimal intersection   ,563f9214fa4add3e984de993c231ac0ff2f4fca6,.github\workflows\ci_test-full.yml
4953,manual_optimization does not work with ddp,  üêõ bug    can t run ddp with manual optimization  fails on the second batch with a error   runtimeerror  expected to mark a variable ready only once  this error is caused by one of the following reasons     use of a module parameter outside the forwardfunction  please make sure model parameters are not shared across multiple concurrent forward backward    reused parameters in multiple reentrant backward passes  for example  if you use multiplecheckpoint functions to wrap the same part of your model  it would result in the same set of parameters been used by different reentrant backward passes multiple times  and hence marking a variable ready multiple times  ddp does not support such use cases yet   to reproduce    change optimization to manual in basic gan bolt   expected behavior    do not fail when n gpus      environment      cuda     gpu     tesla        tesla        tesla        tesla            available          true  version                     packages     numpy             ,239347435029c0a02b305201ebbfa39d62746ca8,benchmarks\test_sharded_parity.py
4974,AttributeError: 'LightningOptimizer' object has no attribute 'state',  i am using pytorch lightning with adam optmiser to train a byol model  the model and pipeline works fine while training  when i stop and resume from checkpoint  it raise this error  i didn t do anything just resume from previous checkpoint  have no idea why there is an error like this  i upgrade my bolts and pytorch lightning to the mast        my opmizer is like this  and adam is the original adam provided by pytorch       ,7755572b4f37b811b83f6a933329b01af4735e66,pytorch_lightning\core\optimizer.py
4978,Fix pipy badges and images not rendering,  it seems that it did not work for pypi page    see our rc     lightning          when i open the pkg info i still see the original paths     downloaded implemented in        ,e2c404bad2eaf90d77d2faf3d2802f783f8dc4f7,.github\workflows\release-pypi.yml
498,Escaping % in add_default_args,  describe the bug  in utilities arg parse py  a percentage symbol is not escaped and would cause an error when printing help information   parser add argument    overfit   default     type float                           help    of dataset to use with this option  float  or    for none    to reproduce  steps to reproduce the behavior   import os  import random  import sys  from pytorch lightning utilities arg parse import add default args  from test tube import hyperoptargumentparser  experiment    if   name         main          root dir   os path split           parent parser   hyperoptargumentparser       add default args       hyperparams   parent parser parse args       execute the file with   help  python temp py   help      throws an error   warning root this   python run does not have gpu support  will run in cpu only mode   traceback       file   users chenghaomou code   temp py   line    in       hyperparams   parent parser parse args     file   users chenghaomou anaconda en,89f7a82157297f32ca12283c0badbc5b50bb5224,pytorch_lightning\utilities\arg_parse.py
529,Training with DDP could fail at startup due to FileExistsError,  in multi gpu mode with ddp  starting the training can fail with the following error   traceback       file  train py   line    in       main     file  train py   line    in main      trainer fit     file   home tanel   lib     site packages pytorch lightning trainer trainer py   line    in fit      mp spawn      file   home tanel   lib     site packages torch multiprocessing spawn py   line    in spawn      while not spawn context join      file   home tanel   lib     site packages torch multiprocessing spawn py   line    in join      raise exception   exception         process   terminated with the following error   traceback       file   home tanel   lib     site packages torch multiprocessing spawn py   line    in  wrap      fn     file   home tanel   lib     site packages pytorch lightning trainer ddp mixin py   line    in ddp train      self run pretrain routine     file   home tanel   lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine  ,539d7bcb4476883771f0492b3a3774048de5163d,pytorch_lightning\callbacks\pt_callbacks.py
537,Summary not working for model on GPU with multiple inputs,  describe the bug  when you want a summary for a model that requires multiple input parameters for forward  then this doesn t work  you can set self example input array to be a tuple and there is some code for passing this to the forward method  however  if the model is on cuda  it tries to pass to move this input directly to cuda without a check whether it is a tuple or list   the line with the error is here   pytorch lightning blob master pytorch lightning root module memory py      example of how it should be checked   pytorch lightning blob master pytorch lightning root module memory py      to reproduce  steps to reproduce the behavior     create a model that requires multiple inputs in the forward method   set self example input array to be a tuple  run the model on gpu    expected behavior  a list with all layers and the input and output shapes of these layers         desktop       os  linux mint      browser chrome  version                 ,d562172b4cd363b86f9d670120932cd333b03cf5,pytorch_lightning\core\memory.py
566,Using print_nan_grads in the Trainer results in an error,  describe the bug  when using  print nan grads true      in the trainer  i am getting the error below   trainer fit   file   users   envs snorkel lib     site packages pytorch lightning trainer trainer py   line    in fit  self run pretrain routine   file   users   envs snorkel lib     site packages pytorch lightning trainer trainer py   line    in run pretrain routine  self train   file   users   envs snorkel lib     site packages pytorch lightning trainer train loop mixin py   line    in train  self run training epoch   file   users   envs snorkel lib     site packages pytorch lightning trainer train loop mixin py   line    in run training epoch  output   self run training batch   file   users   envs snorkel lib     site packages pytorch lightning trainer train loop mixin py   line    in run training batch  self print nan gradients   file   users   envs snorkel lib     site packages pytorch lightning trainer training tricks mixin py   line    in print nan gradients  if torch isnan  ,d4571d1d6f524b0b9284e84ea8f95bb8eb656c86,pytorch_lightning\trainer\training_tricks_mixin.py
604,failing Docs build,  description    not sure what is happening but it seems that the documentation fails         williamfalcon   could you pass details about documentation build    ,ea59a99426c050cf301783f411c41a732af8c752,.github\BECOMING_A_CORE_CONTRIBUTOR.md
606,Early Stopping kicks in at min_epochs + 2 instead of min_epochs,  describe the bug    i was working on a fix for      and found that early stopping starts to kick in at epoch   despite min epochs       to reproduce    run basic examples gpu template py and log the callback calls every epoch   expected behavior    when setting min epochs n    we should evaluate early stopping at the end of epoch n   proposed fix     i propose to change this   line in the training loop     to        why the       the epoch variable in the training loop starts at    but the trainer argument min epochs starts counting at         why the       the early stop check is done at the end of each epoch  hence the epoch counter will be   to min epochs after min epochs have passed       desktop       os  linux  version  master     ,e2ee4ddbdb75a91132394208fabc8c62ca39f3e9,pytorch_lightning\trainer\training_loop.py
618,Comet PAPI Depreciated,  use of the comet api logger reports an unecessary depreciation warning relating to the use of comet ml papi  rather than the newer comet ml api   example   comet warning  you have imported comet ml papi  this interface is deprecated  please use comet ml api instead  for more information  see    sdk releases  release     ,d1633aac112fb35878aae83deb8b57259acea75b,pytorch_lightning\logging\comet.py
638,"Pytorch lightning spawns processes after each epoch of training, causing training script to crash unexpectedly",  üêõ bug    when running my training script with ptl  i noticed that after the script is running process spawning is happening at the end of every epoch  which reloads the script  this unintended behavior often causes the training to crash if i m in the middle of changing the code  and essentially locks the script running from any edits to it   a possible fix for this is to create the data loader object only once here     to reproduce    train with ptl using ddp and num workers      and place a breakpoint at the top of the main file of your project    ,3c2fd560aa4d31b4f48ee225b83361deec53d9c7,CHANGELOG.md
675,Mismatch of displayed 'epoch',  üêõ bug    the display of epoch s number mismatches between the progress bar and the checkpoint indicator  i wonder this mismatch could confuse users     progress bar  the number of epochs starts from     checkpoint indicator  the number of epochs starts from     metrics csv also starts from       i think that to change checkpoint and metrics csv causes a serious problem   so progress bar should be changed in my opinion   what do you think about it   epoch       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    s  batch idx    loss      train batch loss      v num    val loss       ainfo root   epoch    val loss reached        saving model to  dummy   checkpoints   ckpt as top      loss         train batch loss         val loss         epoch         ‚ñå                             s  batch idx    loss      train batch loss      v num    val loss           environment      pytorch version          os   macos        how you installed pytorch   pip install git   lightning git master   upgrade  python version  ,734b28ed2dcd0feb23b44744a3d3d40de0b20a08,pytorch_lightning\callbacks\pt_callbacks.py
688,Checkpoint saving isn't atomic,  üêõ bug    saving checkpoints happens non atomically  in some cases  this causes an incomplete write of a checkpoint    causing any subsequent loading to fail with  runtimeerror  unexpected eof  expected   more bytes  the file might be corrupted   to reproduce    this is difficult to reproduce  since it relies on timing outside of code  for me  it happens with fast running models that run at      seconds per epoch   expected behavior    checkpointing should be resistant to such issues  and instead simply continue as is    ,9aad69d85635a8a65e1f0ee995516c0f8183c0f3,pytorch_lightning\trainer\training_io.py
694,JIT problem with `torchvision` 0.5,  üêõ bug    there are some jit problems with newly released       in      we freeze version to      but in future  we want to support all s  maybe it is just a temporal bug in  and they will handle it     to reproduce            environment      lightning       ,af445830506061680bd41744926c8bee1cca1104,MANIFEST.in
703,Fitting with log_gpu_memory=True fails in python3.6.,  bug    fitting with log gpu memory true in the trainer fails in     version   to reproduce      use     version  create any trainer with log gpu memory true option   then fit it   see error      a pytorch lightning pytorch lightning core memory py in get gpu memory map                 encoding  utf                    capture output true                 check true               convert lines into a dictionary            gpu memory    int  for x in result stdout strip  split       usr lib     subprocess py in run                 kwargs  stdin     pipe                    with popen  as process                 try                     stdout  stderr   process communicate     typeerror    init    got an unexpected keyword argument  capture output         code sample    trainer   trainer       log gpu memory true                 trainer fit       expected behavior    for the same code there is no errors for      environment    pytorch                  ubuntu      pytorch lightning     insta,06242c200a318a37d1f882c786e60354ec04533f,pytorch_lightning\core\memory.py
704,TensorBoardLogger and ModelCheckpoint are not using the same folder by default,  üêõ bug       by default  the tensorboardlogger writes logs into lightning logs   but modelcheckpoint writes checkpoint into lightning logs      ,de2ccc03a8df997b8841f33ae70050498960f08c,pytorch_lightning\logging\tensorboard.py
708,LR Schedulers shouldn't get `epoch` argument in `step` function,  üêõ bug    pytorch lr schedulers now shouldn t get any arguments in  function  see here   and here     looks like the calls in pytorchlightning are not in line with the new interface  see here     this results in unexpected lr changes  removing the epoch argument from step call solves the issue for me   environment    pytorch      pytorchlightning           ,c58aab0b0024c36a9bd4d5a0a472c84b80edc61d,pytorch_lightning\trainer\training_loop.py
709,imagenet_example cannot run,  üêõ bug  imagenet example cannot be executed  to reproduce    steps to reproduce the behavior     download imagenet  cd pl examples full examples imagenet  python imagenet example py   data path imagenet path    error   traceback       file  imagenet example py   line    in       main      file  imagenet example py   line    in main      model   imagenetlightningmodel   typeerror  can t instantiate abstract class imagenetlightningmodel with abstract methods forward      expected behavior    the example should run   environment    pytorch version         is debug build  no  cuda used to build pytorch         os  ubuntu       lts  gcc version           cmake version  could not collect  python version       is cuda available  yes  cuda runtime version  could not collect  gpu models and configuration  gpu    geforce rtx   ti  nvidia driver version       cudnn version  could not collect  versions of relevant libraries       numpy             pytorch lightning             torch             t,eeb48ceb965f0cc140728676a3bb77e7271d9e35,pl_examples\full_examples\imagenet\imagenet_example.py
712,Trainer is setting parameters with requires_grad=False to requires_grad=True (bug),  üêõ bug    when training a model that has some parameters where requires grad false   the trainer  is actually setting requires grad true for these parameters and changing them  the bug appears to originate in the trainertrainloopmixin code   to reproduce    steps to reproduce the behavior       create a model with some parameters which have requires grad false  fit the model using the trainer  check to see if the parameters which were set with  requires grad false  have changed     code sample      import torch  import numpy as np  import os  from torch nn import functional as f  from torch utils data import dataloader  import pytorch lightning as pl      make toy dataset  features   torch from numpy   float   targets   torch from numpy    train   torch utils data tensordataset   train loader   torch utils data dataloader        define lightning model  class coolsystem          def   init              super    init             self     torch nn linear           self     torch nn linea,a2b20b46bca5101627ed392aec17611ac0e97133,pytorch_lightning\trainer\training_loop.py
760,Test metrics not logging to Comet after training,  üêõ bug    when testing a model with trainer test metrics are not logged to comet if the model was previously trained using trainer fit  while training metrics are logged correctly   code sample        comet logger   cometlogger       trainer   trainer       model   get model         trainer fit    metrics are logged to comet      trainer test    no metrics are logged to comet      expected behavior    test metrics should also be logged in to comet   environment      pytorch version         is debug build  no  cuda used to build pytorch           os  ubuntu       lts  gcc version           cmake version  version          python version       is cuda available  yes  cuda runtime version         gpu models and configuration   gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti  gpu    geforce gtx   ti    nvidia driver version       cudnn version   usr local cuda     t,4ac9925dad71edda26db486795341f0b0ba4ed38,pytorch_lightning\loggers\comet.py
796,new profiler has failing tests,   jeremyjordan    tests fail on osx  tests test profiler py  test advanced profiler failed        ,fc0ad03008f5b725814a9091dc6a874950f49b42,tests\test_profiler.py
850,Epoch end checkpoint restarts previous epoch,  üêõ bug    if restarting the training and reloading the model  the epoch that the checkpoint had just completed is restarted rather than beginning the next   expected behavior    when a checkpoint upon epoch end is saved  restarting it should resume its state and start the next epoch    ,6e7dc9c2363779a12e8122ee1e2d470a0b0f013e,pytorch_lightning\trainer\training_io.py
922,Init'ing Dataloader calls get_train_dataloader,  it seems like the code of initializing the dataloader calls into getting the dataloader         pytorch lightning pytorch lightning trainer data loading py               line          in                        if exist iter dataset and isinstance  dataset  iterabledataset               this means that all the effort for wrapping get dataloader to sync through barriers for multi gpu   tpu is not used on this first call      ,1015a0050621828c9e8af2c934e19c5c68d61a5e,CHANGELOG.md
939,logger is NoneType hence doesn't have any experiment or other functionality in a lightning module,  üêõ bug    when trying to use the logging abilities of lightning  i hit a wall  the default and tensorboard loggers both seem to stay uninitialized when calling trainer fit   resulting in crashes everytime i try to log something   to reproduce    create a lightning module as such  class simpleregressor                 use the logger anywhere to get this kind of stacktrace   d  documents projects metawatch metawatch notebooks audio video interest simple regressor py in configure optimizers                  see   lightning readthedocs io en latest pytorch lightning core lightning html pytorch lightning core lightning lightningmodule configure optimizers                  required                self logger experiment add hparams   hidden layer size  self hidden layer size                                                      linear layer size  self linear layer size                                                      lstm layers  self lstm layers      attributeerror   nonetype  object has,f5e0df390c6e1eaf11ad488e297aa2d383daa177,docs\source\experiment_logging.rst
997,Precision=16 with TPUs bug,  üêõ bug    setting precision   when training with a tpu throws an error  to reproduce    see colab            relavent stack trace   exception in device tpu    str expected  not int  traceback       file   usr local lib     dist packages torch xla distributed xla multiprocessing py   line    in  start fn      fn     file   usr local lib     dist packages pytorch lightning trainer distrib parts py   line    in tpu train      os environ             file   usr lib     os py   line    in   setitem        value   self encodevalue     file   usr lib     os py   line    in encode      raise typeerror    name     typeerror  str expected  not int      to fix this all that should be needed should be casting   to a string      os environ        str       environment    collecting environment information     pytorch version         is debug build  no  cuda used to build pytorch         os  ubuntu       lts  gcc version           cmake version  version          python version       is cuda availabl,29cbc9e7230dd4c5e5e8e8ff789b838ed4a79e20,pytorch_lightning\trainer\distrib_parts.py
