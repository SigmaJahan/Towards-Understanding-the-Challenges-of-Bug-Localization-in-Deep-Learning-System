BR_id,BRsummary,BRdescription,commit_id,file_new_name
10303,TensorflowDebugger does not dump Stack/Pack/Concat nodes,  system information      have i written custom code     yes  os platform and distribution     linux ubuntu      tensorflow installed from     binary  tensorflow version          bazel version     cuda cudnn version              gpu model and memory   titan x pascal  exact command to reproduce     import sys  import tensorflow as tf  from tensorflow python import debug as tf debug    base   tf ones   stacked   tf stack   concat   tf concat     session   tf session   session   tf debug localclidebugwrappersession     with session as default        res   session run   print res      describe the problem    when using the tensorflowdebugger with stacked concated  the stacked concated nodes do not appear in the set of dumped nodes once a run has completed   in addition nodes that fed into these nodes are not dumped    ,6f36e6b27106fb4de065db18b9333a3c6c2fbb89,tensorflow\docs_src\programmers_guide\debugger.md
10428,TensorBoard graph key does not match documentation,  the key in the tensorboard ui indicates a  reference edge  as a single headed arrow       while the documentation shows these as double headed arrows       moreover  it appears that the edges indicated as references edges in the ui   are not in fact such edges  for example neither  cs   tf constant   vs   tf variable   tf add   tf add           note  tf add           should include reference edges    but in both cases the key in the ui says that they do    ,65ce8c723da2da639af0f1dd237d50d2680a4cd9,tensorflow\tensorboard\components\tf_graph\tf-graph-scene.html
10519,tf.contrib.data: tf-slim training pipeline gets stuck,  system information        have i written custom code     yes      os platform and distribution     linux               smp debian               gnu linux  version id      version            tensorflow installed from     binary      tensorflow version     tf version            tf git version                tf compiler version                    bazel version     none      cuda cudnn version                gpu model and memory   titan x           exact command to reproduce   python   mwe py      describe the problem    i recently ported my dataset handling to the new dataset api from   now it seems that the  training pipeline stalls if i request just   or   cpus for my job    i does work if i grab   cpus  i tried to come up with a mwe    the interesting thing is that it is not getting stuck if i remove one of the s or  at line    i suspect this issue is related to        source code   logs    import os  import tensorflow as tf  import tensorflow contrib data as tcd  import tensorflow c,f5fcd1fdcf896f46aed03c7e61525b48b75d1acc,tensorflow\contrib\data\python\kernel_tests\iterator_ops_test.py
10641,bug: BeamSearchDecoder should not assume that  when time &gt; 0 beam will be full,    scores flat   control flow ops cond         time             lambda  array ops reshape          lambda  scores     num available beam   control flow ops cond         time             lambda  math ops reduce prod          lambda  math ops reduce prod          pick the next beams according to the specified successors function    next beam size   math ops minimum         ops convert to tensor             beam width  dtype dtypes    name  beam width           num available beam     next beam scores  word indices   nn ops top k     next beam scores set shape     word indices set shape       code start from     python ops beam search decoder py      correct me if i am wrong  but i think this code is assuming that  when time     the beam will be full  it is true when the vocabulary is big such as is the case in machine translation  but if the vocabulary is small  the beam might won t be full when time     and might pose a problem   the value of next beam size  in the code seems must be be,be1b702ff357e851eb4a7237728d80fe08220816,tensorflow\contrib\seq2seq\python\kernel_tests\beam_search_decoder_test.py
10729,tf.nn.max_pool wrong docs?,  system information    not applicable  describe the problem    api   states that ksize has length       the size of window for each dimension of the input tensor  however  value is a   d tensor so doesn t this mean that ksize should be length       same for strides   digging into maxpooling op cc shows that there s some check that does     line         op requires                         errors  invalidargument  sliding window ksize field must                                             specify   dimensions           ,d4a21196ac06e49f2581e27af62efc4efd5387c4,tensorflow\python\ops\nn_ops.py
10741,[go] bug in Shape.size for dim == NumDimensions,  system information    this does not matter   describe the problem      when dim equals s numdimensions   the function should return     instead it panics   source code   logs    in shape go     method      func   size             if dim  s numdimensions             return         should be       func   size             if dim   s numdimensions             return          ,76a0a15cb90c370e41766d124a7a11b28c18089a,tensorflow\go\shape.go
11016,map_func of tf.contrib.data.Dataset.map gets dict keys instead of values when the nested structure of Dataset is dict,  system information      have i written custom code    no  os platform and distribution    linux ubuntu      tensorflow installed from    source  tensorflow version    b                     bazel version           cuda cudnn version         gpu model and memory     describe the problem    if the nested structure of  is    will call map func    and pass the keys of  instead of components in the dataset to   it seems that  or  need to be passed to   so that  could transform the elements in the dataset   source code   logs    import tensorflow as tf    def foo        print              return                 tf contrib data dataset from tensors  map    ,9b11f458196f6f0528c9974238497a6c8b6da547,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py
11017,Tfdbg does not work with Coordinator/QueueRunners,  system information      have i written custom code    no  os platform and distribution    linux mint    tensorflow installed from    binary    tensorflow version                       bazel version    n a  cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  n a    describe the problem    the tensorflow debugger does not seem to be working with queues  data never seems to be fetched by the queuerunner threads  be it from a file   or preloaded    instead  the coordinator should stop  is true right away  this is only the case after wrapping the session in a tf python debug localclidebugwrappersession  the example should make things clearer   moreover  another error occurs at coordinator join    i am aware of the faq entry on threads    but that does not explain why the data fetching threads would not be working   source code   logs    to make it easiest to replicate  i simply took the example on working with preloaded data    and wrapped the session in there ,41bc76d28b8b301c546cc5624abd37fd8b97b64c,tensorflow\python\client\session.py
11091,tf.nn.elu: incorrect second derivative,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    binary  tensorflow version           bazel version    n a  cuda cudnn version  cuda       cudnn      gpu model and memory  gtx   ti    exact command to reproduce  see below    tf nn elu gives incorrect second derivatives   consider the graph y       elu    x   tf placeholder    y       tf nn elu   we ll be evaluating at x     x       we can evaluate first derivatives with automatic differentiation   dy    tf gradients   dy eval            this lines up with the analytic answer  y          however  for the second derivative   ddy    tf gradients   ddy eval           whoops  this doesn t look right  analytically  the derivative is y          evaluated at x    this is       workaround    just in case anyone else needs to work around this until it s fixed   def elu        return tf where        looks like second derivatives work with that    ,e121535a7d04cfc7c7dbb09d8694c01eb29da26f,tensorflow\python\kernel_tests\relu_op_test.py
11132,Go: SIGABRT when executing the same node more than once,  problem    in go  when we pass the same node to the fetches list more then once sigabrt is raised   source code   logs    package poc test    import             fmt           tf  github com tensorflow tensorflow tensorflow go            github com tensorflow tensorflow tensorflow go op            testing        func testfunc                create root scope          root    op newscope                define graph               create a constant matrix          a    op const                                          create a constant column vector          b    op const                                  create a matmul operation          mul    op matmul   a  b                finalize the graph          graph       root finalize                create the session          var sess  tf session          sess      tf newsession              run          var results    tf tensor          var err error          if results  err   sess run   err    nil                    t errorf               ,66604b0355b32961f9a532792be2e008cc22221f,tensorflow\core\common_runtime\direct_session.cc
11411,Fetching data in Distributed Tensorflow has too much latency,  system information      have i written custom code               the above is a simple benchmark which tests the overhead of distributed tf  it fetches a configurable sized variable from the parameter server and does a matmul on the worker  it also does a matmul from a locally stored variable on the worker  the time difference between these two operations would be the overhead i am measuring       os platform and distribution     linux redhat      tensorflow installed from     source      tensorflow version     tensorflow          python version   python            exact command to reproduce       python matmul benchmark py   num features     batch size     num hidden     job name ps    python matmul benchmark py   num features     batch size     num hidden     job name worker  by increasing batch size  the timing difference between local remote computation eventually becomes negligible   however  for small batch sizes the overhead can become       for example  here are two runs for ,11e2aef14f7f2d862363c350ca1d67b87ea6a57b,configure.py
11692,A bug of tf.reduce_logsumexp with `-inf`,  system information      have i written custom code    yes  os platform and distribution     linux centos    tensorflow installed from    binary  tensorflow version           python version         bazel version     cuda cudnn version             gpu model and memory  tesla       exact command to reproduce   python  c  import tensorflow as tf  print tf session  run        describe the problem    the doc of tf reduce logsumexp says it    computes log        however  it does not when the tensor is  inf   source code   logs    python  c  import tensorflow as tf  print tf session  run          prints  nan          python  c  import tensorflow as tf  print tf session  run            prints   inf       ,30c13a450841b213d72dea93d9447a25169be0a7,tensorflow\python\ops\math_ops.py
11725,tf_cnn_benchmarks.py stuck when running with multiple GPUs and ImageNet data with protocol grpc+verbs,  system information      have i written custom code    no  running tf cnn benchmarks py from benchmarks repo  os platform and distribution    ubuntu       lts  tensorflow installed from    unmodified source with rdma verbs enabled  tensorflow version             python version         bazel version           cuda cudnn version         gpu model and memory  nvidia tesla   pcie      exact command to reproduce     ps  cuda visible devices    python tf cnn benchmarks py   ps hosts             worker hosts                       batch size     model     variable update parameter server   local parameter device cpu   job name ps   task index     server protocol grpc verbs     cuda visible devices                   python tf cnn benchmarks py   ps hosts             worker hosts                       batch size     model     variable update parameter server   local parameter device cpu   job name worker   task index     num gpus     data dir  data imagenet data    train dir  data imagenet trai,e650dcfb462a9efc9236e33cae87ac5bbf55d9f7,tensorflow\contrib\verbs\rdma.cc
11829,Slow to import tensorflow.contrib with Python 3 because inspect.stack is slow,  system information      have i written custom code    no  os platform and distribution    macos sierra        tensorflow installed from    source  tensorflow version                       python version         bazel version          homebrew  cuda cudnn version  cpu only build  gpu model and memory  cpu only build  exact command to reproduce  time    c  import tensorflow contrib     the problem    doing import tensorflow contrib take     seconds on my machine when doing it with python        with python       it takes     seconds   investigating this revealed that a lot of time is spent in  inspect stack  in the function make decorator in  python util tf decorator py  the stack is inspected to find the name of the caller of the function  with   inspect stack  is fast  but with python   each call to inspect stack  take approximately     seconds and there are   calls made  which account for the difference in time between python   and     references    keras by default imports tensorfl,d42ca5a1462e75e80536aa9c46c6834bd9455f2b,tensorflow\python\util\tf_decorator.py
11948,Memory leak in Java API when using GPU,  system information      custom code     os  centos    tensorflow installed from    binary  tensorflow version    n a  python version  n a  bazel version    n a  cuda cudnn version       gpu model and memory  geforce gtx    exact command to reproduce  see      describe the problem    main memory on the machine is continuously consumed when running on the gpu  memory consumption hovers around   when running on the cpu   source code   logs    see        ,03d310cc61a864600d24977f73138e643659986c,tensorflow\java\src\main\native\operation_builder_jni.cc
1198,reverse_sequence's inability to accept int32 can break bidirectional_rnn,  in the latest releases bidirectional rnn has been changed to accept   tensors for the sequence length argument  but tf reverse sequence only accepts   tensors  and this is currently causing an error when an   tensor is passed to bidirectional rnn    ,484a80ce99998b59cf9f606a7c2a9ad1c14ea29a,tensorflow\python\ops\rnn.py
11985,windows bazel build failed: undeclared inclusion,  system information      have i written custom code     no  os platform and distribution     windows    tensorflow installed from     source  tensorflow version        python version          bazel version            cuda cudnn version   none  gpu model and memory   none  exact command to reproduce   bazel   output base c  t  build    tensorflow tools pip package build pip package    describe the problem    error  c  os tensorflow tensorflow core build      undeclared inclusion  in rule    tensorflow core lib internal    this rule is missing dependency declarations for the following files included by  tensorflow core framework variant tensor data cc     c  os tensorflow tensorflow core framework tensor h    c  os tensorflow tensorflow core framework allocator h    c  os tensorflow tensorflow core framework numeric types h    c  os tensorflow tensorflow core framework type traits h    c  os tensorflow tensorflow core framework variant h    c  os tensorflow tensorflow core framework typ,6e7f1dac288acda411a21949a5720b2ca2f1a3eb,tensorflow\core\BUILD
12205,BUG: TypeError in DNNClassifier.eval() when using same name for feature in feature_engineering_fn,  describe the problem    if we use the  same key to replace a feature  tensorflow might throw typeerror when evaluating   eg   def feature engineering fn      features  x     some func   when  is   it is a mutable object  hence the bug is caused by  method which runs  again  see code     i ll open a pr later    ,f47c3ad964f42121cbab02afed52e2df367ea9ef,tensorflow\contrib\learn\python\learn\estimators\estimator.py
12249,tf.estimator.Estimator breaks when using python 3.5 type annotations,  minimal example   import tensorflow as tf    def model fn        pass    estimator   tf estimator estimator       results in  file   usr local lib     dist packages tensorflow python estimator estimator py   line    in   init         verify model fn args     file   usr local lib     dist packages tensorflow python estimator estimator py   line    in  verify model fn args      args   set      file   usr local lib     dist packages tensorflow python estimator estimator py   line    in  model fn args      return tuple  args     file   usr local lib     dist packages tensorflow python util tf inspect py   line    in getargspec      if d decorator argspec is not none    inspect getargspec      file   usr lib     inspect py   line    in getargspec      raise valueerror  function has keyword only arguments or annotations   valueerror  function has keyword only arguments or annotations  use getfullargspec  api which can support them      system information      os platform and distribution  ,93a652ef5b635ffbd678d3992767c4862bffeb15,tensorflow\python\estimator\util.py
12436,zeros_like doesn't fully respect the optimize argument,  the definition of zeros like   is   def zeros like      with ops name scope  as name       tensor   ops convert to tensor         if tensor shape is fully defined            we can produce a zeros tensor independent of the value of  tensor            since the shape is known statically         return zeros         if dtype is not none and dtype    tensor dtype         return zeros             shape internal   dtype dtype  name name       else         return gen array ops  zeros like   we can see that if the shape of  is already known  the  parameter is ignored  which is inconsistent with the documented behavior      ,9ba48146942219a2d97e9a2110f88f20d62c4cb6,tensorflow\python\ops\array_ops.py
12569,missing Documentation of the method AttentionWrapper.zero_state(...),  hello    i have noticed that the method attentionwrapper zero state  does not have any description of its functionality in the  documentation website   below is a reference link      docs python tf contrib   attentionwrapper    i really hope that this gets fixed   i have spent a couple of days trying to debug a code that i have written until i realized that i was misusing the method    thank you   ,aae34fa7e35d9c3931cae49bfc20384dd20dffec,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py
12608,gather_nd bounds checking not working,  when using gather nd  sometimes out of bounds indices lead to errors   and sometimes it seems to just read zeros  i expect it reading just other memory from the gpu  but i ve never observed anything other than zeros so i m not sure  when i run on the cpu the bounds seem to be appropriately checked i e  i get the errors desired  here s some example code   import tensorflow as tf  sess   tf session   print            print            print        the first two print statements execute successfully  which is a bug  the indices are clearly out of range  and the arrays are clearly all   s  but instead it returns an appropriately shaped array of   s     the third line  for some reason  has the bounds checking operate correctly  and says that    yes    the index         is not in the bounds  it appears to something based on what the previous op is  maybe  where some ops  such as stack  allow me to go outside the bounds  while others such as reshape don t  here s an example interactive sessi,5386775e64aac0bb5020974122645da900bc312a,tensorflow\core\api_def\base_api\api_def_GatherNd.pbtxt
12641,Improve all-in-memory file copy architecture (Python at least),  current file copy  at least via python   gfile py  → file io py   → file io i     involves copying the source contents into memory  and then writing memory to the destination  for scenarios like      which is working with an   asset  this is unacceptable design   file system h s writablefile is not stubbed to allow anything like a streaming  though its randomaccessfile is    could be employed in a streamable fashion  ish    to cull the python low hanging fruit  at least  please implement file io i   using a regular streaming design instead of the above described current design    ,b1f5f433959406c7aad634c05e85ccd62fd06e87,tensorflow\core\platform\env.cc
12902,Change TanhGrad() operation definition with respect to documentation,  hello   tanhgrad  documentation says   specifically  grad   dy      where y   tanh   and dy  is the corresponding input gradient         tensorflow tensorflow core ops math ops cc               line          in                        specifically   grad   dy       where  y   tanh    and  dy             which is correct and looks good   but operation has following declaration of inputs   input    input         tensorflow tensorflow core ops math ops cc               line          in                        input                                                               what doesn t correlate with the documentated formula  grad   dy       could you please rename inputs with respect to documentation like this   input    input   thanks    ,9b5bf2b6786a58df679b4be2249da8a235b9f4fd,tensorflow\core\ops\math_ops.cc
13202,tf.InteractiveSession leaks sessions,  the following works fine with tf session  but will fail to release resources in tf interactivesession  sess   tf interactivesession     do stuff    sess close   del sess      the reason is that interactive session enters a context using   enter    and never quits it  leaving a reference from a defaultstack object  i found this when debugging why my notebook was hogging all gpu ram   the two work arounds     force c api to close the session using sess   del     get rid of the dangling reference        sess  default session   exit         del sess      import gc      gc collect       i think a better solution would be to have sess close  call both tf closesession and tf deletesession  or have a method that will reset all sessions like session lib reset   ,0f508d4de379e800ad7f990de08959bbd6fcabb5,tensorflow\python\client\session.py
13431,Windows nightly build Dataset.from_generator fails with pyfunc error,  system information      have i written custom code    yes  os platform and distribution    windows    tensorflow installed from   pip  tensorflow version          python version         bazel version      cuda cudnn version    gpu model and memory    exact command to reproduce see below    describe the problem    as described in the so question        the code   import tensorflow as tf    dataset   tf contrib data dataset      dataset range  make one shot iterator       dataset from generator need tensorflow          with tf session  as sess       print       def  dataset generator            while true               try                   yield sess run                except tf errors outofrangeerror                   return      das dataset   dataset from generator       das dataset it   das dataset make one shot iterator       while true           try               print             except tf errors outofrangeerror               break  fails with   c  dropbox   pycharmvirtual tf ni,9b027db459ff771c246a266ac3ec40cfbb4a63ce,tensorflow\contrib\data\python\kernel_tests\dataset_constructor_op_test.py
13506,tf.image.pad_to_bounding_box crashes when passed bounds with dtype int64,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from   from pip in virtualenv  tensorflow version                      python version            n gcc            bazel version     cuda cudnn version   gpu model and memory   exact command to reproduce     description    passing arguments of type   to tf image pad to bounding box triggers a crash of the python interpreter  this is a bug because the type required by tf image pad to bounding box not documented anywhere and just causes a crash with a cryptic error message   sources logs    the following snippet crashes the whole python interpreter with a core dump   import tensorflow as tf  i   tf constant   img   tf ones   sess   tf session   sess run        and leaves the following                 f tensorflow core framework tensor cc    check failed  dtype     expected dtype         ,cbb705f10149a11b8d17182343ef12ab2dbfd7a8,tensorflow\core\kernels\pad_op.cc
13526,Importing TF in Python yields 'cannot import name 'build_info',  system information    fedora        tensorflow installed from source   tf version          tf git version   b               tf compiler version   b                   python version          bazel installed from their fedora copr repositories  version           no cuda    intel mkl        c                bazel build  c opt   config mkl   tensorflow tools pip package build pip package  notice the mkl flag in the bazel build  describe the problem    configuration and bazel build finished without error  when attempting to import tensorflow in python  i get this       import tensorflow as tf  traceback       file     line    in     file   home torstein progs tensorflow tensorflow   init   py   line    in       from tensorflow python import      file   home torstein progs tensorflow tensorflow python   init   py   line    in       from tensorflow python import pywrap tensorflow    file   home torstein progs tensorflow tensorflow python pywrap tensorflow py   line    in       from tensorfl,251a1e70dc04b10fb25e8013d1ad1f27d5eda30b,tensorflow\python\platform\self_check.py
13536,BeamSearchDecoder incorrectly truncates results when used with dynamic_decode,  system information        have i written custom code     os platform and distribution    linux ubuntu     any  tensorflow installed from    binary  tensorflow version                       python version  python          continuum analytics  inc   bazel version    n a  cuda cudnn version  irrelevant  gpu model and memory  irrelevant  exact command to reproduce  irrelevant    describe the problem    tf contrib   beamsearchdecoder incorrectly truncates some of the results because the same index was previously used for a beam member that ended at a earlier step   the root of the problem is that the while loop body in dynamic decode assumes that sequences are independent and will finish only once  in the same time beamsearchdecoder creates a tree like structure where a beam index can be reused in a later step for a state that originates from a different parent index   this causes the decoding loop to sometimes record the wrong sequence length for a beam member  then this wrong sequence l,18f89c81d288f191abd56501ec6f86fe29265bdd,tensorflow\contrib\seq2seq\kernels\beam_search_ops.cc
13558,segfaults in GPU tf.matrix_inverse,  i m running into segfaults in tf matrix inverse  i m adding identity     so matrices should be invertible  and same procedure works fine in numpy and in tensorflow cpu version     segfault py      this non deterministically crashes after     seconds with various backtraces   ie        in             in tensorflow  tensor  totalbytes  const       from  home yaroslav   envs   lib     site packages tensorflow python    libtensorflow framework so        in tensorflow  tensor  tensor data  const       from  home yaroslav   envs   lib     site packages tensorflow python    libtensorflow framework so        in bool tensorflow  internal  transposeusingtile        from  home yaroslav   envs   lib     site packages tensorflow python  pywrap tensorflow internal so        in tensorflow  status tensorflow  dotranspose        from  home yaroslav   envs   lib     site packages tensorflow python  pywrap tensorflow internal so        in tensorflow  svdopgpu  performsvd mgeqn        from  home yarosla,3629fc4e98254c37e614ac3f77fa250b75c70f8d,tensorflow\core\kernels\matrix_inverse_op.cc
13576,sparse_softmax_cross_entropy_with_logits wrong annotation,        tensorflow tensorflow python ops nn ops py               line          in                              of the labels is not equal to the rank of the labels minus one              it should be if logits are scalars   or if the rank of the labels is not equal to the rank of the logits minus one    ,edfb9bb100f9814bf1bbcff2e8a32f12f049bfcc,tensorflow\python\ops\nn_ops.py
13764,Failure in TestNewTensor when running go test,  system information      have i written custom code    no  os platform and distribution    linux ubuntu      tensorflow installed from    source    tensorflow version          dev  python version       bazel version           cuda cudnn version           gpu model and memory  nvidia      exact command to reproduce  go test  v github com tensorflow tensorflow tensorflow go    describe the problem    i m trying to use the go bindings to the tensorflow c library  when i run the tests  i get a nil pointer dereference and a segfault  the details are below  note that i ve built the c library from source using the following options   bazel build  c opt   config cuda   config mkl  c opt   copt  mavx   copt      copt  mfma   copt  mfpmath both   copt       c opt   cxxopt          tensorflow libtensorflow so  source code   logs    when i run go test  v github com tensorflow tensorflow tensorflow go i get the following error                  e tensorflow core common runtime bfc allocator cc    t,db10718b38b2884cb5ed46d33c135c079f649d16,tensorflow\go\tensor.go
13827,"Tensorflow 1.3: tf.constant with dtype=[float32, float64, float16] may have inconsistent behavior.",  system information        have i written custom code    no      os platform and distribution     ubuntu     with docker running gcr io tensorflow tensorflow latest      tensorflow installed from    na      tensorflow version           python version           bazel version    na      cuda cudnn version  na      gpu model and memory  na      exact command to reproduce          works  test   numpy array   sess   tf session   print            works  sess   tf session   print            returns error  sess   tf session   print        typeerror  expected    got none of type   message  instead       describe the problem    a tensorflow constant with none in array with dtype      seem to throw an error  however  if they are first wrapped by a numpy array  none is accepted and turned into nan  this behavior seems inconsistent    ,c43d777b56a17832f7de288d0fe966bf537ffeb7,tensorflow\python\framework\ops.py
13885,tf.reduce_mean is not compatible with np.mean,  tf reduce mean   emphasized that this function is compatible with numpy     equivalent to np mean    but it doesn t in the output type  consider the following code for example   import tensorflow as tf  x   tf variable   init   tf global variables initializer   sess   tf session   sess run   print           the output is zero  it seems that tf reduce mean infer the output type from the input tensor because casting the input tensor to float values  solve the problem  this attribute is not compatible to np mean   import numpy as np  print        system information      os platform and distribution  linux ubuntu      tensorflow installed from    source  tensorflow version         python version          ,a43f911e103aa5910d4e2405d77bdee8f9314fac,tensorflow\python\ops\math_ops.py
14292,Can't import contrib.boosted_trees,  system information      have i written custom code    yes     cat  etc issue                                                  linux             microsoft    microsoft wed dec         pst         gnu linux  version        lts     version id        version codename xenial       are we in docker                                                no     compiler                                                        c              copyright     free software foundation  inc   this is free software  see the source for copying conditions   there is no  warranty  not even for merchantability or fitness for a particular purpose      uname  a                                                        linux             microsoft      microsoft wed dec         pst         gnu linux     check pips                                                      numpy    protobuf    tensorflow    tensorflow tensorboard       check for virtualenv                                            false     tensorflow import ,e52706d1696faa2ab926c2d91a0d85ec99dac314,tensorflow\contrib\cmake\python_modules.txt
14455,Tensorflow cannot be installed with default Windows Python 3.5 stack,  after installing python       using the windows   bit installer       install   upgrade tensorflow  collecting tensorflow  could not find a version that satisfies the requirement tensorflow    no matching distribution found for tensorflow  you are using pip version        however version       is available   you should consider upgrading via the  python  m pip install   upgrade pip  command   i tried on a different machine that worked  and found the only difference to be the pip version   updating to pip       solved the issue   it s not explicitly stated anywhere that you need a newer version of pip   when an old version of something is required to run something  people tend to ignore the messages indicating there is a newer version of it because that s exactly what they are expecting   yeah i know there s a newer version  i meant to do this    if this cannot be resolved for older version of pip    could you please state this in the documentation    ,2ae9c6c7a20dbd8f05e4b60e921e60986e2968bf,tensorflow\docs_src\install\install_windows.md
14542,'Model' object has no attribute 'container_nodes',  problem    model   tf keras models model   model add   tf keras utils plot model   output   traceback       file  model py   line    in       k utils plot model     file   usr local lib     dist packages tensorflow python keras  impl keras utils vis utils py   line    in plot model      dot   model to dot     file   usr local lib     dist packages tensorflow python keras  impl keras utils vis utils py   line    in model to dot      if node key in model container nodes   attributeerror   model  object has no attribute  container nodes       environment     system  ubuntu       tensorflow gpu bin                     ,3e53570d3bf518ec2b6cfeed4b5fd57d11370289,tensorflow\python\keras\_impl\keras\utils\vis_utils.py
14739,Eager: Warn with invalid policy,  if a user accidentally writes tfe enable eager execution  instead of the correct tfe enable eager execution   they won t get an error until later in their program   for example  tfe num gpus  after  the incorrect enable call produces                                                                               attributeerror                            traceback     in            tfe num gpus         lib     site packages tensorflow python eager context py in num gpus             the number of available gpu devices                        return context  num gpus         lib     site packages tensorflow python eager context py in num gpus           def num gpus                 the number of gpus available to execute operations                self  initialize handle and devices             return self  num gpus                lib     site packages tensorflow python eager context py in  initialize handle and devices                 with errors raise exception on not ok status  as status ,ba87a8030aa30f24c354cf705e79734658bb0a8b,tensorflow\python\framework\ops.py
14776,tf.keras.estimator.estimator_from_model does not respect options set in RunConfig,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    binary  tensorflow version    tf version         tf git version                python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version               gpu model and memory  nvidia tesla     gb  exact command to reproduce  see below    describe the problem    when trying to use an estimator that is derived from tf keras estimator estimator from model  and training with tf estimator train and evaluate   setting gpu options in the session config of tf estimator runconfig does not cause the settings to be respected when passed to the estimator from model function  for example setting per process gpu memory fraction     does not decrease the memory allocated to the process on the gpu  similarly setting allow growth true continues to allocate all of the memory and does not allow memory growth   i also tested this with t,355fb5e14b325a1d106c4046f478da4bda350205,tensorflow\python\keras\_impl\keras\estimator.py
14800,Potential memory leak from deleting array and closing file handler,  here are couple of minor memory leak for review             tensorflow tensorflow c c api cc              lines   to          in                        char  base   new char size            char  data start   base   sizeof    srcarray size            char  dst   data start      where next string is encoded           size t dst len   size   static cast            tensorflow     offsets   reinterpret cast            for      i                offsets                 offsets             const string  s   srcarray            size t consumed   tf stringencode   s size   dst  dst len  status            if                   status  status   invalidargument            invalid string tensor encoding  string     i    of                     srcarray size          status  status error message             return nullptr                          dst    consumed             dst len    consumed                      if                status  status   invalidargument            invalid string tensor en,e17ae378063b46c894a8c193823f029d7d87de81,tensorflow\c\c_api.cc
14819,Keras Dropout support_masking gets reset to False,  system information      have i written custom code    yes  os platform and distribution    mac os x        tensorflow installed from    binary  tensorflow version                       python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  see below    you can collect some of this information using our environment capture script     env collect sh    you can obtain the tensorflow version with  python  c  import tensorflow as tf  print    describe the problem    describe the problem clearly here  be sure to convey here why it s a bug in tensorflow or a feature request   the keras dropout layer constructor   sets support masking true and then calls its super constructor  which sets it back to false  other layers defined in that module appear to set support masking true after the super constructor call   source code   logs    include any logs or source code that would be helpful to diagnos,fd1263fb9b9a81b4c8d7e7922308146b4f57428d,tensorflow\python\keras\_impl\keras\layers\core.py
14942,tensorflow 1.4 is 8 times slower than tensorflow 1.3 when read data,  system information      have i written custom code     os platform and distribution     tensorflow installed from    python wheel  tensorflow version        and      python version         bazel version     gcc compiler version     cuda cudnn version  none  gpu model and memory  none  exact command to reproduce     when i run     script using estimator  the script is   times slower than tensorflow      source code   logs    main script     usr bin env python    author      zj     import argparse  import os  import sys  import numpy as np  import time  try       import better exceptions  except importerror       pass  import tensorflow as tf  from src model ori import crnn fn  from src data handler import data loader  from src config import params  alphabet  from src input utils import input fn      def main        models path   flags input model dir      if not os path exists            assert filenotfounderror        models list    os path join  for x in os listdir  if x endswith   ,2d4c29cd6a0627fdd71a752e6bd919204c7cb8bf,tensorflow\python\training\server_lib.py
14985,tf.nn.fractional_max_pool output have same batch size when feed with different input batch size,  describe the problem    tf nn fractional max pool output have same batch size when feed with different input batch size   attached is test code i write    different input is feed in with different batch size   outputs get same batch size   pool test py txt       code result  shape of input a    shape of output a    shape of input b    shape of output b    system information       cat  etc issue                                                  linux c           generic            ubuntu smp thu nov         utc         gnu linux  version        lts     version id        version codename xenial     are we in docker                                                no     compiler                                                        c              copyright     free software foundation  inc   this is free software  see the source for copying conditions   there is no  warranty  not even for merchantability or fitness for a particular purpose      uname  a                                   ,5f0d3395d4c61000cf0cfb3dc681177147be938d,tensorflow\core\kernels\BUILD
15034,Optimize graph & graph transform tools do not support NCHW,  i tried optimizing graph using both graph transform tool   and optimize graph for inference    both cases produced the same error because the fused batchnorm used not nchw  but nhwc  i ve got the error like this   invalidargumenterror    must provide as many biases as the channel dimension of the input tensor      vs    in                node  prefix convblock batchnorm fusedbatchnorm   biasadd t dt float  data format  nhwc    device   job localhost replica   task   device gpu            although nchw is faster than nhwc in gpu environment  why the tools do not support nchw    ,6afe900f543e0005ce69b3152330f1b7b16cb286,tensorflow\python\tools\optimize_for_inference_lib.py
15239,No gradient defined for op: Pow,  system information      have i written custom code    yes  os platform and distribution    windows      tensorflow installed from    binary  tensorflow version         bazel version  n a  python version  none  cuda cudnn version  none  gpu model and memory  none  exact command to reproduce       describe the problem    it seems there is no gradient defined for the pow operation in the c   api   i am actually transferring this issue from migueldeicaza tensorflowsharp      similar to the case of select         it seems there is also no gradient for the pow operation in the c   api    ,e1ded7fa7cfacaeea43a903e738dd3fe2baabc57,tensorflow\cc\gradients\math_grad.cc
15345,Using wrong location for x86_64 android build,  system information      have i written custom code     a  yes  os platform and distribution     a  osx        tensorflow installed from     a  source  tensorflow version     a         python version   a       bazel version     a       gcc compiler version     a     configured with    prefix  applications xcode app contents developer usr   with gxx include dir  usr include c          apple llvm version          target    apple        thread model  posix  installeddir   applications xcode app contents developer toolchains xcodedefault xctoolchain usr bin        cuda cudnn version   gpu model and memory   exact command to reproduce   make  f tensorflow contrib makefile makefile target android android arch      describe the problem    android   build fails with makefile using make  f tensorflow contrib makefile makefile target android android arch   because it cannot find the binary     linux android g    it can be fixed by changing the tensorflow contrib makefile makefile at line   from,df9189cc4671facfecd3e8249c9e8b01b11c0df5,tensorflow\contrib\makefile\Makefile
15611,'saved_model_cli.py' bug fix!,  in file python tools saved model cli py  at function def  print tensor info    the first line should be     print  in   datatype items     not be    print    because tensor info dtype  is an integer which is the value of types     ,2e2715baa84720f786b38d1f9cb6887399020d6f,tensorflow\python\tools\BUILD
15766,tf.assert_equal raises incorrect traceback in Eager mode,  system information      have i written custom code     no  os platform and distribution    linux ubuntu       lts  tensorflow installed from    pip binary  tensorflow version             python version         bazel version     gcc compiler version     cuda cudnn version  none  gpu model and memory  none  exact command to reproduce  python main py    describe the problem    in eager mode  tf assert equal only shows    in traceback message when two inputs are different  however  in graph mode  it does show different values in the message   source code   logs    import tensorflow as tf  import tensorflow contrib eager as tfe    tfe enable eager execution     x   tf constant   y   tf constant     with tf control dependencies          output   tf reduce sum   eager mode traceback   traceback       file   users matt pycharmprojects scratch main py   line    in       with tf control dependencies        file   usr local var pyenv versions         lib     site packages tensorflow python ops ,89cd0cd81ae829610fcbf4437597451ae5a59fe6,tensorflow\python\kernel_tests\check_ops_test.py
15882,"tfdbg error ""Dump root directory does not exist"" with empty fetches",  system information      have i written custom code  yes  os platform and distribution  linux ubuntu      tensorflow installed from  binary    tensorflow version      tensorflow import                                               tf version          tf git version              tf compiler version              sanity check  array   python version         cuda cudnn version      cuda libs                                                        usr local cuda     targets   linux lib libcudart static a   usr local cuda     targets   linux lib libcudart so         usr local cuda     doc man   libcudart so     usr local cuda     doc man   libcudart     usr local cuda     targets   linux lib libcudart so         usr local cuda     targets   linux lib libcudart static a   usr local cuda     doc man   libcudart so     usr local cuda     doc man   libcudart    gpu model and memory  geforce gtx       exact command to reproduce  see code below    describe the problem    localclidebugwrappersessio,1f26c65254268730b7409f517d1ed1b554d01e50,tensorflow\python\debug\wrappers\dumping_wrapper_test.py
15891,Dependencies of tensors created within a tf.while_loop() might not be executed,  system information      have i written custom code    yes  see test case below   os platform and distribution    macos  sierra  version          tensorflow installed from    both  i have compiled tensorflow at   with my small change in pr     i have also tried using the pip package   tensorflow version         python version         bazel version          homebrew  gcc compiler version    apple llvm version          cuda cudnn version  cuda        cudnn     osx      gpu model and memory  nvidia geforce gt   with   mb device memory    exact command to reproduce     python repro py     where repro py contains the test case to reproduce  listed below   describe the problem    here is my test case     part i  from   future   import division  print function  import numpy as np  import tensorflow as tf  from tensorflow python ops import resource variable ops as rr    rs   np random randomstate   a   rs normal    print      b   rs normal    print              part ii  a var   tf variable   ,f8f921c828fb2c97da7c7b80c01390ccec90ae40,tensorflow\python\kernel_tests\control_flow_ops_py_test.py
16100,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,  system information      have i written custom code    yes  see below  os platform and distribution   tensorflow installed from    install   user tensorflow gpu  tensorflow version         python version         cuda       gpu  nvidia titan x    describe the problem    using a timefreqlstmcell in a dynamic rnn or static rcnn without providing the optional parameter frequency skip results in an exception   typeerror  unsupported operand type  for     int  and  nonetype       the line which throws this exception is       tensorflow tensorflow contrib rnn python ops rnn cell py              lines   to          in                        num feats   int               self  frequency skip                   frequency skip has it s default value none here   maybe the default should be changed to     source code   logs    sadly i am not allowed to share my full source code  however  this is how i create the rnn layers   lstmcell   tf contrib rnn timefreqlstmcell                      stacked ls,4b6abfd95254910d01e886123cd24c29f722e8d7,tensorflow\contrib\rnn\python\ops\rnn_cell.py
16152,DeprecationWarning from `inspect.getargspec()`,   is deprecated in python       library inspect html inspect getargspec    i solved the problem in keras like this   keras team keras      system information      using tensorflow as a keras backend    linux ubuntu      installed from conda  version        python          describe the problem    we recently switched from theano to tensorflow and this warning message is filling up my test output   source code   logs     home   conda envs  lib     site packages tensorflow python util tf inspect py    deprecationwarning         inspect getargspec  is deprecated  use inspect signature  or inspect getfullargspec        ,1744b8c0519cec31764d205b813bd4fd6028cbf9,tensorflow\python\util\tf_inspect.py
16163,Dataset.from_generator doesn't release memory after recreating the session,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    binary  tensorflow version             python version  python      bazel version       gcc compiler version       cuda cudnn version     gpu model and memory     exact command to reproduce  see below    describe the problem    after closing the session and creating new one an iterator creates the generator instance but doesn t free the memory of the previous one   every calling of the line session run    increases memory consumption of the script       mib after the first     mib after the second     mib after the third and so on     as you can see the delta is equal to   mib   n   sizeof      source code   logs    import numpy as np  import tensorflow as tf    n                def generate      data   np random rand     for k in range        yield data k  copy     graph   tf graph   with graph as default      x   tf data dataset        from genera,be862d5b91e9b9044f4e028dcdae0b6ad283e8b4,tensorflow\core\api_def\base_api\api_def_GeneratorDataset.pbtxt
16167,Documentation Method Templates Improvement,  system information    n a  describe the problem    the method class templates in documentation should include a full  functioning path to the method instead of just truncating to the method s name   i e  this is what we have at present         this is a more practical and copy paste friendly version       i m constantly just grabbing method templates  pasting to my text editor and then coming back to docs to copy paste the package path which is now the header of the page  which is an awful workflow   source code   logs    n a   ,a1a34b1440c4c4792f945275529e6c5b3c7aa2ca,tensorflow\tools\docs\pretty_docs.py
16238,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError,  system information      have i written custom code    no  os platform and distribution    linux ubuntu         tensorflow installed from    source  tensorflow version           python version         bazel version           gcc compiler version           cuda cudnn version  no gpu  gpu model and memory  na  exact command to reproduce  bazel test  c opt   tensorflow contrib gan losses impl test    describe the problem    one of the sub test test stable global norm unchanged fails on   with  assertionerror                      seems like a minor difference  so i tried changing the tolerance slightly as below            self assertnear            self assertnear       with this the test is passing   is it ok to create a pr with this change  could you please share your thoughts on this   source code   logs                           f                                                                                                                                                            f,4383f3d002ddb0712a7aac3303cde6e599de65eb,tensorflow\contrib\gan\python\losses\python\losses_impl_test.py
16481,Container localhost does not exist.,  hi   i upgraded from         to the current master branch and i started receiving the following error                  w tensorflow core framework op kernel cc    op requires failed at lookup table op cc     not found  container localhost does not exist                    w tensorflow core framework op kernel cc    op requires failed at iterator ops cc     not found  container localhost does not exist         node    lookuptablefind     tin dt string  tout        exception in thread  main  org platanios tensorflow jni notfoundexception  container localhost does not exist         node    lookuptablefind     tin dt string  tout            node  model model iterator next   iteratorgetnext output shapes                                  output types                   device   job localhost replica   task   device cpu             it s hard to reproduce this error but a summary of the context is that i have a lookup table op inside a dataset map operator and i get this error when i try to e,77e6a452188e83ae4498cc3ae23e20e60061b367,tensorflow\core\kernels\data\iterator_ops.cc
16747,No documentation on the order of eigenvalues returned by tf.self_adjoint_eig,  system information      os platform and distribution   ubuntu      tensorflow installed from   binary  tensorflow version      python version           describe the problem    from the documentation of tf self adjoint eig i cannot see what the order of eigenvalues is  i tried with several examples and found they were sorted in ascending order  does this always hold    ,6f0dd0425c51360fe2be5a938a8f3fb39e420fa3,tensorflow\core\api_def\base_api\api_def_SelfAdjointEig.pbtxt
16829,tf.contrib.estimator.replicate_model_fn fails when a trainable variable doesn't have gradient,  tf contrib estimator replicate model fn fails when the gradient of a trainable variable is none  the error messages are     file   usr local lib     dist packages tensorflow python estimator estimator py   line    in train      loss   self  train model     file   usr local lib     dist packages tensorflow python estimator estimator py   line    in  train model      features  labels  model fn lib modekeys train  self config     file   usr local lib     dist packages tensorflow python estimator estimator py   line    in  call model fn      model fn results   self  model fn     file   usr local lib     dist packages tensorflow contrib estimator python estimator replicate model fn py   line    in replicated model fn      local ps devices ps devices     file   usr local lib     dist packages tensorflow contrib estimator python estimator replicate model fn py   line    in  get loss towers        optional params     file  model                 net py   line    in model fn      train op   op,96564330fb0508a50a0515be11c9202c64b0f5b7,tensorflow\contrib\estimator\python\estimator\replicate_model_fn.py
16954,Iterator.get_next() documentation improvement request,  system information    n a  describe the problem    recently i ve written some code using dataset api and i would like to request a problem with documentation     instead of hardcoding comments here    please  move  mrry   s annotation about  and  into  method documentation   i don t know why i didn t get that beautiful warning on my console output but i think i m not the first person with funny thread bomb running and consuming system resources     you know about that also     so    it would be great if you could move all critical annotation into main documentation  i m thinking now about all ml newcomers rather than me     that s all   source code   logs    n a   ,620dc3f097d047346943c416823f5e370df9fe4b,tensorflow\python\data\ops\iterator_ops.py
17130,Java: SIGSEGV when `Tensors.create`-ing from an uninitialized array,  system information      have i written custom code    yes  os platform and distribution     macos        jre version  java  se runtime environment      tensorflow installed from    maven                org tensorflow        tensorflow                       tensorflow version           python version  n a  bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce            junit test case      public void testsigsegv             byte     bb   new byte           note  new byte       doesn t crash  presumably it is initialized by the compile            bb      new byte                   bb      new byte                      no bb                  next line sigsegv s          tensors create           describe the problem    using tensors create to get a tensor from an uninitialized   byte array   results in a sigsegv from jni  see full log below   source code   logs    when running the above test case  i get the fol,624a2e47329fefa1f17373954ac541b0e42a9fca,tensorflow\java\src\main\native\tensor_jni.cc
17246,Fetching value of Variable unnecessarily slow,  doing sess run  is about   slower than sess run    python variable fetch bug report py      variable  fetch cpu variable        gb sec  min       median       mean       fetch cpu variable add      gb sec  min       median       mean       fetch cpu variable concat      gb sec  min       median       mean       tensorflow version info   version                                ,879fc3440495d9388754cb7d1878caf034d03d61,tensorflow\python\lib\core\ndarray_tensor.cc
17284,tensor-valued seeds in tf.data API can result in nondeterministic results,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    source  tensorflow version                     python version         bazel version           gcc compiler version           cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  see code below    describe the problem    the tf data api allows requires seeds to be provided that are tf tensors  this is an issue when the graph level seed has been set to    and the provided op level seed tensor takes on a value of    as noted in the comments in the code for tf get seed  a   seed is problematic because the c   ops assume this means nondeterminism  of course  when a user is specifying these seeds  they re expecting deterministic behaviour  unfortunately  tf get seed only checks for this issue for the case where the seeds are ints  not tensors  see the code below for an example   i would have been happy to submit a pr for this  ,f6bda409206dc642d7a6f02842e76b0be7234491,tensorflow\contrib\data\python\ops\random_ops.py
17360,C++ api: use of op::Attrs methods in gradients,  the generated op  attrs struct returns new instances on its chainable methods  and doesn t change the original object         tensorflow tensorflow cc framework cc op gen cc               line          in                        strings  strappend               there are a few related issues e g        tensorflow tensorflow cc gradients nn grad cc               line          in                        grad attrs dataformat              where the code assumes the underlying object is being mutated and the parameters don t actually pass through   i guess there might be a couple of ways forward  depending on how tensorflow prefers the c   api     decide the attrs chaining methods mutate the underlying object and fix the code generation   decide the attrs chaining methods return new instances  and fix the uses     suggestions   fwiw if option    it might be nice to add tf must use result to the generated api   unfortunately a long standing bug in gcc   means this may be unreliable as an ac,5279cf29cea96b3ec50df506bb51d8ffabdabac9,tensorflow\cc\framework\cc_op_gen.cc
17374,Potential resource leaks caused by unclear Java examples,  system information    java examples at     for tensorflow         describe the problem     returns list of closables  javadoc clearly states that the caller is responsible to free all of them  none of the java examples i found at     highlights that  i realized it by happy accident during in depth reading implementation in session java quite long time after i wrote my code that uses tensorflow                   execute the graph fragments necessary to compute all requested fetches                  warning  the caller assumes ownership of all returned   link tensor s  i e   the         caller must call   link tensor close   on all elements of the returned list to free up         resources                                    public list  run           return runhelper  outputs              i m not sure if the examples them self contain any resource leak or not  they free only the first element of the list  but there may be more of them    i would expect an explicit loop properly freeing,de72c8cccef2ee77667c041b68a34be6fb61ea65,tensorflow\docs_src\install\install_java.md
1748,Optimizers incompatible with sampling -- missing docs?,  hi   perhaps tensorflow docs should mention that   out of   available optimizers will not work with sampling losses  for now  they fail with mysterious messages   the following script will fail for momentum  adagrad  adadelta  rmsprop and ftrl   also  where should i look if i d like to implement my own optimizers for gpu   import tensorflow as tf  import numpy random as nr  import numpy as np      config   num classes      num sampled      num true      activation dim      batch sz           model  setup  activations   tf placeholder    labels   tf placeholder      nce w   tf variable     nce b   tf variable       nce loss   tf reduce mean                   tf nn nce loss nce w  nce b                                   activations  labels                                   num sampled  num classes  num true        optimizer setup  global step     tf variable   initial alpha   tf variable   alpha           tf variable     optimizer   tf train ftrloptimizer       fetches  step   optimize,c34a0d7ea6b557588a7a0c9f9c4e60d59f593af7,tensorflow\core\BUILD
17614,Error command in installation guild,  please go to stack overflow for help and support        there is an small error in the installation guild    which is   step under the  section   the site give an example command of installing tensorflow in the active virtualenv for macos  python which is actually for   with   command        install   upgrade               none any whl       ,df2b8447dc026d1402e3c0cbf7c0071ad5c67178,tensorflow\docs_src\install\install_mac.md
17802,Importing a meta graph which contains a SummaryWriter doesn't work,  system information      have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    binary via pip  tensorflow version                     python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version           gpu model and memory  gtx      exact command to reproduce     first run this   import tensorflow as tf      tf placeholder       tf placeholder              vx   tf variable           vx  writer   tf contrib summary create file writer   saver   tf train saver   sess   tf session   sess run    sess run     result   sess run   print   saver save   then in a different python instance    import tensorflow as tf  saver   tf train import meta graph   sess   tf session   saver restore   describe the problem    trying to restore the meta graph via import meta graph does not work if the graph contains a summarywriter as shown in the example above  the example works if import meta graph is called within,88334807a5beb8b61a967d21e534ed238e7916c0,tensorflow\contrib\cmake\tf_python.cmake
17932,tf.contrib.data.bucket_by_sequence_length fails for nested Dataset element,  hello everyone   i just tried the new function to group variable length inputs for the dataset api  namely  tf contrib data bucket by sequence length  for a small estimator model   i implemented the  such that it returns a dataset  where each element is a tuple      however  when i run it  i get following exception     traceback          file   home leo   lib     site packages tensorflow python data ops dataset ops py   line    in apply  dataset   transformation func   file   home leo   lib     site packages tensorflow contrib data python ops grouping py   line    in  apply fn  window size func window size fn    file   home leo   lib     site packages tensorflow python data ops dataset ops py   line    in apply  dataset   transformation func   file   home leo   lib     site packages tensorflow contrib data python ops grouping py   line    in  apply fn  window size func   file   home leo   lib     site packages tensorflow contrib data python ops grouping py   line    in init  self  ma,2219b88a3d5154b9158a1902b061cad6cae2d0a8,tensorflow\contrib\data\python\kernel_tests\bucketing_test.py
17946,Formatting issue in tf debugger documentation,      in the faq section    there seem to be some markdown formatting problems    ,cde06a39592a849a2bc0ba022e858e6366c87cc5,tensorflow\docs_src\programmers_guide\debugger.md
17949,incorrect description of num_sampled parameter in tf.nn.nce_loss() function: num_sampled is number of negative examples per 1 positive example (NOT per batch),  hi all   looks like parameter  in   function defines the number of negative examples per a positive example but not per a batch as described in tensorflow documentation    docs python tf nn nce loss        import tensorflow as tf  import numpy as np      compute sampled logits  is invoked in nce loss to generate negative sample and calculate logits  from tensorflow python ops nn impl import  compute sampled logits    embedding size      words number      batch size      num sampled        graph   tf graph   with graph as default          input data       train inputs   tf placeholder       train labels   tf placeholder         with tf device            embeddings   tf variable                   tf random uniform            embed   tf nn embedding lookup           nce weights   tf variable                   tf random uniform            nce biases   tf variable                  logits  labels    compute sampled logits                          weights nce weights                        ,600caf99897e82cd0db8665acca5e7630ec1a292,tensorflow\python\ops\nn_impl.py
18094,"`tf.keras.estimator._create_ordered_io` casts everything to floatx, which breaks non-floatx inputs",  system information      have i written custom code    yes  os platform and distribution    debian        tensorflow installed from    installed via pip  tensorflow version       python version         bazel version     gcc compiler version     cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  requires significant code  let me know if necessary    describe the problem    this is kind of a simple issue with using keras models as tensorflow estimators  i unfortunately need to do this awkward conversion in order to use sagemaker  which is even more awkwardly behind by two versions of tensorflow  which is fun   basically  i have a  model that expects a  input   which is then passed through to a lookup layer for some text embeddings  this works fine as a keras model and works fine if i extract the input layers myself and connect them into an estimator  however  if i go to create an estimator from the model using  i run into this code path       tensorflow pyth,3fa8795c511931b55a9703956bdf564fde817c2a,tensorflow\python\keras\_impl\keras\estimator.py
18180,Eager: tf.size() does not respect `out_type`,  system information      have i written custom code    yes  os platform and distribution    linux  ubuntu      tensorflow installed from    binary  tensorflow version           python version          bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce     import tensorflow as tf    tf enable eager execution     print  dtype   describe the problem    as per the documentation of tf size    the  of the returned tensor should default to  and it can be optionally overridden by providing an  argument   however  in the snippet above   returns a  tensor  and in a related stackoverflow question     gradient error occurred when calculate two embeddings on eager mode   the  used by  is resulting in a  tensor   long story short  this is a buggy discrepancy between eager execution and graph construction    likely introduced in commit       cc  alextp     ,46df4a1afd50f69966e63245e7758cc0d5656c4e,tensorflow\python\kernel_tests\array_ops_test.py
1853,shuffle_batch gives ZeroDivisionError when computing capacity stat,  import tensorflow as tf  raw   tf ones    tf train shuffle batch         this fails with  zerodivisionerror                         traceback     in            import tensorflow as tf          raw   tf ones            tf train shuffle batch      users yaroslav anaconda envs tim   lib     site packages tensorflow python training input py in shuffle batch              allow smaller final batch allow smaller final batch              shared name shared name              name name                    users yaroslav anaconda envs tim   lib     site packages tensorflow python training input py in  shuffle batch             full       min after dequeue                                    dtypes                                          note that name contains a     at the end so we intentionally do not place              a     after  s below         unlike tf tensors  python doesn t handle float division by zero  one solution is to wrap  capacity  and  min after dequeue  into tf tensors so that,70ade1b64f65d0a2275672d27129627ff116a997,tensorflow\python\training\input.py
18769,"InvalidArgumentError for save/restore of variables (same version, same OS, same directory)",  i get an invalidargumenterror with no further information when i try to save and then restore parts of my model later to continue training it     initialization   saver   tf train saver   save   saver save   load   saver restore                  w tensorflow core framework op kernel cc    op requires failed at   cc     invalid argument   users nroth documents      trained model     embeddings ckpt data   of    invalid argument  traceback       file   users nroth tf python lib     site packages tensorflow python client session py   line    in  do call      return fn     file   users nroth tf python lib     site packages tensorflow python client session py   line    in  run fn      options  feed dict  fetch list  target list  run metadata     file   users nroth tf python lib     site packages tensorflow python client session py   line    in  call tf sessionrun      status  run metadata     file   users nroth tf python lib     site packages tensorflow python framework errors impl py   l,f2be10a6d278f9a4546fa9cded94074959e67302,tensorflow\core\platform\posix\posix_file_system.cc
19043,Code documetnantion. Probably misprint in function parameter description.,        tensorflow tensorflow contrib slim python slim learning py               line          in                            log every n steps  the frequency  in terms of global steps  that the loss             i believe there was a typo in the description in the penultimate word    which is probably worth replacing with  are    example   log every n steps  the frequency  in terms of global steps  that the loss and global step are logged    ,ebcde41d721ec554a7840cb18e4e8a7a489e424a,tensorflow\contrib\slim\python\slim\learning.py
19360,tf.split's -1 support doesn't handle zero dimensions,  system information      have i written custom code    yes   os platform and distribution    colab  tensorflow installed from    colab  tensorflow version      and        python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  tf split            axis        describe the problem    the variable size version of tf split   allows one of the sizes to be      the corresponding output will expand as necessary so that the total output size matches the input   unfortunately  the    support currently assumes the    dimension corresponds to positive size   it should handle zero as well   e g   this should work  but it doesn t       tf split            axis      traceback       file   users irving anaconda envs openai lib     site packages tensorflow python framework ops py   line    in  create c op      c op   c api tf finishoperation   tensorflow python framework errors impl invalidargumenterror ,88ba64b668044c5cb776fa27c8429e2c0e64e665,tensorflow\core\ops\array_ops.cc
19496,Segfault with rpc ops in eager mode,  system information      have i written custom code    yes  os platform and distribution    ubuntu      tensorflow installed from    binary  pip  tensorflow version         python version        bazel version    na  gcc compiler version    na  cuda cudnn version  na  gpu model and memory  na  exact command to reproduce  see below    describe the problem    rpc ops    are not working in eager mode  see minimal examples below   source code   logs    non eager version works  import tensorflow as tf  from tensorflow contrib rpc python ops gen rpc op import try rpc    with tf graph  as default        response   try rpc       session   tf interactivesession       print     eager version does not     import tensorflow as tf  from tensorflow contrib rpc python ops gen rpc op import try rpc  tf enable eager execution   response   try rpc    ,2c75dbfd2d37a3c06d34cc4b12682a63a75503f7,tensorflow\core\util\rpc\call_container.h
19499,tf.data.Dataset iterators are not cleaned when the loop ends with a break,  system information      have i written custom code   yes  os platform and distribution      tensorflow installed from   binary  tensorflow version          python version         bazel version     gcc compiler version     cuda cudnn version   gpu model and memory   exact command to reproduce     describe the problem    tf data dataset iterators are not cleaned when the loop ends with a break   the code below opens one file per epoch  this eventually hits a system limit     replacing the break by a continue works better since the files are closed  however  this is inefficient if we only need to iterate over a small fraction of the data  source code   logs    dataset   tf data textlinedataset        for epoch in xrange                 batches          for   in dataset           batches               if batches   max batches               break                    ,70674b950ab48f913ed1c99e48c4162287595d46,tensorflow\core\api_def\base_api\api_def_AnonymousIterator.pbtxt
19551,Cannot use AdagradOptimizer with MirroredStrategy,  system information      have i written custom code    yes  os platform and distribution    ubuntu      tensorflow installed from    binary  tensorflow version                     python version          bit  bazel version     gcc compiler version     cuda cudnn version             gpu model and memory  nvidia gtx     gb  exact command to reproduce  python train model py    describe the problem    it took me a while to work out what was going on  but it seems that  tf train adagradoptimizer has some specific implementation detail that causes an error when used with mirroredstrategy  i did a spot check with gradientdescentoptimizer and rmspropoptimizer and they both appear to work in my environment  i m happy to use a different optimizer as a workaround but i thought at the very least this might save others some time hunting down the cause of the error   source code   logs    this is almost exactly copied from the example at        import tensorflow as tf    def model fn        layer  ,d0b51d7d9d50dd73c46ba2f2daaa9d26cc0666d0,tensorflow\python\ops\variable_scope.py
20516,Cannot restore variables with Checkpoint because keys do not align,  system information      have i written custom code    yes  os platform and distribution       tensorflow installed from    binary  tensorflow version    tf nightly           python version         bazel version       gcc compiler version       cuda cudnn version     gpu model and memory     exact command to reproduce     i get the error that is thrown here        tensorflow python training checkpointable util py      i cannot provide code to reproduce it  but basically what happens is i have a class that inherits from checkpointable  it assigns all variables to itself to make them checkpointable  it also assigns an optimizer to itself  i then save the model and restore it  when calling  assert consumed  on the load status object of restore  it throws an error because some key does not match  the variable it tries to restore is actually in the saved checkpoint it just has a different key then the one it gets from enumerate    this describes it as good as i can  sorry for not being abl,f46627f9ed9cd41b5a1ad9cebbdd4c240846c4e0,tensorflow\python\training\checkpointable\base.py
21277,Using TensorFlow's Datasets API causes process to hang in session destructor,  system information      have i written custom code       yes    os platform and distribution       macos high sierra    though we ve also seen this happen on linux as well  we believe     tensorflow installed from       source      tensorflow version                          python version     python            bazel version                gcc compiler version       apple llvm version            cuda cudnn version     n a    gpu model and memory     n a    exact command to reproduce     unfortunately the issue isn t that easy to reproduce without running our application     describe the problem    summary   we are using tensorflow s datasets api  more specifically  we re using tf data dataset from generator to create a dataset based on a generator function   when python comes to garbage collect our tf session object its destructor makes a call into tensorflow to delete the session    this call hangs because it s trying to execute a tf py func function  but cannot acquire python s glo,8cd2d6fe9389e93a4182ae9287f2f8325913fe6c,tensorflow\python\lib\core\py_func.cc
22013,tf.scatter_nd_update - Segmentation fault (core dumped),      system information      have i written custom code     yes  os platform and distribution     linux ubuntu      mobile device   if the issue happens on mobile device   tensorflow installed from     source  tensorflow version       tf checkpoint i have built   tmp tensorflow  git log     commit    author  a  unique tensorflower   date    sun aug                     compat  update forward compatibility horizon to                  piperorigin revid            cat  etc issue                                                  linux           generic          ubuntu smp wed mar         utc         gnu linux  version        lts     version id        version codename xenial       are we in docker                                                yes       compiler                                                        c              copyright     free software foundation  inc   this is free software  see the source for copying conditions   there is no  warranty  not even for merchantability or,85f4f6b7ced7afab7e77e65c2b21448cfbf2d6f2,tensorflow\core\framework\common_shape_fns.cc
22039,Session.run () takes a long time,  system information      have i written custom code     os platform and distribution   ubuntu      mobile device   if the issue happens on mobile device n a  tensorflow installed from    source  tensorflow version          python version      bazel version   n a  gcc compiler version          cuda cudnn version            gpu model and memory geforce    exact command to reproduce n a    describe the problem    each time tensorflow performs session run  for target detection  the detection time of the first image is very long    while the detection time of other images is normal  suppose i detect the image under a certain path    the time of detecting the first image is relatively long  and the remaining nine images are relatively short    however  my operation in practical application is as follows  the session run  is called every once in a while to detect a picture  i hope that the detection time after the first one is normal except for the long time  however  through my test    afte,59166604c26b8ffde742389fcd99c8090bf8ec04,tensorflow\compiler\mlir\lite\python\graphdef_to_tfl_flatbuffer.cc
22438,InvalidArgumentError when SparseTensorValue is not ordered by row then col,  system information      have i written custom code    yes  os platform and distribution    mac os        mobile device   if the issue happens on mobile device  no  tensorflow installed from    binary  tensorflow version                       python version         bazel version    no  gcc compiler version    no  cuda cudnn version  no  gpu model and memory  no  exact command to reproduce  python bug py    describe the problem    if we feed a sparsetensorvalue to tf data dataset from tensor slices such that its indices are not lexicographically sorted by row then col  then we will get a invalidargumenterror   maybe it could be said in the docs  or the error should provide a clearer message  it was hard to guess that indices            is out of order meant the indices were not provided in lexicographic order   i finally saw that it was explained in the sparsetensor docs    but i feel it should be said in the sparsetensorvalue docs   as well   source code   logs    from scipy sparse im,eb625fb51a302dc812c97879697642db9aa8cfc1,tensorflow\core\util\sparse\sparse_tensor.h
23083,Documentation for learning_rate_power in the FTRL optimizer,  please make sure that this is a documentation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag doc template  system information    tensorflow version         doc link    docs python tf train ftrloptimizer      this is probably a silly  small issue  but the docs for the ftrl optimizer do not describe what  and how it interacts with    i tried to figure it out based on the source  but couldn t actually find where learning rate power is used in the code     the   has a very detailed explanation and an equation   something similar for learning rate power would be nice    ,5097def5847270353d7d8a37eacbd0f85e98f2a0,tensorflow\python\training\ftrl.py
23195,Segfault reading dataset more than once (`make_batched_features_dataset`),  system information    have i written custom code    no  os platform and distribution    macos     mojave  mobile device   if the issue happens on mobile device  na  tensorflow installed from    binary   pip  tensorflow version       python version         bazel version    na  gcc compiler version    na  cuda cudnn version  na  gpu model and memory  na    gist of full output of    tools tf env collect sh    exact command to reproduce     wget    raw     txt    python   txt      describe the current behavior  current behavior in              is that the included script segfaults     describe the expected behavior  expected behavior is no segfault  version               and       do not segfault  i have not checked lower versions   code to reproduce the issue  see a full script which reproduces this issue along two different code paths here        the segfault occurs when reading the dataset a second time  the first read works as expected   for reference the two code samples which produ,95de98b58a35aaac2804716a70979e68596f3dae,tensorflow\core\kernels\data\parse_example_dataset_op.cc
23443,Python client link is broken,  please make sure that this is a documentation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag doc template  system information    tensorflow version  unrelated  i m using github ui viewing the code  doc link    lib py        this link   guides python client   gives    we welcome contributions by users  will you be able to update submit a pr   to fix the doc issue    ,09f82cbffdcaf5298bcbd4703c18b64ddd287aab,tensorflow\contrib\crf\__init__.py
23449,Bug in tensorflow lite java wrapper,  in file        line        in this function  output tensors length should be used instead of inputtensors length   ,816426f66a9b3edbbdf4203684f7753b2974866d,tensorflow\lite\java\BUILD
23748,tensorflow.keras Dense layers complain if the input is a sparse Input layer.,  system information    have i written custom code     yes   os platform and distribution     osx mojave        mobile device   if the issue happens on mobile device   na  tensorflow installed from     binary  tensorflow version              python version        bazel version     na  gcc compiler version     cuda cudnn version   gpu model and memory     you can collect some of this information using our environment capture script    you can also obtain the tensorflow version with  python  c  import tensorflow as tf  print    describe the current behavior  see below  which i ran on a osx mojave macbook pro    ipython running python      tensorflow         in      from tensorflow keras models import model    anaconda envs dssm lib     importlib  bootstrap py    runtimewarning  compiletime version     of module  tensorflow python framework fast tensor util  does not match runtime version        return f     in      from tensorflow keras layers import input  dense    in      i   input   s,74195b50fb5e1f22eb95ffd1646b4b0ceca2ea9b,tensorflow\python\client\session_test.py
23878,Bug: instantiating dynamic_rnn with tf.int32 in input and state raises TypeError,  system information    os platform  os x        custom code  tensorflow version         python version           describe the current behavior  tensorflow raises a typeerror when creating a dynamic rnn with tf   type in its input and state  when changing the type to tf   the error is not raised   describe the expected behavior  ideally  a dynamic rnn should support tf   types  if there s any reason why instantiating a dynamic rnn with tf   type in its input and state should not be allowed  a custom error should be raised   code to reproduce the issue  the code below reproduces the error   import tensorflow as tf    x   tf placeholder   cell   tf nn rnn cell lstmcell   output  state   tf nn dynamic rnn         the code below doesn t     import tensorflow as tf    x   tf placeholder   cell   tf nn rnn cell lstmcell   output  state   tf nn dynamic rnn         note the change in dtype   other info   logs  traceback     typeerror                                 traceback     in      x   tf,36304bc4ceb6140e470420b65ce470092fc47ab2,tensorflow\python\kernel_tests\rnn_test.py
23924,Memory leak when using tf.contrib.data.unbatch(),  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version                     python version  python          anaconda  inc   bazel version     gcc compiler version     cuda cudnn version       gpu model and memory       describe the current behavior  memory usage continuously increase when using tf contrib data unbatch    describe the expected behavior  memory usage should not increase   code to reproduce the issue  from absl import app  from absl import flags  from absl import logging  import tensorflow as tf      flags   flags flags  flags define integer   flags define boolean       def create dataset        dataset   tf data dataset from tensor slices          def generate random tensor            return tf random uniform         dataset   dataset map       if flags use unbatch           dataset   dataset apply        else,bb425754adacc784c2ad50ed94307eaa03626b41,tensorflow\core\kernels\data\unbatch_dataset_op.cc
23987,LARSOptimizer does not initialize _learning_rate_tensor and _momentum_tensor,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     os platform and distribution  linux ubuntu      tensorflow installed from  binary  tensorflow version  tags      python version       cpu mode    describe the current behavior   larsoptimizer  object has no attribute   learning rate tensor   code to reproduce the issue   tensorflow tensorflow contrib opt python training lars optimizer test py   ,6d3d0de39f3eaecdbf7fbdb52a7a02b2539efeeb,tensorflow\contrib\opt\python\training\lars_optimizer.py
24286,wrong command for cloning tensorflow in TF for microcontroller doc,  please make sure that this is a documentation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag doc template  system information    tensorflow version  master  doc link       describe the documentation issue  in  getting started  chapter  the command for cloning tensorflos is   download the tensorflow source with git clone      it should be    git clone      we welcome contributions by users  will you be able to update submit a pr   to fix the doc issue   yes   ,caf2d701d27bf5b2e9378db5dfa63e08054ff497,tensorflow\lite\experimental\micro\README.md
24414,ppc64le: no_mkl_dnn_contraction_kernel define causes build failure,  please make sure that this is a build installation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag build template  system information    os platform and distribution    ubuntu        mobile device   if the issue happens on mobile device  n a  tensorflow installed from    source  tensorflow version  master  python version       installed using virtualenv  pip  conda    source  bazel version           gcc compiler version          cuda cudnn version            gpu model and memory         a recent commit       changed the default contraction kernel to be mkl based  code was added for non intel platforms to avoid mkl  but it has an error   provide the exact sequence of commands   steps that you executed before running into the problem  basic build  bazel build   tensorflow tools pip package build pip package  any other info   logs  include any logs or source code that would be helpful to di,1a26797203e62390623756fd67e69e7665b69389,tensorflow\core\kernels\BUILD
24598,Potential tf.boolean_mask bug when the mask array is empty,  system information    os platform and distribution   windows    tensorflow installed from    binary  tensorflow version           python version         cuda cudnn version         gpu model and memory  geforce gtx          i have actually experiencing almost the similar problem like in thread         again  i want to partition a minibatch into different parts  process them in parallel using different computation units and then stitch them back together  however this time i used  instead of  for the partition operation  since the latter runs into problems when one of the partitions is empty  this code is below     import tensorflow as tf  import numpy as np      def build conv layer          ok      conv weights   tf variable           tf truncated normal  filter size  filter size  num of input channels  num of output channels                                stddev      dtype tf            ok      conv biases   tf variable           tf constant        conv   tf nn         relu   tf nn ,961e9f4505dff0c5a91d12b1f4e476b3cf2d295d,tensorflow\core\kernels\where_op.cc
24632,Interrupting tf.keras training while using the TensorBoard callback wreaks havoc,  system information    have i written custom code     yes  os platform and distribution     mac os x        mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version     version              git version  b              python version          bazel version     n a  gcc compiler version     n a  cuda cudnn version   n a  gpu model and memory   n a    describe the current behavior  using tf keras in jupyter   with the tensorboard callback  some problems occur if i interrupt training  these problems did not occur in tf         i get an exception if i call fit  again on the same model     tensorflow python framework errors impl notfounderror  resource  localhost logdir logs     does not exist    op writescalarsummary  name  epoch loss       i can workaround this problem by recompiling the model     i also get an exception if i interrupt training  then i delete the logs directory  then i try to use the tensorboard callback on the ,826027dbd4277a2636fc2935ed245700fd01e7cd,tensorflow\python\BUILD
2523,Incorrect error message,  this error message is out of date   valueerror  cannot execute operation using run   no default session is registered  use  with default session   or pass an explicit session to run       as far as i can tell  there is no longer any default session function  instead  one must call sess as default     ,3e7aae461b657fd7960ff49b64bad125eabfe1e3,tensorflow\python\framework\ops.py
25262,Usage of tf_stack.extract_stack in registry.py breaks TensorFlow R client,  system information    have i written custom code    no  os platform and distribution    linux fedora    mobile device   if the issue happens on mobile device  na  tensorflow installed from    binary  tensorflow version    nightly  python version       bazel version    na  gcc compiler version    na  cuda cudnn version  na  gpu model and memory  na    hi   this usage of tf stack in registry py  stack   tf stack extract stack   user function   stack         breaks the tensorflow for r client  because at that point  when called from r stack is of length    with both elements being of length     this is analogous to the recently fixed        thank you  jtkeeling     it would be awesome if this could still be fixed for the     release  as i m aware of no workaround and we have users that want to register a custom gradient   many thanks    ,823b694639a3f49b6adbf9e73a08c529d583878e,tensorflow\python\framework\registry.py
25426,"Segmentation Fault with tf.io.decode_csv , numpy record_defaults and tensor input",  system information    have i written custom code     no  os platform and distribution     ubuntu       lts on windows linux subsystem  mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version            preview     b              python version   python        bazel version     gcc compiler version     cuda cudnn version  no  gpu model and memory  no    you can collect some of this information using our environment capture script    you can also obtain the tensorflow version with  python  c  import tensorflow as tf  print     describe the current behavior  segmentation fault    describe the expected behavior  prints result  code to reproduce the issue  import numpy as np  import tensorflow as tf  record defaults np zeros       parsed fields   tf io decode csv   record defaults       other info   logs  include any logs or source code that would be helpful to diagnose the problem  if including tracebacks  please include the f,3ae3c3c9d43f870d6340cec529de03175e914595,tensorflow\python\eager\pywrap_tfe_src.cc
25463,TensorFlow GCS access does not work from colab,  system information  using colab research google com  describe the current behavior  hangs   import tensorflow as tf  tf io gfile exists     which is a public gcs bucket      describe the expected behavior  should not hang    ,a252fe83f4dc29c0614983046825ec75a3529cb9,tensorflow\core\platform\cloud\google_auth_provider.cc
2573,avg/max_pool3d description has a bug.,  in file   tensorflow   api docs python functions and classes   tf nn   md      tensorflow   api docs python functions and classes   tf nn   md      original     a list of  that has length      d tensor of length    the size of the window for each dimension of  the input tensor  must have    i think  ksize      ksize         should change to ksize      ksize         according to the test file       tensorflow tensorflow python kernel tests   py               line          in                        ksize     window     window     window                      ,1a5364efe43f76ab72a1f3651df394d6b121c915,tensorflow\core\ops\nn_ops.cc
25844,[TF 2.0 API Docs] tf.lite.TFLiteConverter,  system information    tensorflow version       doc link       api docs python tf lite tfliteconverter    describe the documentation issue    links      incorrect        correct          description      this is used to convert from a tensorflow  graphdef or savedmodel  into either a tflite flatbuffer or graph visualization   should be   graphdef  saved model or tf keras model      usage example      this line of code should be repeated for each of the methods  open  write     parameters defined      missing   parameters    optimizations  representative dataset    we welcome contributions by users  will you be able to update submit a pr   to fix the doc issue   yes   ,39d1b3c7ee8a619b28832ac95e689ba34bac9500,tensorflow\lite\python\lite.py
25882,tf.image.random_jpeg_quality only products images of single jpeg quality,  system information    have i written custom code    no  os platform and distribution    linux debian    tensorflow installed from    pip install  tensorflow version               python version         cuda cudnn version               gpu model and memory  geforce gtx   ti    mb    describe the current behavior  tf image random jpeg quality generates random jpeg quality on graph creation  which is then fixed   describe the expected behavior  tf image random jpeg quality generates random jpeg quality for each image batch of images passed through it   code to reproduce the issue  import numpy as np  import tensorflow as tf    img   np random randint   dtype np       tf img   tf placeholder   jpeg augment   tf image random jpeg quality tf img                                               min jpeg quality                                                 max jpeg quality       sess config   tf configproto   sess config gpu options allow growth   true    with tf session  as sess       sess ,d87710ed89e01cf479d2b6ba619a879c32ad77d9,tensorflow\core\api_def\base_api\api_def_EncodeJpegVariableQuality.pbtxt
25985,reset_states() failure in a stateful network with initial_states set and training in batch - TypeError: 'NoneType' object is not subscriptable,  system information    have i written custom code    n  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n  tensorflow installed from    binary  tensorflow version             python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version     gpu model and memory  gtx        describe the current behavior  as  manojrege   said from keras team keras      when we use  with rnn in some case  we will get an exception   traceback       file   usr local cellar python       frameworks python framework versions     lib     runpy py   line    in  run module as main         main     mod spec     file   usr local cellar python       frameworks python framework versions     lib     runpy py   line    in  run code      exec     file   users manoj desktop repos yane yane lstm manytomanylstm py   line    in       incremental train     file   users manoj desktop repos yane yane lstm manytomanylstm py   line    in inc,83df61b4d4ad11f3b8cf05ee98d29e6fb5e25506,tensorflow\python\keras\layers\recurrent.py
26048,Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).,  system information    have i written custom code    no  os platform and distribution    macos        tensorflow installed from    binary  tensorflow version                         python version  python          anaconda  inc     when tf custom gradient is used incorrectly  in this case  the returned grad function returns an empty list  the script segfaults   import tensorflow as tf    tf enable eager execution      tf custom gradient  def identity        def grad            return       this return value is wrong       return x  grad    x   tf variable   with tf gradienttape  as t       y   identity   t gradient   the t gradient call fails with                 f   tensorflow c eager tape h    check failed  state op tape empty    abort trap         i think it d be preferable to raise an exception instead of crashing   if i instead return too many values from grad  then the script runs  but this is most likely a bug and should probably raise an exception   import tensorflow as tf    ,710b322a8be78b8aff6b148575fcfe5301f42b64,tensorflow\c\eager\tape.h
26099,tf.one_hot crashes when indices is tf.uint8,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     yes   os platform and distribution    ubuntu       windows    tensorflow installed from    official pip source    tensorflow version           python version       cuda cudnn version             gpu model and memory           describe the current behavior  tf one hot crashes when the indices tensor has dtype tf    the error message shows check failed  new num elements    numelements           other info   logs  i ve also tested under tf       and tf         on different machines  both have the same problem    ,a311216a9f028eec9e6b0d2ef175f5d46dff19b7,tensorflow\core\kernels\one_hot_op.cc
26143,[TF2.0] Error Logging for GradientTape,  hello everyone   i was wondering if there is an option for error logging   or could we have tf output error message for gradient calculation  in the example below  the below will output none values in the current setting but will output correct gradients when the tf variables are a float type  my question is  could we please add an error message stating something like gradient calculation supports only float types   best regards   seung jae bang  def forward           f   a   b         return a   b    params    tf variable   tf variable      with tf gradienttape  as tape       result   forward     tape gradient       system information    linux  tensorflow installed from pip install  u tf nightly     preview              python version         ccing   random forests     ,764fdc6fb1a9ac377440e1b537ff8a6b7e9f2063,tensorflow\python\eager\backprop.py
26394,Allow building TF + nvidia GPU targeting &lt; sm35 if XLA is not enabled,  please make sure that this is a build installation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag build template  system information    os platform and distribution    gentoo  tensorflow installed from    source  tensorflow version         python version         installed using virtualenv  pip  conda   pip in venv  bazel version         gcc compiler version           cuda cudnn version       gpu model and memory  gtx   ti    i was able to successfully build tf from source with xla enabled and compute capability       however  when a session is created the python interpreter exits         import tensorflow as tf      tf session                  i tensorflow core platform profile utils cpu utils cc    cpu frequency    hz                 i tensorflow compiler xla service service cc    xla service   executing computations on platform host  devices                  i tensorflow compiler xla ,8dc2d0eedac7760deb65254a8ef89878743299d7,configure.py
26502,IDE cannot resolve module tf.keras,  system information    have i written custom code     no   os platform and distribution     linux ubuntu      tensorflow installed from     binary    tensorflow version          python version          describe the current behavior  import tensorflow as tf  then pycharm cannot resolve module tf keras and report an error   cannot find reference  keras  in    init   py   but when running program  everything works well   describe the expected behavior  tf keras imported successfully with autocomplete of pycharm   code to reproduce the issue  import tensorflow as tf  other info   logs  it seems that there is no import command for keras module in   init  py of tensorflow package   when i added from tensorflow python import keras to   init   pymanually  everything work well   maybe there are some problem for package importing after keras was moved from  api to python    ,88ca0db75e6daf66c6fb21609ee4100126f1b727,tensorflow\api_template.__init__.py
26533,[TF 2.0 API Docs] tf.argsort,  please make sure that this is a documentation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag doc template  system information    tensorflow version        doc link       api docs python tf argsort    describe the documentation issue      usage example  no usage example is provided       visuals  if applicable  no visuals are included       we welcome contributions by users  will you be able to update submit a pr   to fix the doc issue    ,bd37836156e5114f15544f984f34f08d38555d4d,tensorflow\python\ops\sort_ops.py
26590,[tf.keras.layers.LSTM] Initializer fails with input_length parameter,  system information    have i written custom code    no  os platform and distribution    linux ubuntu                generic          ubuntu smp tue feb         utc         gnu linux        tensorflow installed from    conda as binary   tensorflow version              and     confirmed  python version  python        anaconda  inc             cuda cudnn version  cuda      gpu model and memory  geforce gtx titan x    mwe  import tensorflow as tf  lstm   tf keras layers lstm       current behavior  here s python error message   traceback       file     line    in     file     local anaconda envs   lib     site packages tensorflow python keras layers recurrent py   line    in   init          kwargs     file     local anaconda envs   lib     site packages tensorflow python keras layers recurrent py   line    in   init        super    init       file     local anaconda envs   lib     site packages tensorflow python training checkpointable base py   line    in  method wrapper      method    ,8fcf86ec70a2a91e33f222d2be85675f0b773581,tensorflow\python\keras\layers\recurrent.py
26602,Partial function specified through keyword on first position in tf.function,  wrapping in tf function a partial with first argument specified   def f      return x   y    partial func   functools partial   tf func   tf function     print        this does not work in   x  because tf inspect getfullargspec cannot represent such construct   using argspec   unfortunately this also does not work in    where argspecs are already capable of representing this   typeerror  tf  f  got multiple values for argument  x        ,4e4943edc3d2409bffb5776f99b941987d6eda82,tensorflow\core\protobuf\saved_object_graph.proto
26639,Nasnet models don't support custom image sizes even if include_top is set to False,  the general idea for fine tuning pre trained models with a custom image size is to set the include top parameter to false when loading the models  however it doesn t seem to be working with the nasnet models in tf keras so far  all other models including inception are working fine   note  i was using tensorflow     alpha so i m not sure if that is the problem   i believe maybe some issue somewhere in checking dimension size along with the include top flag but i might be wrong   following is the stack trace     code executed   nasnet   tf keras applications nasnet nasnetlarge include top false  weights  imagenet                                                                                       input shape       error message                                                                                valueerror                                traceback     in           nasnet   tf keras applications nasnet nasnetlarge include top false  weights  imagenet                           ,f7ee1bff1d90aa0ac0a5e16a71c3c60f7ad96fdb,tensorflow\python\keras\applications\nasnet.py
26645,Testing guide page not exist (404),  please make sure that this is a documentation issue  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag doc template  system information    tensorflow version       doc link    guides python test      testing guide   not exist which linked from tf test page     we welcome contributions by users  will you be able to update submit a pr   to fix the doc issue   i don t know if the page originally exists  🤔   ,13cca52d62148fb5e103c1265c95184b75f577f5,tensorflow\python\platform\test.py
26665,"tensorflow 2.0, variable_scope(), TypeError: __call__() got an unexpected keyword argument 'partition_info'",  i have convert a cnn model from   x to     by using    but when i used this converted model  i got an error   file   home hsw virtual env     lib     site packages tensorflow python ops variable scope py   line    in default variable creator import scope import scope  distribute strategy distribute strategy  file   home hsw virtual env     lib     site packages tensorflow python ops variables py   line    in   call   return super    call    file   home hsw virtual env     lib     site packages tensorflow python ops resource variable ops py   line    in   init   constraint constraint  file   home hsw virtual env     lib     site packages tensorflow python ops resource variable ops py   line    in  init from args initial value  if init from fn else initial value  file   home hsw virtual env     lib     site packages tensorflow python ops variable scope py   line    in  shape as list   dtype dtype  partition info partition info  typeerror    call    got an unexpected keyword argument  p,a236ae23782c04a057d17a8ad845500c7f15c432,tensorflow\tools\compatibility\tf_upgrade_v2.py
26684,Repeatedly allocating a graph and summary writer leaks memory,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    i have  os platform and distribution    macos        mobile device   if the issue happens on mobile device  n a  tensorflow installed from    source  tensorflow version                     python version         bazel version           gcc compiler version    apple llvm version          cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  repeatedly allocating a graph and making a summary writer leaks memory   describe the expected behavior  memory should be freed when the graph leaves scope   code to reproduce the issue     usr bin env      import resource  import tensorflow as tf    prev      while true       peak   resource getrusage  ru maxrss      print         prev   peak        with tf graph  as default   tf,097fc1cdef5c56d4bb239a5a44bf950f0b1c4d37,tensorflow\python\kernel_tests\summary_ops_test.py
26808,[TF 2.0] unconnected_gradients = 'zero' does not work,  system information    os platform and distribution  macos        tensorflow installed from binary  tensorflow version         python version           i try to get gradients w r t  model parameters  though i was getting none values  here is an example     import tensorflow as tf    import tensorflow keras layers as layers      model   tf keras sequential     model add       with tf gradienttape  as tape       loss   tf random normal      grads   tape gradient     print    none  none       i expect these values to be zero  though they are not    ,a79ed9c304bf9c1971fe3df4f61a0d0ab515eff9,tensorflow\python\eager\backprop.py
26902,tf_upgrade_v2 does not preserve file attributes and symbolic links,  system information    have i written custom code   n a  os platform and distribution   archlinux  mobile device   if the issue happens on mobile device n a  tensorflow installed from   binary  tensorflow version      preview nightly yesterday  python version      bazel version   n a  gcc compiler version   n a  cuda cudnn version n a  gpu model and memory n a          changes executable files to non executable files  i expect executable files are still executable after the upgrade         always changes symbolic links to regular files  however i expect    for in place upgrade  modify the file the link points to  but the symbolic link should be the same     for non in place upgrade  if   intree and   outtree is used  symbolic links which point to files within the tree should become symbolic links pointing to the new file in the outtree  symbolic links which point to external files should become a regular file     for non in place single file upgrade  the output should be a regular fil,0fa0d44944abd86578fa076802f5a8a7490d5656,tensorflow\tools\compatibility\ast_edits.py
27202,Tflite JNI wraps seems failing to release int array.,  hi  it seems that current impl of tflite jni overlooked ref release for array  and the current tflite really could make jni reference table overflow some phones with android           how   just invoke resizeinput every time you run interpreter  even put the same int array to it  you can see the reference table is booming   version   i ve tried       and       nightly  it s the same   the relevant code is here    the strange thing is the code above  althrough didn t deal with release in some situation  should work fine with same int array    ,009fde664530e6616e5aa1f882ff497e9e435924,tensorflow\lite\java\src\main\native\nativeinterpreterwrapper_jni.cc
27282,/tensorflow/lite/experimental/c/c_api_types.h is not readable on Windows filesystem.,  system information    os platform and distribution    windows  mobile device   if the issue happens on mobile device  no  tensorflow installed from    source  tensorflow version     python version  no  installed using virtualenv  pip  conda   no  bazel version    no  gcc compiler version    gcc      cuda cudnn version  no  gpu model and memory  no    describe the problem   tensorflow lite experimental c c api types h is symbolic link to  tensorflow lite c c api internal h   on dos compatible file system  it is replaced with following text file         c c api internal h      provide the exact sequence of commands   steps that you executed before running into the problem  clone repository  and make sure file  tensorflow lite experimental c c api types h  is not a symbolic link on windows   any other info   logs  include any logs or source code that would be helpful to diagnose the problem  if including tracebacks  please include the full traceback  large logs and files should be attac,04e311bf7628eac8b0334a7419442f1009487d7f,tensorflow\lite\c\BUILD
27292,keras.layers.RNN with contants,  system information    have i written custom code    yes  os platform and distribution    arch linux  mobile device   if the issue happens on mobile device   tensorflow installed from    no  tensorflow version        and      python version       bazel version     gcc compiler version     cuda cudnn version      and      gpu model and memory    ti    describe the current behavior  typeerror  can only concatenate list   to list in rnn  build  if a call the rnn with a tensor as constants   describe the expected behavior  basically the build  function of the rnncellwithconstants should be called  with the input shape           code to reproduce the issue  import tensorflow as tf      class rnncellwithconstants          def   init              self state size              super    init           def build            print           self built   true        def call            print           return inputs   inputs         test basic case   x   tf keras input    c   tf keras input    cell ,3e8a80bce0f7ef0ab2ee49f3528a2652f26110f0,tensorflow\python\keras\layers\recurrent.py
27305,Document stride parameter for XlaBuilder::Slice,  doc link    semantics    the documentation for xlabuilder  slice   does not mention the stride parameter    ,369a886aab96fc081ad6637d7b413a339382b758,tensorflow\compiler\xla\g3doc\operation_semantics.md
2740,ExponentialMovingAverage.average duplicates the current scope name,  using tensorflow nightly   import tensorflow as tf  with tf name scope        x   tf variable       ema   tf train exponentialmovingaverage       apply op   ema apply       average   ema average       print average name      scope scope variable exponentialmovingaverage         print ema average name      scope variable exponentialmovingaverage    ,a2b9788ce440c350d4e3fef53fe0c51ba1c10c1a,tensorflow\python\training\moving_averages.py
27431,Using layer classes as attribute throw an exception,  system information    have i written custom code  yes  os platform and distribution  ubuntu      tensorflow installed from  binary  tensorflow version           python version           describe the current behavior  when a layer class is used as attribute  the code will throw a typeerror exception when calling self  gather children attribute  it appears that the layer class is tracked   describe the expected behavior  only layer instances should be tracked  not classes   code to reproduce the issue  import tensorflow as tf    class layer          def   init              super    init             self layer fn   tf keras layers dense    layer   layer   print   other info   logs  traceback       file    class py   line    in       print     file   tmp   local lib     site packages tensorflow python keras engine base layer py   line    in variables      return self weights    file   tmp   local lib     site packages tensorflow python keras engine base layer py   line    in weights     ,9d724a8e6034d321e97cdc9972d4d6e7adb3e3ca,tensorflow\python\keras\engine\base_layer_test.py
27455,TF2.0 gradient problem of using tf.nn.relu in tf.keras.Model.,  system information    os platform and distribution  linux ubuntu      tensorflow installed from  binary  tensorflow version           python version           describe the current behavior  i built a keras model with only a tf nn relu  but the gradient seems to be none after being decorated by  tf function  code to reproduce the issue    tf nn relu   tf keras model    tf function      import tensorflow as tf    z   tf keras input    h   tf nn relu   m   tf keras model      tf function  def f      with  tf function      with tf gradienttape  as t           t watch           z   m       return t gradient     print           none      tf nn relu   tf keras model without  tf function  def f      without  tf function      with tf gradienttape  as t           t watch           z   m       return t gradient     print           tf tensor   dtype       tf keras layers relu    tf keras model    tf function    import tensorflow as tf    z   tf keras input    h   tf keras layers relu    m   tf k,244cb0b925902a29c6a39c62fd1b80cb3797051b,tensorflow\python\keras\engine\base_layer.py
27565,[TF==2.0.0a0] @tf.function raises ValueError when computing gradients,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    yes  os platform and distribution    ubuntu      tensorflow version    pip install tensorflow          python version         describe the current behavior  the code executes normally  but raise valueerror when computing gradients   if i decorate the training function with  tf function  the traceback is as follows                                                                                valueerror                                traceback      workspaces fgenl run py in               for batch id in range                   batch data   data generator get data                      loss  outputs   train one step                           loss  outputs  inputs   sess run                  if loss metrics is none        pyenv versions         envs,110f0610ed0cf52d256e414906cf91d4e9d657e7,tensorflow\python\eager\backprop.py
27632,[doc/keras] incorrect comment in the example for `tf.keras.layers.Add`,    doc link    docs python tf keras layers add class add    describe the documentation issue  see the code example    docs python tf keras layers add class add         added   keras layers add      equivalent to added        keras layers add   it should be      added   keras layers add      equivalent to added   keras layers add        ,e90399a37b7b3984e2f49a89d886d4dfd78d42db,tensorflow\python\keras\layers\merge.py
27705,Keras subclassing and explicit dtype of Input,  system information  have i written custom code     os platform and distribution    arch linux  tensorflow version           description  when using keras subclassing there is no apparent way of defining the dtype of the input node of the network  in some cases  it would be neccecary to use tf   instead of   but as of now i cannot find any way to adjust this  also trying to set the dtype using self dtype   tf   is not permitted    ,1b96e5212002b7ac5027a7538a8f6e5780b669f5,tensorflow\python\keras\engine\base_layer.py
27769,[TF 2.0 keras] Unable save and load weights for double nested models,  system information    have i written custom code    yes  os platform and distribution    mac  tensorflow installed from    binary  tensorflow version           python version         describe the current behavior  load weights throw exception on a doubly nested model  describe the expected behavior  load weights should work  this problem only happens on two  layers of nested model with non trainable weights   the reason is save weights and load weights handles nested model differently  save weights    call layer weights for each layer  load weights    recursively call model weights if layer is a nested model  code to reproduce the issue  import tensorflow as tf  from tensorflow keras import model  from tensorflow keras layers import input     batchnormalization    shape        def bnmodel        x   inputs   input       x            x   batchnormalization        return model     x   inner inputs   input   x   bnmodel    x   bnmodel    inner model   model     inputs   input   model   ,f42549a91a3759a9264ef4d44e9224be4ee3bdc3,tensorflow\python\keras\saving\hdf5_format.py
27829,Cannot create a stateful RNN with recurrent dropout,  system information    have i written custom code     yes  os platform and distribution     macosx        mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version     tf version version          tf version git version            python version          bazel version     n a  gcc compiler version     n a  cuda cudnn version   n a  gpu model and memory   n a    describe the current behavior  i get an exception when trying to use recurrent dropout in a stateful rnn       tensorflow python ops resource variable ops py in   imul                  def   imul               raise runtimeerror  variable    value not supported  use                                  var assign   to modify the variable or                                  var   var   value  to get a new tensor object       runtimeerror  variable    value not supported  use  var assign   to modify the variable or  var   var   value  to get a new tensor object       the full,6a6e8c2586dfd2aeeebe0d94d60dcca4604ab481,tensorflow\python\keras\layers\recurrent.py
27845,Wrong derivatives for complex second order derivatives.,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    osx  mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version    tensorflow         python version           you can collect some of this information using our environment capture script    you can also obtain the tensorflow version with  python  c  import tensorflow as tf  print    describe the current behavior  derivatives of non holomorphic functions are incorrect when compared both against ad and finite differences   describe the expected behavior  derivatives of non holomorphic functions should becorrect   code to reproduce the issue  provide a reproducible test case that is the bare minimum necessary to generate the problem   import numpy as onp  ,2518fc3ef8b962b8487b930d9798d4696f0e53ee,tensorflow\python\kernel_tests\cwise_ops_unary_test.py
27847,BUG: tfdbg session cannot be used with SessionRunHooks,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code   yes  os platform and distribution   archlinux  mobile device   if the issue happens on mobile device   tensorflow installed from   binary  tensorflow version   b                     python version      bazel version   n a  gcc compiler version   n a  cuda cudnn version n a  gpu model and memory n a    the following code        coding  utf         file       import numpy as np  import tensorflow as tf  from tensorflow python import debug as tf debug    a   tf placeholder   b   a      c   b        class hook        def before run            return tf train sessionrunargs     class          def before run            return tf train sessionrunargs     sess   tf session   sess   tf debug localclidebugwrappersession     class sessioncreator        def c,e2d269edb9217411fc4119338df949e1a741432b,tensorflow\python\debug\wrappers\framework.py
28070,tf2.0a0 tf.nn.ctc_loss with AttributeError: Tensor.op is meaningless when eager execution is enabled.,  system information    have i written custom code    yes   os platform and distribution    ubuntu          mobile device   if the issue happens on mobile device   tensorflow installed from    pip install tensorflow gpu        alpha  tensorflow version                         python version         bazel version     gcc compiler version     cuda cudnn version      fogotten  gpu model and memory     code to reproduce the issue  provide a reproducible test case that is the bare minimum necessary to generate the problem   when running  python loss py  error raised  see the error txt for details  maybe there is some internal implementation error on the function namely tf nn ctc loss        helper py    author  jiarenyf        pylint  disable invalid name    pylint  disable too many locals    pylint  disable missing docstring    pylint  disable redefined outer name    import tensorflow as tf                                                   def dense to sparse        eos token   tf constant,778ca5c0bcf87c9e2df73fe9b8074bae5b8c3e58,tensorflow\python\kernel_tests\ctc_loss_op_test.py
28158,Keras ValueError stops autograph building,  system information    have i written custom code    no  tensorflow installed from     pip  tensorflow version              python version          bazel version     gcc compiler version     cuda cudnn version   cudatoolkit          cudnn            gpu model and memory   geforce rtx   ti    describe the current behavior  calling keras layer without calling build  automatically infers the shapes of the trainable variables  this works both in eager mode and graph mode in the current     alpha version  however  running the provided code in         version  it gives the following error message               tf logging py    entity  could not be transform  ed and will be executed as is  some features   may not work as expected  erro  r details can be found in the logs when running with the env variable autograph verbosity       please report this to the  autograph team  cause  valueerror during conversion  weights for model sequential have not yet been created  weights are c  reated when ,008300cc7667da8f8a7d36806470c01a524153d0,tensorflow\python\autograph\converters\directives.py
28346,TrtGraphConverterV2 does not preserve output names in the signature_def,  system information    have i written custom code    no  os platform and distribution    ubuntu      tensorflow installed from    source  tensorflow version     master from april    python version         bazel version         gcc compiler version         cuda cudnn version               gpu model and memory  gtx   ti    describe the current behavior  if you use   to convert a function in a saved model to use trt it does not preserve the output names in the signature def of the saved model   if the saved function   returned a dict   output a   a   output b   b  the names  output a  and  output b  are in the saved model  after conversion with   they are changed to the default names     and       describe the expected behavior  the names of the outputs should not change  this breaks all code that loads the model and relies on the correct names   code to reproduce the issue  take any saved model that contains a function returning a dict   then run this   conversion params   trt convert d,e03ab548c4696efcdbe1cca599da1289c25093b4,tensorflow\python\compiler\tensorrt\trt_convert.py
28406,[tflite doc] CONV_2D_TRANSPOSE -&gt; TRANSPOSE_CONV,  existing urls containing the issue       compatibility    description of issue       tensorflow         op is not present in tensorflow lite schema   after glimpsed toco source code   tf nn    is converted to transpose conv        tensorflow lite toco import tensorflow cc      so updating tflite documentation  would be nice    ,8651de2f625d6fcc63437b5964b5fffca98c411e,tensorflow\lite\g3doc\guide\ops_compatibility.md
28495,Move the Dockerfiles to ubuntu-18.04,  current dockerfile   we have is based out of      its better if we can move to       the corresponding version of tf serving is already using     based ubuntu in their dockerfile    ,6206385a0b8dcb0a71e716c3b019cca820062a06,tensorflow\tools\dockerfiles\partials\ubuntu\devel-nvidia.partial.Dockerfile
28585,The package org.tensorflow.lite.nnapi  does not exist,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     os platform and distribution：ubuntu       mobile device：pixel    tensorflow installed from：source  tensorflow version        python version      bazel version             when i run the demo which in      so  i found the bug don t have  org tensorflow lite nnapi   ,f0836d2a3bdc83b9487d703f30669723bd2662fb,tensorflow\lite\delegates\nnapi\java\src\main\java\org\tensorflow\lite\nnapi\NnApiDelegate.java
28614,Keras RNN example from docs does not support statefulness when multilayer,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information      have i written custom code         os platform and distribution         tensorflow installed from    binary      tensorflow version               python version               cuda cudnn version                 gpu model and memory  gtx   ti        modifying the example code given here   to have  leads to the following error   traceback       file  tmp py   line    in       y   layer     file   home davis software   envs   lib     site packages tensorflow python keras layers recurrent py   line    in   call                                                                                                                                        return super    call       file   home davis software   envs   lib     site packages tensorflow python keras engine base layer py   line ,12250556493fe7757bd97f397e3483e7c0e022b1,tensorflow\python\keras\backend.py
28685,The cycle detection algorithm in the variable creation has bad performance,  system information    have i written custom code    yes  os platform and distribution    arch linux  mobile device   if the issue happens on mobile device  don t know  tensorflow installed from    binary  tensorflow version           python version         bazel version    not used  gcc compiler version    not used  cuda cudnn version  not related  gpu model and memory  not related    describe the current behavior  maybe related         i noticed some slow variable creations  it happens when the initial value is a tensor with complex dependencies   after some digging  i found that it may be caused by the algorithm used in the cycle detection code         tensorflow tensorflow python ops variables py              lines   to          in                        def  has cycle               detect cycles in the dependencies of  initial value               if op name in path           return true          path add           for op input in op inputs           if  has cycle            retur,fc20e9fe8223336b491cedd4bc428867bf0e7daa,tensorflow\python\kernel_tests\variables_test.py
28725,Autograph fails for keyword-only arguments,  system information    have i written custom code    yes  os platform and distribution    linux  tensorflow installed from    pip install tf nightly gpu     preview  tensorflow version                       python version           describe the current behavior  autograph complains when compiling functions with keyword only arguments   example output               ag logging py    entity  could not be transformed and will be executed as is  some features   may not work as expected  error details can be found in the logs when running with the env variable autograph verbosity       please report this to the autograph team  cause  unexpected error transforming   if you believe this is due to a bug  please set the verbosity to     and attach the full output when filing the bug report  caused by  inconsistent nodes  none   and none    describe the expected behavior  autograph works for keyword only arguments  code to reproduce the issue  import tensorflow as tf   tf function  def f        ,22ba2ebfc9779ca61e574ddf6411ee5565381cec,tensorflow\python\autograph\impl\BUILD
28849,Python3 type annotation does not work with @tf.function + for loop -&gt; tf.while_loop conversion,  system information    have i written custom code    yes  os platform and distribution         mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version        alpha  python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  if you use   type annotation such as x tf tensor   tf constant    in a  tf function and the function contains a for loop to be translated to tf while loop    the code will fail as if you did not turn on eager execution   describe the expected behavior    type hinting should not fail the code   code to reproduce the issue   tf function  def tf for tf break        x  tf tensor   tf constant       for i in tf range            x    i      return x    print        other info   logs  warning  logging before flag parsing goes to stderr               tf logging py    entity  could not be transformed and will be stag,d3dd0726fc2a6be6235bc1e9c8825056278d3470,tensorflow\python\autograph\pyct\static_analysis\activity.py
29060,"""Cache iterator is in an invalid state"" error",  system information    os platform and distribution  macos high sierra        tensorflow for cpu installed from pypi  tensorflow version                      python version           describe the current behavior  minimal not working example   import tensorflow as tf  from tensorflow python framework errors impl import outofrangeerror      dataset   tf data dataset range   dataset   dataset cache   dataset   dataset map   dataset   dataset batch     batch   dataset make one shot iterator  get next     with tf session  as sess       while true           try               res   sess run               print           except outofrangeerror               print               break  the code above properly iterates through the dataset only the first run when a cache doesn t exist  but when it loads the dataset from the cache file it crashes with an error   tensorflow python framework errors impl internalerror  cache iterator is in an invalid state           node iteratorgetnext         a wo,003e400902b85626d32727589142d12269306703,tensorflow\core\kernels\data\cache_dataset_ops.cc
29187,TF 2.0: Cannot use recurrent_dropout with LSTMs/GRUs,  system information    have i written custom code    no    os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version    tensorflow gpu             python version       bazel version    n a  gcc compiler version    n a  cuda cudnn version  tried multiple  gpu model and memory  tried multiple    describe the current behavior  the program crashes with a typeerror as below   typeerror  an op outside of the function building code is being passed a  graph  tensor  it is possible to have graph tensors leak out of the function building context by including a tf init scope in your function building code  for example  the following function will fail   tf function def has init scope   my constant   tf constant  with tf init scope   added   my constant     the graph tensor has name  encoder unified gru ones like    this occurs when trying to backprop the gradients through the lstm gru with,180f28a26660ca2e1ba27477f4f9592db5f9c4e8,tensorflow\python\keras\layers\recurrent_v2.py
29191,tf.function spuriously fails for branched super() calls,  tf function fails spuriously under python       for the following example   import tensorflow as tf    class base        def   call              return x         class sub        def   call              return super    call    if true else        tf function  def test        return sub       print     colab         this produces the following error   runtimeerror  in converted code         bug py   test             return sub         bug py     call               return super    call    if true else           runtimeerror  super   no arguments  observations    everything works correctly without the tf function decoration  the branch in   call   seems necessary to trigger the bug  skipping the branch doesn t trigger it  replacing true with false doesn t trigger it   the bug can be triggered even if the condition evaluates to false    replacing tf constant  with    doesn t trigger the bug    tested on tensorflow     nightly         on ubuntu     with python         ,4d4f7ed0a4605975e45efe9ca1750653190aeedf,tensorflow\python\autograph\converters\call_trees.py
29250,[TF 2.0 API Docs] tf.autograph.set_verbosity,  url  with the issue          api docs python tf autograph set verbosity    description of issue       clear description    description could be clearer     reference to abseil s logging format could be referenced rather than only to abseil  itself    there s a slight misspelling in the args for alsologtostdout    it is recommended to set this value to a larges number  like    should be   it is recommended to set this value to a large number  like       submit a pull request     yes    ,61133370602a5fc7b3313ea7f8f745dae66b3c38,tensorflow\python\autograph\utils\ag_logging.py
29270,tf.autograph.experimental.Feature,  url  with the issue          api docs python tf autograph experimental feature    description of issue       clear description    there is no description provided   parameters defined    parameters are undefined   returns defined    return values are not defined   raises listed and defined    errors are not defined   usage example    there is not a usage example    ,629e5a8a56653b16cb784a52009ff1ceaf3db73b,tensorflow\python\autograph\core\converter.py
29277,[TF 2.0 API Docs] tf.nn.dropout,  url  with the issue          api docs python tf nn dropout    description of issue       usage example    usages are linked but none are detailed inline on the page   ,951be80fc6873434b8ab2bef65d437b094037c86,tensorflow\python\distribute\BUILD
29342,tf.config.set_soft_device_placement() seems to have no effect,  system information    have i written custom code     yes  os platform and distribution     macosx        mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version     version            git version              python version        bazel version     n a  gcc compiler version     n a  cuda cudnn version   cuda        gpu model and memory   tesla        describe the current behavior  the tf config set soft device placement  function seems to have no effect when i create an integer variable and try to place it on a gpu  i still get an exception   describe the expected behavior  i expect soft placement to fallback to using the cpu  no error   code to reproduce the issue  import tensorflow as tf  tf config set soft device placement   with tf device        f   tf variable   other info   logs  the code above causes the following exception                                                                                notfounderror ,7360531c13113a19120d798278cd20bec2e5e0c3,tensorflow\c\eager\c_api.cc
29393,[2.0alpha0 AutoGraph] tf.function does not automatically transform nested class methods,  system information    have i written custom code    yes  tensorflow installed from    binary  tensorflow version           python version           describe the current behavior  when we define multiple methods for a class and only decorate one of them with  tf function  the nested methods are not automatically transformed and some errors raise   describe the expected behavior  we only need decorate the outermost method   code to reproduce the issue        coding  utf           author    lin lan      from   future   import absolute import  from   future   import division  from   future   import print function    import tensorflow as tf      class foo        def   init              super    init             self dense   tf keras layers dense           self embeddings   tf variable    dtype tf            tf function      def call            embeddings   tf nn embedding lookup               self embeddings  inputs           return self  inner            tf function      def  inner      ,665bd441195ce352b0d5ce74d5fd2dc19fa4a614,tensorflow\python\autograph\impl\conversion.py
29406,[TF 2.0 API Docs] tf.data.experimental.make_saveable_from_iterator,  url  with the issue          api docs python tf data experimental make saveable from iterator    description of issue       returns defined    the returns section is missing   raises listed and defined    raises are neither listed nor defined    ,82ccb44214ae7d9019826f658dc27a87e37c89f3,tensorflow\python\data\experimental\ops\iterator_ops.py
29439,Unittest and test_session interaction,  system information    have i written custom code    yes  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version                           python version                   bazel version    n a  gcc compiler version    n a  cuda cudnn version         gpu model and memory           environment capture available at        describe the current behavior  additional  ghost  tests are being run but skipped when using unittest with tensorflow testcase class  this behavior is present in        when upgrading to       or        the tests are being skipped entirely as the  ghost  test is in regards to the test session method that you have within the tensorflow python framework testutils and unittest believes that the tests are not actually tests   describe the expected behavior  no  ghost  tests should be run at all in       and the tests work in       and         code to reproduce the ,1e30076f57bf30838b5cb2e59f05e13deb944d1b,tensorflow\python\framework\test_util.py
29501,tensorflow debugger `run -t` fails on keras,  see the description at    tensorflow debugger run t failed running keras model    system information    have i written custom code    yes  os platform and distribution    mac os  mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version           python version       bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     describe the current behavior  exception thrown  describe the expected behavior  run the number of iteration as specified in the run  t command  code to reproduce the issue  see     tensorflow debugger run t failed running keras model    other info   logs   ,b2bdbfb9260fe58d9c5bfe9df11fc51535e5fef3,tensorflow\python\debug\BUILD
29509,How to convert a tensorlfow SpaceToBatchND-Conv2D-BatchToSpaceND to a single Conv2D in tflite,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code   no  os platform and distribution   linux ubuntu      mobile device   if the issue happens on mobile device na  tensorflow installed from   source  tensorflow version          python version       bazel version           gcc compiler version     cuda cudnn version   gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf          i m trying to train my own deeplab model using this code   and convert it to tflite   my target is to get a model similar to this    however  the model is obtained contains operations like       spacetobatchnd and batchtospacend operations are not supported by tflite   opengles backend  they reduced the mod,f54bb6f5578b931d79884302768996ba1073f685,tensorflow\compiler\mlir\lite\BUILD
29656,Bug on `gather_nd` with gradient.,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device   tensorflow installed from    pip  tensorflow version      gpu beta  python version         bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  a simple test code  v   tf variable   dtype tf       with tf gradienttape  as tape       l   tf gather nd       l   tf reduce sum         grads   tape gradient   print   gives following error message                                                                               l,a7ef0da19be94d5f189c8a3af960f1da77e58b41,tensorflow\python\kernel_tests\BUILD
29856,tf.keras.layers.UpSampling2D(interpolation='bilinear') has a smearing defect on the right & bottom edges,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    i ve provided a link to a colab notebook demonstrating the issue below  comparing keras upsampling to what it should look like with a correct implementation as seen in tf image resize   os platform and distribution    google colab  mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version                 python version     bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  upsampling using tf keras layers    results in unnatural smearing of the right and bo,15f6c30d7977c92ba452eb5c1873b8c9f0968a5f,tensorflow\python\keras\backend.py
29881,The call method of DenseFeatures and SequenceFeatures use deprecated attribute _num_buckets,  system information    have i written custom code    no  os platform and distribution    macos        tensorflow installed from    from pip install  tensorflow version                       python version           oct               describe the current behavior  by simply calling the call method of densefeatures and sequencefeatures defined with categorical column with identity and sequence categorical column with identity feature columns    we get warnings that the deprecated attributes  num buckets are used     also note  on the code example below  that a third warning about a deprecated method  add dispatch support  wrapper arises  i do not understand it but there are instructions for updating given in the warning  see the code below   describe the expected behavior  i think that we should not get warnings about deprecated objects when we are not calling any deprecated method  attribute  etc  i think that somewhere in the code of the call method of densefeatures and sequencefeatur,d7e858192d1de827bc03705ac62e1bd38daf06d8,tensorflow\python\feature_column\feature_column_v2.py
29989,Segmentation fault when saving checkpoints with saveable Dataset Iterator,  system information      have i written custom code    yes      os platform and distribution    centos            mobile device   if the issue happens on mobile device  no      tensorflow installed from    binary      tensorflow version               python version             bazel version    none      gcc compiler version    none      cuda cudnn version  none      gpu model and memory  none      tf version                         describe the current behavior  segmentation fault in saving an initializable dataset iterator when entering the tf train monitoredsession context manager   describe the expected behavior  the initializable iterator is saved and restored properly  behaving the same with the one shot iterator   code to reproduce the issue     illustrate saveable dataset iterator       import tensorflow as tf    dataset size      save steps      train step      checkpoint dir     tmp tf dataset saveable     def test saveable           test saveable         graph   tf graph    ,51d1f486fcbe5bc8857586250fd5b10ca110d631,tensorflow\core\kernels\data\iterator_ops.cc
30028,Python package is missing ModuleSpec in tensorflow.__spec__ in tf 1.14.0,  system information    have i written custom code    no  os platform and distribution    linux       linuxkit   with ubuntu     bionic  mobile device   if the issue happens on mobile device  no  tensorflow installed from    preinstalled in docker image  tensorflow version                       python version         bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     describe the current behavior  in tf       the module spec in tensorflow   spec   is none       import tensorflow      print   none      describe the expected behavior  this is different from tf       where it works as expected       import tensorflow      print   modulespec       missing spec causes some problems  e g  pkgutil now fails when trying to find tensorflow  note that the first call to find loader is successful  it only fails after tensorflow is imported   python            gcc            trunk revision     on linux  type  help    copyright    credits  or  license  for more ,b789a3b37b59e6795f799645a6e8b1a6c70fc346,tensorflow\python\util\deprecation_wrapper.py
30113,tf.image.encode_png doesn't support 16 bit and inconsistent behavior in eager mode,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    win    mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version    tested with              and        python version       bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  creating a numpy array with   datatype and passing it to tf image encode png  yields different results in eager execution mode  the first time the array is passed it somehow gets transformed to a   array and for the following encodings it works as expected   using ,7807ec92bf8f44b5fd6de5b5342f041b168cf1f3,tensorflow\python\eager\BUILD
30149,"Autograph ""Failed to parse source code"" error when using lambda in for loop",  system information    have i written custom code     yes  os platform and distribution     macosx        mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version     version          git version            python version          bazel version     n a  gcc compiler version     n a  cuda cudnn version   n a  gpu model and memory   n a    describe the current behavior  i get an autograph error when running the following code     import tensorflow as tf  ds   tf data dataset range  window   for window in ds flat map         print    the error is valueerror  failed to parse source code of  at     describe the expected behavior  everything works fine when i define the dataset on the previous line like this   import tensorflow as tf  ds   tf data dataset range  window   ds   ds flat map    for window in ds       print    code to reproduce the issue  see above   other info   logs  full stack trace with autograph verbosity         ,38df5d8ef43e884674f22670dbfd19ec26782f17,tensorflow\python\autograph\pyct\parser.py
30165,TF 2.0 - Put Tensor into some Numpy functions continuously increases memory usage,  system information    have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    pip package tensorflow           tensorflow version                         python version         cuda cudnn version               gpu model and memory  titan xp      describe the current behavior  memory leak when we put tensor into some numpy functions    np zeros like    following attached code continuously increases memory usage   describe the expected behavior  no memory usage explosion   code to reproduce the issue  import tensorflow as tf  import numpy as np  import time    x   tf random normal    for i in range         y   np array       time sleep        ,d5b287d6c93332ba73b99b375bd21f81266e3112,tensorflow\c\eager\c_api.cc
30248,tf.io.write_file not working in tf.function decorated function,  system information    os platform and distribution    ubuntu       and windows    tensorflow installed from    binary  tensorflow version             python version         describe the current behavior  tf io write file creates file in eager execution but produces no output file when decorated with  tf function   describe the expected behavior  tf io write file should create an output file whether or not being decorated with  tf function   code to reproduce the issue  import tensorflow as tf     tf function  def writejpeg graph        out   tf cast       out   tf image encode jpeg       tf io write file         def writejpeg eager        out   tf cast       out   tf image encode jpeg       tf io write file     img   tf fill    example gray image  writejpeg graph     tfwrite graph jpg  not created  writejpeg eager     tfwrite eager jpg  created   ,3baef3b569344f0af6071950a5fc9d828a4ee6a6,tensorflow\core\ops\io_ops.cc
30378,Problems with keras model saving when there's a loss added with add_loss,  system information  system  windows    wsl with ubuntu   lts  tensorflow version        in cpu mode    python version         it also happens in real linux environments    describe the current behavior  i m having many problems when saving loading keras models with custom loss    i ll describe each one of the scenarios below    code to reproduce the issue  inp   tf keras input    tensor   tf keras layers      model   tf keras model   model add loss    model compile   tf keras experimental export saved model     when not using a keras layer as loss  it produces a non valid json     traceback     file   home nguerinjr documents deep coding project teste py   line    in   tf keras experimental export saved model   file   home nguerinjr documents deep coding project venv lib     site packages tensorflow python keras saving saved model py   line    in export saved model   export model json   file   home nguerinjr documents deep coding project venv lib     site packages tensorflow python k,a377701899b71b5f6bb0f157be763c283c7ff7e9,tensorflow\python\keras\distribute\distribute_strategy_test.py
30474,[TF2.0] Bug allowing misuse of the batch dimension of a convolution layer,  tensorflow       rightfully complains about the following minimal example with valueerror  could not broadcast input array from shape   into shape     tensorflow          however happily runs it and prints     import numpy as np  import tensorflow keras backend as k  from tensorflow keras layers import input     lambda  from tensorflow keras models import model    def custom reshape        return k reshape      inputs   input    x   lambda    x         model   model   model compile   print    batch size      result   model predict    batch size batch size   print   as per discussion   this seems to be a bug in tf        ,37fcf0a0e04b2014864936397c25e6c398135772,tensorflow\python\keras\engine\training_test.py
30574,Decode_wav sample rate output cannot be passed to tf.signal.linear_to_mel_weight_matrix,  system information    have i written custom code    combined output of decode wav with the sample in signal mfccs from log mel spectrograms  os platform and distribution    windows    mobile device   if the issue happens on mobile device  no  tensorflow installed from    conda binary  tensorflow version               python version         bazel version    no  gcc compiler version    no  cuda cudnn version   gpu model and memory  surface book nvidia gpu    describe the current behavior  the output of decode wav is tuple of    sample rate is   but linear to mel weight matrix expects a   sample rate   if the sample rate is cast using tf cast  and then a typeerror is thrown with the message   typeerror  using a tf tensor as a python bool is not allowed  use if t is not none  instead of if t  to test if a tensor is defined  and use tensorflow ops such as tf cond to execute subgraphs conditioned on the value of a tensor   describe the expected behavior  sample rate output of decode wav ca,03ff87bfdeec43b9d3a208746ae19ebf9c139c14,tensorflow\python\kernel_tests\signal\mel_ops_test.py
30642,‘scatter_nd_update’ doesn't work with string,    i reproduced this issue in newest tensorflow official docker image       have i written custom code   no  os platform and distribution   ubuntu      tensorflow installed from   binary  tensorflow version          python version             in my model  i need to maintain an extremely long   d variable tensor，which has several columns and many rows  and its dtype is string  in every training step  i need to update only several individual rows of that tensor  tf scatter nd update   meets my requirements perfectly   except that it doesn t work with string in fact  as a contrast   tf scatter nd   does work  since the document doesn t mention that  can t be string  i think it may be a bug   describe the expected behavior  i hope tf scatter nd update support string ref and i really need this feature in my project  so if it can t be fixed quickly  any walk around suggestions   is also welcome   code to reproduce the issue  import tensorflow as tf  ref   tf variable   indices   tf constant,c3e32b03e187fc2854c34add42ee3d1fe1f17628,tensorflow\core\kernels\scatter_nd_op.cc
30685,`TensorArray` objects used as `Dataset.reduce` state lose inferred shapes,  system information    tensorflow version         python version       describe the current behavior  tensorarray objects passed as accumulators to dataset reduce lose inferred shapes  subsequent calls to tensorarray concat returns a fully unknown shape   describe the expected behavior  the element shape of the tensorarray should be partially known  consistent with the behavior of an equivalent tf while loop   code to reproduce the issue   tf function  def compute        arr   tf tensorarray       def body            real logits   tf random normal           arr   arr write   real logits           i               return i  arr      def cond          return i             arr   tf while loop          c   arr concat       tf print  shape    c shape   rank    c shape rank       return c     tf function  def compute ds        arr   tf tensorarray       def body            i  arr   state          real logits   tf random normal           arr   arr write   real logits           i              ,6cd69820a7ec68363647bf918d312b5d10e0e07a,tensorflow\python\data\util\structure_test.py
31596,TFLiteConverter fails with tf.gather when the params argument is a layer attribute,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    os platform and distribution    linux ubuntu       lts  tensorflow installed from    conda  tensorflow version             python version         cuda cudnn version       gpu model and memory    x tesla   pcie      describe the current behavior  i am not able to convert a savedmodel to a flatbuffer using tfliteconverter when the corresponding tf keras model contains a layer with a tf gather op for which the params argument comes from a variable that was initialized in the build method of that said layer   when the params argument is from a locally defined variable  or when using tf nn embedding lookup instead of tf gather  everything works perfectly fine   it also applies to tf gather nd   describe the expected behavior  i expect tf gather to work for the case in which the pa,252e6183523d226e50137c06a101df0aa5d4d5d9,tensorflow\python\keras\saving\metrics_serialization_test.py
31952,[TF 2.0] tf.gather doesn't work alongside @tf.function,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     yes  os platform and distribution     darwin kernel version        mobile device   if the issue happens on mobile device   n a  tensorflow installed from     binary  tensorflow version              python version   python          anaconda  inc   bazel version     n a  gcc compiler version     n a  cuda cudnn version   n a  gpu model and memory   n a    describe the current behavior  it seems that when tf gather  is called after a tf function  the gradient cannot be calculated  the example code blow shows the bug  the code itself raises the following error message     assertionerror  expected all args to be tensors or variables  but got compositetensor    the code will work if we remove the tf function decorator  or  put the tf gather line insid,d5ee347de231b55f8ef7c11402db1673ff111d53,tensorflow\python\eager\backprop_test.py
32029,tensorflow.keras.Model.compute_output_shape gives wrong results,  system information      have i written custom code     yes      os platform and distribution     linux ubuntu          tensorflow installed from     conda      tensorflow version     tried with       and            python version            describe the current behavior  using a keras model   in tensorflow keras i would like to calculate the output shape for a given input  this works correctly only the first time i call mm compute output shape   the subsequent results for calling the same function with different shapes are inconsistent   using standard keras methods i get different and consistent results   an example for the problem is implemented in the tf bug py script that you find in the zip  if you call it without parameters it loads a fully convolutional model  from a json file   and does  import json  import tensorflow keras as keras  with open  as fi         kk json load         mm keras models model from json       for n in range           ss           print         the resu,3ba8bd697faf4b831f78c3fa547d7956f1b1a0aa,tensorflow\python\keras\engine\network.py
32049,Creating a boolean constant prints a deprecation warning,  system information    have i written custom code  yes  os platform and distribution  ubuntu      tensorflow installed from  binary  tensorflow version         python version         describe the current behavior  creating a boolean constant prints a deprecation warning                 deprecation py    from  lib     site packages tensorflow core python framework constant op py     eagertensorbase cpu   is deprecated and will be removed in a future version   instructions for updating   use tf identity instead     describe the expected behavior  no deprecation warning   code to reproduce the issue  import tensorflow as tf  tf zeros    ,8d4ecb0f24c4f9fc18c248838d2496b8410961f6,tensorflow\python\framework\constant_op.py
32162,[lite doc] broken link in`TensorFlow Lite and TensorFlow operator compatibility`,  the link for  on page   compatibility compatible operations    is broken    ,50f0bb045ca2483c516938a867df806d12b6ee49,tensorflow\lite\g3doc\guide\ops_compatibility.md
32487,[TF -2] Multi gpu training error,  i am trying to train a keras model on two           have i written custom code  os platform and distribution    smp debian            tensorflow version             python version         cuda cudnn version       gpu model and memory  tesla      here is the the keras model that i am trying to fit   import tensorflow as tf  import numpy as np    class sparseslice        def   init              super    init             self fc   feature column        def build              self kernel   self add weight   shape    dtype tf           def call            ids   self fc  transform input tensor           return tf expand dims   axis         strategy   tf distribute mirroredstrategy   with strategy scope          batch size          sparse col   tf feature column categorical column with hash bucket       dense col   tf feature column numeric column       example spec   tf feature column make parse example spec         sparse inputs   tf keras layers input   batch size batch size  sparse true,144e0ebb1c9ede81886e215904a4e4598cd8b0b0,tensorflow\core\common_runtime\copy_tensor.cc
32501,Error when using stateful RNN with multiple inputs,  system information    have i written custom code    no  os platform and distribution    windows    mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version           python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version               gpu model and memory  gtx   ti    describe the current behavior  the stock example of rnns with multiple inputs from here   rnns with listdict inputs or nested inputs   produces an error if you set    this seems to be a problem with any multi input rnn with stateful true   describe the expected behavior  there should be no error  multi input rnns with stateful true should work the same as with stateful false     code to reproduce the issue  note  this code is copied from   rnns with listdict inputs or nested inputs    with the exception that i changed the line  rnn   tf keras layers rnn       to  rnn   tf keras layers rnn       import collections    import t,38b748907e04fb212c1183b4999425d768de0233,tensorflow\python\keras\layers\recurrent.py
32543,RNN layer does not reset dropout masks of RNNCell,  system information    have i written custom code    yes  os platform and distribution    macos mojave        tensorflow installed from    binary  tensorflow version                        python version           describe the current behavior  the rnn layer with an rnncell does not reset the states of dropout masks compared to the layer implementations of the cells  thus the behavior of tf keras layers gru     tf keras layers rnn     this is especially problematic  because the keras rnn api tutorial   states both approaches are mathematically equivalent   describe the expected behavior  the rnn layer should check the type of the rnncell and  if it is a subclass of dropoutrnncellmixin  reset the dropout masks after each call  by calling cell reset recurrent dropout mask  and cell reset dropout mask     partially copied from       from   future   import absolute import  division  print function  import numpy as np    import tensorflow as tf  tf enable eager execution       tf enable ea,2eb6dc0f2e7f5455d368c59c35458709eef03a55,tensorflow\python\keras\layers\convolutional_recurrent.py
32570,Assertion error when using mask with unrolled stacked LSTM,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     os platform and distribution    windows    mobile device   if the issue happens on mobile device  n a  tensorflow installed from    source  tensorflow version         python version       bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  no gpu    gb ram    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  i receive an assertion error when creating a forward pass for an unrolled multi layer lstm while using a mask   describe the expected behavior  no assertion error  or at least a better explanation as to the cause   code to reproduce the issue  import tensorflow ,4d011793076f6c62a560085b8fe03fbf732d67e6,tensorflow\python\keras\backend.py
32586,RNN does not forward the training flag to StackedRNNCells,  system information    have i written custom code  yes  os platform and distribution  ubuntu      tensorflow installed from  binary  tensorflow version         python version           describe the current behavior  when using tf keras layers stackedrnncells with tf keras layers rnn  the rnn layer does not forward the training flag to the cell  this is because the rnn code checks that cell explictly defines the training flag as argument  which tf keras layers stackedrnncells does not        tensorflow python keras layers recurrent py        describe the expected behavior  the training flag should be passed to tf keras layers stackedrnncells  and to each stacked cell   code to reproduce the issue  the code below should not raise the assertionerror   import tensorflow as tf    class cellwrapper          def   init              super    init             self cell   cell         property      def state size            return self cell state size         property      def output size      ,df2b252fa380994cd9236cc56b06557bcf12a9d3,tensorflow\python\keras\layers\recurrent.py
32755,"Bincount Op test ""test_negative"" fails with TF 2.0",  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version             python version       bazel version    n a  gcc compiler version    n a  cuda cudnn version       gpu model and memory  rtx   ti       describe the current behavior  the test negative test in tensorflow python kernel tests bincount op test py fails  as the bincount call with negative values does not throw an invalidargumenterror   this behavior might be the result of the op being called on the gpu  as only the cpu call is expected to throw the error  as per the comment here       tensorflow python kernel tests bincount op test py       setting cuda visible devices to be empty forces the op to run on cpu and the test passes    but passing use gpu false as an option to the session wrapper does not have this effect   describe the expected behavior  the test ,9b5f9aabd84b0d1db782286143c560792d711449,tensorflow\python\kernel_tests\BUILD
3277,Broadcast 0-rank tensors when computing gradients for tf.nn.relu,  environment info    operating system  osx    cpu only  version        perhaps this is desired behavior  but i would have much appreciated a more descriptive warning at least  which would have saved much debugging   i haven t found a small reproducible case for this  but in the code i originally found this bug  no error is raised as the other variables are trained  leaving me scratching my head as to why the linear rectified variable was not being trained   the same issue also occurs for tf nn softplus  and perhaps other methods as well   steps to reproduce    import tensorflow as tf  sess   tf session     x   tf variable   y   tf nn relu   loss   y       optimizer   tf train adamoptimizer   train op   optimizer minimize   sess run      sess run                                                                                  invalidargumenterror                      traceback     users andrew anaconda envs tf dev lib     site packages tensorflow python client session py in  do call   ,5d5db35ed2e9e90b95ab27f8b37898fd4543457f,tensorflow\core\framework\numeric_op.h
32923,No documentation for tf.strings.reduce_join,  thank you for submitting a tensorflow documentation issue  per our github  policy  we only address code doc bugs  performance issues  feature requests  and  build installation issues on github   the tensorflow docs are open source  to get involved  read the documentation  contributor guide       url  with the issue       docs python tf strings reduce join    description of issue       add documentation for this method   clear description    correct links    is the link to the source code correct   parameters defined    are all parameters defined and formatted correctly   returns defined    are return values defined   raises listed and defined    are the errors defined  for example        api docs python tf feature column categorical column with vocabulary file raises    usage example    is there a usage example   request visuals  if applicable    are there currently visuals  if not  will it clarify the content   submit a pull request     are you planning to also submit a pull request,de38438ba192145ec7d5a04257e0935b96b2053f,tensorflow\python\ops\string_ops.py
33094,Tensorflow 2.0.0 / tf.keras.layers.TimeDistributed layer can't be save to saved Model,  system information    have i written custom code    no  os platform and distribution   colaboratory    tensorflow version         python version           describe the current behavior  the model defined which has tf keras layers timedistributed layer cannot be save by model save  function   it shows the error below  valueerror                                traceback     usr lib     inspect py in getfullargspec                                               skip bound arg false                                               sigcls signature            except exception as ex       frames  valueerror  no signature found for builtin     the above exception was the direct cause of the following exception     typeerror                                 traceback     usr lib     inspect py in getfullargspec                  else  so to be fully backwards compatible  we catch all                 possible exceptions here  and reraise a typeerror                raise typeerror  from ex          ,66b9b602af695fddb76c113d823d2fa4d0646c04,tensorflow\python\keras\saving\saved_model\saved_model_test.py
33148,Masking LSTM: OP_REQUIRES failed at cudnn_rnn_ops.cc:1498 : Unknown: CUDNN_STATUS_BAD_PARAM,  system information      have i written custom code  yes  os platform and distribution    linux ubuntu       lts  tensorflow installed from    binary  pip  tensorflow version                       python version  python        cuda cudnn version  cuda      cudnn            gpu model and memory  quadro rtx   major    minor    memoryclockrate          describe the problem    it seems there is an issue with the cudnn lstm implementation when using a tf keras layers masking layer   batch size      num tsteps      num features      num units      model   tf keras sequential        tf keras layers inputlayer   batch size batch size        tf keras layers masking         tf keras layers lstm   return sequences true  stateful false        tf keras layers timedistributed         tf keras layers activation            similar to      i receive this error during training and i have strictly right padded data    however  in contrast to this issue  i confirmed that i do not have any inputs containi,4d582a660b4e84fb283eba598127ae40fdd8d1ed,tensorflow\python\keras\layers\gru_v2_test.py
33340,Significant prediction slowdown after model.compile(),  system information    os platform and distribution    windows    tensorflow installed from    pip install tensorflow  tensorflow version         python version       cuda cudnn version  cuda      cudnn        gpu model and memory  gtx        describe the current behavior  the prediction speed is slowed down a lot after model compile  call   describe the expected behavior  speed should not be affected  predict function is used by users assuming that it will work fast because we use it all the time in production  it should not cause surprise to users        tensorflowexperiments blob master test prediction speed after compile ipynb flush cache true         ,42f469be0f3e8c36624f0b01c571e7ed15f75faf,tensorflow\python\keras\engine\training.py
33365,No float64 support with batch normalization in Tensorflow 2.0?,  stock ubuntu     with cuda      tensorflow       installed via    python            i have a   valued dataset with a simple   network that includes tf keras layers batchnormalization  which is where the error is being thrown i think   the first set of issues   warning tensorflow layer   is casting an input tensor from dtype   to the layer s dtype of    which is new behavior in tensorflow     the layer has dtype   because it s dtype defaults to floatx     if you intended to run this layer in    you can safely ignore this warning  if in doubt  this warning is likely only an issue if you are porting a tensorflow   x model to tensorflow       to change all layers to have dtype   by default  call  tf keras backend set floatx    to change just this layer  pass dtype     to the layer constructor        after setting tf keras backend set floatx   next set of errors   traceback       file   home aj ga py   line    in       encoder   make encoder model     file   home aj ga py   line    in mak,2ebc291f6a94163c49fc835d3afc93892e645e45,tensorflow\python\keras\layers\normalization.py
33376,importing tensorflow inside a function/object causes a memory leak,  system information    have i written custom code    yes  os platform and distribution    osx      mobile device   if the issue happens on mobile device  no  tensorflow installed from    pip install tensorflow       tensorflow version           python version         bazel version       gcc compiler version       cuda cudnn version     gpu model and memory       describe the current behavior  when importing tensorflow from a function or object  the import statement somehow keeps a reference to the function and increasing it s reference count  the full import stacktrace is never freed  making it impossible for the object   to be freed from memory   describe the expected behavior  it should be possible to free the function calling import tensorflow  this is not an issue with any other imports     code to reproduce the issue  import gc      class tfimporter       def   init              self  name   name          print         def get tf            print           import tensorflow      ,413d0fa9d75e99e02b74bb079465bea728eb3a44,tensorflow\python\framework\test_util.py
33425,"Tensorflow eager execution not working with tf.math.unsorted_segment_max, Gradient output is null",  system information    have i written custom code    yes  os platform and distribution    windows    professional edition  mobile device   if the issue happens on mobile device   tensorflow installed from    binary  installed using conda  tensorflow version    unknown         python version         cuda cudnn version            gpu model and memory       vram    describe the current behavior  when using tf math unsorted segment max with tensorflow eager execution and gradient tape  the source code   produces following error   traceback       file  c  projects iotmap py segmented max error py   line    in       grads   tape gradient     file  c  programdata   lib site packages tensorflow python eager backprop py   line    in gradient      unconnected gradients unconnected gradients     file  c  programdata   lib site packages tensorflow python eager imperative grad py   line    in imperative grad      compat as str      file  c  programdata   lib site packages tensorflow python eager b,cb9319253d81374e6c9b0dc27c28fe8f5ba2ebb1,tensorflow\python\eager\pywrap_tfe_src.cc
33526,Error while trying to use tf.broadcast_weights,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    linux    mobile device   if the issue happens on mobile device  google colab  tensorflow installed from    binary  tensorflow version         python version    x  bazel version       gcc compiler version       cuda cudnn version     gpu model and memory       you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  unable to import tf broadcast weights in tf       describe the expected behavior  should be able to import tf broadcast weights in tf      code to reproduce the issue  provide a reproducible test case that is the bare minimum necessary to generate the problem   method    p,5dd0b6a589f4f2c66b66a0d022a3d753588bca80,tensorflow\python\keras\metrics.py
33572,[tflite] Support INT8 quantization for PACK with TFLITE_BUILTINS_INT8 OpsSet,  system information    have i written custom code    yes  os platform and distribution    linux ubuntu      tensorflow installed from    binary  tensorflow version         python version           similar to the unpack node issue in       the new tfliteconverter post training quantisation flow  as described in   training quantization full integer quantization of weights and activations    does not support quantization of pack stack operation when only integer operations are requested in the output model  when such conversion is attempted the following error is reported     runtimeerror  quantization not yet supported for op  pack    code to reproduce the issue  for example  the script below   import tensorflow as tf  import numpy as np    def representative dataset gen         np ones        np ones    for   in range      yield             tf graph input  foo   tf compat   placeholder   bar   tf compat   placeholder   out stacked   tf stack     with tf compat   session  as sess    tf ,d8668d9e03b65c4a6d9ecb08e74b0b67798fbbab,tensorflow\lite\tools\optimize\operator_property.cc
33724,Infinite loop with generators wrapping a dataset in tf.function,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    yes  os platform and distribution    ubuntu      mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version                       python version         bazel version     gcc compiler version     cuda cudnn version               gpu model and memory  titan xp       you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  my use case was to use tqdm to track progress on a training loop over a tf data dataset    tf function  def train one epoch        for x in tqdm            train step   however  when the function train one epoch is wrapped in a tf function  the autog,eba45c548371e29cd141c32d367b582b9ca656be,tensorflow\python\autograph\g3doc\reference\common_errors.md
33776,LSTMCell name is ignored in trainable_variables when wrapped in keras.layers.RNN,  i created a model with several lstmcell cells  wrapped in keras layers rnn   when i print trainable variables  cell names are ignored  which results into duplicate variable names    which confuses tensorboard for example   collab notebook to reproduce        environment  tensorflow   release   ,91bdf64a8b3778d70213446418b2a3d2d7a04a68,tensorflow\python\keras\integration_test.py
33888,Bug in saving model in hdf5 format,  system information    have i written custom code    yes  os platform and distribution    macos mojave version        mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version           python version   bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  when try to save the below model in keras format  we get the following error   valueerror  unable to create group    this happens as this model has three layers with name as below   tf op layer pad paddings    tf op layer pad paddings  tf op layer pad  such name causes error in keras as described here   keras team keras      describe the expected behavior  model saving should not fail   code to reproduce the issue  import tensorflow as tf  from tensorflow import keras    x   keras input   dtype      name  input    t   tf shape      to pad    t      y   tf pad   model   keras model   model save  ,0e884391beabe2fadb6398b1fc5f48a9662c333c,tensorflow\python\keras\saving\hdf5_format.py
33974,assert_shapes broken code in documentation,  url  with the issue       docs python tf debugging assert shapes    description of issue       the source code in the example is incorrect   is   tf assert shapes                            should be   tf assert shapes                             note that     is not allowed in python to form   tuples    ,beef1a7bc10883e142813dfbd68da113a94cd6b7,tensorflow\python\ops\check_ops.py
34020,Checkpoint.restore doesn't restore Dataset iterator state when Dataset contains shuffle(),  system information    have i written custom code    yes  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version                       python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  not installed  gpu model and memory  no gpu    describe the current behavior  the code at the end results in the following output                  i tensorflow core platform cpu feature guard cc    your cpu supports instructions that this tensorflow binary was not compiled to use    fma                 i tensorflow core platform profile utils cpu utils cc    cpu frequency    hz                 i tensorflow compiler xla service service cc    xla service   executing computations on platform host  devices                  i tensorflow compiler xla service service cc      streamexecutor device    host  default version  tf tensor   dtype     tf tensor   dt,53d244502fe0de438c939438d00e86f0409b2cb5,tensorflow\core\kernels\data\shuffle_dataset_op.cc
34055,model.reset_states() does not work for bidirectional-RNNs in tf.keras.,  system information    have i written custom code    yes   os platform and distribution    macos        tensorflow installed from    binary  tensorflow version           python version         gpu model and memory  none      describe the current behavior  state handling in rnns with a bidirectional wrapper has changed in tf keras from keras with tf   x   in the old keras with tf   x  using stateful true in a bidi rnn had no effect    i e   all bidi rnn models behaved as if  stateful false   therefore model reset states  did not do anything   in the new tf keras  stateful true in a bidi rnn does have an effect    the fwd rnn is stateful and the bwd rnn is stateful   this is a good change imo    even though stateful bidi rnns are unusual  this is the best way to implement   however  in tf keras  the model reset states  does not do anything for bidi rnn models     describe the expected behavior  for the minimal example script provided below  here is the output   fwd    non stateful      ,7cea9a4edfd181771da97db944e89551b62195ce,tensorflow\python\keras\layers\wrappers.py
34124,[Docs] Doc example of DeviceSpec doesn't work with tf 2.0,  doc link    docs python tf devicespec    import tensorflow as tf  device spec   tf devicespec   with tf device      pass  got error                                                                                keyerror                                  traceback     usr local lib     dist packages tensorflow core python eager context py in   enter              try              new device name  new device spec    device parsing cache cache key            except typeerror     keyerror       during handling of the above exception  another exception occurred     valueerror                                traceback      frames   usr local lib     dist packages tensorflow core python eager context py in   enter                  if not isinstance                   raise valueerror                                         new device name                 device spec   pydev devicespec from string                if old device name     valueerror  expecting a string device name  got        system,5006295cf7a20de9ef9087127569e9d58b28022d,tensorflow\python\framework\device_spec.py
34165,tf.keras.backend.sqrt(tf.constant(-1.0)) is 0 which is misleading and tf.sqrt(tf.constant(-1.0)) is 'nan' which is the way it should be.,  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  no  tensorflow installed from    source  tensorflow version         python version       bazel version    no  gcc compiler version    no  cuda cudnn version  no  gpu model and memory  run in cpu  describe the current behavior  tf keras backend sqrt   returns   as clip by value is being done in the source code   whereas tf sqrt   returns  nan  which is the expected behavior of any sqrt function  this causes some bugs which are very difficult to track     describe the expected behavior  make sqrt functions return only the expected behavior and remove the clip by value   code to reproduce the issue  import tensorflow as tf  tf enable eager execution   tf keras backend sqrt   numpy   tf sqrt   numpy   other info   logs  include any logs or source code that would be helpful to diagnose the problem  if including tracebacks  please ,8134c918424e5f99683617ca5afa8303e5c90642,tensorflow\python\keras\backend.py
34194,tf.size() has no documentation,    docs python tf size    example   returns the number of elements in the tensor  it equals the length of the flattened tensor     ,b70b3e7032cfbbd6d237dc01cdbd1399378b8351,tensorflow\python\ops\array_ops.py
34297,tf.function hangs on ragged tensor input,  system information    have i written custom code     os platform and distribution   no  tensorflow installed from   colab  tensorflow version          python version      gpu model and memory none    describe the current behavior  when using tf function with a number of for loops on a raggedtensor  the function call hangs      when running the function directly    the function executes immediately   when converting the ragged tensor to dense tensor  the function executes immediately   i couldn t pinpoint the exact combination of operations that causes this autograph behavior  but i tried to reduce my code to the simplest combination that still causes this behavior   i struggled for hours with my own code  trying to get tf function to work  until i figured it was due to the ragged tensor   for loops   tf function hanging the kernel  i observed similar behavior on my machine and a colab machine as well   describe the expected behavior  should execute in a comparable time to a dense ten,75af7b4750cd757cd82b32896eb98414e1b38898,tensorflow\python\autograph\operators\control_flow.py
34701,"std::uniform_int_distribution&lt;int8_t&gt; is undefined in the C++17 standard, but TFLite violates this limitation.",  as the c   reference   mentioned  std  uniform int distribution is undefined in the c      therefore microsoft visual c     will give the following build error when the code includes benchmark tflite model cc        in fact  the recent tflite model benchmark couldn t build on windows as my ci environment   shows   bazel build  c opt   verbose failures   tensorflow lite tools benchmark benchmark model  c  program files   microsoft visual studio   enterprise vc tools msvc       include random   error    invalid template argument for uniform int distribution             rand req genl    requires one of short  int  long  long long  unsigned short  unsigned int  unsigned long  or unsigned long long  tensorflow lite tools benchmark benchmark tflite model cc   note  see reference to class template instantiation  std  uniform int distribution  being compiled  c  program files   microsoft visual studio   enterprise vc tools msvc       include random   error    note  char  signed char  unsigne,c16614ef36b990ce9633e25aa00467ea4ce85844,tensorflow\lite\tools\benchmark\benchmark_tflite_model.cc
34789,GRUCell is not compatible with its own initial state,  system information    have i written custom code  yes  os platform and distribution  ubuntu      tensorflow installed from  binary  tensorflow version         python version           describe the current behavior  the initial state returned by tf keras layers grucell get initial state  can not be passed to the first cell call without error  it raises an invalidargumenterror error   describe the expected behavior  rnn cells should accept their own initial states   code to reproduce the issue  import tensorflow as tf  batch size      cell   tf keras layers grucell   initial state   cell get initial state   output  state   cell   initial state   other info   logs  traceback       file  test gru incompat py   line    in       output  state   cell   initial state     file   lib     site packages tensorflow core python keras engine base layer py   line    in   call        outputs   self call     file   lib     site packages tensorflow core python keras layers recurrent py   line    in cal,48b920246f9f06a645a9b864c39171c5b0c2c4ef,tensorflow\python\keras\layers\gru_test.py
34873,Using GPU delegate causes app to crash,  system information    os platform and distribution    windows    tensorflow installed from    binary  tensorflow version     org tensorflow tensorflow lite gpu       nightly     command used to run the converter or code if you re using the python api  model   tf keras models load model   converter   tf lite tfliteconverter from keras model   converter optimizations    tf lite optimize default   converter target spec supported types    tf     tflite model   converter convert   open  write       the output from the converter invocation  it successfully converts the model to tflite        also  please include a link to the saved model or graphdef   link          the model is a simple cnn which takes a   grayscale image and outputs probabilities for   classes   failure details  i want to run the   version of the model using tflite on the gpu   however using the gpu delegate on this model causes the app to crash  i have tested on other devices and this model crashes on all phones when run,f48446d6d301d261f4fa7d73baa80e593e965abf,tensorflow\lite\delegates\gpu\common\model_builder.cc
34975,[TF 2.0 API Docs] `tf.keras.callbacks.LearningRateScheduler` (Very small update),  url  with the issue     please provide a link to the documentation entry  for example   doc link     docs python tf keras callbacks learningratescheduler    code link        tensorflow python keras callbacks py        description of issue       parameters    the next api for scheduler parameter of learningratescheduler takes in   parameters  epoch and lr   instead of just epoch  this is evident in the on epoch begin of the learningratescheduler method   the documentation for this method is still outdated  the docs and the example code still shows the scheduler function takes in only epoch instead of both epoch and lr  i think the doc should be updated to reflect the new api   proposed change to the doc     update the description of scheduler     schedule  a function that takes an epoch index as input   and current learning rate and returns a new learning rate as output          update the example usage to include a scheduler that utilize the current learning rate as well     i hope t,ec684ae119051481f4435ecfa7d0cc7c06eb0fa8,tensorflow\python\keras\callbacks.py
35100,Error occurred when finalizing GeneratorDataset iterator,  system information    os platform and distribution  arch linux            arch  tensorflow installed from  binary  tensorflow version           keras version        tf  python version       gpu model and memory    gtx   ti        describe the current behavior  executing tensorflow s mnist handwriting example produces error   the error dissapears if the code doesn t use onedevicestrategy or mirroredstrategy    w tensorflow core kernels data generator dataset op cc    error occurred when finalizing generatordataset iterator  cancelled  operation was cancelled    code to reproduce the issue  import tensorflow as tf   import tensorflow datasets as tfds   import time      from tensorflow keras optimizers import adam      def build model         filters           units           kernel size           learning rate             model   tf keras sequential           tf keras layers     activation  relu   input shape             tf keras layers             tf keras layers flatten           tf ,b6edd34c5858ab0ab4380da774e7e2fd81a92da0,tensorflow\core\kernels\data\captured_function.cc
35306,A possible bug in ConvRNN2D __call__,  referring to    call       kwargs  initial state     initial state and   kwargs  constants     constants should be added in the else block at    the current situation contradicts with the use of full input at     i am not sure if we can simply replicate the code from its parent rnn call     i went ahead and tried it  but after loading the saved model weights in a complete new python session the results on validation data doesn t match at all   i would appreciate a quick fix for local edit at least   thanks   ,74c9e141067c804bb9a5f94df9342d270cc01f75,tensorflow\python\keras\layers\convolutional_recurrent.py
35335,Dataset scan loses variable modifications,  system information    have i written custom code    yes  providing source   os platform and distribution    mac os        most likely irrelevant   tensorflow installed from    binary from pip  tensorflow version                         python version         cuda cudnn version  using cpu only     describe the current behavior  while writing a unit test i created a function that iterates a tf data dataset and accumulates the values in a local variable  this worked fine using eager mode  but then i noticed that the returned result was zero when using tf function   i ve produced a small simple code that reproduces the problem  in particular  returning the accumulator variable produces a result of    but accessing the variable directly works fine  also  using tf print on the accumulator while iterating the dataset shows the correct value  but printing it after the iteration still within the method shows    suggesting perhaps some kind of scoping problem   please see the attached source t,c4ec9389364cb8d1bff451ab8baf55d25cabdd1f,tensorflow\python\data\kernel_tests\iterator_test.py
35379,Cannot export keras model to SavedModel if mixed-precision policy is enabled,  system information    have i written custom code    no  os platform and distribution    linux debian    tensorflow installed from    binary  tensorflow version    starting from      nightly version tested  python version         describe the current behavior  when keras mixed precision policy     is in use  we can t save the keras model in savedmodel format with method keras models model save without a specific signatures  it seems like a mismatch between input signature inferred by the model itself and the auto casted inputs   valueerror  python inputs incompatible with input signature  inputs     input signature    dtype tf    name none          although we can use graph rewrite as mixed precision training method to bypass this autocasting issue  but graph rewrite is not working in some cases   thus it is not recommended by tensorflow official guide  for flexibility we do hope to use mixed precision policy in mixed precision training  and directly exporting mixed precision trained ,cd6184047e9e497955c473b88387b54818ff23a0,tensorflow\python\keras\mixed_precision\experimental\keras_test.py
35547,“Cloud TPU” console spam on every TensorFlow import,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    glinux    mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version                       python version  python        bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  importing tensorflow prints an unnecessary and unhelpful warning     warning tensorflow falling back to tensorflow client  its recommended to install the cloud tpu client directly with pip install cloud tpu client      describe the expected behavior  importing tensorflow should not print any messages about cloud tpus   this is a normal desktop installation that doesn t have anything to do  with tpus  an,3ff0960fee4927db7baea2f354e60c3328b066fe,tensorflow\python\distribute\cluster_resolver\tpu_cluster_resolver.py
35551,TFLite expermimental_new_converter error with tf.keras Bidirectional Wrapper or attribute go_backwards=True,  system information    os platform and distribution  linux ubuntu       tensorflow installed from     tensorflow version     used to build model         used to run converter              command used to run the converter or code if you re using the python api  import tensorflow as tf  import os  os environ  cuda visible devices             print     model   tf keras models load model   model summary   converter   tf lite tfliteconverter from keras model   converter target spec supported ops    tf lite opsset tflite builtins                                          tf lite opsset select tf ops   converter experimental new converter   true    add this line    tflite model   converter convert       the output from the converter invocation  warning tensorflow falling back to tensorflow client  its recommended to install the cloud tpu client directly with pip install cloud tpu client                            i tensorflow stream executor platform default dso loader cc    successfully ope,e8f6431f53f49f8cab7e15bef24ab2ee775f2ed9,tensorflow\compiler\mlir\lite\ir\tfl_ops.td
35765,Autograph failure with `\`,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code     os platform and distribution    ubuntu      mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version                       python version       bazel version     gcc compiler version     cuda cudnn version   gpu model and memory  nvidia rtx      describe the current behavior  tensorflow shows warning about  failure of autograph  warning tensorflow autograph could not transform   and will run it as is   please report this to the tensorflow team  when filing the bug  set the verbosity to     and attach the full output   cause  expected exactly one node node  found           the warning seems to be caused by the backslash       describe the expected behavior  there should be no such warning  co,8e3adf77b2a148fb2c6fea8fea2d0217bca3339f,tensorflow\python\autograph\pyct\parser.py
35980,Tensorflow predict call crashes when loading a model with gevent enabled,  system information    have i written custom code    yes  os platform and distribution    macos        also tried it on the docker container nvidia cuda       runtime  tensorflow installed from    binary  tensorflow version                       python version        on mac  python          anaconda  inc  in docker  cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  tensorflow crashes after calling predict on the model  this happens with gevent       and also      describe the expected behavior  tensorflow doesn t crash  code to reproduce the issue  from gevent import monkey  monkey patch all     import numpy as np  import tensorflow as tf    classifier   tf keras models load model   classifier predict np array       np zeros            other info   logs  traceback       file   opt conda envs   lib     site packages tensorflow core python keras backend py   line    in eager learning phase scope       graph learning phases  dummy eager graph    value  ,75286a79e7cdf9fdc27b15919f453786eee8936d,tensorflow\python\keras\backend.py
36067,saved_model_cli breaks nightly packages,  our in house nightly builds were broken since       when auditwheel tries to repair my nightly packages  the reason under the hood seems to be an incorrect link from the recent change of adding xla support to  in       install the latest nightly  and navigate to the directory of tensorflow core compiler aot     ldd  pywrap tfcompile so   linux vdso so       libtensorflow framework so       usr local lib     dist packages tensorflow core compiler aot         libtensorflow framework so        pywrap tensorflow internal so    not found   libdl so       lib   linux gnu libdl so       libpthread so       lib   linux gnu libpthread so       libm so       lib   linux gnu libm so       libstdc   so       usr lib   linux gnu libstdc   so       libgcc s so       lib   linux gnu libgcc s so       libc so       lib   linux gnu libc so          ld linux     so       librt so       lib   linux gnu librt so          obviously it links to  pywrap tensorflow internal so but it is not found with the r,0a57a64a022a180abf7a95584a9570e9f126c42e,tensorflow\compiler\aot\BUILD
36198,model.summary() Does not Work in Some Cases,  tf version      having been suggested by  reedwm   i am filing this bug  please see the last comment in issue      for details    cc  reedwm     ,bb2e09ad7207c504296962192fa5f1b7ec53a659,tensorflow\python\keras\tests\model_subclassing_test.py
3624,Basic Element-wise Complex Number Calculations Not Available On GPU,  basic element wise addition  subtraction  multiplication or division for any tensor of type tf   is not implemented on gpu   environment info    operating system  centos                      installed version of cuda and cudnn   cuda     and cudnn         rw r  r      root root   jul        usr local cuda     lib libcudadevrt a  lrwxrwxrwx    root root       jul        usr local cuda     lib libcudart so    libcudart so      lrwxrwxrwx    root root       jul        usr local cuda     lib libcudart so        libcudart so         rwxr xr x    root root   jul        usr local cuda     lib libcudart so         rw r  r      root root   jul        usr local cuda     lib libcudart static a  tensorflow installed from source     commit hash    bazel information   build label                 build target  bazel out local fastbuild bin src main java com google devtools build lib bazel bazelserver deploy jar  build time  fri jul              build timestamp     build timestamp as int       steps,53af29cb8503c7ed55a23d22090dd39ce0056a7a,tensorflow\core\kernels\cwise_op_add.cc
36268,tf.debugging.assert_shapes() does not work for SparseTensor,  system information    have i written custom code    yes  os platform and distribution    mac os x      tensorflow installed from    binary  tensorflow version         python version         describe the current behavior  tf debugging assert shapes cannot be used with sparse tensors   describe the expected behavior  tf debugging assert shapes should allow you to mix and match dense and sparse tensors when checking for dimensional consistency   code to reproduce the issue  import tensorflow as tf  a   tf range   tf debugging assert shapes        works    raises  valueerror  attempt to convert a value   with an unsupported type   to a tensor   tf debugging assert shapes              ,11fc1489d01822a2e728103c3af998976b4e7cd1,RELEASE.md
36394,file_io.get_matching_files indefinitely hangs,  system information    have i written custom code    no  os platform and distribution    gcp cloud shell  mobile device   if the issue happens on mobile device  n a  tensorflow installed from    comes pre installed in  gcp cloud shell  tensorflow version    tf      python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  file io get matching files indefinitely hangs  describe the expected behavior  file io get matching files should not hang   code to reproduce the issue  note the    in the first command  gsutil cp readme cloudshell txt gs    test  bug txt  gsutil cp readme cloudshell txt gs    test bug txt      this creates a weird   folder under the test folder  now open python  from tensorflow python lib io import file io  file io get matching files       this will hang   delete the   folder and this would work fine   one of our training jobs hung because tf somehow created a   fold,7bfbd3f7be0725ee9c220047fe85032cf126d92b,tensorflow\core\platform\cloud\gcs_file_system.cc
36462,Autograph is incompatible with typeguard,  please make sure that this is a bug  as per our github policy  we only address code doc bugs  performance issues  feature requests and build installation issues on github  tag bug template  system information    have i written custom code    yes  os platform and distribution    ubuntu      mobile device   if the issue happens on mobile device  no  tensorflow installed from    binary  tensorflow version           python version         bazel version    no  gcc compiler version    no  cuda cudnn version  no  gpu model and memory  no gpu    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  import tensorflow as tf  from typeguard import typechecked   tf function    typechecked  def add     tf tensor       return a   b    print   tf zeros                                                                                  nameerror                           ,a962580295172539a3a6ae5b02836aac1cabf100,tensorflow\python\autograph\converters\break_statements.py
36624,LSTM return_state=True fail with tf.keras.Sequencial model,    have i written custom code    no  os platform and distribution    macos        tensorflow installed from    binary  tensorflow version                       python version           oct               describe the current behavior  the call method of a tf keras sequential object fails and throws an error when one layer is an instance of the tf keras layers lstm class constructed with return state true  given the error message  i believe it is because the output of the call method of such lstm layer is a list instead of a tensor  and the call method of sequential does not know what to do with a list   describe the expected behavior  i think that the call method of sequential should know that the tensor output of lstm is the first element of the list when return state true   code to reproduce the issue  setting    import tensorflow as tf  import numpy as np    print   format      batch size      ts      input dim      nump   np arange  reshape   dataset   tf data dataset from tensor s,619ca02f2d9ff61aedf7de6e6b43116e859f6913,tensorflow\python\keras\engine\sequential.py
36700,tf2 isn't enabled in tensorflow_core.python.keras.layers.__init__,  system information    have i written custom code    no  os platform and distribution    ubuntu      tensorflow installed from    pip install tensorflow       tensorflow version         python version         cuda cudnn version  not used  gpu model and memory  not used    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  whenever you import a layer using the path   from tensorflow python keras layers  it will import the layer using the tensorflow   x behavior   which isn t the case when we use tensorflow keras layers   the issue is that every networks from tf keras applications   use those import which can lead to some severe bugs     describe the expected behavior  importing from from tensorflow python keras layers and from tensorflow keras layers should have exactly the same behavior     code to reproduce the issue  from tensorflow python keras lay,35a382295ad81f7080d306c9b09b0edaa451fcfc,tensorflow\api_template.__init__.py
37230,tf.function second derivative error,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information      have i written custom code  as opposed to using a stock  example script provided in tensorflow   yes      os platform and distribution  e g    linux ubuntu       macos mojave            mobile device   if  the issue happens on mobile device  n a      tensorflow installed from  source or  binary     tensorflow version   binary                 python version     bazel  version    python            gcc compiler version  if compiling from  source        cuda cudnn version    gpu model and memory       describe the current behavior  i have a custom function need to use slicing in a for loop  the first order derivative is working properly but the second order derivative gives the error  the error only occurs when the function is decorated with tf function  below is a simplifie,cf09044d9e7b232080f95f0f910a6803904df1de,tensorflow\python\eager\pywrap_gradient_exclusions.cc
37480,'third_party/tensorflow/compiler/aot:codegen_test' No such directory found,  thank you for submitting a tensorflow documentation issue  per our github  policy  we only address code doc bugs  performance issues  feature requests  and  build installation issues on github   the tensorflow docs are open source  to get involved  read the documentation  contributor guide       url  with the issue       test cc    please provide a link to the documentation entry  for example        api docs python tf mymethod    description of issue          to update the golden file  flip update golden to true and run the     following      bazel test   test strategy local        third party tensorflow compiler aot codegen test  clear description    line    i wanted to update the golden file  but the given directory is not present in the third party library   for example  why should someone use this method  how is it useful   correct links    is the link to the source code correct   parameters defined    are all parameters defined and formatted correctly   returns defined    are re,282828af67de29d13dd2c69d96413c030b02543c,tensorflow\compiler\aot\codegen_test.cc
37777,It seems that Tensorflow needs a check for the unreasonable parameter `input_dim=0` in the layer `Embedding`.,  system information      have i written custom code     os platform and distribution    windows     linux ubuntu      tensorflow version：      cpu    python version         cuda cudnn version     gpu model and memory       describe the current behavior    when i build the model with an illogical parameter input dim     the layer embedding  tensorflow uses this unreasonably parameter to build and even save the model  the detailed performance in building the model is shown in the following picture。      key insights    to sum up  input dim or output dim     are unreasonable corner cases     tensorflow seems to lack in checking this corner case    this may lead tensorflow users to create and even save a wrong model  which will bring potential risks in the subsequent usage   code to reproduce the issue    import numpy as np  import tensorflow keras layers as l   from tensorflow keras import model  input  import tensorflow  import os    print      kwargs           input dim        you can ,f61175812426009a4c96e51befb2951612990903,tensorflow\python\keras\layers\embeddings.py
37983,Calling next with a default value on an exhausted Dataset iterator raises an OutOfRangeError in graph mode,  system information    have i written custom code  yes  os platform and distribution   windows    tensorflow installed from binary           describe the current behavior  next  is supposed to give the next element in the iterator or the value given as default if the iterator is at the end   however  when using the above construction in a function with  tf function  the default value is not returned and an error   is produced when trying to call next on an iterator that is at the end   when running this code in eager mode  the default value is returned as expected   describe the expected behavior  in graph mode the default value should be returned when at the end of an iterator   standalone code to reproduce the issue  import tensorflow as tf    x   tf convert to tensor   ds   tf data dataset from tensor slices   dsi   iter        tf function   remove this to get the expected behaviour  def func        for   in range            tf print        func       output                        ,95ea3404528afcb1a74dd5f0946ea8d17beda28b,tensorflow\python\autograph\operators\py_builtins.py
3824,Unclear documentation and behavior for sampler in Tensorflow,  for the samplers implemented in tensorflow  e g  tf nn fixed unigram candidate sampler  the behavior is not well defined in the document  for instance  i would expect the labels specified in true classes will be excluded from the sampling pool  and the sampling will be conducted for each batch  but according to my experiments  neither of above is true   consider the following code    import tensorflow as tf  labels matrix   tf reshape      sampled ids         tf nn fixed unigram candidate sampler   true classes   labels matrix   num true       num sampled       unique   true   range max       distortion         unigrams   range      init   tf initialize all variables   with tf session  as sess   sess run   print sess run      the output can be    which actually belongs to the set of true classes    also  the output has the dimension      which basically means that the sampling is only conducted once  not for each batch   can someone help to clarify this    ,71319c58a3436fbd7081e49c52878dd1ba1772b5,tensorflow\core\ops\candidate_sampling_ops.cc
38349,`nan` gradient when `tf.where` is used,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code  as opposed to using a stock  example script provided in tensorflow    yes  os platform and distribution  e g    linux ubuntu       debian gnu linux      mobile device   if  the issue happens on mobile device   tensorflow installed from  source or  binary   binary  tensorflow version                                           python version         bazel version     gcc compiler version     cuda cudnn version    gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  well defined function with tf where has nan gradients at points where tf where inactive branch is undefined   describe the expect,d6c0858665de6036de24991b29d74b182cfcf5ae,tensorflow\python\ops\array_ops.py
38403,model.reset_states() does not work for bidirectional-RNNs in tf.keras,  system information      have i written custom code  as opposed to using a stock  example script provided in tensorflow   yes      os platform and distribution  e g    linux ubuntu       ubuntu     lts      tensorflow installed from  source or  binary   binary      tensorflow version     tf      and  tf nightly                  python version              cuda cudnn version                  gpu model and memory         bug is on both cpu gpu       describe the current behavior  model reset states  does not work for bidirectional  stateful recurrent layers     tf      model reset states  does nothing for stateful bidi rnns   tf nightly  calling model reset states  for stateful bidi rnns causes a crash    this was reported as a bug in tf        model reset states  does nothing for bidi rnns     i thought this was fixed in tf nightly at that time  but has returned in tf       model reset states  for standard rnns changed in tf      has the following behavior     if model is stateful with,60b167181081c14ff88c77ae62049cab8a5ba4c7,tensorflow\python\keras\layers\recurrent.py
38459,Both 'mean' and 'variance' must be None when is_training is True and exponential_avg_factor == 1.0,  system information    have i written custom code  as opposed to using a stock  example script provided in tensorflow   yes  os platform and distribution  e g    linux ubuntu       ubuntu      mobile device   if  the issue happens on mobile device   no  tensorflow installed from  source or  binary     tensorflow version             python version         bazel  version     gcc compiler version  if compiling from  source    cuda cudnn version       gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf          tf        describe the current behavior  when instantiating a batch norm layer like this   tf keras layers batchnormalization   i get the error   both  mean  and  variance  must be none when is training is true and exponential avg factor         describe the expected behavior  it is not always the expected behavior  consider meta learning for example  we are going to see ju,3cfba9571bcc4be237bfdfa3498c66073ae59280,tensorflow\python\ops\nn_impl.py
38516,Cannot use set_visible_devices with mixed_precision,  system information    have i written custom code    kind of  combination of   example scripts  os platform and distribution    linux   fedora    mobile device   if the issue happens on mobile device   tensorflow installed from  source or  binary   source  tensorflow version                         python version         bazel version           gcc compiler version    gcc              cuda cudnn version  cuda      cudnn          gpu model and memory    geforce rtx   ti      describe the current behavior  when attempting to use tf config set visible devices  in conjunction with tf python keras mixed precision experimental policy set policy   the tensorflow errors with   runtimeerror  tensorflow device   is being mapped to multiple cuda devices    which is not supported  this may be the result of providing different gpu configurations   when creating multiple sessions in the same process  this is not  currently supported  see         describe the expected behavior  no error  standalone ,2730e4b0bcba80799ddc10f52081927848540f30,tensorflow\python\keras\mixed_precision\experimental\device_compatibility_check.py
38640,"K.cast_to_floatx() will convert ""None"" to ""Nan"" and lead the ReLU to Nan output.",  system information    have i written custom code     os platform and distribution      linux ubuntu      tensorflow backend     yes  tensorflow version         cpu  python version          cuda cudnn version      gpu model and memory        describe the current behavior  i found that if i used relu  in tensorflow keras  without any errors or warnings  tensorflow will return a matrix with nan        for this reason  i did some investigations and found that when the parameters in relu are passed to  tensorflow python keras layers advanced activations py line near line    k cast to floatx   will incorrectly convert the  none  parameter to  nan  and pass it to the backend for calculation      nan  and  none  should have different meanings  but k cast to floatx did not distinguish between  nan  and  none  during the calculation  which led to the usage of a  nan  parameter in the tensorflow calculation  this further affects the final output result and makes the output with   nan   this ope,3db8df8ffafe5bcd83a12b92bc4c8287cd80237f,tensorflow\python\keras\layers\advanced_activations.py
38906,MemoryOptimizer produces broken graph with AlreadyExistsError exception while running GRU layer on Tensorflow 2.2.0rc_3,  system information    custom model built using keras  macbook pro    core intel core    macos catalina        tensorflow installed from pip in virtual environment  tensorflow                      python        running on cpu    describe the current behavior  the code snippet listed below outputs multiple tensorflow core framework op kernel cc    op requires failed at variable ops cc     already exists  resource warnings and finally exists with tensorflow python framework errors impl alreadyexistserror exception   note  the code works correctly if the gru layer size is decreased from   to    it also works if tensorflow is downgraded to version         the issue is related to      issue reported in    this issue offers code to reproduce it and occurs on the latest version of tensorflow   describe the expected behavior  the code should work without exception   standalone code to reproduce the issue  import numpy as np    from tensorflow keras models import sequential  from tensorflow ke,80a93674eafc224a45cbe96c65e993e9735634a3,tensorflow\core\kernels\variable_ops.cc
38932,New TFLiteConverter not working with tf.complex64,  system information    os platform and distribution    colab cpu  tensorflow version               describe the current behavior  new tfliteconverter not working with tf    disabling the new converter   works   standalone code to reproduce the issue   tf function  def foo        return x   y      x   tf constant   y   tf constant     foo concrete   foo get concrete function   converter   tf lite tfliteconverter from concrete functions   foo tflite   converter convert   colab example    thanks   ,85c637969a25228065a276044691dab020984361,tensorflow\compiler\mlir\lite\python\tf_tfl_flatbuffer_helpers.cc
39075,ForwardAccumulator fails with `experimental_run_functions_eagerly(True)`,  system information    have i written custom code    no  os platform and distribution    macos catalina  tensorflow installed from    binary  tensorflow version            python version         bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a      running the examples in tf forwardaccumulator docs   fail with  when running with      running the examples in tf forwardaccumulator docs   with  work the same way as when running with      this is the standard example from   docs python tf autodiff forwardaccumulator    with just the  call added   import tensorflow as tf    tf config experimental run functions eagerly       v   tf variable   with tf autodiff forwardaccumulator       v         the  vector  in hessian vector product       tf constant   as acc     with tf gradienttape  as tape       y   tf reduce sum     backward   tape gradient   backward    gradient from backprop    acc jvp     forward over backward hessian vector product,3e6697b916c9e775dc61375b913d21ba9d22126f,tensorflow\python\eager\forwardprop.py
39186,tf.data.experimental.make_csv_dataset modifies mutable variables passed to it,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    tensorflow installed from    docker  tensorflow version    tf          describe the current behavior  tf data experimental make csv dataset modifies passed variables in place  so if you call    the variable  is changed  it s in line          tensorflow python data experimental ops readers py     but it may happen to other variables passed  specifically the list sent to  is replaced by a list of the indices of those columns in the file to read   describe the expected behavior  a function should never modify the mutable objects passed to it  this is only ever appropriate for methods of a class    ,79acb0824b8bbb1fb887d2ff625f2f170d80fe1f,tensorflow\python\data\experimental\kernel_tests\csv_dataset_test.py
39222,Typos in source code docs,  here   a minor typo in the source code          ,fe972004ab02ff454749bea5780e70d4a4633c3a,tensorflow\python\eager\backprop.py
39462,ReduceLROnPlateau keeps executing lr reduction block of code after min_lr has been reached,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    windows  mobile device   if the issue happens on mobile device  none  tensorflow installed from    through pip  tensorflow version         python version       bazel version    n a  gcc compiler version    n a  cuda cudnn version  n a  gpu model and memory  n a    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  reducelr will execute the part of the code that reduces lr even if lr  equals  min lr  while this is not a problem if we are just dealing with lr since the value will technically ,b3461c38fd09abc6f31f69ade9bc632fabbdb73a,tensorflow\python\keras\callbacks.py
39649,tf.math.reduce_mean takes too long and produces wrong result when input_tensor is uint32/64 and axis is array,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    yes  os platform and distribution    ubuntu      macos        mobile device   if the issue happens on mobile device  na  tensorflow installed from    binary  tensorflow version                                          python version        bazel version   na  gcc compiler version   na  cuda cudnn version na  gpu model and memory na    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  tf math reduce mean hangs  or takes forever to compute for certain input    around when the slow down occurs  the function produces incorrect r,e972c5572634efd188696038e9241b75cdcd69bc,tensorflow\core\framework\register_types.h
39718,"TF Lite nightly: Model with Fully Connected layer can't be converted, fully quantization, int8",  system information    os platform and distribution    linux  tensorflow installed from    tf nightly  tensorflow version    tf nightly    command used to run the converter or code if you re using the python api  if possible  please share a link to colab jupyter any notebook      usp sharing    import numpy as np  import tensorflow as tf    mnist   tf keras datasets mnist  train data  test data   mnist load data     pre process   lambda x  x        num calib      calib data   pre process               train data        num calib  astype                model   tf keras sequential                                  tf keras layers inputlayer                     tf keras layers reshape                     tf keras layers                         filters    kernel size    activation tf nn relu                                      tf keras layers                       tf keras layers flatten                    tf keras layers dense                              model summary     train images  ,268a0ea1502532e0e127e71d0ad42cc4e8ad81c6,tensorflow\lite\tools\optimize\quantize_model.cc
39756,problem running visualize.py at import flatbuffersn,  system information    have i written custom code    no  os platform and distribution     mac os        mobile device   if the issue happens on mobile device  n a  tensorflow installed from    from source  tensorflow version                     python version         bazel version           gcc compiler version    apple clang version            cuda cudnn version  n a  gpu model and memory  n a    describe the current behavior  visualize py script fails with this error   importerror  cannot import name  flatbuffersn  from  flatbuffers python     i am running it with the command    from the top of my tensorflow source directory   the build phase seems to work fine  then the import error seems to happen on running visualize py   full output in attached file   vis dump txt    the target tflite file can be downloaded here    but i don t think the tflite file ever gets loaded  so i doubt that the specific file is relevant   the directory where it is trying to import from is listed here   t,f9fb66cdb7419d2eadf7faea995f1aacb032104b,third_party\flatbuffers\build_defs.bzl
39976,RandomContrast Layer - confusing __init__ error message,  system information    have i written custom code    yes  os platform and distribution    colab  tensorflow installed from    colab  tensorflow version           python version           describe the current behavior  when instantiating a randomcontrast layer object with a value        for the factor parameter  a valueerror is raised with the following error message which is somewhat confusing in this scenario   factor cannot have negative values  got     describe the expected behavior  a valueerror should be raised with a more appropriate error message  something like   factor cannot be greater than    got     standalone code to reproduce the issue  random contrast layer   tf keras layers experimental preprocessing randomcontrast    ,25213f58c433d3712931b4071d2498bb67f8c2ca,tensorflow\python\keras\layers\preprocessing\image_preprocessing.py
40050,Unclear shape dependency of `value` in  `tf.keras.backend.moving_average_update` documentation,  url  with the issue       docs python tf keras backend moving average update    description of issue       clear description    unclear shape dependency of input value  according to the document  value should have the same shape as variable  but it is unclear what is variable   parameters defined    yes  returns defined    yes  raises listed and defined    no   ,cd3ed4fe8cfa83418ff5937cb52560013667cba4,tensorflow\python\keras\backend.py
40103,Unclear type/dimension dependency of `filters` in  `conv1d/3d_transpose` documentation,  url  with the issue       docs python tf nn        docs python tf nn      description of issue       clear description    unclear type and dimension dependency of input filters  according to the document  filters should have the same type as value and the in channel dimension must match that of value  but it is unclear what value is   parameters defined    yes  returns defined    yes  raises listed and defined      docs python tf nn      yes    docs python tf nn       no  the  raises  list is not provided or defined  system information      have i written custom code    no  os platform and distribution    macos mojave      tensorflow installed from    binary  tensorflow version             python version            ,cef0c8cf713a718c01dd3da582c94d4e09a54753,tensorflow\python\ops\nn_ops.py
40328,Subclassed model with ConvLSTM2D layer can't be saved as a SavedModel in TF2.2,  system information    have i written custom code    custom code  extended an example from tf guides  os platform and distribution    ubuntu     mac os      mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version        and      python version       bazel version     gcc compiler version     cuda cudnn version       gpu model and memory         describe the current behavior  as a header states the model build with subclassing api with   layer inside can t be saved as a savedmodel  given that keras   model format doesn t support saving subclassed models i am left with no option to save the model architecture to file   the issues appears in     while there seems to be no bug in earlier version      describe the expected behavior  the code is to work without issues in both     and      standalone code to reproduce the issue  provide a reproducible test case that is the bare minimum necessary to generate  the problem  if possible  pl,47582983cb1064b5bb81233db4f0adeeaa10b74d,tensorflow\python\keras\saving\saved_model\layer_serialization.py
40839,"tf.io.decode_image(img, channels=3) outputs 4 channels when reading 4-channel BMP",   attached a sample bmp file    zip    system information    have i written custom code    no  os platform and distribution  windows    tensorflow installed from    binary    tensorflow version         python version           describe the current behavior  when reading in a   channel bmp     tf io decode image  gives shape   instead of    tf io decode bmp  gives the following error    traceback       file  channels py   line    in       loop     file  channels py   line    in loop      img   tf io decode bmp     file  c  users mattchee   lib site packages tensorflow python ops gen image ops py   line    in decode bmp       ops raise from not ok status     file  c  users mattchee   lib site packages tensorflow python framework ops py   line    in raise from not ok status           six raise from   none     file     line    in raise from  tensorflow python framework errors impl invalidargumenterror  channels attribute   does not match bits per pixel from file    op decodebmp       i m ,0859ec0386ffa55739cbe831f38942c53027c12f,tensorflow\core\BUILD
4084,Process hanging when using TF_SessionRun with multiple times the same input,  it seems that if the same input appears multiple times in the inputs argument of tf sessionrun   then the tf sessionrun call never returns   this issue can be reproduced by modifying c api test cc and replacing the line   csession setinputs       with   csession setinputs     feed          according to gdb  the process is waiting for a mutex in the runstate destructor from directsession   what related github issues or stackoverflow threads have you found by searching the web for your problem     none  environment info    operating system  linux      installed version of cuda and cudnn  none  if installed from source  provide    the commit hash       the output of bazel version  build label         build target  bazel out local fastbuild bin src main java com google devtools build lib bazel bazelserver deploy jar  build time  fri jul              build timestamp     build timestamp as int       if possible  provide a minimal reproducible example      see above   what other attempted s,c35e69c56941d79163dc9f054f57c199b1a4cc44,tensorflow\core\common_runtime\direct_session_test.cc
40895,nested gradients for convolution layer fail under tf.function,  system information    have i written custom code     os platform and distribution    ubuntu      mobile device   if the issue happens on mobile device  no  tensorflow installed from    pip tf nightly  tensorflow version                       python version        bazel version       gcc compiler version       cuda cudnn version       gpu model and memory  geforce gtx   ti with max q   gb    describe the current behavior  the code below works in eager mode  but fails with the following error if using tf function  traceback       file  bugreport py   line    in       value   func     file   home abdo tmp venv lib     site packages tensorflow python eager def function py   line    in   call        result   self  call     file   home abdo tmp venv lib     site packages tensorflow python eager def function py   line    in  call      self  initialize     file   home abdo tmp venv lib     site packages tensorflow python eager def function py   line    in  initialize       args    kwds      ,3d1f1b062dafc5cea4561d0a48538c60aef5aa5e,tensorflow\python\eager\backprop_test.py
41270,"GPU delegate library is libtensorflowlite_gpu_delegate.so, not libtensorflowlite_gpu_gl.so.",  thank you for submitting a tensorflow documentation issue  per our github  policy  we only address code doc bugs  performance issues  feature requests  and  build installation issues on github   the tensorflow docs are open source  to get involved  read the documentation  contributor guide       url  with the issue     please provide a link to the documentation entry  for example        api docs python tf mymethod      advanced    description of issue       bazel build  c opt   config   tensorflow lite delegates gpu gl delegate                    for static library  bazel build  c opt   config   tensorflow lite delegates gpu libtensorflowlite gpu gl so    for dynamic library      should be changed to   bazel build  c opt   config   tensorflow lite delegates gpu delegate                    for static library  bazel build  c opt   config   tensorflow lite delegates gpu libtensorflowlite gpu delegate so    for dynamic library      because gl delegate is not gpu delegate runtime library ,052d45f39a62f4684801ca29ad6b6a593ce4a8fa,tensorflow\lite\g3doc\performance\gpu_advanced.md
4131,reduce_max and maximum give different results for negative infinity,  using tf maximum with negative inf inputs as follows   tf maximum  eval       gives the expected result  inf  however  tf reduce max  on the same inputs   tf reduce max  eval       gives         which is the min     for positive infinity inputs  both functions result in inf   what related github issues or stackoverflow threads have you found by searching the web for your problem     i posted this as an so question first      bug in tensorflow reduce max for negative infinity    environment info    operating system  ubuntu      installed version of cuda and cudnn    rw r  r     root root     may        usr local cuda       libcudadevrt a  lrwxrwxrwx   root root         may        usr local cuda       libcudart so    libcudart so      lrwxrwxrwx   root root         may        usr local cuda       libcudart so        libcudart so         rw r  r     root root     may        usr local cuda       libcudart so         rw r  r     root root     may        usr local cuda       libcudart stat,fadc1f3c869c15b1890221bb6dfb0f7bd0f7c23d,tensorflow\core\kernels\constant_op_gpu.cu.cc
41674,TensorFlow Lite for Microcontrollers sigaborts with a MobileNetV2 alpha=0.1 model,  system information    have i written custom code    yes  os platform and distribution    macos        linux  gcc compiler version    apple clang version         c       describe the current behavior  i am using tensorflow lite for microcontrollers at commit       i ve trained two   models in keras with   input size and a single input channel  then converted to   quantized   i am attempting to run both models using tensorflow lite for microcontrollers on    built with clang on macos and with gcc on ubuntu   the first model has a   filter scaling factor   of      this model runs perfectly   the second model has a scaling factor of      this model sigaborts during the invoke  call   strangely  both models run perfectly when executed from the openmv       and the smaller model runs perfectly on the    it might be worth noting that on the openmv devices the model is stored in dynamic memory  that said  i ve tried declaring the model without static on   and it has no impact   i ve attached,dd918be82cb9702cc9ca022179629fbd8c6d3ed9,tensorflow\lite\kernels\internal\reference\integer_ops\add.h
41712,"Conv1DTranspose Dilation support - Might be a bug, IDK.",  thank you for submitting a tensorflow documentation issue  per our github  policy  we only address code doc bugs  performance issues  feature requests  and  build installation issues on github   the tensorflow docs are open source  to get involved  read the documentation  contributor guide       url  with the issue     please provide a link to the documentation entry  for example     docs python tf keras layers      description of issue           dilation   does not inform uses that dilation doesn t work for any value  of dilation   because it isn t implemented yet   clear description    currently documentation says    an integer  specifying the dilation rate to use for dilated convolution  currently  specifying a dilation rate value      is incompatible with specifying a stride value         this may not be implemented yet in the newest of nightly build  but with my tf nightly        build this didn t work  i fear updating to new nightly builds in case in breaks my research code whi,75801da4cd321aabbf79e78da1e5de1a10ba4c2a,tensorflow\python\keras\layers\convolutional.py
42217,Typo in TFLite CoreML framework build command example,  system information    have i written custom code     os platform and distribution     mobile device   if the issue happens on mobile device   tensorflow installed from     tensorflow version     python version   bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     describe the current behavior  there is a single typo in build command example         tensorflow tensorflow lite experimental ios build apple              lines   to          in                          bazel build  c opt   config ios fat   tensorflow lite experimental ios tensorflowliteccoreml framework          tflite ios static framework               name    tensorflowliteccoreml framework                hdrs                        coreml delegate h                                allowlist symbols file     allowlist tensorflowliteccoreml txt                bundle name    tensorflowliteccoreml                minimum os version   tfl minimum os version               deps               ,916bd91c2bc1ded18ca460520e922a71c3033418,tensorflow\lite\experimental\ios\BUILD.apple
42281,tf.nn.ctc_beam_search_decoder crashes(bad_alloc) when top_paths is large,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version          python version        bazel version   n a  gcc compiler version   n a  cuda cudnn version n a  gpu model and memory n a    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  tf nn ctc beam search decoder crashes  when top paths is extremely large  describe the expected behavior  expect no crashes  standalone code to reproduce the issue  provide a rep,7bd42cf6ba061ba7c06c072c9d962abe331461eb,tensorflow\core\framework\node_def_util.cc
42331,tf.nest.flatten crashes (abort) when expand_composites's constraints are violated,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    linux ubuntu      mobile device   if the issue happens on mobile device  n a  tensorflow installed from    binary  tensorflow version          python version        bazel version   n a  gcc compiler version   n a  cuda cudnn version n a  gpu model and memory n a    you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  tf nest flatten crashes   when expand composites is   boolean is violated   describe the expected behavior  expect no crash  standalone code to reproduce the issue  provide a rep,35c2a97ddca6da7d5a21d5ee3e2869eec68299f9,tensorflow\python\util\nest.py
42364,`tf.data.experimental.snapshot()` hangs when using GCS paths,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    yes  os platform and distribution    debian     stretch  mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version         python version         bazel version    na  gcc compiler version    na  cuda cudnn version  cuda      gpu model and memory  nvidia tesla      you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  tf data experimental snapshot  hangs when using a google storage path   describe the expected behavior  tf data experimental snapshot  works   standalone code to,532966cad34471bded2b0483737e8e8d23bc4720,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc
42386,GradientTape.gradient needs to check target type.,        tensorflow tensorflow python eager backprop py               line          in                              target  a list or nested structure of tensors or variables to be             recently  i wrote some code below which is very simple   tape gradient   but it raises typeerror  cannot convert value none to a tensorflow dtype   it turns out my custom loss function is returning none  yes  i know i was dumb   i had to check all of the related tensorflow code lines to find this dumb problem   so  i suggest the type checking block inside of this code   i think this suggestion can be helpful for fools like me   thanks in advance      ,f25125e50bab365642335413356466883bf7f361,tensorflow\python\eager\backprop.py
42458,CosineSimilarity documentation range incorrect,  url  with the issue       docs python tf keras losses cosinesimilarity    description of issue       clear description     note that it is a negative quantity between    and    should be changed to  note that it is a negative quantity between    and     correct links    parameters defined    returns defined    raises listed and defined    usage example    request visuals  if applicable    submit a pull request     don t plan to submit pull request   ,7e7641d95c6c9b7e46b129c10ec7a965fb2f848d,tensorflow\python\keras\losses.py
43449,tf.autodiff.ForwardAccumulator fails for Embedding layer,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    windows    mobile device   if the issue happens on mobile device   tensorflow installed from    pip  tensorflow version             python version         bazel version     gcc compiler version     cuda cudnn version               gpu model and memory   geforce gtx        you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  calculating the jacobian vector product of an embedding layer produces  attributeerror   indexedslices  object has no attribute   as tf output   describe the expected beha,79fce11a6cc8c3d9fd85e9a04b596fd4ea4d7b79,tensorflow\python\eager\forwardprop_test.py
43529,'tf.TensorScatterUpdate' Conversion to tflite,  system information    os platform and distribution     windows    tensorflow installed from     python binary  tensorflow version            provide the text output from tflite convert  error   tf tensorscatterupdate  op is neither a custom op nor a flex op       copy and paste here      standalone code to reproduce the issue  provide a reproducible test case that is the bare minimum necessary to generate  the problem  if possible  please share a link to colab jupyter any notebook   also  please include a link to a graphdef or the model if possible   any other info   logs  include any logs or source code that would be helpful to diagnose the problem   if including tracebacks  please include the full traceback  large logs and files  should be attached    ,8dbab4f830b2f77199c22e7c1763bea8e523079b,tensorflow\lite\delegates\flex\allowlisted_flex_ops.cc
4358,Wrong example script in the docs for preprocessing data,  i was reading the docs for preprocessing    which is a small paragraph linking to the cifar   network   as an example  however  that script does not perform any preprocessing  do we have a better example illustrating preprocessing steps like data normalization  distorting images  etc    ,345132fb42c05807206e892a3cb497c0bcd58af1,tensorflow\g3doc\how_tos\reading_data\index.md
4361,Update tf.contrib.layers.batch_norm() docs,  tensorflow version that i use              i took heavy use of tf contrib layers batch norm  the last weeks   after facing some problems on how to use it correctly  i figured out that there are many devs out there who are confused as well  such as here            how could i use batch normalization in tensorflow    i would suggest to do following improvements to make it more clear      update example in doc string   the example tells in case we use update collections on its defaults  we have to include this   update ops   tf get collection   if update ops       updates   tf group       total loss   control flow ops with dependencies       but this is actually not working or deprecated  as it throws errors  instead  we have to do some tiny changes  i would suggest to update the docs as follows   from tensorflow python import control flow ops    update ops   tf get collection   if update ops       updates   tf tuple       total loss   control flow ops with dependencies       as a side ,a6e6d0aa2cad5e1d50b4f5cbed427a5df9267098,tensorflow\contrib\layers\python\layers\layers.py
43781,[MLIR] TFlite contains unused file,  tensorflow compiler mlir lite transforms load quantization recipe cc is called via  tf opt  allow unregistered dialect  tfl load recipe  s   filecheck  s  however  according to this commit      allow unregistered dialect  is disabled     hence  in  can be removed as it is not involved in any mlir  tfl passes    ,1831af3c012643a5d61012bf71b653e075dcfd22,tensorflow\compiler\mlir\lite\BUILD
43816,"TypeError in ""Bidirectional.compute_output_shape""",  hello   while analyzing tensorflow on sonarcloud i saw what looks like an error in tensorflow python keras layers wrappers py         you can see the issue in sonarcloud here     after reviewing the code myself i can indeed see that state shape will have the type tuple  and using the operator   on a list and a tuple will fail with the exception     typeerror  can only concatenate list   to list    here is the flow which leads to this issue   output shape   tuple      output shape is a tuple       if self return state         state shape   output shape        state shape is a tuple       return  output shape    state shape   copy copy     the expression   output shape    state shape  will fail      in case you have any question  suggestion or if you see a false positive on sonarcloud you can reach out on sonarsource community forum     a few notes in case you want to use sonarcloud     i am currently testing the python analyzer so the project on sonarcloud will only show python issues,5a8b0d3e80cef5ffba362d747a0f449c90862b5d,tensorflow\python\keras\layers\wrappers.py
43998,Update BatchNormalization documentation,  thank you for submitting a tensorflow documentation issue  per our github  policy  we only address code doc bugs  performance issues  feature requests  and  build installation issues on github   the tensorflow docs are open source  to get involved  read the documentation  contributor guide       url  with the issue       docs python tf keras layers batchnormalization    description of issue       clear description    the document should mention that the axis can take in a list of integers  not just an integer  i tested it and it is already implemented in the tensorflow  i was not aware of it and used reshape and transpose  which is inefficient   correct links    is the link to the source code correct   parameters defined    are all parameters defined and formatted correctly   the axis can be a list of integers   returns defined    are return values defined   raises listed and defined    are the errors defined  for example        api docs python tf feature column categorical column wi,9318f787e6d33adabc5c1b18c6663d1432f646f9,tensorflow\python\keras\layers\normalization.py
44011,ops defined inside tf.while_loop's cond/body or tf.cond's true_fn/false_fn functions ignore their enclosed tf.device if the tf.while_loop/tf.cond itself is inside a tf.device,  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution    macos catalina  mobile device   if the issue happens on mobile device   tensorflow installed from    binary  tensorflow version                       python version  python        bazel version     gcc compiler version     cuda cudnn version   gpu model and memory     you can collect some of this information using our environment capture  script    you can also obtain the tensorflow version with     tf      python  c  import tensorflow as tf  print    tf      python  c  import tensorflow as tf  print      describe the current behavior  tf print doesn t print on specified device   if it is inside a for loop in graph mode   initially i though it s an autograph issue  but if you trace the graph and inspect it i,1ce28a3f8a7670eda543176c7fe3a78b5db11a1e,tensorflow\core\common_runtime\function_test.cc
44278,Unexpected `snapshot` behaviour with `flat_map` in tf-nightly,  system information    have i written custom code    yes  os platform and distribution    ubuntu      tensorflow installed from    binary    tensorflow version               python version           describe the current behavior  a dataset formed by flat mapping multiple snapshotted datasets which have each been iterated over individually   results in a dataset which seemingly does not use those files on disk  this is different to the behaviour of cache in tf     and tf nightly and snapshot in tf      describe the expected behavior  snapshot to work equivalently in     as tf nightly  and similarly to cache     colab here     import os  from tempfile import temporarydirectory    import numpy as np  import tensorflow as tf      def as numpy        return np array  for x in ds        def get data       num repeats         snap false       preprocess early false       preprocess late false       del rng false                    get numpy results from a data pipeline         the pipeline l,cbc7f31b7d6aac81158be084201d3b3e8e346907,tensorflow\core\kernels\data\experimental\snapshot_dataset_op.cc
44646,Bug when a custom tf.keras.models.Model has multiple class inheritance,  system information    have i written custom code    yes  os platform and distribution    windows and linux ubuntu      tensorflow installed from    pip  tensorflow version         python version  conda env with python        cuda cudnn version       gpu model and memory  geforce rtx   super with max q design      describe the current behavior  creating a custom model that inherit of at least one other class than tf keras models model  the following exception is raised   file  c  users snake   envs transformers lib site packages tensorflow python training tracking base py   line    in  method wrapper      result   method     file  c  users snake   envs transformers lib site packages tensorflow python keras engine training py   line    in   init        inject functional model class     file  c  users snake   envs transformers lib site packages tensorflow python keras engine training py   line    in inject functional model class      cls   bases     tuple     file  c  users snake   envs,8f68aad1107df679843da96a990773e9fc30201c,tensorflow\python\keras\engine\functional_test.py
44983,tf.image.per_image_standardization unexpected behavior with unsigned integer input,  system information    have i written custom code    yes  os platform and distribution    linux ubuntu        tensorflow installed from    pip  tensorflow version                     python version         cuda cudnn version         gpu model and memory  nvidia tesla   dgxs       describe the current behavior  tf image per image standardization has no special handling for unsigned integers but still converts back to unsigned before returning  thus uint inputs get saturated at zero   and at their maximum value  currently no explanation of uint behavior is given in the docstring  and no warning or other indication that behavior might be unexpected is shown   describe the expected behavior  in previous versions of tensorflow    per image standardization just returned a float rather than converting back to the original datatype  so it didn t have this problem  i would expect something safe like that  leaving it up to the developer to decide how they want this conversion handled  or at lea,b6be9714e878a7dd0d1405bd7a83e021ba4b561a,tensorflow\python\ops\image_ops_impl.py
45054,"Concatenating sparse keras input layers results in None shape, preventing use in Dense layer",  system information    have i written custom code    yes  os platform and distribution    colab  mobile device   if the issue happens on mobile device  no  tensorflow installed from    colab  tensorflow version         python version         bazel version    none  gcc compiler version    none  cuda cudnn version  none  gpu model and memory  none    describe the current behavior  using the keras functional api  when concatenating together multiple instances of tensorflow keras layers input with sparse true  the resulting sparsetensor has a none shape    feeding the merged layer with shape none into a dense layer results in an error   valueerror  the last dimension of the inputs to  dense  should be defined  found  none        describe the expected behavior  concatenating non sparse instances of tensorflow keras layers input   results in a tensor which has a defined shape   and can be fed into a dense layer successfully  additionally a single sparse input layer has a shape and can be fe,8438d59abbd7a2c3d1c48bfd91b118aae53bbb14,tensorflow\python\keras\layers\BUILD
45113,Documentation for using a private CocoaPod spec for TensorFlow Lite is not correct,  url  with the issue       ios using local tensorflow lite core    description of issue       clear description    the documentation linked above says to change the following line in the tensorflowlitec podspec file     s source            http     file            it then asks you to follow the instructions on creating a private cocoapod repo   to use this pod in your project   that guide asks you to run the following   pod repo push tfliteswift tensorflowlitec podspec      but when you do this  you get an error message about the podspec line       error installing tensorflowlitec      tensorflowlitec          error    ios  unknown  encountered an unknown error       usr local   bin curl  f  l  o  var folders     t       file zip file    tensorflow bazel bin tensorflow lite experimental ios tensorflowlitec framework zip   create dirs   netrc optional   retry    a  cocoapods       cocoapods downloader           curl    url using bad illegal format or missing url      i don t believe th,cfe685d8127222d01125893909be092b4dd7ae89,tensorflow\lite\g3doc\guide\build_ios.md
45195,//tensorflow/lite/python:lite_v2_test failed,  with current master branch  commit       test   tensorflow lite python   failed     bazelisk test   tensorflow lite python        error   mirror tensorflow tensorflow lite python build      in deps attribute of py test rule   tensorflow lite python       tenso  rflow lite kernels hashtable hashtable op kernels  does not have mandatory providers   py  or  pyinfo   error  analysis of target    tensorflow lite python    failed  build aborted  analysis of target    tensorflow lite python    failed     ,077fe29d9d1f2149dd8c74bcd2f99de7b5fd1506,tensorflow\lite\kernels\hashtable\BUILD
45243,tf.keras.applications.mobilenet_v3.preprocess_input documentation not according source code,  in   docs python tf keras applications   preprocess input    said    that   preprocessed numpy array or a tf tensor with type     the inputs pixel values are scaled between    and    sample wise   however in source code in github you find this    keras export   def preprocess input      pylint  disable unused argument  return x  and if you check it in a notebook you see that as source code state it is doing nothing  do not scale and do not change dtype to   of the input    ,52cdef300393aa1e2a4220030f77839edbae8bd3,tensorflow\python\keras\applications\mobilenet_v3.py
45662,Wrong device returned for GPUCompatibleFIFOQueueTests.testEnqueueDequeue test,  system information    have i written custom code    no  os platform and distribution    centos    tensorflow installed from    source  tensorflow version           python version         bazel version           gcc compiler version    gcc        cuda cudnn version       gpu model and memory       describe the current behavior  in the   tensorflow python kernel tests fifo queue test target the gpucompatiblefifoqueuetests testenqueuedequeue returns the wrong device    job localhost replica   task   device cpu   vs  job localhost replica   task   device gpu    see below log  standalone code to reproduce the issue  run bazel test  other info   logs  fail  testenqueuedequeue    gpucompatiblefifoqueuetests testenqueuedequeue                                                                          traceback       file   tmp bazel tf   execroot org tensorflow bazel out ppc opt bin tensorflow python kernel tests fifo queue test runfiles org tensorflow tensorflow python framework test util py ,671c78343c3427858bd5674baab80c3c8c429815,tensorflow\python\kernel_tests\fifo_queue_test.py
45894,tf.data.experimental.assert_cardinality incompatible with INFINITE_CARDINALITY,  system information    have i written custom code  yes  os platform and distribution  ubuntu      tensorflow installed from  binary  tensorflow version                     python version           describe the current behavior  tf data experimental assert cardinality  can be used to fix dataset cardinality in instances where it cannot be inferred  in situations where the cardinality is infinite  this raises an error where it shouldn t   describe the expected behavior if ds   ds base apply   then an error should be raised if ds base stops producing elements  as opposed to when the first element is produced     notebook    code     import tensorflow as tf    ds   tf data dataset range  repeat  flat map    print     tf data infinite cardinality     false  ds   ds apply    print     tf data infinite cardinality     true    for example in ds take        pass    tensorflow python framework errors impl failedpreconditionerror     input dataset was expected to contain    elements but containe,59f5abfbc8dc5559c361f80f4fa4a006db825e40,tensorflow\core\kernels\data\experimental\assert_cardinality_dataset_op.cc
46020,Common location for portable bash helper functions / aliases,   tensorflow micro    pr      fixes the use of  to be compatible with a mac  we are already using  in additional places too   while we should get      merged  it would be better to have a common location for these helper functions   aliases   i can imagine collecting these into a common helper script   and have something like   uname s  uname  s     if   uname s    linux   then    alias        else if   uname s    darwin    then    alias       r   fi    we would then need to change the different download scripts to determine the directory in which the script lives  something like        tensorflow tensorflow lite micro tools ci build test bluepill sh              lines   to          in                        script dir         pwd            root dir   script dir                          cd    root dir               and then have  source tensorflow lite micro tools bash helper sh   ,c5ce162da8efb465e2ba8a8825049614813892a9,tensorflow\lite\micro\tools\make\bash_helpers.sh
46128,"This throws ERROR: features = tf.io.parse_example(..., features=make_parse_example_spec(columns))",  please make sure that this is a bug  as per our  github policy   we only address code doc bugs  performance issues  feature requests and  build installation issues on github  tag bug template  system information    have i written custom code    no  os platform and distribution         mobile device   if the issue happens on mobile device   tensorflow installed from    source  tensorflow version    latest from this week  python version      x  bazel version           gcc compiler version     cuda cudnn version       gpu model and memory     describe the current behavior  i was trying this code  but it throws exception    docs python tf keras experimental sequencefeatures  valueerror  attempt to convert a value   with an unsupported type   to a tensor          here is the full code from that page            behavior of some cells or feature columns may depend on whether we are in    training or inference mode  e g  applying dropout   training   true  rating   sequence numeric column   ,2cc955f533a9ba70512cf4a07024aaf65708e103,tensorflow\python\keras\feature_column\sequence_feature_column.py
46423,writer_test of serialization failed for squeeznet,  system information    os platform and distribution  linux ubuntu      tensorflow installed from source  tensorflow version     python version  python        bazel version    build label         gcc compiler version   gcc              describe the current behavior    download squeezenet    build write test of serialization   bazel build  c opt   tensorflow lite tools serialization writer test  bazel bin tensorflow lite tools serialization writer test  model folder  squeezenet tflite    error  tensorflow lite kernels reshape cc   num input elements    num output elements    error  node number     failed to prepare     allocatetensors failed on the round tripped model       describe the expected behavior  pass write test  standalone code to reproduce the issue    download squeezenet    build write test of serialization   bazel build  c opt   tensorflow lite tools serialization writer test  bazel bin tensorflow lite tools serialization writer test  model folder  squeezenet tflite    othe,f12082d7af509e9549d8e8fb2b514ccd0db0e84e,tensorflow\lite\tools\serialization\option_writer_generator.cc
4861,Example mnist_rnn Not Working with Docker Image,  issue  example mnist rnn does run on docker image                                                                                  importerror                               traceback     in            from   future   import print function                    from sklearn import metrics  preprocessing                    import tensorflow as tf    importerror  no module named sklearn        what related github issues or stackoverflow threads have you found by searching the web for your problem     an example in the code base does not work with the docker image  it is the opinion of the filer that all examples should run without any need for configuration on the docker image because the project has control over what is installed on the docker image     rnn py    environment info    operating system   docker run  it  p     gcr io tensorflow tensorflow    installed version of cuda and cudnn     none  cpu based container  if installed from binary pip package  provide     the output from pyt,7c79d528f43c69b6719da0c7846cd3aa56df57ef,tensorflow\tools\docker\Dockerfile
5115,timeout breaks FIFOQueue,  ubuntu       lts         using timeout with notebooks is very useful in case you dequeue an empty queue or enqueue a full one  the problem is that after a timeout occurs a enqueue or dequeueop  throws an error  import tensorflow as tf  with tf device        ph   tf placeholder       q   tf fifoqueue       enq   q enqueue       deq   q dequeue       timeout option   tf runoptions   sess   tf session   sess run       here i get the usual timeout error  the problem is that when i then run  sess run   i get the error                                                                                cancellederror                            traceback     in            sess run      usr local lib     dist packages tensorflow python client session pyc in run             try               result   self  run none  fetches  feed dict  options ptr                                  run metadata ptr               if run metadata                 proto data   tf session tf getbuffer      usr local lib  ,f46fe646a26f0514fdfbfcea3882fd0120f24388,tensorflow\core\common_runtime\direct_session.cc
5380,'tensorflow/core/public/session.h' file not found,  i didn t find anything about this problem  i m creating the python package from the source with bazel  the commands used are the same of the official guide in the section create pip package     the file  whl generated is working but doesn t have some include files that i use in a new op created in c    the missing files are session options h and session h which  however  are present in the official  whl of tensorflow that you can download     to add those files i had to insert some requirements in tensorflow core build           tensorflow core build               file changed    insertions     diff   git a tensorflow core build b tensorflow core build  index             a tensorflow core build      b tensorflow core build                  cc library             platform strong hash h              platform thread annotations h              platform types h              public version h              public session h              public session options h                  visibility    ,b725df4aaf4fde1139f7204bae31650a25348e0f,tensorflow\tools\pip_package\BUILD
5543,Constant folding doesn't remove control edges,  i believe that when constant folding takes place  and a section of a graph is replaced by a constant  that only the data output edge of the replaced node is removed   i believe that i can see that a graph of nodes ending up in a div   is replaced by a const   the output of the div goes to a mul  and this is changed to the new const correctly   however  there is a control output from the div going to a const     this is not changed to the div replacement   consequently the dead node pruning doesn t remove the original div   here is some trace   during the constant folding   graph before  nodes    edges    graph constant graph  nodes    edges    constant foldable        replacing  name  gradients mean grad truediv  id   op device   job localhost replica   task   device ipu    def  gradients mean grad truediv   div t dt float   device   job localhost replica   task   device ipu             with a constant  replacing edge to gradients square grad      during the post constant folding pru,73bc428901dfd6507bbea1315c8813f2048233ff,tensorflow\core\common_runtime\simple_placer.cc
5652,cifar10_multi_gpu_train.py breaks with more than 1 GPU,  environment info    operating system  ubuntu  installed version of cuda and cudnn      and      the commit hash       the output of bazel version   build label         build target  bazel out local fastbuild bin src main java com google devtools build lib bazel bazelserver deploy jar  build time  fri oct              build timestamp     build timestamp as int       if possible  provide a minimal reproducible example      python   py   num gpus    both   py and   py   work   logs or other output that would be helpful    i tensorflow stream executor dso loader cc    successfully opened cuda library libcublas so locally  i tensorflow stream executor dso loader cc    successfully opened cuda library libcudnn so locally  i tensorflow stream executor dso loader cc    successfully opened cuda library libcufft so locally  i tensorflow stream executor dso loader cc    successfully opened cuda library libcuda so   locally  i tensorflow stream executor dso loader cc    successfully opened cuda ,1b531b9d88361a3b8506399d5edae155125c5371,tensorflow\models\image\cifar10\cifar10_multi_gpu_train.py
6602,fatal error: tensorflow/stream_executor/lib/status.h: No such file or directory,  i try to write my own op and i have installed tensorflow       with gpu support on linux   this code fails    include  tensorflow core platform stream executor h       with error   fatal error  tensorflow stream executor lib status h  no such file or directory      that files does not exists   some more include files seem to be missing  when grepping for devicememory in the include path  the only file it finds is include tensorflow core util stream executor util h    ,02d2385b8c33e89b53e49bc89e646b54de920bad,tensorflow\tools\pip_package\setup.py
6717,Incorrect gradient for categorical distribution entropy,  the categorical distribution class provides an awesome entropy operator but apparently the gradient calculation w r t  the input operators doesn t work   logits   tf variable     probabilities   tf nn softmax   log probabilities   tf nn log softmax   entropy     tf reduce sum       using the actual distribution would be nicer but gradients seem buggy  categorical distribution   tf contrib distributions categorical   categorical distribution entropy   categorical distribution entropy       initialize  init   tf global variables initializer   sess   tf session   sess run       works  print    print         apparently loses gradient information  print    print     in the outputs we see that the entropy calculation works but the gradients are somehow lost  this obviously also doesn t work when i try to maximize this entropy using any optimizer                 array                                                   dtype                    array                                            ,b39773478542bf812a12715f7f753f9a88e5e86c,tensorflow\contrib\distributions\python\kernel_tests\categorical_test.py
6738,How to train Multibox object detector included in the TF Detect Android demo,  there is a way to train a custom model for the multibox object detector that is included in the tf detect android demo      ,53aabd5cb0ffcc1fd33cbd00eb468dd8d8353df2,WORKSPACE
6766,"softmax_cross_entropy_with_logits aborts the process, if a tensor with zero first dimension is passed as an argument",  environment info    operating system  ubuntu      installed version of cuda and cudnn  cuda      cudnn        tensorflow version        installed from    gpu             whl    reproduced also using tf        cuda      cudnn        minimal reproducible example    import tensorflow as tf  y   tf placeholder   one hot y tf one hot   ce   tf nn softmax cross entropy with logits   sess   tf session   sess run       result on gpu   e tensorflow core common runtime bfc allocator cc    tried to allocate   bytes  w tensorflow core common runtime allocator retry cc    request to allocate   bytes  f tensorflow core common runtime gpu gpu device cc    eigenallocator for gpu ran out of memory when allocating    see error logs for more detailed info   aborted        result on cpu   array        ,07e6ca0ac7cdfcb6105fb3410fc68355c04df1d5,tensorflow\core\kernels\xent_op.cc
6823,Upgrade HighwayHash,  the highwayhash module which is downloaded as an external dependency in tensorflow produces different hash results on big endian and little endian architectures  this causes the test  from  to fail on big endian  after raising an issue   with highwayhash community  they have added a change to make hash values consistent across architectures through commit     will it be possible to pick this or higher commit of highwayhash in tensorflow    ,6ed5eaaac13540c1b9d5a7549b2c82aac91ba318,tensorflow\workspace.bzl
7025,"Getting ""Dst tensor is not initialized."" when really the problem is out of GPU memory",  this is the stack trace we sometimes get when trying to use tensorflow on a gpu that s occupied by another process  it would help debugging if the error said something about memory    zheng xq    tf version             dirty      w tensorflow core platform cpu feature guard cc    the tensorflow library wasn t compiled to use     instructions  but these are available on your machine and could speed up cpu computations   w tensorflow core platform cpu feature guard cc    the tensorflow library wasn t compiled to use avx instructions  but these are available on your machine and could speed up cpu computations   w tensorflow core platform cpu feature guard cc    the tensorflow library wasn t compiled to use   instructions  but these are available on your machine and could speed up cpu computations   w tensorflow core platform cpu feature guard cc    the tensorflow library wasn t compiled to use fma instructions  but these are available on your machine and could speed up cpu computations ,0a9b39caefd437fec742ae48b25061abd6e2699b,tensorflow\core\common_runtime\gpu\gpu_device.cc
7065,pytorch 2.5x faster on VGG16,  what related github issues or stackoverflow threads have you found by searching the web for your problem     started on so  and was told to post here  so post     environment info    operating system   ubuntu       maxwell titan x  installed version of cuda and cudnn   cuda      cudnn          ls  l  usr local cuda   libcud    rw r  r     root root      jan        usr local cuda   libcudadevrt a  lrwxrwxrwx   root root          jan        usr local cuda   libcudart so    libcudart so      lrwxrwxrwx   root root          jan        usr local cuda   libcudart so        libcudart so         rwxr xr x   root root      jan        usr local cuda   libcudart so         rw r  r     root root      jan        usr local cuda   libcudart static a  lrwxrwxrwx     users         jul        usr local cuda   libcudnn so    libcudnn so    lrwxrwxrwx     users         jul        usr local cuda   libcudnn so      libcudnn so         rwxrwxr x     users   jul        usr local cuda   libcudnn so         r,0318cf082ee88ff0e226a5bf7da0487f44d82182,tensorflow\core\kernels\conv_grad_filter_ops.cc
7077,TensorBoard ImportError: No module named werkzeug,  using the current head of tensorflow  i have bumped into an issue when i execute tensorboard   version is reported as      head  git rev parse head       linux  linux tensor         generic      ubuntu smp fri dec         utc         gnu linux                 i tensorflow stream executor dso loader cc    successfully opened cuda library libcudnn so   locally               i tensorflow stream executor dso loader cc    successfully opened cuda library libcufft so     locally               i tensorflow stream executor dso loader cc    successfully opened cuda library libcuda so   locally               i tensorflow stream executor dso loader cc    successfully opened cuda library libcurand so     locally  traceback     file   usr local bin tensorboard   line    in   load entry point    file   home greg  local lib     site packages pkg resources init py   line    in load entry point  return get distribution  load entry point   file   home greg  local lib     site packages pkg resources in,bc72653cf7968115fe9f714d7d2bc63004524479,tensorflow\g3doc\get_started\os_setup.md
7088,Running optimized graph with two output nodes on android gives “Session was not created with a graph before Run()” error.,  hi  please would you please be so kind and help me with one issue that prevents me from moving forward  i have graph with two output layers   and i am unable to strip optimize on inference it in order to run on android device    when i run   bazel bin tensorflow python tools optimize for inference   –input  tmp output pb   –output  tmp optimized pb   –input names mul   –output names  final result orig final result added   then in my android application  i get  session was not created with a graph before run   error  and both final result orig and final result added are not found   when i run   bazel bin tensorflow python tools optimize for inference   –input  tmp output pb   –output  tmp optimized pb   –input names mul   –output names  final result orig   it works fine  final result orig is available and works correctly  however final result added is obviously not found and not available for my app to use   and when i run   bazel bin tensorflow python tools optimize for inference   –,c3df5d40ef8240ede980ccb740d6af87837d8eef,tensorflow\core\kernels\BUILD
7186,missing fclose,   contrib pi examples label image label image cc line    missing fclose     ,82a0f347baf7284345cbf9e830310604da9b1b73,tensorflow\contrib\pi_examples\label_image\label_image.cc
7300,"API documentation ""Core graph data structures""",  a few things are wrong in the documentation around    api docs python framework md tfgraphadd to collectionname value graphadd to collection       near the end of tf graph name scope  the line about valueerror is incomplete   the paragraph at the very end of tf graph name scope should be in tf graph add to collection   both tf graph add to collection and tf graph add to collections   exist      ,dfe3bb7cdc888767c2fff32efea405c8d6aa5c88,tensorflow\python\framework\ops.py
7404,No attribute 'outer_context' when calculating gradient from imported graph,  it seems when you import a graph with a  while  loop  you can t calculate gradients as you could on the original graph  e g   import tensorflow as tf  i tf constant   out tf while loop   lambda i   tf add     i   name  output    graph def   tf get default graph  as graph def     g   tf graph   with g as default        tf import graph def   s   tf session   i imported   g get tensor by name   out imported   g get tensor by name   tf gradients                                                                                attributeerror                            traceback     in            tf gradients      users malmaud anaconda lib     site packages tensorflow python ops gradients impl pyc in gradients             pending count  loop state    pendingcount   to ops                                                       from ops                                                       colocate gradients with ops                       iterate over the collected ops        users malmaud anac,1d7c2fa60f717dea7239970d96f7d4bf96842039,tensorflow\python\ops\control_flow_ops.py
7406,Should check whether n_class is zero before calling sample_n() in mixture.py,  problem description    mixture model first use categorical to sample how much samples it need for each mixture components  this is variable  at line      but it actually means    and then it pass  to    the problem is  could be   and you can t pass  to   which is used in beta distribution   see line     in mixture py   if possible  provide a minimal reproducible example      it s easy to reproduce  just create a mixture of beta   uniform with     probability  half of the time it ll sample from uniform  and half of the time it ll sample from beta      usr bin python  import tensorflow as tf  ds   tf contrib distributions      create mixture distribution of beta   uniform  components    ds beta   ds uniform    cat   ds categorical   mix   ds mixture       get only   sample  x   mix sample n     with tf session  as sess       sess run              repeats until crash      for i in range            print sess run   what other attempted solutions have you tried     two possible solutions ,3116fa80450735e907907bc57a6834e9e212570a,tensorflow\core\kernels\random_op.cc
7906,Erroneous number of channels in the Guide to TF Layers tutorial.,  the tutorial a guide to tf layers  building a convolutional neural network seems to have the wrong number of channels in the tensor dimensions given at the end of the paragraph convolutional layer        our output tensor produced by    has a shape of  batch size            the same width and height dimensions as the input  but now with   channels holding the output from each of the filters     to be consistent with the rest of the tutorial  it should probably say  shape of  batch size             since the rightmost dimension denotes the number of channels    ,4e63540076921d2c08d03aa9efb76fd483920593,tensorflow\docs_src\tutorials\layers.md
8011,TypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;,  feature request for a better error description or for better summary handling   the following code works fine if some summaries where defined before   ops      ops     tf summary merge all    session run       however if there were no summaries we get   typeerror  fetch argument none has invalid type   which is really saying    one of the session run ops where empty  which is forbidden    alternatively let merge all return a noop if there are no summaries    ,cffd79f4b102c2082cbcc258abf7ed06df8c141c,tensorflow\core\kernels\slice_op.cc
8364,Documentation formatting broken,  see   docs python tf contrib copy graph copy op to graph      or   docs python tf contrib graph editor copy       either the doc generator needs to understand python doc comments better or the doc comments need to be updated to work better with markdown    what do you think   thanks   andreas   ,4b86783ac6c3af9c35d3af36d4db0e9bc11f21c0,tensorflow\contrib\copy_graph\python\util\copy_elements.py
8718,Incorrect reference for tf.learn in Linear Model tutorials,  this page   and this   mentions  multiple times   however  looking at the links and source code  i believe that  should actually be either  or   not        ,4f52ce514bc83c5adbfdcd8a342a0fa0f42e55d0,tensorflow\docs_src\tutorials\linear.md
8809,an update for the tf.contrib.learn Quickstart example is needed,  hi all   just tried to start the script from here    started tflearn    found one issue for   users   import urllib  raw   urllib urlopen  read   this returns                                                                                attributeerror                            traceback     in            raw   urllib urlopen  read     attributeerror  module  urllib  has no attribute  urlopen     the solution   import urllib request as ur  raw   ur urlopen  read   but  the main reason for this issue ticket is warning messages like   warning tensorflow from  users vadimborisov anaconda lib     site packages tensorflow contrib learn python learn estimators head py    scalar summary   is deprecated and will be removed after         instructions for updating   please switch to tf summary scalar  note that tf summary scalar uses the node name instead of the tag  this means that tensorflow will automatically de duplicate summary names based on the scope they are created in  also  passing ,0c08c585591152046ca1e6781d1f2fa573427dfc,tensorflow\docs_src\deploy\distributed.md
9047,beta2_power is applied incorrectly in Adam optimizer,  in adam py and in the applyadam op  the denominator is effectively       epsilon t    tf sqrt       however  this appears incorrect – per the paper  the correct ema adjustment should give   tf sqrt     epsilon t      otherwise  when epsilon t is large relative to tf sqrt   the effective epsilon used in the denominator is also scaled up by the correction factor  which doesn t match what s in the paper   does this seem right  or am i missing something here    ,1926b9d3b91375f5a4433303a27b49b4da53f64e,tensorflow\python\training\adam.py
9089,cpp protobuf instructions out-of-date for MacOS,  instructions to upgrade to cpp protobuf implementation on mac from   mac     don t work work  makes tf fails with following stacktrace  traceback       file  kronecker benchmark py   line    in       import tensorflow as tf    file   users yaroslav anaconda envs   lib     site packages tensorflow   init   py   line    in       from tensorflow python import      file   users yaroslav anaconda envs   lib     site packages tensorflow python   init   py   line    in       from tensorflow core framework   import      file   users yaroslav anaconda envs   lib     site packages tensorflow core framework   py   line    in       from google protobuf import descriptor as  descriptor    file   users yaroslav anaconda envs   lib     site packages google protobuf descriptor py   line    in       from google protobuf pyext import  message  importerror  dlopen   library not loaded   usr local lib libprotobuf   dylib    referenced from   users yaroslav anaconda envs   lib     site packages google pr,3dc4907a0d7ae113547fcf716f618e3c8e1b9c77,tensorflow\docs_src\install\install_mac.md
9103,BUG: tensorflow.placeholder shape does not serialize with protobuf,  profobuf serialization       attr       dtype       type    dt float        shape       shape               name    x     op    placeholder       tensorflow code  x   tf placeholder    ,24a95ae389e1c76e771ac33d66e0ec40a236260f,tensorflow\cc\client\client_session_test.cc
9136,Issues when using Queues + tf.train.Server,  note  issues that are not bugs or feature requests will be closed  please ask usage questions on stackoverflow   you must complete this information or else your issue will be closed      have i written custom code     yes  tensorflow installed from     binary  tensorflow version        cpu                      bazel version     cuda cudnn version  n a  gpu model and memory  n a  exact command to reproduce  cf below     this problem has been reproduced on both linux and various mac os machines   describe the problem clearly    we seem to experience issues when using both queues   tf train server  when executed in a simple python       console  the following script hangs   import tensorflow as tf  import time    cluster   tf train clusterspec   server   tf train server     with tf graph  as default  as graph         queue      input queue   tf train input producer            useless variable      variable   tf variable           session and queue runners      session   tf session      ,1f210ad7c2a81fe27196dd1a85c9bb92f19bc94a,tensorflow\core\distributed_runtime\master_session.cc
9161,"Hi, I am unable to access the documentation",  note  issues that are not bugs or feature requests will be closed  please ask usage questions on stackoverflow   you must complete this information or else your issue will be closed      have i written custom code      tensorflow installed from      tensorflow version   bazel version     cuda cudnn version   gpu model and memory   exact command to reproduce     describe the problem clearly    source code   logs    include any logs or source code that would be helpful to diagnose the problem  if including tracebacks  please include the full traceback  large logs and files should be attached  try to reproducible test case code the bare minimum necessary to generate the problem   ,9cc21f04ed051d6a6d3a21e909aa547110aa8b0d,tensorflow\python\debug\examples\README.md
9312,Typo in seq2seq.attention_wrapper.py,  hi   i think there is a small typo in contrib   attention wrapper py  would someone like to check it   code url     python ops attention wrapper py       i guess it should be  rather than  to be checked   thanks    ,0557e8f90c1a2f027f4561c70558c6c836138058,tensorflow\contrib\seq2seq\python\ops\attention_wrapper.py
9633,SIGSEGV with sparse_add and broadcasting,  system information      have i written custom code     yes  enclosed below  os platform and distribution     ubuntu      tensorflow installed from     binary via pip  tensorflow version        bazel version     n a  using pip installation  cuda cudnn version   n a  cpu only  gpu model and memory   none  exact command to reproduce     from   future   import print function  import numpy as np  import tensorflow as tf    dense sz              dense   tf constant     sparse sz              nnz      nz ind   np random choice   size nnz  replace false   nz ind   np unravel index   nz ind   np array  t  assert np all      ensure canonical ordering   ind   np lexsort  for i in reversed      nz ind   nz ind ind      print     sparse plc   tf sparse placeholder   sparse sum   tf sparse add   init   tf global variables initializer     with tf session  as sess       sess run       print       res   sess run    sparse sz         print       describe the problem    running the code above results i,50b836addfed6b49fc823987e9301f1b6eeef90c,tensorflow\core\kernels\sparse_tensor_dense_add_op.cc
9931,Go: SIGSEGV when using int32 instead of int64 and missing error in Resize functions,  problem    in go  some operation causes a sigsegv when using an   instead of an       the resize  operations don t define the output shape correctly when the input is not a  batch   they just let the dimensions undefined instead of raising some errors   the tests below are commented so i hope that s enough to let you understand what the problems are   source code   logs    package poc test    import             fmt             tf  github com tensorflow tensorflow tensorflow go            github com tensorflow tensorflow tensorflow go op            testing        func testresizewithoutbatchisnosense                create root scope          root    op newscope                define graph                  read image content          imagepath     test jpg           contents    op readfile   op const   imagepath                    decode jpeg          value    op decodejpeg   contents  op decodejpegchannels                 i d like to add noise to the image  so i d like to define a nose,fe41d05e7c8343ed53fc788d6c312792b390f679,tensorflow\go\graph.go
