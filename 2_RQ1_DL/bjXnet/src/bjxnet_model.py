# -*- coding: utf-8 -*-
"""bjxnet_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UcrdG_S7ztc960Bjj-nzdL8lyD5IATnT
"""

import torch
import torch.nn as nn
from text_encoder import TextCNN
from graph_encoder import GraphEncoder
from attention_layer import AttentionLayer

class bjXnet(nn.Module):
    def __init__(self, vocab_size, embed_size, num_filters, kernel_sizes, in_feats, hidden_feats, n_steps, n_etypes):
        super(bjXnet, self).__init__()
        self.text_encoder = TextCNN(vocab_size, embed_size, kernel_sizes, num_filters)
        self.graph_encoder = GraphEncoder(in_feats, hidden_feats, n_steps, n_etypes)
        self.attention = AttentionLayer(feature_dim=300)

    def forward(self, report_text, code_text, graph, node_features):
        report_features = self.text_encoder(report_text)
        code_text_features = self.text_encoder(code_text)
        graph_features = self.graph_encoder(graph, node_features)
        weighted_graph_features = self.attention(report_features, graph_features)
        combined_features = torch.cat([code_text_features, weighted_graph_features], dim=1)
        return combined_features