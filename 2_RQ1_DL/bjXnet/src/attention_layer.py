# -*- coding: utf-8 -*-
"""attention_layer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UcrdG_S7ztc960Bjj-nzdL8lyD5IATnT
"""

import torch
import torch.nn as nn

class AttentionLayer(nn.Module):
    def __init__(self, feature_dim):
        super(AttentionLayer, self).__init__()
        self.query = nn.Linear(feature_dim, feature_dim)
        self.key = nn.Linear(feature_dim, feature_dim)
        self.value = nn.Linear(feature_dim, feature_dim)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, report_features, graph_features):
        query = self.query(report_features)
        key = self.key(graph_features)
        value = self.value(graph_features)

        attention_weights = self.softmax(torch.bmm(query.unsqueeze(1), key.transpose(1, 2)))
        weighted_graph_features = torch.bmm(attention_weights, value).squeeze(1)

        return weighted_graph_features