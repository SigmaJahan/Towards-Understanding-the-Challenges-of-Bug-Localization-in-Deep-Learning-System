# -*- coding: utf-8 -*-
"""training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UcrdG_S7ztc960Bjj-nzdL8lyD5IATnT
"""

import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from bjxnet_model import bjXnet
from data_processing import BugDataset
from graph_encoder import load_cpg_data

EPOCHS = 50
BATCH_SIZE = 32
LEARNING_RATE = 1e-3


vocab_size = 5000
dataset = BugDataset(bug_reports=[], source_files=[], labels=[], processor=None)
data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

model = bjXnet(vocab_size=vocab_size, embed_size=300, num_filters=100, kernel_sizes=[3, 4, 5],
               in_feats=100, hidden_feats=128, n_steps=3, n_etypes=4)
optimizer = Adam(model.parameters(), lr=LEARNING_RATE)
loss_fn = torch.nn.CosineEmbeddingLoss()

def train():
    model.train()
    for epoch in range(EPOCHS):
        for bug_vec, source_vec, label in data_loader:
            optimizer.zero_grad()
            output = model(bug_vec, source_vec, graph=None, node_features=None)
            loss = loss_fn(output, label)
            loss.backward()
            optimizer.step()
        print(f'Epoch [{epoch}/{EPOCHS}], Loss: {loss.item()}')

if __name__ == '__main__':
    train()